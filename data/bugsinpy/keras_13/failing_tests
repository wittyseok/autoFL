coverage run -m pytest tests/keras/engine/test_training.py::test_model_methods
============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-5.4.3, py-1.8.1, pluggy-0.13.1 -- /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/bin/python
cachedir: .pytest_cache
rootdir: /home/user/BugsInPy/temp/projects/keras, inifile: pytest.ini
plugins: forked-1.1.3, flaky-3.6.1, xdist-1.32.0
gw0 I / gw1 I

[gw0] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras

[gw1] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras

[gw0] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]

[gw1] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]
gw0 [1] / gw1 [1]

scheduling tests via LoadScheduling

tests/keras/engine/test_training.py::test_model_methods 
[gw0] [100%] FAILED tests/keras/engine/test_training.py::test_model_methods 

=================================== FAILURES ===================================
______________________________ test_model_methods ______________________________
[gw0] linux -- Python 3.7.3 /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/bin/python

    def test_model_methods():
        a = Input(shape=(3,), name='input_a')
        b = Input(shape=(3,), name='input_b')
    
        a_2 = Dense(4, name='dense_1')(a)
        dp = Dropout(0.5, name='dropout')
        b_2 = dp(b)
    
        model = Model([a, b], [a_2, b_2])
    
        optimizer = 'rmsprop'
        loss = 'mse'
        loss_weights = [1., 0.5]
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        # training/testing doesn't work before compiling.
        with pytest.raises(RuntimeError):
            model.train_on_batch([input_a_np, input_b_np],
                                 [output_a_np, output_b_np])
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
    
        # test train_on_batch
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   [output_a_np, output_b_np])
        out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                   {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # test fit
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np], epochs=1, batch_size=4)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4)
    
        # test validation_split
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5)
    
        # test validation data
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4,
                        validation_data=([input_a_np, input_b_np],
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        [output_a_np, output_b_np],
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=({'input_a': input_a_np,
                                          'input_b': input_b_np},
                                         [output_a_np, output_b_np]))
        out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},
                        {'dense_1': output_a_np, 'dropout': output_b_np},
                        epochs=1, batch_size=4, validation_split=0.5,
                        validation_data=(
                            {'input_a': input_a_np, 'input_b': input_b_np},
                            {'dense_1': output_a_np, 'dropout': output_b_np}))
    
        # test_on_batch
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  [output_a_np, output_b_np])
        out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},
                                  {'dense_1': output_a_np, 'dropout': output_b_np})
    
        # predict_on_batch
        out = model.predict_on_batch([input_a_np, input_b_np])
        out = model.predict_on_batch({'input_a': input_a_np,
                                      'input_b': input_b_np})
    
        # predict, evaluate
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # with sample_weight
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        sample_weight = [None, np.random.random((10,))]
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np],
                                   sample_weight=sample_weight)
    
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np],
                                  sample_weight=sample_weight)
    
        # test accuracy metric
        model.compile(optimizer, loss, metrics=['acc'],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 5
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 5
    
        # this should also work
        model.compile(optimizer, loss, metrics={'dense_1': 'acc'},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # and this as well
        model.compile(optimizer, loss, metrics={'dense_1': ['acc']},
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        assert len(out) == 4
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == 4
    
        # test starting from non-zero initial epoch
        trained_epochs = []
        trained_batches = []
    
        # define tracer callback
        def on_epoch_begin(epoch, logs):
            trained_epochs.append(epoch)
    
        def on_batch_begin(batch, logs):
            trained_batches.append(batch)
    
        tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin,
                                    on_batch_begin=on_batch_begin)
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np], epochs=5, batch_size=4,
                        initial_epoch=2, callbacks=[tracker_cb])
        assert trained_epochs == [2, 3, 4]
    
        # test starting from non-zero initial epoch for generator too
        trained_epochs = []
    
        @threadsafe_generator
        def gen_data(batch_sz):
            while True:
                yield ([np.random.random((batch_sz, 3)),
                        np.random.random((batch_sz, 3))],
                       [np.random.random((batch_sz, 4)),
                        np.random.random((batch_sz, 3))])
    
        out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,
                                  initial_epoch=2, callbacks=[tracker_cb])
        assert trained_epochs == [2, 3, 4]
    
        # test with a custom metric function
        def mse(y_true, y_pred):
            return K.mean(K.pow(y_true - y_pred, 2))
    
        model.compile(optimizer, loss, metrics=[mse],
                      sample_weight_mode=None)
    
        out = model.train_on_batch([input_a_np, input_b_np],
                                   [output_a_np, output_b_np])
        out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)
        assert len(out) == out_len
        out = model.test_on_batch([input_a_np, input_b_np],
                                  [output_a_np, output_b_np])
        assert len(out) == out_len
    
        input_a_np = np.random.random((10, 3))
        input_b_np = np.random.random((10, 3))
    
        output_a_np = np.random.random((10, 4))
        output_b_np = np.random.random((10, 3))
    
        out = model.fit([input_a_np, input_b_np],
                        [output_a_np, output_b_np],
                        batch_size=4, epochs=1)
        out = model.evaluate([input_a_np, input_b_np],
                             [output_a_np, output_b_np],
                             batch_size=4)
        out = model.predict([input_a_np, input_b_np], batch_size=4)
    
        # enable verbose for evaluate_generator
        out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)
    
        # empty batch
        with pytest.raises(ValueError):
            @threadsafe_generator
            def gen_data():
                while True:
                    yield (np.asarray([]), np.asarray([]))
    
            out = model.evaluate_generator(gen_data(), steps=1)
    
        # x is not a list of numpy arrays.
        with pytest.raises(ValueError):
            out = model.predict([None])
    
        # x does not match _feed_input_names.
        with pytest.raises(ValueError):
            out = model.predict([input_a_np, None, input_b_np])
        with pytest.raises(ValueError):
            out = model.predict([None, input_a_np, input_b_np])
    
        # all input/output/weight arrays should have the same number of samples.
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np[:2]],
                                       [output_a_np, output_b_np],
                                       sample_weight=sample_weight)
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np[:2]],
                                       sample_weight=sample_weight)
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=[sample_weight[1],
                                                      sample_weight[1][:2]])
    
        # `sample_weight` is neither a dict nor a list.
        with pytest.raises(TypeError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=tuple(sample_weight))
    
        # `validation_data` is neither a tuple nor a triple.
        with pytest.raises(ValueError):
            out = model.fit([input_a_np, input_b_np],
                            [output_a_np, output_b_np],
                            epochs=1, batch_size=4,
                            validation_data=([input_a_np, input_b_np],))
    
        # `loss` does not match outputs.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss=['mse', 'mae', 'mape'])
    
        # `loss_weights` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})
    
        # `loss_weights` does not match outputs.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', loss_weights=[0.5])
    
        # `loss_weights` is invalid type.
        with pytest.raises(TypeError):
            model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))
    
        # `sample_weight_mode` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse',
                          sample_weight_mode={'lstm': 'temporal'})
    
        # `sample_weight_mode` does not match output_names.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])
    
        # `sample_weight_mode` matches output_names partially.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss='mse',
                          sample_weight_mode={'dense_1': 'temporal'})
    
        # `loss` does not exist.
        with pytest.raises(ValueError):
            model.compile(optimizer, loss=[])
    
        model.compile(optimizer, loss=['mse', 'mae'])
        model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,
                                                           'dropout': 0.8})
        model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])
    
        # the rank of weight arrays should be 1.
        with pytest.raises(ValueError):
            out = model.train_on_batch(
                [input_a_np, input_b_np],
                [output_a_np, output_b_np],
                sample_weight=[None, np.random.random((10, 20, 30))])
    
        model.compile(optimizer, loss='mse',
                      sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})
        model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])
    
        # the rank of output arrays should be at least 3D.
        with pytest.raises(ValueError):
            out = model.train_on_batch([input_a_np, input_b_np],
                                       [output_a_np, output_b_np],
                                       sample_weight=sample_weight)
    
        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,
                      sample_weight_mode=None)
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  steps_per_epoch=3,
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  validation_steps=3,
                                  max_queue_size=1,
                                  callbacks=[tracker_cb])
        assert trained_epochs == [0, 1, 2, 3, 4]
        assert trained_batches == list(range(3)) * 5
        assert len(val_seq.logs) <= 4 * 5
    
        # steps_per_epoch will be equal to len of sequence if it's unspecified
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  initial_epoch=0,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb])
        assert trained_epochs == [0, 1, 2, 3, 4]
        assert trained_batches == list(range(12)) * 5
        assert len(val_seq.logs) == 12 * 5
    
        # test for workers = 0
        trained_epochs = []
        trained_batches = []
        val_seq = RandomSequence(4)
        out = model.fit_generator(generator=RandomSequence(3),
                                  epochs=5,
                                  validation_data=val_seq,
                                  callbacks=[tracker_cb],
>                                 workers=0)

tests/keras/engine/test_training.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
keras/engine/training.py:1418: in fit_generator
    initial_epoch=initial_epoch)
keras/engine/training_generator.py:233: in fit_generator
    workers=0)
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
keras/engine/training.py:1472: in evaluate_generator
    verbose=verbose)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

model = <keras.engine.training.Model object at 0x7f43940d19e8>
generator = <generator object iter_sequence_infinite at 0x7f4396984ed0>
steps = None, max_queue_size = 10, workers = 0, use_multiprocessing = False
verbose = 0

    def evaluate_generator(model, generator,
                           steps=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
                           verbose=0):
        """See docstring for `Model.evaluate_generator`."""
        model._make_test_function()
    
        if hasattr(model, 'metrics'):
            for m in model.stateful_metric_functions:
                m.reset_states()
            stateful_metric_indices = [
                i for i, name in enumerate(model.metrics_names)
                if str(name) in model.stateful_metric_names]
        else:
            stateful_metric_indices = []
    
        steps_done = 0
        wait_time = 0.01
        outs_per_batch = []
        batch_sizes = []
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps is None:
            if is_sequence:
                steps = len(generator)
            else:
>               raise ValueError('`steps=None` is only valid for a generator'
                                 ' based on the `keras.utils.Sequence` class.'
                                 ' Please specify `steps` or use the'
                                 ' `keras.utils.Sequence` class.')
E               ValueError: `steps=None` is only valid for a generator based on the `keras.utils.Sequence` class. Please specify `steps` or use the `keras.utils.Sequence` class.

keras/engine/training_generator.py:300: ValueError
----------------------------- Captured stdout call -----------------------------
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.8906 - dense_1_loss: 0.4878 - dropout_loss: 0.8055
10/10 [==============================] - 0s 180us/step - loss: 0.9917 - dense_1_loss: 0.6147 - dropout_loss: 0.7540
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.7942 - dense_1_loss: 0.4992 - dropout_loss: 0.5900
10/10 [==============================] - 0s 165us/step - loss: 0.8688 - dense_1_loss: 0.6025 - dropout_loss: 0.5325
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.8636 - dense_1_loss: 0.5967 - dropout_loss: 0.5338
10/10 [==============================] - 0s 156us/step - loss: 0.9122 - dense_1_loss: 0.5924 - dropout_loss: 0.6396
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.6340 - dense_1_loss: 0.5332 - dropout_loss: 0.2017
5/5 [==============================] - 0s 4ms/step - loss: 0.6672 - dense_1_loss: 0.5470 - dropout_loss: 0.2403 - val_loss: 0.7190 - val_dense_1_loss: 0.6176 - val_dropout_loss: 0.2027
Train on 5 samples, validate on 5 samples
Epoch 1/1

4/5 [=======================>......] - ETA: 0s - loss: 0.8603 - dense_1_loss: 0.5511 - dropout_loss: 0.6182
5/5 [==============================] - 0s 374us/step - loss: 0.8244 - dense_1_loss: 0.5410 - dropout_loss: 0.5668 - val_loss: 0.7134 - val_dense_1_loss: 0.6120 - val_dropout_loss: 0.2027
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.6172 - dense_1_loss: 0.4888 - dropout_loss: 0.2568
10/10 [==============================] - 0s 273us/step - loss: 0.7351 - dense_1_loss: 0.5720 - dropout_loss: 0.3261 - val_loss: 0.6615 - val_dense_1_loss: 0.5664 - val_dropout_loss: 0.1902
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.9182 - dense_1_loss: 0.5161 - dropout_loss: 0.8041
10/10 [==============================] - 0s 285us/step - loss: 0.8753 - dense_1_loss: 0.5645 - dropout_loss: 0.6215 - val_loss: 0.6542 - val_dense_1_loss: 0.5591 - val_dropout_loss: 0.1902
Train on 10 samples, validate on 10 samples
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.7608 - dense_1_loss: 0.5030 - dropout_loss: 0.5156
10/10 [==============================] - 0s 242us/step - loss: 0.7881 - dense_1_loss: 0.5573 - dropout_loss: 0.4617 - val_loss: 0.6468 - val_dense_1_loss: 0.5517 - val_dropout_loss: 0.1902

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 115us/step
Epoch 3/5

 4/10 [===========>..................] - ETA: 0s - loss: 1.4751 - dense_1_loss: 0.8948 - dropout_loss: 0.5803 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 220us/step - loss: 1.1608 - dense_1_loss: 0.5951 - dropout_loss: 0.5656 - dense_1_acc: 0.3000
Epoch 4/5

 4/10 [===========>..................] - ETA: 0s - loss: 1.2326 - dense_1_loss: 0.6941 - dropout_loss: 0.5386 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 178us/step - loss: 1.0738 - dense_1_loss: 0.5783 - dropout_loss: 0.4955 - dense_1_acc: 0.3000
Epoch 5/5

 4/10 [===========>..................] - ETA: 0s - loss: 0.9419 - dense_1_loss: 0.6400 - dropout_loss: 0.3020 - dense_1_acc: 0.2500
10/10 [==============================] - 0s 162us/step - loss: 0.8988 - dense_1_loss: 0.5652 - dropout_loss: 0.3336 - dense_1_acc: 0.3000
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6578 - dense_1_loss: 0.3425 - dropout_loss: 0.3152 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 1ms/step - loss: 0.9077 - dense_1_loss: 0.4329 - dropout_loss: 0.4747 - dense_1_acc: 0.3333
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.9365 - dense_1_loss: 0.3741 - dropout_loss: 0.5623 - dense_1_acc: 0.7500
3/3 [==============================] - 0s 669us/step - loss: 0.9089 - dense_1_loss: 0.4002 - dropout_loss: 0.5087 - dense_1_acc: 0.5000
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 1.0081 - dense_1_loss: 0.6529 - dropout_loss: 0.3551 - dense_1_acc: 0.2500
3/3 [==============================] - 0s 655us/step - loss: 1.1058 - dense_1_loss: 0.6178 - dropout_loss: 0.4881 - dense_1_acc: 0.1667
Epoch 1/1

 4/10 [===========>..................] - ETA: 0s - loss: 0.9563 - dense_1_loss: 0.4691 - dropout_loss: 0.4872 - dense_1_mse: 0.4691 - dropout_mse: 0.4872
10/10 [==============================] - 0s 210us/step - loss: 0.9182 - dense_1_loss: 0.4579 - dropout_loss: 0.4603 - dense_1_mse: 0.4579 - dropout_mse: 0.4603

 4/10 [===========>..................] - ETA: 0s
10/10 [==============================] - 0s 107us/step

1/3 [=========>....................] - ETA: 0s
3/3 [==============================] - 0s 801us/step
Epoch 1/5

1/3 [=========>....................] - ETA: 0s - loss: 0.8318 - dense_1_loss: 0.5169 - dropout_loss: 0.6299
3/3 [==============================] - 0s 83ms/step - loss: 0.7055 - dense_1_loss: 0.4837 - dropout_loss: 0.4435 - val_loss: 0.6214 - val_dense_1_loss: 0.5409 - val_dropout_loss: 0.1611
Epoch 2/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7961 - dense_1_loss: 0.4095 - dropout_loss: 0.7732
3/3 [==============================] - 0s 1ms/step - loss: 0.8248 - dense_1_loss: 0.5603 - dropout_loss: 0.5290 - val_loss: 0.4251 - val_dense_1_loss: 0.3309 - val_dropout_loss: 0.1883
Epoch 3/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6235 - dense_1_loss: 0.4940 - dropout_loss: 0.2590
3/3 [==============================] - 0s 1ms/step - loss: 0.5830 - dense_1_loss: 0.4408 - dropout_loss: 0.2844 - val_loss: 0.4967 - val_dense_1_loss: 0.4025 - val_dropout_loss: 0.1882
Epoch 4/5

1/3 [=========>....................] - ETA: 0s - loss: 0.6540 - dense_1_loss: 0.4348 - dropout_loss: 0.4385
3/3 [==============================] - 0s 992us/step - loss: 0.6261 - dense_1_loss: 0.3640 - dropout_loss: 0.5240 - val_loss: 0.4587 - val_dense_1_loss: 0.3937 - val_dropout_loss: 0.1300
Epoch 5/5

1/3 [=========>....................] - ETA: 0s - loss: 0.7671 - dense_1_loss: 0.4608 - dropout_loss: 0.6126
3/3 [==============================] - 0s 39ms/step - loss: 0.5516 - dense_1_loss: 0.3406 - dropout_loss: 0.4220 - val_loss: 0.5072 - val_dense_1_loss: 0.4464 - val_dropout_loss: 0.1217
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 1.0160 - dense_1_loss: 0.6884 - dropout_loss: 0.6553
12/12 [==============================] - 0s 5ms/step - loss: 0.7391 - dense_1_loss: 0.4586 - dropout_loss: 0.5610 - val_loss: 0.4899 - val_dense_1_loss: 0.3953 - val_dropout_loss: 0.1892
Epoch 2/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6192 - dense_1_loss: 0.3557 - dropout_loss: 0.5271
12/12 [==============================] - 0s 9ms/step - loss: 0.6218 - dense_1_loss: 0.3899 - dropout_loss: 0.4639 - val_loss: 0.4581 - val_dense_1_loss: 0.3938 - val_dropout_loss: 0.1286
Epoch 3/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6866 - dense_1_loss: 0.4242 - dropout_loss: 0.5248
12/12 [==============================] - 0s 9ms/step - loss: 0.6634 - dense_1_loss: 0.3788 - dropout_loss: 0.5691 - val_loss: 0.4320 - val_dense_1_loss: 0.3412 - val_dropout_loss: 0.1816
Epoch 4/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6159 - dense_1_loss: 0.4589 - dropout_loss: 0.3141
12/12 [==============================] - 0s 9ms/step - loss: 0.5843 - dense_1_loss: 0.3522 - dropout_loss: 0.4642 - val_loss: 0.4031 - val_dense_1_loss: 0.3289 - val_dropout_loss: 0.1483
Epoch 5/5

 1/12 [=>............................] - ETA: 0s - loss: 0.8272 - dense_1_loss: 0.4134 - dropout_loss: 0.8275
12/12 [==============================] - 0s 9ms/step - loss: 0.5893 - dense_1_loss: 0.3314 - dropout_loss: 0.5159 - val_loss: 0.3804 - val_dense_1_loss: 0.3038 - val_dropout_loss: 0.1532
Epoch 1/5

 1/12 [=>............................] - ETA: 0s - loss: 0.6466 - dense_1_loss: 0.3229 - dropout_loss: 0.6474
----------------------------- Captured stderr call -----------------------------
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

------------------------------ Captured log call -------------------------------
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING  tensorflow:deprecation.py:506 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.
--------------------------- Captured stderr teardown ---------------------------
WARNING:tensorflow:From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.

---------------------------- Captured log teardown -----------------------------
WARNING  tensorflow:module_wrapper.py:139 From /home/user/BugsInPy/temp/projects/keras/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.
=============================== warnings summary ===============================
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:521: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
    tensor_proto.tensor_content = nparray.tostring()

/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:339: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    if not isinstance(values, collections.Sequence):

keras/utils/data_utils.py:651
keras/utils/data_utils.py:651
keras/utils/data_utils.py:651
  /home/user/BugsInPy/temp/projects/keras/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.
    DeprecationWarning)

/opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/generator_io.py:26
  /opt/conda/envs/30be27653f737e13d505dcd8372bd58d/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/learn_io/generator_io.py:26: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Container

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========================== slowest 20 test durations ===========================
2.41s call     tests/keras/engine/test_training.py::test_model_methods

(0.00 durations hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED tests/keras/engine/test_training.py::test_model_methods - ValueError: ...
======================== 1 failed, 12 warnings in 5.10s ========================
Using TensorFlow backend.
