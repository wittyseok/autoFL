[
    {
        "name": "scrapy.spiderloader.SpiderLoader.__init__#19",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.spider_modules = settings.getlist('SPIDER_MODULES')\n        self._spiders = {}\n        self._load_all_spiders()",
        "begin_line": 19,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019968051118210862,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_spiders#24",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_spiders(self, module)",
        "snippet": "    def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._spiders[spcls.name] = spcls",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_all_spiders#28",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_all_spiders(self)",
        "snippet": "    def _load_all_spiders(self):\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError as e:\n                msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n                       \"Check SPIDER_MODULES setting\".format(\n                            modname=name, tb=traceback.format_exc()))\n                warnings.warn(msg, RuntimeWarning)",
        "begin_line": 28,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.from_settings#40",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings)",
        "begin_line": 40,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019968051118210862,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.load#43",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.load(self, spider_name)",
        "snippet": "    def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(\"Spider not found: {}\".format(spider_name))",
        "begin_line": 43,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.find_by_request#53",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.find_by_request(self, request)",
        "snippet": "    def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [name for name, cls in self._spiders.items()\n                if cls.handles_request(request)]",
        "begin_line": 53,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.list#60",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.list(self)",
        "snippet": "    def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())",
        "begin_line": 60,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.default.UrlContract.adjust_request_args#16",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.UrlContract",
        "signature": "scrapy.contracts.default.UrlContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.__init__#42",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ReturnsContract, self).__init__(*args, **kwargs)\n\n        assert len(self.args) in [1, 2, 3]\n        self.obj_name = self.args[0] or None\n        self.obj_type = self.objects[self.obj_name]\n\n        try:\n            self.min_bound = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound = int(self.args[2])\n        except IndexError:\n            self.max_bound = float('inf')",
        "begin_line": 42,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.post_process#59",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if isinstance(x, self.obj_type):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = '%s..%s' % (self.min_bound, self.max_bound)\n\n            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n                (occurrences, self.obj_name, expected))",
        "begin_line": 59,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.default.ScrapesContract.post_process#84",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ScrapesContract",
        "signature": "scrapy.contracts.default.ScrapesContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        for x in output:\n            if isinstance(x, (BaseItem, dict)):\n                for arg in self.args:\n                    if not arg in x:\n                        raise ContractFail(\"'%s' field is missing\" % arg)",
        "begin_line": 84,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators.xmliter#18",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.xmliter(obj, nodename)",
        "snippet": "def xmliter(obj, nodename):\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    nodename_patt = re.escape(nodename)\n\n    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename_patt, re.S)\n    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename_patt, re.S)\n    text = _body_or_str(obj)\n\n    header_start = re.search(HEADER_START_RE, text)\n    header_start = header_start.group(1).strip() if header_start else ''\n    header_end = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end[1]:].strip() if header_end else ''\n\n    r = re.compile(r'<%(np)s[\\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = header_start + match.group() + header_end\n        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]",
        "begin_line": 18,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators.xmliter_lxml#44",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.xmliter_lxml(obj, nodename, namespace=None, prefix='x')",
        "snippet": "def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n    from lxml import etree\n    reader = _StreamReader(obj)\n    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n    for _, node in iterable:\n        nodetext = etree.tostring(node, encoding='unicode')\n        node.clear()\n        xs = Selector(text=nodetext, type='xml')\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader.__init__#61",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader.__init__(self, obj)",
        "snippet": "    def __init__(self, obj):\n        self._ptr = 0\n        if isinstance(obj, Response):\n            self._text, self.encoding = obj.body, obj.encoding\n        else:\n            self._text, self.encoding = obj, 'utf-8'\n        self._is_unicode = isinstance(self._text, six.text_type)",
        "begin_line": 61,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader.read#69",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader.read(self, n=65535)",
        "snippet": "    def read(self, n=65535):\n        self.read = self._read_unicode if self._is_unicode else self._read_string\n        return self.read(n).lstrip()",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader._read_string#73",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader._read_string(self, n=65535)",
        "snippet": "    def _read_string(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e]",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader._read_unicode#78",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader._read_unicode(self, n=65535)",
        "snippet": "    def _read_unicode(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e].encode('utf-8')",
        "begin_line": 78,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.iterators._body_or_str#130",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators._body_or_str(obj, unicode=True)",
        "snippet": "def _body_or_str(obj, unicode=True):\n    expected_types = (Response, six.text_type, six.binary_type)\n    assert isinstance(obj, expected_types), \\\n        \"obj must be %s, not %s\" % (\n            \" or \".join(t.__name__ for t in expected_types),\n            type(obj).__name__)\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        elif isinstance(obj, TextResponse):\n            return obj.text\n        else:\n            return obj.body.decode('utf-8')\n    elif isinstance(obj, six.text_type):\n        return obj if unicode else obj.encode('utf-8')\n    else:\n        return obj.decode('utf-8') if unicode else obj",
        "begin_line": 130,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.ItemMeta.__new__#27",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.ItemMeta",
        "signature": "scrapy.item.ItemMeta.__new__(mcs, class_name, bases, attrs)",
        "snippet": "    def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)",
        "begin_line": 27,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__init__#52",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._values = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in six.iteritems(dict(*args, **kwargs)):\n                self[k] = v",
        "begin_line": 52,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021123785382340515,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__getitem__#58",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._values[key]",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002101723413198823,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__setitem__#61",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(\"%s does not support field: %s\" %\n                (self.__class__.__name__, key))",
        "begin_line": 61,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__getattr__#71",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(\"Use item[%r] to get field value\" % name)\n        raise AttributeError(name)",
        "begin_line": 71,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__setattr__#76",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                (name, value))\n        super(DictItem, self).__setattr__(name, value)",
        "begin_line": 76,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__len__#82",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._values)",
        "begin_line": 82,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__iter__#85",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self._values)",
        "begin_line": 85,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00026737967914438503,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.keys#90",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.keys(self)",
        "snippet": "    def keys(self):\n        return self._values.keys()",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022266755733689602,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.__repr__#93",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return pformat(dict(self))",
        "begin_line": 93,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.item.DictItem.copy#96",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.copy(self)",
        "snippet": "    def copy(self):\n        return self.__class__(self)",
        "begin_line": 96,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.__init__#9",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.__init__(self, user_agent='Scrapy')",
        "snippet": "    def __init__(self, user_agent='Scrapy'):\n        self.user_agent = user_agent",
        "begin_line": 9,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.from_crawler#13",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings['USER_AGENT'])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 13,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.spider_opened#18",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.user_agent = getattr(spider, 'user_agent', self.user_agent)",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.process_request#21",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if self.user_agent:\n            request.headers.setdefault(b'User-Agent', self.user_agent)",
        "begin_line": 21,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager._get_mwlist_from_settings#15",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))",
        "begin_line": 15,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager._add_middleware#18",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager._add_middleware(self, pipe)",
        "snippet": "    def _add_middleware(self, pipe):\n        super(ItemPipelineManager, self)._add_middleware(pipe)\n        if hasattr(pipe, 'process_item'):\n            self.methods['process_item'].append(pipe.process_item)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager.process_item#23",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        return self._process_chain('process_item', item, spider)",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.SpiderInfo.__init__#21",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.SpiderInfo",
        "signature": "scrapy.pipelines.media.SpiderInfo.__init__(self, spider)",
        "snippet": "        def __init__(self, spider):\n            self.spider = spider\n            self.downloading = set()\n            self.downloaded = {}\n            self.waiting = defaultdict(list)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.__init__#27",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.__init__(self, download_func=None)",
        "snippet": "    def __init__(self, download_func=None):\n        self.download_func = download_func",
        "begin_line": 27,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021786492374727668,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._key_for_pipe#31",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._key_for_pipe(self, key, base_class_name=None, settings=None)",
        "snippet": "    def _key_for_pipe(self, key, base_class_name=None,\n                      settings=None):\n        \"\"\"\n        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n        'IMAGES'\n        >>> class MyPipe(MediaPipeline):\n        ...     pass\n        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n        'MYPIPE_IMAGES'\n        \"\"\"\n        class_name = self.__class__.__name__\n        formatted_key = \"{}_{}\".format(class_name.upper(), key)\n        if class_name == base_class_name or not base_class_name \\\n            or (settings and not settings.get(formatted_key)):\n            return key\n        return formatted_key",
        "begin_line": 31,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.open_spider#57",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)",
        "begin_line": 57,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.process_item#60",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=1)\n        return dfd.addCallback(self.item_completed, item, info)",
        "begin_line": 60,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._process_request#67",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._process_request(self, request, info)",
        "snippet": "    def _process_request(self, request, info):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info)\n        dfd.addCallback(self._check_media_to_download, request, info)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)  # it must return wad at last",
        "begin_line": 67,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._check_media_to_download#96",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._check_media_to_download(self, result, request, info)",
        "snippet": "    def _check_media_to_download(self, result, request, info):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd",
        "begin_line": 96,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters#113",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters(self, result, fp, info)",
        "snippet": "    def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)",
        "begin_line": 113,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_to_download#125",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        \"\"\"Check request before starting download\"\"\"\n        pass",
        "begin_line": 125,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.get_media_requests#129",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass",
        "begin_line": 129,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_downloaded#133",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        \"\"\"Handler for success downloads\"\"\"\n        return response",
        "begin_line": 133,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_failed#137",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_failed(self, failure, request, info)",
        "snippet": "    def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure",
        "begin_line": 137,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.item_completed#141",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item",
        "begin_line": 141,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__init__#19",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)",
        "snippet": "    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None",
        "begin_line": 19,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009433962264150943,
            "pseudo_dstar_susp": 0.009433962264150943,
            "pseudo_tarantula_susp": 0.009433962264150943,
            "pseudo_op2_susp": 0.009433962264150943,
            "pseudo_barinel_susp": 0.009433962264150943
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.meta#41",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.meta(self)",
        "snippet": "    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta",
        "begin_line": 41,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000197667523225934,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_url#46",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 46,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011904761904761904,
            "pseudo_dstar_susp": 0.011904761904761904,
            "pseudo_tarantula_susp": 0.011904761904761904,
            "pseudo_op2_susp": 0.011904761904761904,
            "pseudo_barinel_susp": 0.011904761904761904
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_url#49",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)",
        "begin_line": 49,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008403361344537815,
            "pseudo_dstar_susp": 0.008403361344537815,
            "pseudo_tarantula_susp": 0.008403361344537815,
            "pseudo_op2_susp": 0.008403361344537815,
            "pseudo_barinel_susp": 0.008403361344537815
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_body#61",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0001973164956590371,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_body#64",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)",
        "begin_line": 64,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011111111111111112,
            "pseudo_dstar_susp": 0.011111111111111112,
            "pseudo_tarantula_susp": 0.011111111111111112,
            "pseudo_op2_susp": 0.011111111111111112,
            "pseudo_barinel_susp": 0.011111111111111112
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.encoding#73",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._encoding",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008403361344537815,
            "pseudo_dstar_susp": 0.008403361344537815,
            "pseudo_tarantula_susp": 0.008403361344537815,
            "pseudo_op2_susp": 0.008403361344537815,
            "pseudo_barinel_susp": 0.008403361344537815
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__str__#76",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022527596305474206,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.copy#81",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.replace#85",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 85,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021331058020477816,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.rpc.XmlRpcRequest.__init__#18",
        "src_path": "scrapy/http/request/rpc.py",
        "class_name": "scrapy.http.request.rpc.XmlRpcRequest",
        "signature": "scrapy.http.request.rpc.XmlRpcRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        encoding = kwargs.get('encoding', None)\n        if 'body' not in kwargs and 'params' in kwargs:\n            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n            kwargs['body'] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault('method', 'POST')\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault('dont_filter', True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs['encoding'] = encoding\n\n        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n        self.headers.setdefault('Content-Type', 'text/xml')",
        "begin_line": 18,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__#24",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n            raise NotConfigured\n\n        self.crawler = crawler\n        self._useragent = crawler.settings.get('USER_AGENT')\n        self._parsers = {}",
        "begin_line": 24,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.from_crawler#33",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request#36",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        d = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d",
        "begin_line": 36,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request_2#43",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request_2(self, rp, request, spider)",
        "snippet": "    def process_request_2(self, rp, request, spider):\n        if rp is not None and not rp.can_fetch(\n                 to_native_str(self._useragent), request.url):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest()",
        "begin_line": 43,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser#50",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser(self, request, spider)",
        "snippet": "    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n\n        if isinstance(self._parsers[netloc], Deferred):\n            d = Deferred()\n            def cb(result):\n                d.callback(result)\n                return result\n            self._parsers[netloc].addCallback(cb)\n            return d\n        else:\n            return self._parsers[netloc]",
        "begin_line": 50,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.cb#69",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.cb(result)",
        "snippet": "            def cb(result):\n                d.callback(result)\n                return result",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._logerror#77",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._logerror(self, failure, request, spider)",
        "snippet": "    def _logerror(self, failure, request, spider):\n        if failure.type is not IgnoreRequest:\n            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                         {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots#85",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots(self, response, netloc)",
        "snippet": "    def _parse_robots(self, response, netloc):\n        rp = robotparser.RobotFileParser(response.url)\n        body = ''\n        if hasattr(response, 'text'):\n            body = response.text\n        else: # last effort try\n            try:\n                body = response.body.decode('utf-8')\n            except UnicodeDecodeError:\n                # If we found garbage, disregard it:,\n                # but keep the lookup cached (in self._parsers)\n                # Running rp.parse() will set rp state from\n                # 'disallow all' to 'allow any'.\n                pass\n        # stdlib's robotparser expects native 'str' ;\n        # with unicode input, non-ASCII encoded bytes decoding fails in Python2\n        rp.parse(to_native_str(body).splitlines())\n\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)",
        "begin_line": 85,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._robots_error#107",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._robots_error(self, failure, netloc)",
        "snippet": "    def _robots_error(self, failure, netloc):\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)",
        "begin_line": 107,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.spider.iterate_spider_output#12",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iterate_spider_output(result)",
        "snippet": "def iterate_spider_output(result):\n    return arg_to_iter(result)",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.spider.iter_spider_classes#16",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iter_spider_classes(module)",
        "snippet": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (ie. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in six.itervalues(vars(module)):\n        if inspect.isclass(obj) and \\\n           issubclass(obj, Spider) and \\\n           obj.__module__ == module.__name__ and \\\n           getattr(obj, 'name', None):\n            yield obj",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.process_results#28",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.process_results(self, response, results)",
        "snippet": "    def process_results(self, response, results):\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (Items or Requests).\n        \"\"\"\n        return results",
        "begin_line": 28,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.adapt_response#38",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.adapt_response(self, response)",
        "snippet": "    def adapt_response(self, response):\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response",
        "begin_line": 38,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.parse_nodes#51",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.parse_nodes(self, response, nodes)",
        "snippet": "    def parse_nodes(self, response, nodes):\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either a BaseItem, a Request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            for result_item in self.process_results(response, ret):\n                yield result_item",
        "begin_line": 51,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.parse#64",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.parse(self, response)",
        "snippet": "    def parse(self, response):\n        if not hasattr(self, 'parse_node'):\n            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n\n        response = self.adapt_response(response)\n        if self.iterator == 'iternodes':\n            nodes = self._iternodes(response)\n        elif self.iterator == 'xml':\n            selector = Selector(response, type='xml')\n            self._register_namespaces(selector)\n            nodes = selector.xpath('//%s' % self.itertag)\n        elif self.iterator == 'html':\n            selector = Selector(response, type='html')\n            self._register_namespaces(selector)\n            nodes = selector.xpath('//%s' % self.itertag)\n        else:\n            raise NotSupported('Unsupported node iterator')\n\n        return self.parse_nodes(response, nodes)",
        "begin_line": 64,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider._iternodes#84",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider._iternodes(self, response)",
        "snippet": "    def _iternodes(self, response):\n        for node in xmliter(response, self.itertag):\n            self._register_namespaces(node)\n            yield node",
        "begin_line": 84,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider._register_namespaces#89",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider._register_namespaces(self, selector)",
        "snippet": "    def _register_namespaces(self, selector):\n        for (prefix, uri) in self.namespaces:\n            selector.register_namespace(prefix, uri)",
        "begin_line": 89,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.from_crawler#12",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('REFERER_ENABLED'):\n            raise NotConfigured\n        return cls()",
        "begin_line": 12,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.process_spider_output#17",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _set_referer(r):\n            if isinstance(r, Request):\n                r.headers.setdefault('Referer', response.url)\n            return r\n        return (_set_referer(r) for r in result or ())",
        "begin_line": 17,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware._set_referer#18",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware._set_referer(r)",
        "snippet": "        def _set_referer(r):\n            if isinstance(r, Request):\n                r.headers.setdefault('Referer', response.url)\n            return r",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest.execute#14",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest.execute(self, args, check_code=True, settings=None)",
        "snippet": "    def execute(self, args, check_code=True, settings=None):\n        env = os.environ.copy()\n        if settings is not None:\n            env['SCRAPY_SETTINGS_MODULE'] = settings\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest._process_finished#24",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest._process_finished(self, pp, cmd, check_code)",
        "snippet": "    def _process_finished(self, pp, cmd, check_code):\n        if pp.exitcode and check_code:\n            msg = \"process %s exit with code %d\" % (cmd, pp.exitcode)\n            msg += \"\\n>>> stdout <<<\\n%s\" % pp.out\n            msg += \"\\n\"\n            msg += \"\\n>>> stderr <<<\\n%s\" % pp.err\n            raise RuntimeError(msg)\n        return pp.exitcode, pp.out, pp.err",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.__init__#36",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.__init__(self)",
        "snippet": "    def __init__(self):\n        self.deferred = defer.Deferred()\n        self.out = b''\n        self.err = b''\n        self.exitcode = None",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.outReceived#42",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.outReceived(self, data)",
        "snippet": "    def outReceived(self, data):\n        self.out += data",
        "begin_line": 42,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.errReceived#45",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.errReceived(self, data)",
        "snippet": "    def errReceived(self, data):\n        self.err += data",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.processEnded#48",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.processEnded(self, status)",
        "snippet": "    def processEnded(self, status):\n        self.exitcode = status.value.exitCode\n        self.deferred.callback(self)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http.HttpDownloadHandler.__init__#9",
        "src_path": "scrapy/core/downloader/handlers/http.py",
        "class_name": "scrapy.core.downloader.handlers.http.HttpDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http.HttpDownloadHandler.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        import warnings\n        from scrapy.exceptions import ScrapyDeprecationWarning\n        warnings.warn('HttpDownloadHandler is deprecated, import scrapy.core.downloader'\n                      '.handlers.http10.HTTP10DownloadHandler instead',\n                      category=ScrapyDeprecationWarning, stacklevel=1)\n        super(HttpDownloadHandler, self).__init__(*args, **kwargs)",
        "begin_line": 9,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpError.__init__#16",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpError",
        "signature": "scrapy.spidermiddlewares.httperror.HttpError.__init__(self, response, *args, **kwargs)",
        "snippet": "    def __init__(self, response, *args, **kwargs):\n        self.response = response\n        super(HttpError, self).__init__(*args, **kwargs)",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.from_crawler#24",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 24,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.__init__#27",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')\n        self.handle_httpstatus_list = settings.getlist('HTTPERROR_ALLOWED_CODES')",
        "begin_line": 27,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_input#31",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_input(self, response, spider)",
        "snippet": "    def process_spider_input(self, response, spider):\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if 'handle_httpstatus_all' in meta:\n            return\n        if 'handle_httpstatus_list' in meta:\n            allowed_statuses = meta['handle_httpstatus_list']\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, 'Ignoring non-200 response')",
        "begin_line": 31,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_exception#47",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_exception(self, response, exception, spider)",
        "snippet": "    def process_spider_exception(self, response, exception, spider):\n        if isinstance(exception, HttpError):\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {'response': response}, extra={'spider': spider},\n            )\n            return []",
        "begin_line": 47,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.common.wrap_loader_context#6",
        "src_path": "scrapy/loader/common.py",
        "class_name": "scrapy.loader.common",
        "signature": "scrapy.loader.common.wrap_loader_context(function, context)",
        "snippet": "def wrap_loader_context(function, context):\n    \"\"\"Wrap functions that receive loader_context to contain the context\n    \"pre-loaded\" and expose a interface that receives only one argument\n    \"\"\"\n    if 'loader_context' in get_func_args(function):\n        return partial(function, loader_context=context)\n    else:\n        return function",
        "begin_line": 6,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__(self, debug=False)",
        "snippet": "    def __init__(self, debug=False):\n        self.jars = defaultdict(CookieJar)\n        self.debug = debug",
        "begin_line": 17,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000231000231000231,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler#22",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COOKIES_ENABLED'):\n            raise NotConfigured\n        return cls(crawler.settings.getbool('COOKIES_DEBUG'))",
        "begin_line": 22,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request#27",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        for cookie in cookies:\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)",
        "begin_line": 27,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response#42",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response",
        "begin_line": 42,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie#54",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie(self, request, spider)",
        "snippet": "    def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in request.headers.getlist('Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Sending cookies to: {}\\n{}\".format(request, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 54,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie#63",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie(self, response, spider)",
        "snippet": "    def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in response.headers.getlist('Set-Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 63,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie#72",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie(self, cookie)",
        "snippet": "    def _format_cookie(self, cookie):\n        # build cookie string\n        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n\n        if cookie.get('path', None):\n            cookie_str += '; Path=%s' % cookie['path']\n        if cookie.get('domain', None):\n            cookie_str += '; Domain=%s' % cookie['domain']\n\n        return cookie_str",
        "begin_line": 72,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies#83",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies(self, jar, request)",
        "snippet": "    def _get_request_cookies(self, jar, request):\n        if isinstance(request.cookies, dict):\n            cookie_list = [{'name': k, 'value': v} for k, v in \\\n                    six.iteritems(request.cookies)]\n        else:\n            cookie_list = request.cookies\n\n        cookies = [self._format_cookie(x) for x in cookie_list]\n        headers = {'Set-Cookie': cookies}\n        response = Response(request.url, headers=headers)\n\n        return jar.make_cookies(response, request)",
        "begin_line": 83,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.__init__#22",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._encoding = kwargs.pop('encoding', None)\n        self._cached_benc = None\n        self._cached_ubody = None\n        self._cached_selector = None\n        super(TextResponse, self).__init__(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014084507042253521,
            "pseudo_dstar_susp": 0.014084507042253521,
            "pseudo_tarantula_susp": 0.014084507042253521,
            "pseudo_op2_susp": 0.014084507042253521,
            "pseudo_barinel_susp": 0.014084507042253521
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_url#29",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, six.text_type):\n            if six.PY2 and self.encoding is None:\n                raise TypeError(\"Cannot convert unicode url - %s \"\n                                \"has no encoding\" % type(self).__name__)\n            self._url = to_native_str(url, self.encoding)\n        else:\n            super(TextResponse, self)._set_url(url)",
        "begin_line": 29,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014084507042253521,
            "pseudo_dstar_susp": 0.014084507042253521,
            "pseudo_tarantula_susp": 0.014084507042253521,
            "pseudo_op2_susp": 0.014084507042253521,
            "pseudo_barinel_susp": 0.014084507042253521
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_body#38",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, six.text_type):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                    type(self).__name__)\n            self._body = body.encode(self._encoding)\n        else:\n            super(TextResponse, self)._set_body(body)",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.022727272727272728,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.022727272727272728,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.022727272727272728
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.replace#48",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.encoding#53",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012658227848101266,
            "pseudo_dstar_susp": 0.012658227848101266,
            "pseudo_tarantula_susp": 0.012658227848101266,
            "pseudo_op2_susp": 0.012658227848101266,
            "pseudo_barinel_susp": 0.012658227848101266
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._declared_encoding#56",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._declared_encoding(self)",
        "snippet": "    def _declared_encoding(self):\n        return self._encoding or self._headers_encoding() \\\n            or self._body_declared_encoding()",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012658227848101266,
            "pseudo_dstar_susp": 0.012658227848101266,
            "pseudo_tarantula_susp": 0.012658227848101266,
            "pseudo_op2_susp": 0.012658227848101266,
            "pseudo_barinel_susp": 0.012658227848101266
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.body_as_unicode#60",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.body_as_unicode(self)",
        "snippet": "    def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        return self.text",
        "begin_line": 60,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.text#65",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.text(self)",
        "snippet": "    def text(self):\n        \"\"\" Body as unicode \"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = 'charset=%s' % benc\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody",
        "begin_line": 65,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.017543859649122806,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.017543859649122806,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.urljoin#75",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)",
        "begin_line": 75,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._headers_encoding#81",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._headers_encoding(self)",
        "snippet": "    def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_native_str(content_type))",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019758940920766647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_inferred_encoding#85",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_inferred_encoding(self)",
        "snippet": "    def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                    auto_detect_fun=self._auto_detect_fun,\n                    default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc",
        "begin_line": 85,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019876764062810574,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._auto_detect_fun#95",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._auto_detect_fun(self, text)",
        "snippet": "    def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_declared_encoding#104",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_declared_encoding(self)",
        "snippet": "    def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)",
        "begin_line": 104,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019782393669634025,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.selector#108",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.selector(self)",
        "snippet": "    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector",
        "begin_line": 108,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002244668911335578,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.xpath#114",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.xpath(self, query, **kwargs)",
        "snippet": "    def xpath(self, query, **kwargs):\n        return self.selector.xpath(query, **kwargs)",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.css#117",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.selector.css(query)",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.serialize.ScrapyJSONEncoder.default#16",
        "src_path": "scrapy/utils/serialize.py",
        "class_name": "scrapy.utils.serialize.ScrapyJSONEncoder",
        "signature": "scrapy.utils.serialize.ScrapyJSONEncoder.default(self, o)",
        "snippet": "    def default(self, o):\n        if isinstance(o, set):\n            return list(o)\n        elif isinstance(o, datetime.datetime):\n            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif isinstance(o, BaseItem):\n            return dict(o)\n        elif isinstance(o, Request):\n            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n        elif isinstance(o, Response):\n            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n        else:\n            return super(ScrapyJSONEncoder, self).default(o)",
        "begin_line": 16,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.from_crawler#17",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.spider_opened#22",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)",
        "begin_line": 22,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.process_request#28",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            request.headers[b'Authorization'] = auth",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider.__init__#20",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider.__init__(self, *a, **kw)",
        "snippet": "    def __init__(self, *a, **kw):\n        super(SitemapSpider, self).__init__(*a, **kw)\n        self._cbs = []\n        for r, c in self.sitemap_rules:\n            if isinstance(c, six.string_types):\n                c = getattr(self, c)\n            self._cbs.append((regex(r), c))\n        self._follow = [regex(x) for x in self.sitemap_follow]",
        "begin_line": 20,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider.start_requests#29",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)",
        "begin_line": 29,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider._parse_sitemap#33",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider._parse_sitemap(self, response)",
        "snippet": "    def _parse_sitemap(self, response):\n        if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n                               {'response': response}, extra={'spider': self})\n                return\n\n            s = Sitemap(body)\n            if s.type == 'sitemapindex':\n                for loc in iterloc(s, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == 'urlset':\n                for loc in iterloc(s):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break",
        "begin_line": 33,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider._get_sitemap_body#56",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider._get_sitemap_body(self, response)",
        "snippet": "    def _get_sitemap_body(self, response):\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        elif is_gzipped(response):\n            return gunzip(response.body)\n        elif response.url.endswith('.xml'):\n            return response.body\n        elif response.url.endswith('.xml.gz'):\n            return gunzip(response.body)",
        "begin_line": 56,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.sitemap.regex#70",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap",
        "signature": "scrapy.spiders.sitemap.regex(x)",
        "snippet": "def regex(x):\n    if isinstance(x, six.string_types):\n        return re.compile(x)\n    return x",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.__init__#22",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool('AJAXCRAWL_ENABLED'):\n            raise NotConfigured\n\n        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n        # middleware parses first 4k. 4k turns out to be insufficient\n        # for this middleware, and parsing 100k could be slow.\n        # We use something in between (32K) by default.\n        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)",
        "begin_line": 22,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.from_crawler#33",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.process_response#36",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != 'GET':\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if 'ajax_crawlable' in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url+'#!')\n        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n                     extra={'spider': spider})\n\n        ajax_crawl_request.meta['ajax_crawlable'] = True\n        return ajax_crawl_request",
        "begin_line": 36,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware._has_ajax_crawlable_variant#60",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware._has_ajax_crawlable_variant(self, response)",
        "snippet": "    def _has_ajax_crawlable_variant(self, response):\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[:self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)",
        "begin_line": 60,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl._has_ajaxcrawlable_meta#71",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl._has_ajaxcrawlable_meta(text)",
        "snippet": "def _has_ajaxcrawlable_meta(text):\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if 'fragment' not in text:\n        return False\n    if 'content' not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None",
        "begin_line": 71,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.__init__#8",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.__init__(self, sender=dispatcher.Anonymous)",
        "snippet": "    def __init__(self, sender=dispatcher.Anonymous):\n        self.sender = sender",
        "begin_line": 8,
        "end_line": 9,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.connect#11",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.connect(self, receiver, signal, **kwargs)",
        "snippet": "    def connect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.connect(receiver, signal, **kwargs)",
        "begin_line": 11,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.send_catch_log#37",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.send_catch_log(self, signal, **kwargs)",
        "snippet": "    def send_catch_log(self, signal, **kwargs):\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log(signal, **kwargs)",
        "begin_line": 37,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.send_catch_log_deferred#47",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.send_catch_log_deferred(self, signal, **kwargs)",
        "snippet": "    def send_catch_log_deferred(self, signal, **kwargs):\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning `deferreds`_ from\n        signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n\n        .. _deferreds: http://twistedmatrix.com/documents/current/core/howto/defer.html\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)",
        "begin_line": 47,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.__init__#17",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n\n        self.close_on = {\n            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),\n            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),\n            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),\n            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n            }\n\n        if not any(self.close_on.values()):\n            raise NotConfigured\n\n        self.counter = defaultdict(int)\n\n        if self.close_on.get('errorcount'):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get('pagecount'):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get('timeout'):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get('itemcount'):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)",
        "begin_line": 17,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.from_crawler#43",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_opened#56",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.task = reactor.callLater(self.close_on['timeout'], \\\n            self.crawler.engine.close_spider, spider, \\\n            reason='closespider_timeout')",
        "begin_line": 56,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_closed#66",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        task = getattr(self, 'task', False)\n        if task and task.active():\n            task.cancel()",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.mail.MailSender.__init__#31",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.__init__(self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False)",
        "snippet": "    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n        self.smtphost = smtphost\n        self.smtpport = smtpport\n        self.smtpuser = smtpuser\n        self.smtppass = smtppass\n        self.smtptls = smtptls\n        self.smtpssl = smtpssl\n        self.mailfrom = mailfrom\n        self.debug = debug",
        "begin_line": 31,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.mail.MailSender.send#48",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None)",
        "snippet": "    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split('/', 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg['From'] = self.mailfrom\n        msg['To'] = COMMASPACE.join(to)\n        msg['Date'] = formatdate(localtime=True)\n        msg['Subject'] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg['Cc'] = COMMASPACE.join(cc)\n\n        if charset:\n            msg.set_charset(charset)\n\n        if attachs:\n            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split('/'))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' \\\n                    % attach_name)\n                msg.attach(part)\n        else:\n            msg.set_payload(body)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                          'mailattachs': len(attachs)})\n            return\n\n        dfd = self._sendmail(rcpts, msg.as_string())\n        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n            callbackArgs=[to, cc, subject, len(attachs)],\n            errbackArgs=[to, cc, subject, len(attachs)])\n        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n        return dfd",
        "begin_line": 48,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log#20",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop('dont_log', _IgnoredException)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender,\n                *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses",
        "begin_line": 20,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log_deferred#47",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d",
        "begin_line": 47,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.signal.logerror#52",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.logerror(failure, recv)",
        "snippet": "    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.signal.disconnect_all#74",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.disconnect_all(signal=Any, sender=Any)",
        "snippet": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)",
        "begin_line": 74,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.trackref.object_ref.__new__#30",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref.object_ref",
        "signature": "scrapy.utils.trackref.object_ref.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj",
        "begin_line": 30,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007462686567164179,
            "pseudo_dstar_susp": 0.007462686567164179,
            "pseudo_tarantula_susp": 0.007462686567164179,
            "pseudo_op2_susp": 0.007462686567164179,
            "pseudo_barinel_susp": 0.007462686567164179
        }
    },
    {
        "name": "scrapy.utils.trackref.format_live_refs#36",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.format_live_refs(ignore=NoneType)",
        "snippet": "def format_live_refs(ignore=NoneType):\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(six.iteritems(live_refs),\n                             key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(six.itervalues(wdict))\n        s += \"%-30s %6d   oldest: %ds ago\\n\" % (\n            cls.__name__, len(wdict), now - oldest\n        )\n    return s",
        "begin_line": 36,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.trackref.print_live_refs#53",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.print_live_refs(*a, **kw)",
        "snippet": "def print_live_refs(*a, **kw):\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))",
        "begin_line": 53,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.trackref.get_oldest#58",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.get_oldest(class_name)",
        "snippet": "def get_oldest(class_name):\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in six.iteritems(live_refs):\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(six.iteritems(wdict), key=itemgetter(1))[0]",
        "begin_line": 58,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.trackref.iter_all#67",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.iter_all(class_name)",
        "snippet": "def iter_all(class_name):\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in six.iteritems(live_refs):\n        if cls.__name__ == class_name:\n            return six.iterkeys(wdict)",
        "begin_line": 67,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.from_crawler#14",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n            raise NotConfigured\n        return cls()",
        "begin_line": 14,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_request#19",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_response#22",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n\n        if request.method == 'HEAD':\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist('Content-Encoding')\n            if content_encoding and not is_gzipped(response):\n                encoding = content_encoding.pop()\n                decoded_body = self._decode(response.body, encoding.lower())\n                respcls = responsetypes.from_args(headers=response.headers, \\\n                    url=response.url)\n                kwargs = dict(cls=respcls, body=decoded_body)\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs['encoding'] = None\n                response = response.replace(**kwargs)\n                if not content_encoding:\n                    del response.headers['Content-Encoding']\n\n        return response",
        "begin_line": 22,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware._decode#44",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware._decode(self, body, encoding)",
        "snippet": "    def _decode(self, body, encoding):\n        if encoding == b'gzip' or encoding == b'x-gzip':\n            body = gunzip(body)\n\n        if encoding == b'deflate':\n            try:\n                body = zlib.decompress(body)\n            except zlib.error:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                body = zlib.decompress(body, -15)\n        return body",
        "begin_line": 44,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.decorators.deco#14",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.deco(func)",
        "snippet": "    def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = \"Call to deprecated function %s.\" % func.__name__\n            if use_instead:\n                message += \" Use %s instead.\" % use_instead\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.decorators.wrapped#16",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.wrapped(*args, **kwargs)",
        "snippet": "        def wrapped(*args, **kwargs):\n            message = \"Call to deprecated function %s.\" % func.__name__\n            if use_instead:\n                message += \" Use %s instead.\" % use_instead\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)",
        "begin_line": 16,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.decorators.defers#30",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.defers(func)",
        "snippet": "def defers(func):\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)\n    return wrapped",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.decorators.wrapped#33",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.wrapped(*a, **kw)",
        "snippet": "    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.__init__#50",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.__init__(self, store_uri, download_func=None, settings=None)",
        "snippet": "    def __init__(self, store_uri, download_func=None, settings=None):\n        super(ImagesPipeline, self).__init__(store_uri, settings=settings,\n                                             download_func=download_func)\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=\"ImagesPipeline\",\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n        )\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field = settings.get(\n            resolve('IMAGES_URLS_FIELD'),\n            self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field = settings.get(\n            resolve('IMAGES_RESULT_FIELD'),\n            self.IMAGES_RESULT_FIELD\n        )\n        self.min_width = settings.getint(\n            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH\n        )\n        self.min_height = settings.getint(\n            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT\n        )\n        self.thumbs = settings.get(\n            resolve('IMAGES_THUMBS'), self.THUMBS\n        )",
        "begin_line": 50,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.from_settings#88",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n        store_uri = settings['IMAGES_STORE']\n        return cls(store_uri, settings=settings)",
        "begin_line": 88,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.convert_image#130",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.convert_image(self, image, size=None)",
        "snippet": "    def convert_image(self, image, size=None):\n        if image.format == 'PNG' and image.mode == 'RGBA':\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode != 'RGB':\n            image = image.convert('RGB')\n\n        if size:\n            image = image.copy()\n            image.thumbnail(size, Image.ANTIALIAS)\n\n        buf = BytesIO()\n        image.save(buf, 'JPEG')\n        return image, buf",
        "begin_line": 130,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.get_media_requests#146",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.images_urls_field, [])]",
        "begin_line": 146,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.item_completed#149",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.images_result_field in item.fields:\n            item[self.images_result_field] = [x for ok, x in results if ok]\n        return item",
        "begin_line": 149,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.file_path#154",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.file_path(self, request, response=None, info=None)",
        "snippet": "    def file_path(self, request, response=None, info=None):\n        ## start of deprecation warning block (can be removed in the future)\n        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n                          'please use file_path(request, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)\n\n        # check if called from image_key or file_key with url as first argument\n        if not isinstance(request, Request):\n            _warn()\n            url = request\n        else:\n            url = request.url\n\n        # detect if file_key() or image_key() methods have been overridden\n        if not hasattr(self.file_key, '_base'):\n            _warn()\n            return self.file_key(url)\n        elif not hasattr(self.image_key, '_base'):\n            _warn()\n            return self.image_key(url)\n        ## end of deprecation warning block\n\n        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n        return 'full/%s.jpg' % (image_guid)",
        "begin_line": 154,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline._warn#156",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline._warn()",
        "snippet": "        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n                          'please use file_path(request, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)",
        "begin_line": 156,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.thumb_path#182",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.thumb_path(self, request, thumb_id, response=None, info=None)",
        "snippet": "    def thumb_path(self, request, thumb_id, response=None, info=None):\n        ## start of deprecation warning block (can be removed in the future)\n        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('ImagesPipeline.thumb_key(url) method is deprecated, please use '\n                          'thumb_path(request, thumb_id, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)\n\n        # check if called from thumb_key with url as first argument\n        if not isinstance(request, Request):\n            _warn()\n            url = request\n        else:\n            url = request.url\n\n        # detect if thumb_key() method has been overridden\n        if not hasattr(self.thumb_key, '_base'):\n            _warn()\n            return self.thumb_key(url, thumb_id)\n        ## end of deprecation warning block\n\n        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)",
        "begin_line": 182,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline._warn#184",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline._warn()",
        "snippet": "        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('ImagesPipeline.thumb_key(url) method is deprecated, please use '\n                          'thumb_path(request, thumb_id, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)",
        "begin_line": 184,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.file_key#208",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.file_key(self, url)",
        "snippet": "    def file_key(self, url):\n        return self.image_key(url)",
        "begin_line": 208,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.image_key#213",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.image_key(self, url)",
        "snippet": "    def image_key(self, url):\n        return self.file_path(url)",
        "begin_line": 213,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.thumb_key#218",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.thumb_key(self, url, thumb_id)",
        "snippet": "    def thumb_key(self, url, thumb_id):\n        return self.thumb_path(url, thumb_id)",
        "begin_line": 218,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.get_settings_priority#24",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.get_settings_priority(priority)",
        "snippet": "def get_settings_priority(priority):\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, six.string_types):\n        return SETTINGS_PRIORITIES[priority]\n    else:\n        return priority",
        "begin_line": 24,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019876764062810574,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__init__#44",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__init__(self, value, priority)",
        "snippet": "    def __init__(self, value, priority):\n        self.value = value\n        if isinstance(self.value, BaseSettings):\n            self.priority = max(self.value.maxpriority(), priority)\n        else:\n            self.priority = priority",
        "begin_line": 44,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.set#51",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.set(self, value, priority)",
        "snippet": "    def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority",
        "begin_line": 51,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__str__#59",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<SettingsAttribute value={self.value!r} \" \\\n               \"priority={self.priority}>\".format(self=self)",
        "begin_line": 59,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__init__#88",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        self.frozen = False\n        self.attributes = {}\n        self.update(values, priority)",
        "begin_line": 88,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0001936858415649816,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__getitem__#93",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value",
        "begin_line": 93,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019747235387045813,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__contains__#98",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__contains__(self, name)",
        "snippet": "    def __contains__(self, name):\n        return name in self.attributes",
        "begin_line": 98,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0001941747572815534,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.get#101",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.get(self, name, default=None)",
        "snippet": "    def get(self, name, default=None):\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return self[name] if self[name] is not None else default",
        "begin_line": 101,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000196078431372549,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getbool#113",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getbool(self, name, default=False)",
        "snippet": "    def getbool(self, name, default=False):\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\"Supported values for boolean settings \"\n                             \"are 0/1, True/False, '0'/'1', \"\n                             \"'True'/'False' and 'true'/'false'\")",
        "begin_line": 113,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getint#141",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getint(self, name, default=0)",
        "snippet": "    def getint(self, name, default=0):\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return int(self.get(name, default))",
        "begin_line": 141,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019665683382497542,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getfloat#153",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getfloat(self, name, default=0.0)",
        "snippet": "    def getfloat(self, name, default=0.0):\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return float(self.get(name, default))",
        "begin_line": 153,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002022653721682848,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getlist#165",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getlist(self, name, default=None)",
        "snippet": "    def getlist(self, name, default=None):\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, six.string_types):\n            value = value.split(',')\n        return list(value)",
        "begin_line": 165,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getdict#184",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getdict(self, name, default=None)",
        "snippet": "    def getdict(self, name, default=None):\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, six.string_types):\n            value = json.loads(value)\n        return dict(value)",
        "begin_line": 184,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getwithbase#205",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getwithbase(self, name)",
        "snippet": "    def getwithbase(self, name):\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: string\n        \"\"\"\n        compbs = BaseSettings()\n        compbs.update(self[name + '_BASE'])\n        compbs.update(self[name])\n        return compbs",
        "begin_line": 205,
        "end_line": 215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002022653721682848,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getpriority#217",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getpriority(self, name)",
        "snippet": "    def getpriority(self, name):\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: string\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority",
        "begin_line": 217,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020132876988121604,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.maxpriority#229",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.maxpriority(self)",
        "snippet": "    def maxpriority(self):\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(self.getpriority(name) for name in self)\n        else:\n            return get_settings_priority('default')",
        "begin_line": 229,
        "end_line": 239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__setitem__#241",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__setitem__(self, name, value)",
        "snippet": "    def __setitem__(self, name, value):\n        self.set(name, value)",
        "begin_line": 241,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.set#244",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.set(self, name, value, priority='project')",
        "snippet": "    def set(self, name, value, priority='project'):\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: string\n\n        :param value: the value to associate with the setting\n        :type value: any\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)",
        "begin_line": 244,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.setdict#272",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.setdict(self, values, priority='project')",
        "snippet": "    def setdict(self, values, priority='project'):\n        self.update(values, priority)",
        "begin_line": 272,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020012007204322593,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.setmodule#275",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.setmodule(self, module, priority='project')",
        "snippet": "    def setmodule(self, module, priority='project'):\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: module object or string\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, six.string_types):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)",
        "begin_line": 275,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.update#297",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.update(self, values, priority='project')",
        "snippet": "    def update(self, values, priority='project'):\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, six.string_types):\n            values = json.loads(values)\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in six.iteritems(values):\n                    self.set(name, value, values.getpriority(name))\n            else:\n                for name, value in six.iteritems(values):\n                    self.set(name, value, priority)",
        "begin_line": 297,
        "end_line": 328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.delete#330",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.delete(self, name, priority='project')",
        "snippet": "    def delete(self, name, priority='project'):\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= self.getpriority(name):\n            del self.attributes[name]",
        "begin_line": 330,
        "end_line": 334,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__delitem__#336",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__delitem__(self, name)",
        "snippet": "    def __delitem__(self, name):\n        self._assert_mutability()\n        del self.attributes[name]",
        "begin_line": 336,
        "end_line": 338,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings._assert_mutability#340",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings._assert_mutability(self)",
        "snippet": "    def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")",
        "begin_line": 340,
        "end_line": 342,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.copy#344",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)",
        "begin_line": 344,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019968051118210862,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.freeze#356",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.freeze(self)",
        "snippet": "    def freeze(self):\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True",
        "begin_line": 356,
        "end_line": 364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020012007204322593,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.frozencopy#366",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.frozencopy(self)",
        "snippet": "    def frozencopy(self):\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy",
        "begin_line": 366,
        "end_line": 374,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020132876988121604,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__iter__#376",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self.attributes)",
        "begin_line": 376,
        "end_line": 377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019519812609798947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__len__#379",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self.attributes)",
        "begin_line": 379,
        "end_line": 380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings._to_dict#382",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings._to_dict(self)",
        "snippet": "    def _to_dict(self):\n        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n                for k, v in six.iteritems(self)}",
        "begin_line": 382,
        "end_line": 384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.copy_to_dict#386",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.copy_to_dict(self)",
        "snippet": "    def copy_to_dict(self):\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()",
        "begin_line": 386,
        "end_line": 400,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.overrides#409",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.overrides(self)",
        "snippet": "    def overrides(self):\n        warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='cmdline')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._overrides\n        except AttributeError:\n            self._overrides = o = _DictProxy(self, 'cmdline')\n        return o",
        "begin_line": 409,
        "end_line": 418,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.defaults#421",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.defaults(self)",
        "snippet": "    def defaults(self):\n        warnings.warn(\"`Settings.defaults` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='default')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._defaults\n        except AttributeError:\n            self._defaults = o = _DictProxy(self, 'default')\n        return o",
        "begin_line": 421,
        "end_line": 430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__init__#435",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__init__(self, settings, priority)",
        "snippet": "    def __init__(self, settings, priority):\n        self.o = {}\n        self.settings = settings\n        self.priority = priority",
        "begin_line": 435,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__getitem__#443",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__getitem__(self, k)",
        "snippet": "    def __getitem__(self, k):\n        return self.o[k]",
        "begin_line": 443,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__setitem__#446",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__setitem__(self, k, v)",
        "snippet": "    def __setitem__(self, k, v):\n        self.settings.set(k, v, priority=self.priority)\n        self.o[k] = v",
        "begin_line": 446,
        "end_line": 448,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__init__#468",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        # Do not pass kwarg values here. We don't want to promote user-defined\n        # dicts, and we want to update, not replace, default dicts with the\n        # values given by the user\n        super(Settings, self).__init__()\n        self.setmodule(default_settings, 'default')\n        # Promote default dictionaries to BaseSettings instances for per-key\n        # priorities\n        for name, val in six.iteritems(self):\n            if isinstance(val, dict):\n                self.set(name, BaseSettings(val, 'default'), 'default')\n        self.update(values, priority)",
        "begin_line": 468,
        "end_line": 479,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019557989438685703,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__init__#484",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__init__(self, settings_module=None, **kw)",
        "snippet": "    def __init__(self, settings_module=None, **kw):\n        self.settings_module = settings_module\n        Settings.__init__(self, **kw)",
        "begin_line": 484,
        "end_line": 486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__getitem__#488",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        if opt_name in self.overrides:\n            return self.overrides[opt_name]\n        if self.settings_module and hasattr(self.settings_module, opt_name):\n            return getattr(self.settings_module, opt_name)\n        if opt_name in self.defaults:\n            return self.defaults[opt_name]\n        return Settings.__getitem__(self, opt_name)",
        "begin_line": 488,
        "end_line": 495,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.iter_default_settings#505",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.iter_default_settings()",
        "snippet": "def iter_default_settings():\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)",
        "begin_line": 505,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.settings.__init__.overridden_settings#512",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.overridden_settings(settings)",
        "snippet": "def overridden_settings(settings):\n    \"\"\"Return a dict of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value",
        "begin_line": 512,
        "end_line": 517,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.gz.read1#23",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.read1(gzf, size=-1)",
        "snippet": "    def read1(gzf, size=-1):\n        return gzf.read1(size)",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.gz.gunzip#27",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gunzip(data)",
        "snippet": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = read1(f, 8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or getattr(f, 'extrabuf', None):\n                try:\n                    output += f.extrabuf[-f.extrasize:]\n                finally:\n                    break\n            else:\n                raise\n    return output",
        "begin_line": 27,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.gz.is_gzipped#56",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.is_gzipped(response)",
        "snippet": "def is_gzipped(response):\n    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n    ctype = response.headers.get('Content-Type', b'')\n    cenc = response.headers.get('Content-Encoding', b'').lower()\n    return (_is_gzipped(ctype) or\n            (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))",
        "begin_line": 56,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.__init__#17",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.__init__(self, *middlewares)",
        "snippet": "    def __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self.methods = defaultdict(list)\n        for mw in middlewares:\n            self._add_middleware(mw)",
        "begin_line": 17,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002008838891120932,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_settings#28",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_settings(cls, settings, crawler=None)",
        "snippet": "    def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler and hasattr(mwcls, 'from_crawler'):\n                    mw = mwcls.from_crawler(crawler)\n                elif hasattr(mwcls, 'from_settings'):\n                    mw = mwcls.from_settings(settings)\n                else:\n                    mw = mwcls()\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': pprint.pformat(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)",
        "begin_line": 28,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_crawler#57",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings, crawler)",
        "begin_line": 57,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._add_middleware#60",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].insert(0, mw.close_spider)",
        "begin_line": 60,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._process_parallel#66",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._process_parallel(self, methodname, obj, *args)",
        "snippet": "    def _process_parallel(self, methodname, obj, *args):\n        return process_parallel(self.methods[methodname], obj, *args)",
        "begin_line": 66,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00026737967914438503,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._process_chain#69",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._process_chain(self, methodname, obj, *args)",
        "snippet": "    def _process_chain(self, methodname, obj, *args):\n        return process_chain(self.methods[methodname], obj, *args)",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.open_spider#76",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        return self._process_parallel('open_spider', spider)",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00026737967914438503,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.close_spider#79",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        return self._process_parallel('close_spider', spider)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00026737967914438503,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.ossignal.install_shutdown_handlers#15",
        "src_path": "scrapy/utils/ossignal.py",
        "class_name": "scrapy.utils.ossignal",
        "signature": "scrapy.utils.ossignal.install_shutdown_handlers(function, override_sigint=True)",
        "snippet": "def install_shutdown_handlers(function, override_sigint=True):\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n    SIGINT handler won't be install if there is already a handler in place\n    (e.g.  Pdb)\n    \"\"\"\n    reactor._handleSignals()\n    signal.signal(signal.SIGTERM, function)\n    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or \\\n            override_sigint:\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, \"SIGBREAK\"):\n        signal.signal(signal.SIGBREAK, function)",
        "begin_line": 15,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__init__#16",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__init__(self, xmltext)",
        "snippet": "    def __init__(self, xmltext):\n        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__iter__#22",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__iter__(self)",
        "snippet": "    def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d",
        "begin_line": 22,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.sitemap.sitemap_urls_from_robots#39",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap",
        "signature": "scrapy.utils.sitemap.sitemap_urls_from_robots(robots_text, base_url=None)",
        "snippet": "def sitemap_urls_from_robots(robots_text, base_url=None):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith('sitemap:'):\n            url = line.split(':', 1)[1].strip()\n            yield urljoin(base_url, url)",
        "begin_line": 39,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient._parsed_url_args#14",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient",
        "signature": "scrapy.core.downloader.webclient._parsed_url_args(parsed)",
        "snippet": "def _parsed_url_args(parsed):\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    b = lambda s: to_bytes(s, encoding='ascii')\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    path = b(path)\n    host = b(parsed.hostname)\n    port = parsed.port\n    scheme = b(parsed.scheme)\n    netloc = b(parsed.netloc)\n    if port is None:\n        port = 443 if scheme == b'https' else 80\n    return scheme, netloc, host, port, path",
        "begin_line": 14,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient._parse#29",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient",
        "signature": "scrapy.core.downloader.webclient._parse(url)",
        "snippet": "def _parse(url):\n    \"\"\" Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)",
        "begin_line": 29,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionMade#44",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionMade(self)",
        "snippet": "    def connectionMade(self):\n        self.headers = Headers() # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.lineReceived#58",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.lineReceived(self, line)",
        "snippet": "    def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleHeader#61",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleHeader(self, key, value)",
        "snippet": "    def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleStatus#64",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleStatus(self, version, status, message)",
        "snippet": "    def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)",
        "begin_line": 64,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleEndHeaders#67",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleEndHeaders(self)",
        "snippet": "    def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)",
        "begin_line": 67,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionLost#70",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionLost(self, reason)",
        "snippet": "    def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000231000231000231,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleResponse#75",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleResponse(self, response)",
        "snippet": "    def handleResponse(self, response):\n        if self.factory.method.upper() == b'HEAD':\n            self.factory.page(b'')\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()",
        "begin_line": 75,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.timeout#84",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.timeout(self)",
        "snippet": "    def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b'https'):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\\\n                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % \\\n                (self.factory.url, self.factory.timeout)))",
        "begin_line": 84,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.__init__#108",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.__init__(self, request, timeout=180)",
        "snippet": "    def __init__(self, request, timeout=180):\n        self._url = urldefrag(request.url)[0]\n        # converting to bytes to comply to Twisted interface\n        self.url = to_bytes(self._url, encoding='ascii')\n        self.method = to_bytes(request.method, encoding='ascii')\n        self.body = request.body or None\n        self.headers = Headers(request.headers)\n        self.response_headers = None\n        self.timeout = request.meta.get('download_timeout') or timeout\n        self.start_time = time()\n        self.deferred = defer.Deferred().addCallback(self._build_response, request)\n\n        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n        # to have _disconnectedDeferred. See Twisted r32329.\n        # As Scrapy implements it's own logic to handle redirects is not\n        # needed to add the callback _waitForDisconnect.\n        # Specifically this avoids the AttributeError exception when\n        # clientConnectionFailed method is called.\n        self._disconnectedDeferred = defer.Deferred()\n\n        self._set_connection_attributes(request)\n\n        # set Host header based on url\n        self.headers.setdefault('Host', self.netloc)\n\n        # set Content-Length based len of body\n        if self.body is not None:\n            self.headers['Content-Length'] = len(self.body)\n            # just in case a broken http/1.1 decides to keep connection alive\n            self.headers.setdefault(\"Connection\", \"close\")\n        # Content-Length must be specified in POST method even with no body\n        elif self.method == b'POST':\n            self.headers['Content-Length'] = 0",
        "begin_line": 108,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._build_response#142",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._build_response(self, body, request)",
        "snippet": "    def _build_response(self, body, request):\n        request.meta['download_latency'] = self.headers_time-self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url)\n        return respcls(url=self._url, status=status, headers=headers, body=body)",
        "begin_line": 142,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._set_connection_attributes#149",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._set_connection_attributes(self, request)",
        "snippet": "    def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n        proxy = request.meta.get('proxy')\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url",
        "begin_line": 149,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.gotHeaders#157",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.gotHeaders(self, headers)",
        "snippet": "    def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers",
        "begin_line": 157,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023430178069353328,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__init__#14",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.default_loader_context = default_loader_context",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__call__#18",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        values = arg_to_iter(value)\n        if loader_context:\n            context = MergeDict(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            next_values = []\n            for v in values:\n                next_values += arg_to_iter(func(v))\n            values = next_values\n        return values",
        "begin_line": 18,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__init__#35",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.stop_on_none = default_loader_context.get('stop_on_none', True)\n        self.default_loader_context = default_loader_context",
        "begin_line": 35,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__call__#40",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        if loader_context:\n            context = MergeDict(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            if value is None and self.stop_on_none:\n                break\n            value = func(value)\n        return value",
        "begin_line": 40,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.TakeFirst.__call__#55",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.TakeFirst",
        "signature": "scrapy.loader.processors.TakeFirst.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        for value in values:\n            if value is not None and value != '':\n                return value",
        "begin_line": 55,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.Identity.__call__#63",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Identity",
        "signature": "scrapy.loader.processors.Identity.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return values",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022197558268590456,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__init__#74",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__init__(self, json_path)",
        "snippet": "    def __init__(self, json_path):\n        self.json_path = json_path\n        import jmespath\n        self.compiled_path = jmespath.compile(self.json_path)",
        "begin_line": 74,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__call__#79",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__call__(self, value)",
        "snippet": "    def __call__(self, value):\n        \"\"\"Query value for the jmespath query and return answer\n        :param value: a data structure (dict, list) to extract from\n        :return: Element extracted according to jmespath query\n        \"\"\"\n        return self.compiled_path.search(value)",
        "begin_line": 79,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__init__#89",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__init__(self, separator=u' ')",
        "snippet": "    def __init__(self, separator=u' '):\n        self.separator = separator",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__call__#92",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return self.separator.join(values)",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.__init__#10",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.from_crawler#14",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o",
        "begin_line": 14,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_opened#23",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_closed#26",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        self.stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n        self.stats.set_value('finish_reason', reason, spider=spider)",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.item_scraped#30",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        self.stats.inc_value('item_scraped_count', spider=spider)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.response_received#33",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.response_received(self, spider)",
        "snippet": "    def response_received(self, spider):\n        self.stats.inc_value('response_received_count', spider=spider)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.__init__#14",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.__init__(self, contracts)",
        "snippet": "    def __init__(self, contracts):\n        for contract in contracts:\n            self.contracts[contract.name] = contract",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.extract_contracts#27",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.extract_contracts(self, method)",
        "snippet": "    def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts",
        "begin_line": 27,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_method#48",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_method(self, method, results)",
        "snippet": "    def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            # calculate request args\n            args, kwargs = get_spec(Request.__init__)\n            kwargs['callback'] = method\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            # create and prepare request\n            args.remove('self')\n            if set(args).issubset(set(kwargs)):\n                request = Request(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request",
        "begin_line": 48,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager._clean_req#71",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager._clean_req(self, request, method, results)",
        "snippet": "    def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper",
        "begin_line": 71,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.cb_wrapper#77",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.cb_wrapper(response)",
        "snippet": "        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.eb_wrapper#85",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.eb_wrapper(failure)",
        "snippet": "        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)",
        "begin_line": 85,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.__init__#97",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.__init__(self, method, *args)",
        "snippet": "    def __init__(self, method, *args):\n        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n        self.args = args",
        "begin_line": 97,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_pre_hook#102",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_pre_hook(self, request, results)",
        "snippet": "    def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response)))\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 102,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_post_hook#125",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_post_hook(self, request, results)",
        "snippet": "    def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 125,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.wrapper#130",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.wrapper(response)",
        "snippet": "            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output",
        "begin_line": 130,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.adjust_request_args#149",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        return args",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__",
        "signature": "scrapy.contracts.__init__._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase.__str__#157",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase.__str__(_self)",
        "snippet": "        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.__init__#33",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self._pool = HTTPConnectionPool(reactor, persistent=True)\n        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n        self._pool._factory.noisy = False\n\n        self._sslMethod = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n        # try method-aware context factory\n        try:\n            self._contextFactory = self._contextFactoryClass(method=self._sslMethod)\n        except TypeError:\n            # use context factory defaults\n            self._contextFactory = self._contextFactoryClass()\n            msg = \"\"\"\n '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n e.g. OpenSSL.SSL.SSLv23_METHOD).\\\n Please upgrade your context factory class to handle it or ignore it.\"\"\" % (\n                settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n            warnings.warn(msg)\n        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n        self._disconnect_timeout = 1",
        "begin_line": 33,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.download_request#56",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,\n            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize))\n        return agent.download_request(request)",
        "begin_line": 56,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.close#63",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.close(self)",
        "snippet": "    def close(self):\n        d = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d",
        "begin_line": 63,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.cancel_delayed_call#75",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.cancel_delayed_call(result)",
        "snippet": "        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result",
        "begin_line": 75,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.__init__#99",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.__init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None)",
        "snippet": "    def __init__(self, reactor, host, port, proxyConf, contextFactory,\n                 timeout=30, bindAddress=None):\n        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost,\n            proxyPort, timeout, bindAddress)\n        self._tunnelReadyDeferred = defer.Deferred()\n        self._tunneledHost = host\n        self._tunneledPort = port\n        self._contextFactory = contextFactory",
        "begin_line": 99,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.requestTunnel#109",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.requestTunnel(self, protocol)",
        "snippet": "    def requestTunnel(self, protocol):\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort,\n                                        self._proxyAuthHeader)\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse\n        self._protocol = protocol\n        return protocol",
        "begin_line": 109,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.processProxyResponse#119",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.processProxyResponse(self, rcvd_bytes)",
        "snippet": "    def processProxyResponse(self, rcvd_bytes):\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        self._protocol.dataReceived = self._protocolDataReceived\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(rcvd_bytes)\n        if respm and int(respm.group('status')) == 200:\n            try:\n                # this sets proper Server Name Indication extension\n                # but is only available for Twisted>=14.0\n                sslOptions = self._contextFactory.creatorForNetloc(\n                    self._tunneledHost, self._tunneledPort)\n            except AttributeError:\n                # fall back to non-SNI SSL context factory\n                sslOptions = self._contextFactory\n            self._protocol.transport.startTLS(sslOptions,\n                                              self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            if respm:\n                extra = {'status': int(respm.group('status')),\n                         'reason': respm.group('reason').strip()}\n            else:\n                extra = rcvd_bytes[:32]\n            self._tunnelReadyDeferred.errback(\n                TunnelError('Could not open CONNECT tunnel with proxy %s:%s [%r]' % (\n                    self._host, self._port, extra)))",
        "begin_line": 119,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.connect#152",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.connect(self, protocolFactory)",
        "snippet": "    def connect(self, protocolFactory):\n        self._protocolFactory = protocolFactory\n        connectDeferred = super(TunnelingTCP4ClientEndpoint,\n                                self).connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred",
        "begin_line": 152,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.tunnel_request_data#161",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11",
        "signature": "scrapy.core.downloader.handlers.http11.tunnel_request_data(host, port, proxy_auth_header=None)",
        "snippet": "def tunnel_request_data(host, port, proxy_auth_header=None):\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_native_str as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))\n    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\\r\\n'\n    tunnel_req += b'Host: ' + host_value + b'\\r\\n'\n    if proxy_auth_header:\n        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\\r\\n'\n    tunnel_req += b'\\r\\n'\n    return tunnel_req",
        "begin_line": 161,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent.__init__#190",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent.__init__(self, reactor, proxyConf, contextFactory=None, connectTimeout=None, bindAddress=None, pool=None)",
        "snippet": "    def __init__(self, reactor, proxyConf, contextFactory=None,\n                 connectTimeout=None, bindAddress=None, pool=None):\n        super(TunnelingAgent, self).__init__(reactor, contextFactory,\n            connectTimeout, bindAddress, pool)\n        self._proxyConf = proxyConf\n        self._contextFactory = contextFactory",
        "begin_line": 190,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent._getEndpoint#198",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent._getEndpoint(self, uri)",
        "snippet": "        def _getEndpoint(self, uri):\n            return TunnelingTCP4ClientEndpoint(\n                self._reactor, uri.host, uri.port, self._proxyConf,\n                self._contextFactory, self._endpointFactory._connectTimeout,\n                self._endpointFactory._bindAddress)",
        "begin_line": 198,
        "end_line": 202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent._requestWithEndpoint#210",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent._requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath)",
        "snippet": "    def _requestWithEndpoint(self, key, endpoint, method, parsedURI,\n            headers, bodyProducer, requestPath):\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key = key + self._proxyConf\n        return super(TunnelingAgent, self)._requestWithEndpoint(key, endpoint, method, parsedURI,\n            headers, bodyProducer, requestPath)",
        "begin_line": 210,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent.__init__#226",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent.__init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None, maxsize=0, warnsize=0)",
        "snippet": "    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n                 maxsize=0, warnsize=0):\n        self._contextFactory = contextFactory\n        self._connectTimeout = connectTimeout\n        self._bindAddress = bindAddress\n        self._pool = pool\n        self._maxsize = maxsize\n        self._warnsize = warnsize\n        self._txresponse = None",
        "begin_line": 226,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._get_agent#236",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._get_agent(self, request, timeout)",
        "snippet": "    def _get_agent(self, request, timeout):\n        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost = to_unicode(proxyHost)\n            omitConnectTunnel = b'noconnect' in proxyParams\n            if  scheme == b'https' and not omitConnectTunnel:\n                proxyConf = (proxyHost, proxyPort,\n                             request.headers.get(b'Proxy-Authorization', None))\n                return self._TunnelingAgent(reactor, proxyConf,\n                    contextFactory=self._contextFactory, connectTimeout=timeout,\n                    bindAddress=bindaddress, pool=self._pool)\n            else:\n                endpoint = TCP4ClientEndpoint(reactor, proxyHost, proxyPort,\n                    timeout=timeout, bindAddress=bindaddress)\n                return self._ProxyAgent(endpoint)\n\n        return self._Agent(reactor, contextFactory=self._contextFactory,\n            connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)",
        "begin_line": 236,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent.download_request#258",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent.download_request(self, request)",
        "snippet": "    def download_request(self, request):\n        timeout = request.meta.get('download_timeout') or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b'Proxy-Authorization')\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        else:\n            bodyproducer = None\n            # Setting Content-Length: 0 even for POST requests is not a\n            # MUST per HTTP RFCs, but it's common behavior, and some\n            # servers require this, otherwise returning HTTP 411 Length required\n            #\n            # RFC 7230#section-3.3.2:\n            # \"a Content-Length header field is normally sent in a POST\n            # request even when the value is 0 (indicating an empty payload body).\"\n            #\n            # Twisted Agent will not add \"Content-Length: 0\" by itself\n            if method == b'POST':\n                headers.addRawHeader(b'Content-Length', b'0')\n\n        start_time = time()\n        d = agent.request(\n            method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d.addCallback(self._cb_bodyready, request)\n        d.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, url, timeout)\n        return d",
        "begin_line": 258,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_timeout#297",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_timeout(self, result, request, url, timeout)",
        "snippet": "    def _cb_timeout(self, result, request, url, timeout):\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))",
        "begin_line": 297,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_latency#308",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_latency(self, result, request, start_time)",
        "snippet": "    def _cb_latency(self, result, request, start_time):\n        request.meta['download_latency'] = time() - start_time\n        return result",
        "begin_line": 308,
        "end_line": 310,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022266755733689602,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodyready#312",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodyready(self, txresponse, request)",
        "snippet": "    def _cb_bodyready(self, txresponse, request):\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return txresponse, b'', None\n\n        maxsize = request.meta.get('download_maxsize', self._maxsize)\n        warnsize = request.meta.get('download_warnsize', self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n\n        if maxsize and expected_size > maxsize:\n            error_msg = (\"Cancelling download of %(url)s: expected response \"\n                         \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n            error_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n\n            logger.error(error_msg, error_args)\n            txresponse._transport._producer.loseConnection()\n            raise defer.CancelledError(error_msg % error_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\"Expected response size (%(size)s) larger than \"\n                           \"download warn size (%(warnsize)s).\",\n                           {'size': expected_size, 'warnsize': warnsize})\n\n        def _cancel(_):\n            txresponse._transport._producer.loseConnection()\n\n        d = defer.Deferred(_cancel)\n        txresponse.deliverBody(_ResponseReader(d, txresponse, request, maxsize, warnsize))\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d",
        "begin_line": 312,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cancel#335",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cancel(_)",
        "snippet": "        def _cancel(_):\n            txresponse._transport._producer.loseConnection()",
        "begin_line": 335,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodydone#346",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodydone(self, result, request, url)",
        "snippet": "    def _cb_bodydone(self, result, request, url):\n        txresponse, body, flags = result\n        status = int(txresponse.code)\n        headers = Headers(txresponse.headers.getAllRawHeaders())\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)",
        "begin_line": 346,
        "end_line": 351,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022598870056497175,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.__init__#357",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.__init__(self, body)",
        "snippet": "    def __init__(self, body):\n        self.body = body\n        self.length = len(body)",
        "begin_line": 357,
        "end_line": 359,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.startProducing#361",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.startProducing(self, consumer)",
        "snippet": "    def startProducing(self, consumer):\n        consumer.write(self.body)\n        return defer.succeed(None)",
        "begin_line": 361,
        "end_line": 363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.pauseProducing#365",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.pauseProducing(self)",
        "snippet": "    def pauseProducing(self):\n        pass",
        "begin_line": 365,
        "end_line": 366,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.__init__#374",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.__init__(self, finished, txresponse, request, maxsize, warnsize)",
        "snippet": "    def __init__(self, finished, txresponse, request, maxsize, warnsize):\n        self._finished = finished\n        self._txresponse = txresponse\n        self._request = request\n        self._bodybuf = BytesIO()\n        self._maxsize  = maxsize\n        self._warnsize  = warnsize\n        self._reached_warnsize = False\n        self._bytes_received = 0",
        "begin_line": 374,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000231000231000231,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.dataReceived#384",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.dataReceived(self, bodyBytes)",
        "snippet": "    def dataReceived(self, bodyBytes):\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.error(\"Received (%(bytes)s) bytes larger than download \"\n                         \"max size (%(maxsize)s).\",\n                         {'bytes': self._bytes_received,\n                          'maxsize': self._maxsize})\n            self._finished.cancel()\n\n        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n            self._reached_warnsize = True\n            logger.warning(\"Received more bytes than download \"\n                           \"warn size (%(warnsize)s) in request %(request)s.\",\n                           {'warnsize': self._warnsize,\n                            'request': self._request})",
        "begin_line": 384,
        "end_line": 400,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000231000231000231,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.connectionLost#402",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.connectionLost(self, reason)",
        "snippet": "    def connectionLost(self, reason):\n        if self._finished.called:\n            return\n\n        body = self._bodybuf.getvalue()\n        if reason.check(ResponseDone):\n            self._finished.callback((self._txresponse, body, None))\n        elif reason.check(PotentialDataLoss):\n            self._finished.callback((self._txresponse, body, ['partial']))\n        else:\n            self._finished.errback(reason)",
        "begin_line": 402,
        "end_line": 412,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.console._embed_standard_shell#34",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console._embed_standard_shell(namespace={}, banner='')",
        "snippet": "def _embed_standard_shell(namespace={}, banner=''):\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n    try: # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter\n        readline.parse_and_bind(\"tab:complete\")\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)\n    return wrapper",
        "begin_line": 34,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.console.wrapper#45",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console.wrapper(namespace=namespace, banner='')",
        "snippet": "    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.console.get_shell_embed_func#55",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console.get_shell_embed_func(shells=None, known_shells=None)",
        "snippet": "def get_shell_embed_func(shells=None, known_shells=None):\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None: # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None: # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue",
        "begin_line": 55,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.open#18",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.open(self)",
        "snippet": "    def open(self):  # can return deferred\n        pass",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.__init__#31",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.__init__(self, path=None, debug=False)",
        "snippet": "    def __init__(self, path=None, debug=False):\n        self.file = None\n        self.fingerprints = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n            self.fingerprints.update(x.rstrip() for x in self.file)",
        "begin_line": 31,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.from_settings#43",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(job_dir(settings), debug)",
        "begin_line": 43,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_seen#47",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_seen(self, request)",
        "snippet": "    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)",
        "begin_line": 47,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint#55",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint(self, request)",
        "snippet": "    def request_fingerprint(self, request):\n        return request_fingerprint(request)",
        "begin_line": 55,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.close#58",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.file:\n            self.file.close()",
        "begin_line": 58,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.log#62",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.log(self, request, spider)",
        "snippet": "    def log(self, request, spider):\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s\"\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request: %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False\n\n        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)",
        "begin_line": 62,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__#17",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__(self, maxlength)",
        "snippet": "    def __init__(self, maxlength):\n        self.maxlength = maxlength",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.from_settings#21",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        maxlength = settings.getint('URLLENGTH_LIMIT')\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output#27",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter#28",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.template.render_templatefile#8",
        "src_path": "scrapy/utils/template.py",
        "class_name": "scrapy.utils.template",
        "signature": "scrapy.utils.template.render_templatefile(path, **kwargs)",
        "snippet": "def render_templatefile(path, **kwargs):\n    with open(path, 'rb') as fp:\n        raw = fp.read().decode('utf8')\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n    with open(render_path, 'wb') as fp:\n        fp.write(content.encode('utf8'))\n    if path.endswith('.tmpl'):\n        os.remove(path)",
        "begin_line": 8,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__init__#10",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__init__(self, seq=None, encoding='utf-8')",
        "snippet": "    def __init__(self, seq=None, encoding='utf-8'):\n        self.encoding = encoding\n        super(Headers, self).__init__(seq)",
        "begin_line": 10,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0078125,
            "pseudo_dstar_susp": 0.0078125,
            "pseudo_tarantula_susp": 0.0078125,
            "pseudo_op2_susp": 0.0078125,
            "pseudo_barinel_susp": 0.0078125
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normkey#14",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019297568506368199,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normvalue#18",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (six.text_type, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]",
        "begin_line": 18,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers._tobytes#29",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers._tobytes(self, x)",
        "snippet": "    def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, six.text_type):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return six.text_type(x).encode(self.encoding)\n        else:\n            raise TypeError('Unsupported value type: {}'.format(type(x)))",
        "begin_line": 29,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__getitem__#39",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        try:\n            return super(Headers, self).__getitem__(key)[-1]\n        except IndexError:\n            return None",
        "begin_line": 39,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.get#45",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        try:\n            return super(Headers, self).get(key, def_val)[-1]\n        except IndexError:\n            return None",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021003990758244065,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.getlist#51",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.getlist(self, key, def_val=None)",
        "snippet": "    def getlist(self, key, def_val=None):\n        try:\n            return super(Headers, self).__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []",
        "begin_line": 51,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlist#59",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlist(self, key, list_)",
        "snippet": "    def setlist(self, key, list_):\n        self[key] = list_",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlistdefault#62",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlistdefault(self, key, default_list=())",
        "snippet": "    def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.appendlist#65",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.appendlist(self, key, value)",
        "snippet": "    def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst",
        "begin_line": 65,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022306491188935982,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.items#70",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.items(self)",
        "snippet": "    def items(self):\n        return list(self.iteritems())",
        "begin_line": 70,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019936204146730463,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.iteritems#73",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.iteritems(self)",
        "snippet": "    def iteritems(self):\n        return ((k, self.getlist(k)) for k in self.keys())",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019936204146730463,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.values#76",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.values(self)",
        "snippet": "    def values(self):\n        return [self[k] for k in self.keys()]",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_string#79",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_string(self)",
        "snippet": "    def to_string(self):\n        return headers_dict_to_raw(self)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_unicode_dict#82",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_unicode_dict(self)",
        "snippet": "    def to_unicode_dict(self):\n        \"\"\" Return headers as a CaselessDict with unicode keys\n        and unicode values. Multiple values are joined with ','.\n        \"\"\"\n        return CaselessDict(\n            (to_unicode(key, encoding=self.encoding),\n             to_unicode(b','.join(value), encoding=self.encoding))\n            for key, value in self.items())",
        "begin_line": 82,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__copy__#91",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 91,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.__init__#12",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.__init__(self, headers)",
        "snippet": "    def __init__(self, headers):\n        self._headers = headers",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.from_crawler#16",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n        return cls(headers.items())",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.process_request#20",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)",
        "begin_line": 20,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf.build_component_list#14",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.build_component_list(compdict, custom=None, convert=update_classpath)",
        "snippet": "def build_component_list(compdict, custom=None, convert=update_classpath):\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))\n\n    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in six.iteritems(compdict):\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError('Some paths in {!r} convert to the same '\n                                     'object, please update your settings'\n                                     ''.format(list(compdict.keys())))\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in six.iteritems(compdict)}\n\n    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in six.iteritems(compdict):\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                 'a real number or None instead'.format(value, name))\n\n    # BEGIN Backwards compatibility for old (base, custom) call signature\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    if custom is not None:\n        compdict.update(custom)\n    # END Backwards compatibility\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]",
        "begin_line": 14,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf._check_components#17",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._check_components(complist)",
        "snippet": "    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf._map_keys#22",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._map_keys(compdict)",
        "snippet": "    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in six.iteritems(compdict):\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError('Some paths in {!r} convert to the same '\n                                     'object, please update your settings'\n                                     ''.format(list(compdict.keys())))\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in six.iteritems(compdict)}",
        "begin_line": 22,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf._validate_values#38",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._validate_values(compdict)",
        "snippet": "    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in six.iteritems(compdict):\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                 'a real number or None instead'.format(value, name))",
        "begin_line": 38,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf.arglist_to_dict#59",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.arglist_to_dict(arglist)",
        "snippet": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)",
        "begin_line": 59,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf.closest_scrapy_cfg#66",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.closest_scrapy_cfg(path='.', prevpath=None)",
        "snippet": "def closest_scrapy_cfg(path='.', prevpath=None):\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if path == prevpath:\n        return ''\n    path = os.path.abspath(path)\n    cfgfile = os.path.join(path, 'scrapy.cfg')\n    if os.path.exists(cfgfile):\n        return cfgfile\n    return closest_scrapy_cfg(os.path.dirname(path), path)",
        "begin_line": 66,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf.get_config#94",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.get_config(use_closest=True)",
        "snippet": "def get_config(use_closest=True):\n    \"\"\"Get Scrapy config file as a SafeConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = SafeConfigParser()\n    cfg.read(sources)\n    return cfg",
        "begin_line": 94,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.conf.get_sources#102",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.get_sources(use_closest=True)",
        "snippet": "def get_sources(use_closest=True):\n    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\n        os.path.expanduser('~/.config')\n    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n               xdg_config_home + '/scrapy.cfg',\n               os.path.expanduser('~/.scrapy.cfg')]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources",
        "begin_line": 102,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.__init__#10",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])",
        "begin_line": 10,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.download_request#14",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler._connect#20",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler._connect(self, factory)",
        "snippet": "    def _connect(self, factory):\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b'https':\n            return reactor.connectSSL(host, port, factory,\n                                      self.ClientContextFactory())\n        else:\n            return reactor.connectTCP(host, port, factory)",
        "begin_line": 20,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.__init__#15",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.__init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None, pqclass=None)",
        "snippet": "    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n                 logunser=False, stats=None, pqclass=None):\n        self.df = dupefilter\n        self.dqdir = self._dqdir(jobdir)\n        self.pqclass = pqclass\n        self.dqclass = dqclass\n        self.mqclass = mqclass\n        self.logunser = logunser\n        self.stats = stats",
        "begin_line": 15,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.from_crawler#26",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n        dupefilter = dupefilter_cls.from_settings(settings)\n        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS', settings.getbool('SCHEDULER_DEBUG'))\n        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)",
        "begin_line": 26,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.has_pending_requests#37",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.has_pending_requests(self)",
        "snippet": "    def has_pending_requests(self):\n        return len(self) > 0",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.open#40",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.open(self, spider)",
        "snippet": "    def open(self, spider):\n        self.spider = spider\n        self.mqs = self.pqclass(self._newmq)\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()",
        "begin_line": 40,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.close#46",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.dqs:\n            prios = self.dqs.close()\n            with open(join(self.dqdir, 'active.json'), 'w') as f:\n                json.dump(prios, f)\n        return self.df.close(reason)",
        "begin_line": 46,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.enqueue_request#53",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.enqueue_request(self, request)",
        "snippet": "    def enqueue_request(self, request):\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        if dqok:\n            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n        return True",
        "begin_line": 53,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.next_request#66",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.next_request(self)",
        "snippet": "    def next_request(self):\n        request = self.mqs.pop()\n        if request:\n            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request:\n                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n        if request:\n            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n        return request",
        "begin_line": 66,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.__len__#78",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqpush#81",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqpush(self, request)",
        "snippet": "    def _dqpush(self, request):\n        if self.dqs is None:\n            return\n        try:\n            reqd = request_to_dict(request, self.spider)\n            self.dqs.push(reqd, -request.priority)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\"Unable to serialize request: %(request)s - reason:\"\n                       \" %(reason)s - no more unserializable requests will be\"\n                       \" logged (stats being collected)\")\n                logger.warning(msg, {'request': request, 'reason': e},\n                               exc_info=True, extra={'spider': self.spider})\n                self.logunser = False\n            self.stats.inc_value('scheduler/unserializable',\n                                 spider=self.spider)\n            return\n        else:\n            return True",
        "begin_line": 81,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._mqpush#101",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._mqpush(self, request)",
        "snippet": "    def _mqpush(self, request):\n        self.mqs.push(request, -request.priority)",
        "begin_line": 101,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqpop#104",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqpop(self)",
        "snippet": "    def _dqpop(self):\n        if self.dqs:\n            d = self.dqs.pop()\n            if d:\n                return request_from_dict(d, self.spider)",
        "begin_line": 104,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._newmq#110",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._newmq(self, priority)",
        "snippet": "    def _newmq(self, priority):\n        return self.mqclass()",
        "begin_line": 110,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqdir#129",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqdir(self, jobdir)",
        "snippet": "    def _dqdir(self, jobdir):\n        if jobdir:\n            dqdir = join(jobdir, 'requests.queue')\n            if not exists(dqdir):\n                os.makedirs(dqdir)\n            return dqdir",
        "begin_line": 129,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.file.FileDownloadHandler.__init__#7",
        "src_path": "scrapy/core/downloader/handlers/file.py",
        "class_name": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.file.FileDownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        pass",
        "begin_line": 7,
        "end_line": 8,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.file.FileDownloadHandler.download_request#11",
        "src_path": "scrapy/core/downloader/handlers/file.py",
        "class_name": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.file.FileDownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        filepath = file_uri_to_path(request.url)\n        with open(filepath, 'rb') as fo:\n            body = fo.read()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)",
        "begin_line": 11,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__init__#190",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__init__(self, seq=None)",
        "snippet": "    def __init__(self, seq=None):\n        super(CaselessDict, self).__init__()\n        if seq:\n            self.update(seq)",
        "begin_line": 190,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007633587786259542,
            "pseudo_dstar_susp": 0.007633587786259542,
            "pseudo_tarantula_susp": 0.007633587786259542,
            "pseudo_op2_susp": 0.007633587786259542,
            "pseudo_barinel_susp": 0.007633587786259542
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__getitem__#195",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))",
        "begin_line": 195,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0001972386587771203,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__setitem__#198",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))",
        "begin_line": 198,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002113718030014796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__delitem__#201",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))",
        "begin_line": 201,
        "end_line": 202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__contains__#204",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))",
        "begin_line": 204,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020885547201336674,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__copy__#208",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 208,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normkey#212",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()",
        "begin_line": 212,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normvalue#216",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value",
        "begin_line": 216,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.get#220",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 220,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019627085377821394,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.setdefault#223",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.setdefault(self, key, def_val=None)",
        "snippet": "    def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 223,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000209819555182543,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.update#226",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.update(self, seq)",
        "snippet": "    def update(self, seq):\n        seq = seq.items() if isinstance(seq, dict) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super(CaselessDict, self).update(iseq)",
        "begin_line": 226,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0001967342120794806,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.fromkeys#232",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.fromkeys(cls, keys, value=None)",
        "snippet": "    def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)",
        "begin_line": 232,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.pop#235",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)",
        "begin_line": 235,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022527596305474206,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.__init__#247",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.__init__(self, *dicts)",
        "snippet": "    def __init__(self, *dicts):\n        self.dicts = dicts",
        "begin_line": 247,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023282887077997672,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.__getitem__#250",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        for dict_ in self.dicts:\n            try:\n                return dict_[key]\n            except KeyError:\n                pass\n        raise KeyError",
        "begin_line": 250,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.has_key#279",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.has_key(self, key)",
        "snippet": "    def has_key(self, key):\n        for dict_ in self.dicts:\n            if key in dict_:\n                return True\n        return False",
        "begin_line": 279,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.SequenceExclude.__init__#312",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.SequenceExclude",
        "signature": "scrapy.utils.datatypes.SequenceExclude.__init__(self, seq)",
        "snippet": "    def __init__(self, seq):\n        self.seq = seq",
        "begin_line": 312,
        "end_line": 313,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.datatypes.SequenceExclude.__contains__#315",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.SequenceExclude",
        "signature": "scrapy.utils.datatypes.SequenceExclude.__contains__(self, item)",
        "snippet": "    def __contains__(self, item):\n        return item not in self.seq",
        "begin_line": 315,
        "end_line": 316,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__#28",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__(self)",
        "snippet": "    def __init__(self):\n        self._formats = {\n            'tar': self._is_tar,\n            'zip': self._is_zip,\n            'gz': self._is_gzip,\n            'bz2': self._is_bzip2\n        }",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar#36",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar(self, response)",
        "snippet": "    def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip#47",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip(self, response)",
        "snippet": "    def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip#59",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip(self, response)",
        "snippet": "    def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 59,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2#69",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2(self, response)",
        "snippet": "    def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 69,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response#78",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in six.iteritems(self._formats):\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response",
        "begin_line": 78,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.__init__#20",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]",
        "begin_line": 20,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.should_cache_request#24",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.should_cache_request(self, request)",
        "snippet": "    def should_cache_request(self, request):\n        return urlparse_cached(request).scheme not in self.ignore_schemes",
        "begin_line": 24,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.should_cache_response#27",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.should_cache_response(self, response, request)",
        "snippet": "    def should_cache_response(self, response, request):\n        return response.status not in self.ignore_http_codes",
        "begin_line": 27,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_fresh#30",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_fresh(self, response, request)",
        "snippet": "    def is_cached_response_fresh(self, response, request):\n        return True",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_valid#33",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_valid(self, cachedresponse, response, request)",
        "snippet": "    def is_cached_response_valid(self, cachedresponse, response, request):\n        return True",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.__init__#41",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_response_cache_controls = [to_bytes(cc) for cc in\n            settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]\n        self._cc_parsed = WeakKeyDictionary()",
        "begin_line": 41,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._parse_cachecontrol#48",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._parse_cachecontrol(self, r)",
        "snippet": "    def _parse_cachecontrol(self, r):\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b'Cache-Control', b'')\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]",
        "begin_line": 48,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_request#58",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_request(self, request)",
        "snippet": "    def should_cache_request(self, request):\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Any other is eligible for caching\n        return True",
        "begin_line": 58,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_response#68",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_response(self, response, request)",
        "snippet": "    def should_cache_response(self, response, request):\n        # What is cacheable - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        elif response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        elif self.always_store:\n            return True\n        # Any hint on response expiration is good\n        elif b'max-age' in cc or b'Expires' in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        elif response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        elif response.status in (200, 203, 401):\n            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        else:\n            return False",
        "begin_line": 68,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_fresh#97",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_fresh(self, cachedresponse, request)",
        "snippet": "    def is_cached_response_fresh(self, cachedresponse, request):\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b'no-cache' in cc or b'no-cache' in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b'max-stale']\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False",
        "begin_line": 97,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_valid#136",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_valid(self, cachedresponse, response, request)",
        "snippet": "    def is_cached_response_valid(self, cachedresponse, response, request):\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b'must-revalidate' not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304",
        "begin_line": 136,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._set_conditional_validators#147",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._set_conditional_validators(self, request, cachedresponse)",
        "snippet": "    def _set_conditional_validators(self, request, cachedresponse):\n        if b'Last-Modified' in cachedresponse.headers:\n            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n\n        if b'ETag' in cachedresponse.headers:\n            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']",
        "begin_line": 147,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._get_max_age#154",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._get_max_age(self, cc)",
        "snippet": "    def _get_max_age(self, cc):\n        try:\n            return max(0, int(cc[b'max-age']))\n        except (KeyError, ValueError):\n            return None",
        "begin_line": 154,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._compute_freshness_lifetime#160",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._compute_freshness_lifetime(self, response, request, now)",
        "snippet": "    def _compute_freshness_lifetime(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#410\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n\n        # Try HTTP/1.0 Expires header\n        if b'Expires' in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute fresshness lifetime\n        return 0",
        "begin_line": 160,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._compute_current_age#191",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._compute_current_age(self, response, request, now)",
        "snippet": "    def _compute_current_age(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#366\n        currentage = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n        if now > date:\n            currentage = now - date\n\n        if b'Age' in response.headers:\n            try:\n                age = int(response.headers[b'Age'])\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage",
        "begin_line": 191,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.__init__#213",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n        self.db = None",
        "begin_line": 213,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.open_spider#219",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n        self.db = self.dbmodule.open(dbpath, 'c')",
        "begin_line": 219,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.close_spider#223",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        self.db.close()",
        "begin_line": 223,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.retrieve_response#226",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 226,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.store_response#238",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)\n        self.db['%s_time' % key] = str(time())",
        "begin_line": 238,
        "end_line": 247,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage._read_data#249",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage._read_data(self, spider, request)",
        "snippet": "    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        db = self.db\n        tkey = '%s_time' % key\n        if tkey not in db:\n            return  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        return pickle.loads(db['%s_data' % key])",
        "begin_line": 249,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage._request_key#262",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage._request_key(self, request)",
        "snippet": "    def _request_key(self, request):\n        return request_fingerprint(request)",
        "begin_line": 262,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.__init__#268",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n        self._open = gzip.open if self.use_gzip else open",
        "begin_line": 268,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.open_spider#274",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        pass",
        "begin_line": 274,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.close_spider#277",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        pass",
        "begin_line": 277,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.retrieve_response#280",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return  # not cached\n        rpath = self._get_request_path(spider, request)\n        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n            body = f.read()\n        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n            rawheaders = f.read()\n        url = metadata.get('response_url')\n        status = metadata['status']\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 280,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.store_response#297",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n            pickle.dump(metadata, f, protocol=2)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n            f.write(request.body)",
        "begin_line": 297,
        "end_line": 320,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage._get_request_path#322",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage._get_request_path(self, spider, request)",
        "snippet": "    def _get_request_path(self, spider, request):\n        key = request_fingerprint(request)\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)",
        "begin_line": 322,
        "end_line": 324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage._read_meta#326",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage._read_meta(self, spider, request)",
        "snippet": "    def _read_meta(self, spider, request):\n        rpath = self._get_request_path(spider, request)\n        metapath = os.path.join(rpath, 'pickled_meta')\n        if not os.path.exists(metapath):\n            return  # not found\n        mtime = os.stat(metapath).st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return  # expired\n        with self._open(metapath, 'rb') as f:\n            return pickle.load(f)",
        "begin_line": 326,
        "end_line": 335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.__init__#340",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        import leveldb\n        self._leveldb = leveldb\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.db = None",
        "begin_line": 340,
        "end_line": 345,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.open_spider#347",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n        self.db = self._leveldb.LevelDB(dbpath)",
        "begin_line": 347,
        "end_line": 349,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.close_spider#351",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        # Do compactation each time to save space and also recreate files to\n        # avoid them being removed in storages with timestamp-based autoremoval.\n        self.db.CompactRange()\n        del self.db",
        "begin_line": 351,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.retrieve_response#357",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 357,
        "end_line": 367,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.store_response#369",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        batch = self._leveldb.WriteBatch()\n        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))\n        batch.Put(key + b'_time', to_bytes(str(time())))\n        self.db.Write(batch)",
        "begin_line": 369,
        "end_line": 380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage._read_data#382",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage._read_data(self, spider, request)",
        "snippet": "    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        try:\n            ts = self.db.Get(key + b'_time')\n        except KeyError:\n            return  # not found or invalid entry\n\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        try:\n            data = self.db.Get(key + b'_data')\n        except KeyError:\n            return  # invalid entry\n        else:\n            return pickle.loads(data)",
        "begin_line": 382,
        "end_line": 397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage._request_key#399",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage._request_key(self, request)",
        "snippet": "    def _request_key(self, request):\n        return to_bytes(request_fingerprint(request))",
        "begin_line": 399,
        "end_line": 400,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.parse_cachecontrol#404",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache",
        "signature": "scrapy.extensions.httpcache.parse_cachecontrol(header)",
        "snippet": "def parse_cachecontrol(header):\n    \"\"\"Parse Cache-Control header\n\n    http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b','):\n        key, sep, val = directive.strip().partition(b'=')\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives",
        "begin_line": 404,
        "end_line": 421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.httpcache.rfc1123_to_epoch#424",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache",
        "signature": "scrapy.extensions.httpcache.rfc1123_to_epoch(date_str)",
        "snippet": "def rfc1123_to_epoch(date_str):\n    try:\n        date_str = to_unicode(date_str, encoding='ascii')\n        return mktime_tz(parsedate_tz(date_str))\n    except Exception:\n        return None",
        "begin_line": 424,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.__init__#16",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.__init__(self, maxdepth, stats=None, verbose_stats=False, prio=1)",
        "snippet": "    def __init__(self, maxdepth, stats=None, verbose_stats=False, prio=1):\n        self.maxdepth = maxdepth\n        self.stats = stats\n        self.verbose_stats = verbose_stats\n        self.prio = prio",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.from_crawler#23",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        maxdepth = settings.getint('DEPTH_LIMIT')\n        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n        prio = settings.getint('DEPTH_PRIORITY')\n        return cls(maxdepth, crawler.stats, verbose, prio)",
        "begin_line": 23,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.process_spider_output#30",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                elif self.stats:\n                    if self.verbose_stats:\n                        self.stats.inc_value('request_depth_count/%s' % depth,\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True\n\n        # base case (depth=0)\n        if self.stats and 'depth' not in response.meta:\n            response.meta['depth'] = 0\n            if self.verbose_stats:\n                self.stats.inc_value('request_depth_count/0', spider=spider)\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 30,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware._filter#31",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                elif self.stats:\n                    if self.verbose_stats:\n                        self.stats.inc_value('request_depth_count/%s' % depth,\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True",
        "begin_line": 31,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler#21",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n            raise NotConfigured\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 21,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.__init__(self, auth_encoding='latin-1')",
        "snippet": "    def __init__(self, auth_encoding='latin-1'):\n        self.auth_encoding = auth_encoding\n        self.proxies = {}\n        for type, url in getproxies().items():\n            self.proxies[type] = self._get_proxy(url, type)",
        "begin_line": 17,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.from_crawler#24",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n            raise NotConfigured\n        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n        return cls(auth_encoding)",
        "begin_line": 24,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._basic_auth_header#30",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._basic_auth_header(self, username, password)",
        "snippet": "    def _basic_auth_header(self, username, password):\n        user_pass = to_bytes(\n            '%s:%s' % (unquote(username), unquote(password)),\n            encoding=self.auth_encoding)\n        return base64.b64encode(user_pass).strip()",
        "begin_line": 30,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._get_proxy#36",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._get_proxy(self, url, orig_type)",
        "snippet": "    def _get_proxy(self, url, orig_type):\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.process_request#47",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        # ignore if proxy is already set\n        if 'proxy' in request.meta:\n            if request.meta['proxy'] is None:\n                return\n            # extract credentials if present\n            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n            request.meta['proxy'] = proxy_url\n            if creds and not request.headers.get('Proxy-Authorization'):\n                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n            return\n        elif not self.proxies:\n            return\n\n        parsed = urlparse_cached(request)\n        scheme = parsed.scheme\n\n        # 'no_proxy' is only supported by http schemes\n        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n            return\n\n        if scheme in self.proxies:\n            self._set_proxy(request, scheme)",
        "begin_line": 47,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._set_proxy#71",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._set_proxy(self, request, scheme)",
        "snippet": "    def _set_proxy(self, request, scheme):\n        creds, proxy = self.proxies[scheme]\n        request.meta['proxy'] = proxy\n        if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds",
        "begin_line": 71,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.__init__#19",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.from_crawler#23",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 23,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.process_spider_output#28",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\"Filtered offsite request to %(domain)r: %(request)s\",\n                                     {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x",
        "begin_line": 28,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.should_follow#44",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.should_follow(self, request, spider)",
        "snippet": "    def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.get_host_regex#50",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.get_host_regex(self, spider)",
        "snippet": "    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('') # allow all by default\n        regex = r'^(.*\\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains if d is not None)\n        return re.compile(regex)",
        "begin_line": 50,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.spider_opened#58",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()",
        "begin_line": 58,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n\n        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')",
        "begin_line": 17,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002244668911335578,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.from_crawler#25",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 25,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002244668911335578,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect#28",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect(self, redirected, request, spider, reason)",
        "snippet": "    def _redirect(self, redirected, request, spider, reason):\n        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n        redirects = request.meta.get('redirect_times', 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta['redirect_times'] = redirects\n            redirected.meta['redirect_ttl'] = ttl - 1\n            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \\\n                [request.url]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                         {'reason': reason, 'redirected': redirected, 'request': request},\n                         extra={'spider': spider})\n            return redirected\n        else:\n            logger.debug(\"Discarding %(request)s: max redirections reached\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest(\"max redirections reached\")",
        "begin_line": 28,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect_request_using_get#48",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect_request_using_get(self, request, redirect_url)",
        "snippet": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected",
        "begin_line": 48,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware.process_response#60",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if (request.meta.get('dont_redirect', False) or\n                response.status in getattr(spider, 'handle_httpstatus_list', []) or\n                response.status in request.meta.get('handle_httpstatus_list', []) or\n                request.meta.get('handle_httpstatus_all', False)):\n            return response\n\n        allowed_status = (301, 302, 303, 307)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['location'])\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)",
        "begin_line": 60,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.__init__#87",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        super(MetaRefreshMiddleware, self).__init__(settings)\n        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',\n                                         settings.getint('METAREFRESH_MAXDELAY'))",
        "begin_line": 87,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.process_response#92",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \\\n                not isinstance(response, HtmlResponse):\n            return response\n\n        interval, url = get_meta_refresh(response)\n        if url and interval < self._maxdelay:\n            redirected = self._redirect_request_using_get(request, url)\n            return self._redirect(redirected, request, spider, 'meta refresh')\n\n        return response",
        "begin_line": 92,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__new__#55",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__new__(metacls, name, bases, clsdict_)",
        "snippet": "        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__init__#61",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__init__(cls, name, bases, clsdict_)",
        "snippet": "        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__#77",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__(cls, inst)",
        "snippet": "        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__#81",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__(cls, sub)",
        "snippet": "        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)",
        "begin_line": 81,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__call__#95",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__call__(cls, *args, **kwargs)",
        "snippet": "        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate._clspath#120",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate._clspath(cls, forced=None)",
        "snippet": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "begin_line": 120,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.deprecate.update_classpath#150",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.update_classpath(path)",
        "snippet": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path",
        "begin_line": 150,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reactor.listen_tcp#3",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor",
        "signature": "scrapy.utils.reactor.listen_tcp(portrange, host, factory)",
        "snippet": "def listen_tcp(portrange, host, factory):\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n    if not hasattr(portrange, '__iter__'):\n        return reactor.listenTCP(portrange, factory, interface=host)\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1]+1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise",
        "begin_line": 3,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.__init__#25",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.__init__(self, func, *a, **kw)",
        "snippet": "    def __init__(self, func, *a, **kw):\n        self._func = func\n        self._a = a\n        self._kw = kw\n        self._call = None",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.schedule#31",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.schedule(self, delay=0)",
        "snippet": "    def schedule(self, delay=0):\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.cancel#35",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.cancel(self)",
        "snippet": "    def cancel(self):\n        if self._call:\n            self._call.cancel()",
        "begin_line": 35,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.__call__#39",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.__call__(self)",
        "snippet": "    def __call__(self):\n        self._call = None\n        return self._func(*self._a, **self._kw)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.__init__#12",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.__init__(self, timeout=180)",
        "snippet": "    def __init__(self, timeout=180):\n        self._timeout = timeout",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.from_crawler#16",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 16,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.spider_opened#21",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self._timeout = getattr(spider, 'download_timeout', self._timeout)",
        "begin_line": 21,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000270929287455974,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.process_request#24",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if self._timeout:\n            request.meta.setdefault('download_timeout', self._timeout)",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00026737967914438503,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_mimetype#43",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_mimetype(self, mimetype)",
        "snippet": "    def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = \"%s/*\" % mimetype.split('/')[0]\n            return self.classes.get(basetype, Response)",
        "begin_line": 43,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_type#53",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_type(self, content_type, content_encoding=None)",
        "snippet": "    def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)",
        "begin_line": 53,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_disposition#61",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_disposition(self, content_disposition)",
        "snippet": "    def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n            filename = filename.strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response",
        "begin_line": 61,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_headers#70",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_headers(self, headers)",
        "snippet": "    def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls",
        "begin_line": 70,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_filename#83",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_filename(self, filename)",
        "snippet": "    def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response",
        "begin_line": 83,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_body#91",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_body(self, body)",
        "snippet": "    def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')",
        "begin_line": 91,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_args#107",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_args(self, headers=None, url=None, filename=None, body=None)",
        "snippet": "    def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls",
        "begin_line": 107,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.defer_fail#10",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_fail(_failure)",
        "snippet": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d",
        "begin_line": 10,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.defer_succeed#21",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_succeed(result)",
        "snippet": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d",
        "begin_line": 21,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.defer_result#32",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_result(result)",
        "snippet": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)",
        "begin_line": 32,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.mustbe_deferred#40",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.mustbe_deferred(f, *args, **kw)",
        "snippet": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)",
        "begin_line": 40,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002464268112370626,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.parallel#56",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.parallel(iterable, count, callable, *args, **named)",
        "snippet": "def parallel(iterable, count, callable, *args, **named):\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: http://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = task.Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])",
        "begin_line": 56,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain#66",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain(callbacks, input, *a, **kw)",
        "snippet": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d",
        "begin_line": 66,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain_both#74",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain_both(callbacks, errbacks, input, *a, **kw)",
        "snippet": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d",
        "begin_line": 74,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.process_parallel#86",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_parallel(callbacks, input, *a, **kw)",
        "snippet": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d",
        "begin_line": 86,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.defer.iter_errback#95",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.iter_errback(iterable, errback, *a, **kw)",
        "snippet": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except:\n            errback(failure.Failure(), *a, **kw)",
        "begin_line": 95,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reqser.request_to_dict#11",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_to_dict(request, spider=None)",
        "snippet": "def request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n    }\n    if type(request) is not Request:\n        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n    return d",
        "begin_line": 11,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reqser.request_from_dict#41",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_from_dict(d, spider=None)",
        "snippet": "def request_from_dict(d, spider=None):\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    cb = d['callback']\n    if cb and spider:\n        cb = _get_method(spider, cb)\n    eb = d['errback']\n    if eb and spider:\n        eb = _get_method(spider, eb)\n    request_cls = load_object(d['_class']) if '_class' in d else Request\n    return request_cls(\n        url=to_native_str(d['url']),\n        callback=cb,\n        errback=eb,\n        method=d['method'],\n        headers=d['headers'],\n        body=d['body'],\n        cookies=d['cookies'],\n        meta=d['meta'],\n        encoding=d['_encoding'],\n        priority=d['priority'],\n        dont_filter=d['dont_filter'])",
        "begin_line": 41,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reqser._find_method#68",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._find_method(obj, func)",
        "snippet": "def _find_method(obj, func):\n    if obj:\n        try:\n            func_self = six.get_method_self(func)\n        except AttributeError:  # func has no __self__\n            pass\n        else:\n            if func_self is obj:\n                return six.get_method_function(func).__name__\n    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))",
        "begin_line": 68,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.reqser._get_method#80",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._get_method(obj, name)",
        "snippet": "def _get_method(obj, name):\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(\"Method %r not found in: %s\" % (name, obj))",
        "begin_line": 80,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.arg_to_iter#17",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.arg_to_iter(arg)",
        "snippet": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]",
        "begin_line": 17,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.load_object#31",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.load_object(path)",
        "snippet": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    object can be a class, function, variable or an instance.\n    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n    \"\"\"\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n\n    module, name = path[:dot], path[dot+1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n\n    return obj",
        "begin_line": 31,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.walk_modules#54",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.walk_modules(path)",
        "snippet": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods",
        "begin_line": 54,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.extract_regex#76",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.extract_regex(regex, text, encoding='utf-8')",
        "snippet": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n\n    if isinstance(regex, six.string_types):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, six.text_type):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]",
        "begin_line": 76,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.md5sum#100",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.md5sum(file)",
        "snippet": "def md5sum(file):\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()",
        "begin_line": 100,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.misc.rel_has_nofollow#116",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.rel_has_nofollow(rel)",
        "snippet": "def rel_has_nofollow(rel):\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return True if rel is not None and 'nofollow' in rel.split() else False",
        "begin_line": 116,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.link.Link.__init__#18",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__init__(self, url, text='', fragment='', nofollow=False)",
        "snippet": "    def __init__(self, url, text='', fragment='', nofollow=False):\n        if not isinstance(url, str):\n            if six.PY2:\n                warnings.warn(\"Link urls must be str objects. \"\n                              \"Assuming utf-8 encoding (which could be wrong)\")\n                url = to_bytes(url, encoding='utf8')\n            else:\n                got = url.__class__.__name__\n                raise TypeError(\"Link urls must be str objects, got %s\" % got)\n        self.url = url\n        self.text = text\n        self.fragment = fragment\n        self.nofollow = nofollow",
        "begin_line": 18,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.link.Link.__eq__#32",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self.url == other.url and self.text == other.text and \\\n            self.fragment == other.fragment and self.nofollow == other.nofollow",
        "begin_line": 32,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.link.Link.__hash__#36",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)",
        "begin_line": 36,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.link.Link.__repr__#39",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n            (self.url, self.text, self.fragment, self.nofollow)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor.__init__#51",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor.__init__(self, link_extractor, allow, deny, allow_domains, deny_domains, restrict_xpaths, canonicalize, deny_extensions, restrict_css)",
        "snippet": "    def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                 restrict_xpaths, canonicalize, deny_extensions, restrict_css):\n\n        self.link_extractor = link_extractor\n\n        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)\n                          for x in arg_to_iter(allow)]\n        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x)\n                         for x in arg_to_iter(deny)]\n\n        self.allow_domains = set(arg_to_iter(allow_domains))\n        self.deny_domains = set(arg_to_iter(deny_domains))\n\n        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n                                          arg_to_iter(restrict_css)))\n\n        self.canonicalize = canonicalize\n        if deny_extensions is None:\n            deny_extensions = IGNORED_EXTENSIONS\n        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}",
        "begin_line": 51,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._link_allowed#73",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._link_allowed(self, link)",
        "snippet": "    def _link_allowed(self, link):\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n            return False\n        return True",
        "begin_line": 73,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor.matches#89",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor.matches(self, url)",
        "snippet": "    def matches(self, url):\n\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)",
        "begin_line": 89,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._process_links#100",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._process_links(self, links)",
        "snippet": "    def _process_links(self, links):\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(urlparse(link.url))\n        links = self.link_extractor._process_links(links)\n        return links",
        "begin_line": 100,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._extract_links#108",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._extract_links(self, *args, **kwargs)",
        "snippet": "    def _extract_links(self, *args, **kwargs):\n        return self.link_extractor._extract_links(*args, **kwargs)",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.__init__#10",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.__init__(self, policy=None, check_expired_frequency=10000)",
        "snippet": "    def __init__(self, policy=None, check_expired_frequency=10000):\n        self.policy = policy or DefaultCookiePolicy()\n        self.jar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()\n        self.check_expired_frequency = check_expired_frequency\n        self.processed = 0",
        "begin_line": 10,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002523977788995457,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.extract_cookies#17",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.extract_cookies(self, response, request)",
        "snippet": "    def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.add_cookie_header#22",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.add_cookie_header(self, request)",
        "snippet": "    def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()",
        "begin_line": 22,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.make_cookies#73",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.make_cookies(self, response, request)",
        "snippet": "    def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002523977788995457,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.set_cookie_if_ok#81",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.set_cookie_if_ok(self, cookie, request)",
        "snippet": "    def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.potential_domain_matches#85",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies",
        "signature": "scrapy.http.cookies.potential_domain_matches(domain)",
        "snippet": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]",
        "begin_line": 85,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.acquire#105",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.acquire(self)",
        "snippet": "    def acquire(self):\n        pass",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.release#108",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.release(self)",
        "snippet": "    def release(self):\n        pass",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.__init__#118",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.__init__(self, request)",
        "snippet": "    def __init__(self, request):\n        self.request = request",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022878059940517045,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_full_url#121",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_full_url(self)",
        "snippet": "    def get_full_url(self):\n        return self.request.url",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_host#124",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_host(self)",
        "snippet": "    def get_host(self):\n        return urlparse_cached(self.request).netloc",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_type#127",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_type(self)",
        "snippet": "    def get_type(self):\n        return urlparse_cached(self.request).scheme",
        "begin_line": 127,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.is_unverifiable#130",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.is_unverifiable(self)",
        "snippet": "    def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)",
        "begin_line": 130,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_origin_req_host#140",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_origin_req_host(self)",
        "snippet": "    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname",
        "begin_line": 140,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.full_url#145",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.full_url(self)",
        "snippet": "    def full_url(self):\n        return self.get_full_url()",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.host#149",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.host(self)",
        "snippet": "    def host(self):\n        return self.get_host()",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.type#153",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.type(self)",
        "snippet": "    def type(self):\n        return self.get_type()",
        "begin_line": 153,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.unverifiable#157",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.unverifiable(self)",
        "snippet": "    def unverifiable(self):\n        return self.is_unverifiable()",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.origin_req_host#161",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.origin_req_host(self)",
        "snippet": "    def origin_req_host(self):\n        return self.get_origin_req_host()",
        "begin_line": 161,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.has_header#164",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.has_header(self, name)",
        "snippet": "    def has_header(self, name):\n        return name in self.request.headers",
        "begin_line": 164,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_header#167",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_header(self, name, default=None)",
        "snippet": "    def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')",
        "begin_line": 167,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.header_items#171",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.header_items(self)",
        "snippet": "    def header_items(self):\n        return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n            for k, v in self.request.headers.items()\n        ]",
        "begin_line": 171,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.add_unredirected_header#178",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.add_unredirected_header(self, name, value)",
        "snippet": "    def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)",
        "begin_line": 178,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.__init__#184",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.__init__(self, response)",
        "snippet": "    def __init__(self, response):\n        self.response = response",
        "begin_line": 184,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002464268112370626,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.info#187",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.info(self)",
        "snippet": "    def info(self):\n        return self",
        "begin_line": 187,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002498126405196103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.get_all#191",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.get_all(self, name, default=None)",
        "snippet": "    def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]",
        "begin_line": 191,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__init__#17",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__init__(self, url, status=200, headers=None, body=b'', flags=None, request=None)",
        "snippet": "    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 17,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010638297872340425,
            "pseudo_dstar_susp": 0.010638297872340425,
            "pseudo_tarantula_susp": 0.010638297872340425,
            "pseudo_op2_susp": 0.010638297872340425,
            "pseudo_barinel_susp": 0.010638297872340425
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.meta#26",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.meta(self)",
        "snippet": "    def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )",
        "begin_line": 26,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_url#35",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015873015873015872,
            "pseudo_dstar_susp": 0.015873015873015872,
            "pseudo_tarantula_susp": 0.015873015873015872,
            "pseudo_op2_susp": 0.015873015873015872,
            "pseudo_barinel_susp": 0.015873015873015872
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_url#38",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n                type(url).__name__))",
        "begin_line": 38,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_body#47",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 47,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012195121951219513,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.012195121951219513,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.012195121951219513
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_body#50",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body",
        "begin_line": 50,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__str__#63",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%d %s>\" % (self.status, self.url)",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.copy#68",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()",
        "begin_line": 68,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.replace#72",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 72,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.urljoin#81",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)",
        "begin_line": 81,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.text#87",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.text(self)",
        "snippet": "    def text(self):\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as text (unicode object in Python 2 and str in Python 3)\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")",
        "begin_line": 87,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.css#93",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.css(self, *a, **kw)",
        "snippet": "    def css(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")",
        "begin_line": 93,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.xpath#99",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.xpath(self, *a, **kw)",
        "snippet": "    def xpath(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")",
        "begin_line": 99,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.__init__#12",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self._dump = crawler.settings.getbool('STATS_DUMP')\n        self._stats = {}",
        "begin_line": 12,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_value#16",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return self._stats.get(key, default)",
        "begin_line": 16,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_stats#19",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_stats(self, spider=None)",
        "snippet": "    def get_stats(self, spider=None):\n        return self._stats",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.set_value#22",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        self._stats[key] = value",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00025621316935690495,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.inc_value#28",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count",
        "begin_line": 28,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019782393669634025,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.max_value#32",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        self._stats[key] = max(self._stats.setdefault(key, value), value)",
        "begin_line": 32,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.min_value#35",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        self._stats[key] = min(self._stats.setdefault(key, value), value)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.open_spider#41",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        pass",
        "begin_line": 41,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021372088053002778,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.close_spider#44",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.close_spider(self, spider, reason)",
        "snippet": "    def close_spider(self, spider, reason):\n        if self._dump:\n            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                        extra={'spider': spider})\n        self._persist_stats(self._stats, spider)",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021372088053002778,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector._persist_stats#50",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        pass",
        "begin_line": 50,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector.__init__#56",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        super(MemoryStatsCollector, self).__init__(crawler)\n        self.spider_stats = {}",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector._persist_stats#60",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        self.spider_stats[spider.name] = stats",
        "begin_line": 60,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.get_value#66",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return default",
        "begin_line": 66,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.set_value#69",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        pass",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.inc_value#75",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        pass",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.max_value#78",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        pass",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.min_value#81",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        pass",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.__init__#24",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module('resource')\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler = crawler\n        self.warned = False\n        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n        self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n        self.mail = MailSender.from_settings(crawler.settings)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)",
        "begin_line": 24,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.from_crawler#45",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.job.job_dir#3",
        "src_path": "scrapy/utils/job.py",
        "class_name": "scrapy.utils.job",
        "signature": "scrapy.utils.job.job_dir(settings)",
        "snippet": "def job_dir(settings):\n    path = settings['JOBDIR']\n    if path and not os.path.exists(path):\n        os.makedirs(path)\n    return path",
        "begin_line": 3,
        "end_line": 7,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.flatten#15",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.flatten(x)",
        "snippet": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))",
        "begin_line": 15,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.iflatten#35",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.iflatten(x)",
        "snippet": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in iflatten(el):\n                yield el_\n        else:\n            yield el",
        "begin_line": 35,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.is_listlike#47",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.is_listlike(x)",
        "snippet": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(six.moves.xrange(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))",
        "begin_line": 47,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021253985122210415,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.unique#71",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.unique(list_, key=lambda x: x)",
        "snippet": "def unique(list_, key=lambda x: x):\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result",
        "begin_line": 71,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.to_unicode#97",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_unicode(text, encoding=None, errors='strict')",
        "snippet": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object `text`. If `text`\n    is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, six.text_type):\n        return text\n    if not isinstance(text, (bytes, six.text_type)):\n        raise TypeError('to_unicode must receive a bytes, str or unicode '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)",
        "begin_line": 97,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011627906976744186,
            "pseudo_dstar_susp": 0.011627906976744186,
            "pseudo_tarantula_susp": 0.011627906976744186,
            "pseudo_op2_susp": 0.011627906976744186,
            "pseudo_barinel_susp": 0.011627906976744186
        }
    },
    {
        "name": "scrapy.utils.python.to_bytes#110",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_bytes(text, encoding=None, errors='strict')",
        "snippet": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of `text`. If `text`\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, six.string_types):\n        raise TypeError('to_bytes must receive a unicode, str or bytes '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)",
        "begin_line": 110,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.to_native_str#123",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_native_str(text, encoding=None, errors='strict')",
        "snippet": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of `text`\n    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n    if six.PY2:\n        return to_bytes(text, encoding, errors)\n    else:\n        return to_unicode(text, encoding, errors)",
        "begin_line": 123,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011363636363636364,
            "pseudo_dstar_susp": 0.011363636363636364,
            "pseudo_tarantula_susp": 0.011363636363636364,
            "pseudo_op2_susp": 0.011363636363636364,
            "pseudo_barinel_susp": 0.011363636363636364
        }
    },
    {
        "name": "scrapy.utils.python.re_rsearch#132",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.re_rsearch(pattern, text, chunk_size=1024)",
        "snippet": "def re_rsearch(pattern, text, chunk_size=1024):\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, six.string_types):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = [match for match in pattern.finditer(chunk)]\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None",
        "begin_line": 132,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python._chunk_iter#145",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python._chunk_iter()",
        "snippet": "    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)",
        "begin_line": 145,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.memoizemethod_noargs#165",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.memoizemethod_noargs(method)",
        "snippet": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n    return new_method",
        "begin_line": 165,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.new_method#171",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.new_method(self, *args, **kwargs)",
        "snippet": "    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]",
        "begin_line": 171,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.binary_is_text#189",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.binary_is_text(data)",
        "snippet": "def binary_is_text(data):\n    \"\"\" Returns `True` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(\"data must be bytes, got '%s'\" % type(data).__name__)\n    return all(c not in _BINARYCHARS for c in data)",
        "begin_line": 189,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.get_func_args#198",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_func_args(func, stripself=False)",
        "snippet": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        func_args, _, _, _ = inspect.getargspec(func)\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n    if stripself:\n        func_args.pop(0)\n    return func_args",
        "begin_line": 198,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.get_spec#225",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_spec(func)",
        "snippet": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test(object):\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getargspec(func)\n    elif hasattr(func, '__call__'):\n        spec = inspect.getargspec(func.__call__)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs",
        "begin_line": 225,
        "end_line": 259,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.equal_attributes#262",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.equal_attributes(obj1, obj2, attributes)",
        "snippet": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True",
        "begin_line": 262,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__init__#282",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__init__(self, default_factory)",
        "snippet": "    def __init__(self, default_factory):\n        self.default_factory = default_factory\n        self._weakdict = weakref.WeakKeyDictionary()",
        "begin_line": 282,
        "end_line": 284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__getitem__#286",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]",
        "begin_line": 286,
        "end_line": 289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.retry_on_eintr#327",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.retry_on_eintr(function, *args, **kw)",
        "snippet": "def retry_on_eintr(function, *args, **kw):\n    \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n    while True:\n        try:\n            return function(*args, **kw)\n        except IOError as e:\n            if e.errno != errno.EINTR:\n                raise",
        "begin_line": 327,
        "end_line": 334,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.python.without_none_values#337",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.without_none_values(iterable)",
        "snippet": "def without_none_values(iterable):\n    \"\"\"Return a copy of `iterable` with all `None` entries removed.\n\n    If `iterable` is a mapping, return a dictionary where all pairs that have\n    value `None` have been removed.\n    \"\"\"\n    try:\n        return {k: v for k, v in six.iteritems(iterable) if v is not None}\n    except AttributeError:\n        return type(iterable)((v for v in iterable if v is not None))",
        "begin_line": 337,
        "end_line": 346,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.start_requests#8",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        self._postinit_reqs = super(InitSpider, self).start_requests()\n        return iterate_spider_output(self.init_request())",
        "begin_line": 8,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.initialized#12",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.initialized(self, response=None)",
        "snippet": "    def initialized(self, response=None):\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop('_postinit_reqs')",
        "begin_line": 12,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.init_request#18",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.init_request(self)",
        "snippet": "    def init_request(self):\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()",
        "begin_line": 18,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.__init__#22",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.__init__(self, concurrency, delay, randomize_delay)",
        "snippet": "    def __init__(self, concurrency, delay, randomize_delay):\n        self.concurrency = concurrency\n        self.delay = delay\n        self.randomize_delay = randomize_delay\n\n        self.active = set()\n        self.queue = deque()\n        self.transferring = set()\n        self.lastseen = 0\n        self.latercall = None",
        "begin_line": 22,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.free_transfer_slots#33",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.free_transfer_slots(self)",
        "snippet": "    def free_transfer_slots(self):\n        return self.concurrency - len(self.transferring)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.download_delay#36",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.download_delay(self)",
        "snippet": "    def download_delay(self):\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n        return self.delay",
        "begin_line": 36,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.close#41",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.close(self)",
        "snippet": "    def close(self):\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()",
        "begin_line": 41,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__._get_concurrency_delay#61",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__",
        "signature": "scrapy.core.downloader.__init__._get_concurrency_delay(concurrency, spider, settings)",
        "snippet": "def _get_concurrency_delay(concurrency, spider, settings):\n    delay = settings.getfloat('DOWNLOAD_DELAY')\n    if hasattr(spider, 'DOWNLOAD_DELAY'):\n        warnings.warn(\"%s.DOWNLOAD_DELAY attribute is deprecated, use %s.download_delay instead\" %\n                      (type(spider).__name__, type(spider).__name__))\n        delay = spider.DOWNLOAD_DELAY\n    if hasattr(spider, 'download_delay'):\n        delay = spider.download_delay\n\n    if hasattr(spider, 'max_concurrent_requests'):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay",
        "begin_line": 61,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.__init__#78",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.settings = crawler.settings\n        self.signals = crawler.signals\n        self.slots = {}\n        self.active = set()\n        self.handlers = DownloadHandlers(crawler)\n        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n        self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n        self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n        self._slot_gc_loop.start(60)",
        "begin_line": 78,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.fetch#92",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.fetch(self, request, spider)",
        "snippet": "    def fetch(self, request, spider):\n        def _deactivate(response):\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd = self.middleware.download(self._enqueue_request, request, spider)\n        return dfd.addBoth(_deactivate)",
        "begin_line": 92,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._deactivate#93",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._deactivate(response)",
        "snippet": "        def _deactivate(response):\n            self.active.remove(request)\n            return response",
        "begin_line": 93,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.needs_backout#101",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.needs_backout(self)",
        "snippet": "    def needs_backout(self):\n        return len(self.active) >= self.total_concurrency",
        "begin_line": 101,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._get_slot#104",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._get_slot(self, request, spider)",
        "snippet": "    def _get_slot(self, request, spider):\n        key = self._get_slot_key(request, spider)\n        if key not in self.slots:\n            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n\n        return key, self.slots[key]",
        "begin_line": 104,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._get_slot_key#113",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._get_slot_key(self, request, spider)",
        "snippet": "    def _get_slot_key(self, request, spider):\n        if 'download_slot' in request.meta:\n            return request.meta['download_slot']\n\n        key = urlparse_cached(request).hostname or ''\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key",
        "begin_line": 113,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._enqueue_request#123",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._enqueue_request(self, request, spider)",
        "snippet": "    def _enqueue_request(self, request, spider):\n        key, slot = self._get_slot(request, spider)\n        request.meta['download_slot'] = key\n\n        def _deactivate(response):\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        deferred = defer.Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred",
        "begin_line": 123,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._deactivate#127",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._deactivate(response)",
        "snippet": "        def _deactivate(response):\n            slot.active.remove(request)\n            return response",
        "begin_line": 127,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._process_queue#137",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._process_queue(self, spider, slot)",
        "snippet": "    def _process_queue(self, spider, slot):\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break",
        "begin_line": 137,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._download#161",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._download(self, slot, request, spider)",
        "snippet": "    def _download(self, slot, request, spider):\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives,  remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            return _\n\n        return dfd.addBoth(finish_transferring)",
        "begin_line": 161,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._downloaded#169",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._downloaded(response)",
        "snippet": "        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response",
        "begin_line": 169,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.finish_transferring#183",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.finish_transferring(_)",
        "snippet": "        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            return _",
        "begin_line": 183,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.close#190",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.close(self)",
        "snippet": "    def close(self):\n        self._slot_gc_loop.stop()\n        for slot in six.itervalues(self.slots):\n            slot.close()",
        "begin_line": 190,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._slot_gc#195",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._slot_gc(self, age=60)",
        "snippet": "    def _slot_gc(self, age=60):\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()",
        "begin_line": 195,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.__init__#27",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.__init__(self, item=None, selector=None, response=None, parent=None, **context)",
        "snippet": "    def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n        self.selector = selector\n        context.update(selector=selector, response=response)\n        if item is None:\n            item = self.default_item_class()\n        self.context = context\n        self.parent = parent\n        self._local_item = context['item'] = item\n        self._local_values = defaultdict(list)",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._values#40",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._values(self)",
        "snippet": "    def _values(self):\n        if self.parent is not None:\n            return self.parent._values\n        else:\n            return self._local_values",
        "begin_line": 40,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.item#47",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.item(self)",
        "snippet": "    def item(self):\n        if self.parent is not None:\n            return self.parent.item\n        else:\n            return self._local_item",
        "begin_line": 47,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.nested_xpath#53",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.nested_xpath(self, xpath, **context)",
        "snippet": "    def nested_xpath(self, xpath, **context):\n        selector = self.selector.xpath(xpath)\n        context.update(selector=selector)\n        subloader = self.__class__(\n            item=self.item, parent=self, **context\n        )\n        return subloader",
        "begin_line": 53,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.nested_css#61",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.nested_css(self, css, **context)",
        "snippet": "    def nested_css(self, css, **context):\n        selector = self.selector.css(css)\n        context.update(selector=selector)\n        subloader = self.__class__(\n            item=self.item, parent=self, **context\n        )\n        return subloader",
        "begin_line": 61,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_value#69",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def add_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._add_value(k, v)\n        else:\n            self._add_value(field_name, value)",
        "begin_line": 69,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_value#79",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def replace_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._replace_value(k, v)\n        else:\n            self._replace_value(field_name, value)",
        "begin_line": 79,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._add_value#89",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._add_value(self, field_name, value)",
        "snippet": "    def _add_value(self, field_name, value):\n        value = arg_to_iter(value)\n        processed_value = self._process_input_value(field_name, value)\n        if processed_value:\n            self._values[field_name] += arg_to_iter(processed_value)",
        "begin_line": 89,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022197558268590456,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._replace_value#95",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._replace_value(self, field_name, value)",
        "snippet": "    def _replace_value(self, field_name, value):\n        self._values.pop(field_name, None)\n        self._add_value(field_name, value)",
        "begin_line": 95,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_value#99",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_value(self, value, *processors, **kw)",
        "snippet": "    def get_value(self, value, *processors, **kw):\n        regex = kw.get('re', None)\n        if regex:\n            value = arg_to_iter(value)\n            value = flatten(extract_regex(regex, x) for x in value)\n\n        for proc in processors:\n            if value is None:\n                break\n            proc = wrap_loader_context(proc, self.context)\n            value = proc(value)\n        return value",
        "begin_line": 99,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.load_item#112",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.load_item(self)",
        "snippet": "    def load_item(self):\n        item = self.item\n        for field_name in tuple(self._values):\n            value = self.get_output_value(field_name)\n            if value is not None:\n                item[field_name] = value\n\n        return item",
        "begin_line": 112,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_value#121",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_value(self, field_name)",
        "snippet": "    def get_output_value(self, field_name):\n        proc = self.get_output_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        try:\n            return proc(self._values[field_name])\n        except Exception as e:\n            raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n                (field_name, self._values[field_name], type(e).__name__, str(e)))",
        "begin_line": 121,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_collected_values#130",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_collected_values(self, field_name)",
        "snippet": "    def get_collected_values(self, field_name):\n        return self._values[field_name]",
        "begin_line": 130,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_input_processor#133",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_input_processor(self, field_name)",
        "snippet": "    def get_input_processor(self, field_name):\n        proc = getattr(self, '%s_in' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'input_processor', \\\n                self.default_input_processor)\n        return proc",
        "begin_line": 133,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00025621316935690495,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_processor#140",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_processor(self, field_name)",
        "snippet": "    def get_output_processor(self, field_name):\n        proc = getattr(self, '%s_out' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'output_processor', \\\n                self.default_output_processor)\n        return proc",
        "begin_line": 140,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022598870056497175,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._process_input_value#147",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._process_input_value(self, field_name, value)",
        "snippet": "    def _process_input_value(self, field_name, value):\n        proc = self.get_input_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        return proc(value)",
        "begin_line": 147,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022197558268590456,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_item_field_attr#152",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_item_field_attr(self, field_name, key, default=None)",
        "snippet": "    def _get_item_field_attr(self, field_name, key, default=None):\n        if isinstance(self.item, Item):\n            value = self.item.fields[field_name].get(key, default)\n        else:\n            value = default\n        return value",
        "begin_line": 152,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._check_selector_method#159",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._check_selector_method(self)",
        "snippet": "    def _check_selector_method(self):\n        if self.selector is None:\n            raise RuntimeError(\"To use XPath or CSS selectors, \"\n                \"%s must be instantiated with a selector \"\n                \"or a response\" % self.__class__.__name__)",
        "begin_line": 159,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_xpath#165",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def add_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 165,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_xpath#169",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def replace_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 169,
        "end_line": 171,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_xpath#173",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_xpath(self, xpath, *processors, **kw)",
        "snippet": "    def get_xpath(self, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 173,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_xpathvalues#181",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_xpathvalues(self, xpaths, **kw)",
        "snippet": "    def _get_xpathvalues(self, xpaths, **kw):\n        self._check_selector_method()\n        xpaths = arg_to_iter(xpaths)\n        return flatten(self.selector.xpath(xpath).extract() for xpath in xpaths)",
        "begin_line": 181,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_css#186",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def add_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 186,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_css#190",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def replace_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 190,
        "end_line": 192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_css#194",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_css(self, css, *processors, **kw)",
        "snippet": "    def get_css(self, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 194,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_cssvalues#198",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_cssvalues(self, csss, **kw)",
        "snippet": "    def _get_cssvalues(self, csss, **kw):\n        self._check_selector_method()\n        csss = arg_to_iter(csss)\n        return flatten(self.selector.css(css).extract() for css in csss)",
        "begin_line": 198,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.setUp#10",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.setUp(self)",
        "snippet": "    def setUp(self):\n        super(SiteTest, self).setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port",
        "begin_line": 10,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.tearDown#15",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.tearDown(self)",
        "snippet": "    def tearDown(self):\n        super(SiteTest, self).tearDown()\n        self.site.stopListening()",
        "begin_line": 15,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.url#19",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.url(self, path)",
        "snippet": "    def url(self, path):\n        return urljoin(self.baseurl, path)",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002479543763947434,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testsite.NoMetaRefreshRedirect.render#24",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.NoMetaRefreshRedirect",
        "signature": "scrapy.utils.testsite.NoMetaRefreshRedirect.render(self, request)",
        "snippet": "    def render(self, request):\n        content = util.Redirect.render(self, request)\n        return content.replace(b'http-equiv=\\\"refresh\\\"',\n            b'http-no-equiv=\\\"do-not-refresh-me\\\"')",
        "begin_line": 24,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.testsite.test_site#30",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite",
        "signature": "scrapy.utils.testsite.test_site()",
        "snippet": "def test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)",
        "begin_line": 30,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.__init__#11",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):\n            raise NotConfigured\n\n        self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency = crawler.settings.getfloat(\"AUTOTHROTTLE_TARGET_CONCURRENCY\")\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)",
        "begin_line": 11,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.from_crawler#22",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.__init__#38",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool('RETRY_ENABLED'):\n            raise NotConfigured\n        self.max_retry_times = settings.getint('RETRY_TIMES')\n        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n        self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')",
        "begin_line": 38,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002464268112370626,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.from_crawler#46",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 46,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002464268112370626,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_response#49",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_retry', False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response",
        "begin_line": 49,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_exception#57",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n                and not request.meta.get('dont_retry', False):\n            return self._retry(request, exception, spider)",
        "begin_line": 57,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware._retry#62",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware._retry(self, request, reason, spider)",
        "snippet": "    def _retry(self, request, reason, spider):\n        retries = request.meta.get('retry_times', 0) + 1\n\n        if retries <= self.max_retry_times:\n            logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                         {'request': request, 'retries': retries, 'reason': reason},\n                         extra={'spider': spider})\n            retryreq = request.copy()\n            retryreq.meta['retry_times'] = retries\n            retryreq.dont_filter = True\n            retryreq.priority = request.priority + self.priority_adjust\n            return retryreq\n        else:\n            logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                         {'request': request, 'retries': retries, 'reason': reason},\n                         extra={'spider': spider})",
        "begin_line": 62,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.__init__#31",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.__init__(self, method=SSL.SSLv23_METHOD, *args, **kwargs)",
        "snippet": "        def __init__(self, method=SSL.SSLv23_METHOD, *args, **kwargs):\n            super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n            self._ssl_method = method",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021542438604049978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getCertificateOptions#35",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getCertificateOptions(self)",
        "snippet": "        def getCertificateOptions(self):\n            # setting verify=True will require you to provide CAs\n            # to verify against; in other words: it's not that simple\n\n            # backward-compatible SSL/TLS method:\n            #\n            # * this will respect `method` attribute in often recommended\n            #   `ScrapyClientContextFactory` subclass\n            #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n            #\n            # * getattr() for `_ssl_method` attribute for context factories\n            #   not calling super(..., self).__init__\n            return CertificateOptions(verify=False,\n                        method=getattr(self, 'method',\n                                       getattr(self, '_ssl_method', None)),\n                        fixBrokenPeers=True,\n                        acceptableCiphers=DEFAULT_CIPHERS)",
        "begin_line": 35,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getContext#55",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getContext(self, hostname=None, port=None)",
        "snippet": "        def getContext(self, hostname=None, port=None):\n            return self.getCertificateOptions().getContext()",
        "begin_line": 55,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.creatorForNetloc#58",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.creatorForNetloc(self, hostname, port)",
        "snippet": "        def creatorForNetloc(self, hostname, port):\n            return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.httpobj.urlparse_cached#8",
        "src_path": "scrapy/utils/httpobj.py",
        "class_name": "scrapy.utils.httpobj",
        "signature": "scrapy.utils.httpobj.urlparse_cached(request_or_response)",
        "snippet": "def urlparse_cached(request_or_response):\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]",
        "begin_line": 8,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021034917963819942,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.common.newsetter#2",
        "src_path": "scrapy/http/common.py",
        "class_name": "scrapy.http.common",
        "signature": "scrapy.http.common.newsetter(self, value)",
        "snippet": "    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n        raise AttributeError(msg)",
        "begin_line": 2,
        "end_line": 5,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.__init__#11",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.__init__(self, jobdir=None)",
        "snippet": "    def __init__(self, jobdir=None):\n        self.jobdir = jobdir",
        "begin_line": 11,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.from_crawler#15",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj",
        "begin_line": 15,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_closed#25",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=2)",
        "begin_line": 25,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_opened#30",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.statefn#38",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.statefn(self)",
        "snippet": "    def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.__init__#28",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        self._configure(kwargs)",
        "begin_line": 28,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter._configure#31",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter._configure(self, options, dont_fail=False)",
        "snippet": "    def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses constructors)\n        \"\"\"\n        self.encoding = options.pop('encoding', None)\n        self.fields_to_export = options.pop('fields_to_export', None)\n        self.export_empty_fields = options.pop('export_empty_fields', False)\n        if not dont_fail and options:\n            raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))",
        "begin_line": 31,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.export_item#42",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        raise NotImplementedError",
        "begin_line": 42,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.serialize_field#45",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', lambda x: x)\n        return serializer(value)",
        "begin_line": 45,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021925016443762334,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.start_exporting#49",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        pass",
        "begin_line": 49,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.finish_exporting#52",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        pass",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter._get_serialized_fields#55",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter._get_serialized_fields(self, item, default_value=None, include_empty=None)",
        "snippet": "    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n        if self.fields_to_export is None:\n            if include_empty and not isinstance(item, dict):\n                field_iter = six.iterkeys(item.fields)\n            else:\n                field_iter = six.iterkeys(item)\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if field_name in item:\n                field = {} if isinstance(item, dict) else item.fields[field_name]\n                value = self.serialize_field(field, field_name, item[field_name])\n            else:\n                value = default_value\n\n            yield field_name, value",
        "begin_line": 55,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonLinesItemExporter.__init__#84",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonLinesItemExporter",
        "signature": "scrapy.exporters.JsonLinesItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        self.file = file\n        kwargs.setdefault('ensure_ascii', not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**kwargs)",
        "begin_line": 84,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonLinesItemExporter.export_item#90",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonLinesItemExporter",
        "signature": "scrapy.exporters.JsonLinesItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + '\\n'\n        self.file.write(to_bytes(data, self.encoding))",
        "begin_line": 90,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.__init__#98",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        self.file = file\n        kwargs.setdefault('ensure_ascii', not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**kwargs)\n        self.first_item = True",
        "begin_line": 98,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.start_exporting#105",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        self.file.write(b\"[\\n\")",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.finish_exporting#108",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        self.file.write(b\"\\n]\")",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.export_item#111",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b',\\n')\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict)\n        self.file.write(to_bytes(data, self.encoding))",
        "begin_line": 111,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.__init__#123",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self.item_element = kwargs.pop('item_element', 'item')\n        self.root_element = kwargs.pop('root_element', 'items')\n        self._configure(kwargs)\n        if not self.encoding:\n            self.encoding = 'utf-8'\n        self.xg = XMLGenerator(file, encoding=self.encoding)",
        "begin_line": 123,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.start_exporting#131",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})",
        "begin_line": 131,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.export_item#135",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        self.xg.startElement(self.item_element, {})\n        for name, value in self._get_serialized_fields(item, default_value=''):\n            self._export_xml_field(name, value)\n        self.xg.endElement(self.item_element)",
        "begin_line": 135,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.finish_exporting#141",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()",
        "begin_line": 141,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._export_xml_field#145",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._export_xml_field(self, name, serialized_value)",
        "snippet": "    def _export_xml_field(self, name, serialized_value):\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, 'items'):\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value)\n        elif is_listlike(serialized_value):\n            for value in serialized_value:\n                self._export_xml_field('value', value)\n        elif isinstance(serialized_value, six.text_type):\n            self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n        self.xg.endElement(name)",
        "begin_line": 145,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._xg_characters#165",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._xg_characters(self, serialized_value)",
        "snippet": "        def _xg_characters(self, serialized_value):\n            if not isinstance(serialized_value, six.text_type):\n                serialized_value = serialized_value.decode(self.encoding)\n            return self.xg.characters(serialized_value)",
        "begin_line": 165,
        "end_line": 168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.__init__#176",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.__init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs)",
        "snippet": "    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        if not self.encoding:\n            self.encoding = 'utf-8'\n        self.include_headers_line = include_headers_line\n        self.stream = io.TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding\n        ) if six.PY3 else file\n        self.csv_writer = csv.writer(self.stream, **kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued",
        "begin_line": 176,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.serialize_field#191",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._join_if_needed)\n        return serializer(value)",
        "begin_line": 191,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._join_if_needed#195",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._join_if_needed(self, value)",
        "snippet": "    def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value",
        "begin_line": 195,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.export_item#203",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value='',\n                                             include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)",
        "begin_line": 203,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._build_row#213",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._build_row(self, values)",
        "snippet": "    def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_native_str(s, self.encoding)\n            except TypeError:\n                yield s",
        "begin_line": 213,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._write_headers_and_set_fields_to_export#220",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._write_headers_and_set_fields_to_export(self, item)",
        "snippet": "    def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                if isinstance(item, dict):\n                    # for dicts try using fields of the first item\n                    self.fields_to_export = list(item.keys())\n                else:\n                    # use fields declared in Item\n                    self.fields_to_export = list(item.fields.keys())\n            row = list(self._build_row(self.fields_to_export))\n            self.csv_writer.writerow(row)",
        "begin_line": 220,
        "end_line": 230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PickleItemExporter.__init__#235",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PickleItemExporter",
        "signature": "scrapy.exporters.PickleItemExporter.__init__(self, file, protocol=2, **kwargs)",
        "snippet": "    def __init__(self, file, protocol=2, **kwargs):\n        self._configure(kwargs)\n        self.file = file\n        self.protocol = protocol",
        "begin_line": 235,
        "end_line": 238,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PickleItemExporter.export_item#240",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PickleItemExporter",
        "signature": "scrapy.exporters.PickleItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)",
        "begin_line": 240,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.MarshalItemExporter.__init__#247",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.MarshalItemExporter",
        "signature": "scrapy.exporters.MarshalItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs)\n        self.file = file",
        "begin_line": 247,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.MarshalItemExporter.export_item#251",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.MarshalItemExporter",
        "signature": "scrapy.exporters.MarshalItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)",
        "begin_line": 251,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PprintItemExporter.__init__#257",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PprintItemExporter",
        "signature": "scrapy.exporters.PprintItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs)\n        self.file = file",
        "begin_line": 257,
        "end_line": 259,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PprintItemExporter.export_item#261",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PprintItemExporter",
        "signature": "scrapy.exporters.PprintItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + '\\n'))",
        "begin_line": 261,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._configure#272",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._configure(self, options, dont_fail=False)",
        "snippet": "    def _configure(self, options, dont_fail=False):\n        self.binary = options.pop('binary', True)\n        super(PythonItemExporter, self)._configure(options, dont_fail)\n        if self.binary:\n            warnings.warn(\n                \"PythonItemExporter will drop support for binary export in the future\",\n                ScrapyDeprecationWarning)\n        if not self.encoding:\n            self.encoding = 'utf-8'",
        "begin_line": 272,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter.serialize_field#282",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._serialize_value)\n        return serializer(value)",
        "begin_line": 282,
        "end_line": 284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._serialize_value#286",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._serialize_value(self, value)",
        "snippet": "    def _serialize_value(self, value):\n        if isinstance(value, BaseItem):\n            return self.export_item(value)\n        if isinstance(value, dict):\n            return dict(self._serialize_dict(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value",
        "begin_line": 286,
        "end_line": 296,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._serialize_dict#298",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._serialize_dict(self, value)",
        "snippet": "    def _serialize_dict(self, value):\n        for key, val in six.iteritems(value):\n            key = to_bytes(key) if self.binary else key\n            yield key, self._serialize_value(val)",
        "begin_line": 298,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter.export_item#303",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        if self.binary:\n            result = dict(self._serialize_dict(result))\n        return result",
        "begin_line": 303,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.request.request_fingerprint#19",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_fingerprint(request, include_headers=None)",
        "snippet": "def request_fingerprint(request, include_headers=None):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (ie. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accesible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    \"\"\"\n    if include_headers:\n        include_headers = tuple(to_bytes(h.lower())\n                                 for h in sorted(include_headers))\n    cache = _fingerprint_cache.setdefault(request, {})\n    if include_headers not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url)))\n        fp.update(request.body or b'')\n        if include_headers:\n            for hdr in include_headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[include_headers] = fp.hexdigest()\n    return cache[include_headers]",
        "begin_line": 19,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.request.request_authenticate#65",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_authenticate(request, username, password)",
        "snippet": "def request_authenticate(request, username, password):\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)",
        "begin_line": 65,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.request.request_httprepr#72",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_httprepr(request)",
        "snippet": "def request_httprepr(request):\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s",
        "begin_line": 72,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.request.referer_str#89",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.referer_str(request)",
        "snippet": "def referer_str(request):\n    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n    referrer = request.headers.get('Referer')\n    if referrer is None:\n        return referrer\n    return to_native_str(referrer, errors='replace')",
        "begin_line": 89,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.http.decode_chunked_transfer#9",
        "src_path": "scrapy/utils/http.py",
        "class_name": "scrapy.utils.http",
        "signature": "scrapy.utils.http.decode_chunked_transfer(chunked_body)",
        "snippet": "def decode_chunked_transfer(chunked_body):\n    \"\"\"Parsed body received with chunked transfer encoding, and return the\n    decoded body.\n\n    For more info see:\n    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n    \"\"\"\n    body, h, t = '', '', chunked_body\n    while t:\n        h, t = t.split('\\r\\n', 1)\n        if h == '0':\n            break\n        size = int(h, 16)\n        body += t[:size]\n        t = t[size+2:]\n    return body",
        "begin_line": 9,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._get_mwlist_from_settings#21",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(\n            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))",
        "begin_line": 21,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._add_middleware#25",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'process_request'):\n            self.methods['process_request'].append(mw.process_request)\n        if hasattr(mw, 'process_response'):\n            self.methods['process_response'].insert(0, mw.process_response)\n        if hasattr(mw, 'process_exception'):\n            self.methods['process_exception'].insert(0, mw.process_exception)",
        "begin_line": 25,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.download#33",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.download(self, download_func, request, spider)",
        "snippet": "    def download(self, download_func, request, spider):\n        @defer.inlineCallbacks\n        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield method(request=request, spider=spider)\n                assert response is None or isinstance(response, (Response, Request)), \\\n                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue((yield download_func(request=request,spider=spider)))\n\n        @defer.inlineCallbacks\n        def process_response(response):\n            assert response is not None, 'Received None in process_response'\n            if isinstance(response, Request):\n                defer.returnValue(response)\n\n            for method in self.methods['process_response']:\n                response = yield method(request=request, response=response,\n                                        spider=spider)\n                assert isinstance(response, (Response, Request)), \\\n                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n                    (six.get_method_self(method).__class__.__name__, type(response))\n                if isinstance(response, Request):\n                    defer.returnValue(response)\n            defer.returnValue(response)\n\n        @defer.inlineCallbacks\n        def process_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_exception']:\n                response = yield method(request=request, exception=exception,\n                                        spider=spider)\n                assert response is None or isinstance(response, (Response, Request)), \\\n                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                    (six.get_method_self(method).__class__.__name__, type(response))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue(_failure)\n\n        deferred = mustbe_deferred(process_request, request)\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred",
        "begin_line": 33,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_request#35",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_request(request)",
        "snippet": "        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield method(request=request, spider=spider)\n                assert response is None or isinstance(response, (Response, Request)), \\\n                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue((yield download_func(request=request,spider=spider)))",
        "begin_line": 35,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_response#46",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_response(response)",
        "snippet": "        def process_response(response):\n            assert response is not None, 'Received None in process_response'\n            if isinstance(response, Request):\n                defer.returnValue(response)\n\n            for method in self.methods['process_response']:\n                response = yield method(request=request, response=response,\n                                        spider=spider)\n                assert isinstance(response, (Response, Request)), \\\n                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n                    (six.get_method_self(method).__class__.__name__, type(response))\n                if isinstance(response, Request):\n                    defer.returnValue(response)\n            defer.returnValue(response)",
        "begin_line": 46,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_exception#62",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_exception(_failure)",
        "snippet": "        def process_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_exception']:\n                response = yield method(request=request, exception=exception,\n                                        spider=spider)\n                assert response is None or isinstance(response, (Response, Request)), \\\n                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                    (six.get_method_self(method).__class__.__name__, type(response))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue(_failure)",
        "begin_line": 62,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.boto.is_botocore#9",
        "src_path": "scrapy/utils/boto.py",
        "class_name": "scrapy.utils.boto",
        "signature": "scrapy.utils.boto.is_botocore()",
        "snippet": "def is_botocore():\n    try:\n        import botocore\n        return True\n    except ImportError:\n        if six.PY2:\n            try:\n                import boto\n                return False\n            except ImportError:\n                raise NotConfigured('missing botocore or boto library')\n        else:\n            raise NotConfigured('missing botocore library')",
        "begin_line": 9,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.project.inside_project#16",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.inside_project()",
        "snippet": "def inside_project():\n    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n    if scrapy_module is not None:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n        else:\n            return True\n    return bool(closest_scrapy_cfg())",
        "begin_line": 16,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.project.project_data_dir#28",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.project_data_dir(project='default')",
        "snippet": "def project_data_dir(project='default'):\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = cfg.get(DATADIR_CFG_SECTION, project)\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n    if not exists(d):\n        os.makedirs(d)\n    return d",
        "begin_line": 28,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.project.data_path#45",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.data_path(path, createdir=False)",
        "snippet": "def data_path(path, createdir=False):\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    if not isabs(path):\n        if inside_project():\n            path = join(project_data_dir(), path)\n        else:\n            path = join('.scrapy', path)\n    if createdir and not exists(path):\n        os.makedirs(path)\n    return path",
        "begin_line": 45,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml._nons#22",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml",
        "signature": "scrapy.linkextractors.lxmlhtml._nons(tag)",
        "snippet": "def _nons(tag):\n    if isinstance(tag, six.string_types):\n        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE)+1] == XHTML_NAMESPACE:\n            return tag.split('}')[-1]\n    return tag",
        "begin_line": 22,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor.__init__#30",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor.__init__(self, tag='a', attr='href', process=None, unique=False)",
        "snippet": "    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n        self.process_attr = process if callable(process) else lambda v: v\n        self.unique = unique",
        "begin_line": 30,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._iter_links#36",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._iter_links(self, document)",
        "snippet": "    def _iter_links(self, document):\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield (el, attrib, attribs[attrib])",
        "begin_line": 36,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._extract_links#46",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._extract_links(self, selector, response_url, response_encoding, base_url)",
        "snippet": "    def _extract_links(self, selector, response_url, response_encoding, base_url):\n        links = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            url = to_native_str(url, encoding=response_encoding)\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(url, _collect_string_content(el) or u'',\n                        nofollow=rel_has_nofollow(el.get('rel')))\n            links.append(link)\n        return self._deduplicate_if_needed(links)",
        "begin_line": 46,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._process_links#71",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._process_links(self, links)",
        "snippet": "    def _process_links(self, links):\n        \"\"\" Normalize and filter extracted links\n\n        The subclass should override it if neccessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)",
        "begin_line": 71,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._deduplicate_if_needed#78",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._deduplicate_if_needed(self, links)",
        "snippet": "    def _deduplicate_if_needed(self, links):\n        if self.unique:\n            return unique_list(links, key=lambda link: link.url)\n        return links",
        "begin_line": 78,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.__init__#86",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.__init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None, deny_extensions=None, restrict_css=())",
        "snippet": "    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                 tags=('a', 'area'), attrs=('href',), canonicalize=True,\n                 unique=True, process_value=None, deny_extensions=None, restrict_css=()):\n        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n        tag_func = lambda x: x in tags\n        attr_func = lambda x: x in attrs\n        lx = LxmlParserLinkExtractor(tag=tag_func, attr=attr_func,\n            unique=unique, process=process_value)\n\n        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n            allow_domains=allow_domains, deny_domains=deny_domains,\n            restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n            canonicalize=canonicalize, deny_extensions=deny_extensions)",
        "begin_line": 86,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023998080153587713,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links#100",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links(self, response)",
        "snippet": "    def extract_links(self, response):\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [subdoc\n                    for x in self.restrict_xpaths\n                    for subdoc in response.xpath(x)]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        return unique_list(all_links)",
        "begin_line": 100,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.__init__#39",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured\n        self.crawler = crawler\n        self.noisy = False\n        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n        self.host = crawler.settings['TELNETCONSOLE_HOST']\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)",
        "begin_line": 39,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.from_crawler#52",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.start_listening#55",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.start_listening(self)",
        "snippet": "    def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.debug(\"Telnet console listening on %(host)s:%(port)d\",\n                     {'host': h.host, 'port': h.port},\n                     extra={'crawler': self.crawler})",
        "begin_line": 55,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.stop_listening#62",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.stop_listening(self)",
        "snippet": "    def stop_listening(self):\n        self.port.stopListening()",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.response.get_base_url#24",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_base_url(response)",
        "snippet": "def get_base_url(response):\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\n            response.encoding)\n    return _baseurl_cache[response]",
        "begin_line": 24,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.018867924528301886,
            "pseudo_dstar_susp": 0.018867924528301886,
            "pseudo_tarantula_susp": 0.018867924528301886,
            "pseudo_op2_susp": 0.018867924528301886,
            "pseudo_barinel_susp": 0.018867924528301886
        }
    },
    {
        "name": "scrapy.utils.response.get_meta_refresh#34",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_meta_refresh(response)",
        "snippet": "def get_meta_refresh(response):\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n            response.encoding, ignore_tags=('script', 'noscript'))\n    return _metaref_cache[response]",
        "begin_line": 34,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.response.response_status_message#43",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_status_message(status)",
        "snippet": "def response_status_message(status):\n    \"\"\"Return status code plus status text descriptive message\n    \"\"\"\n    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), \"Unknown Status\")))",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.response.response_httprepr#49",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_httprepr(response)",
        "snippet": "def response_httprepr(response):\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n        to_bytes(http.RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n    if response.headers:\n        s += response.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += response.body\n    return s",
        "begin_line": 49,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.response.open_in_browser#63",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.open_in_browser(response, _openfunc=webbrowser.open)",
        "snippet": "def open_in_browser(response, _openfunc=webbrowser.open):\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = '<head><base href=\"%s\">' % response.url\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: %s\" %\n                        response.__class__.__name__)\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(\"file://%s\" % fname)",
        "begin_line": 63,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.test.skip_if_no_boto#23",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.skip_if_no_boto()",
        "snippet": "def skip_if_no_boto():\n    try:\n        is_botocore()\n    except NotConfigured as e:\n        raise SkipTest(e)",
        "begin_line": 23,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.test.get_crawler#48",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_crawler(spidercls=None, settings_dict=None)",
        "snippet": "def get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(settings_dict)\n    return runner.create_crawler(spidercls or Spider)",
        "begin_line": 48,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020903010033444816,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.test.get_pythonpath#59",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_pythonpath()",
        "snippet": "def get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')",
        "begin_line": 59,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021978021978021978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.test.get_testenv#65",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_testenv()",
        "snippet": "def get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env",
        "begin_line": 65,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021978021978021978,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.test.assert_samelines#73",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.assert_samelines(testcase, text1, text2, msg=None)",
        "snippet": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)",
        "begin_line": 73,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.__init__#18",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self._crawler = crawler\n        self._schemes = {}  # stores acceptable schemes on instancing\n        self._handlers = {}  # stores instanced handlers for schemes\n        self._notconfigured = {}  # remembers failed handlers\n        handlers = without_none_values(\n            crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n        for scheme, clspath in six.iteritems(handlers):\n            self._schemes[scheme] = clspath\n\n        crawler.signals.connect(self._close, signals.engine_stopped)",
        "begin_line": 18,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._get_handler#30",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._get_handler(self, scheme)",
        "snippet": "    def _get_handler(self, scheme):\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = 'no handler available for that scheme'\n            return None\n\n        path = self._schemes[scheme]\n        try:\n            dhcls = load_object(path)\n            dh = dhcls(self._crawler.settings)\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                         {\"clspath\": path, \"scheme\": scheme},\n                         exc_info=True,  extra={'crawler': self._crawler})\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n        return self._handlers[scheme]",
        "begin_line": 30,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.download_request#59",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(\"Unsupported URL scheme '%s': %s\" %\n                               (scheme, self._notconfigured[scheme]))\n        return handler.download_request(request, spider)",
        "begin_line": 59,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._close#68",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._close(self, *_a, **_kw)",
        "snippet": "    def _close(self, *_a, **_kw):\n        for dh in self._handlers.values():\n            if hasattr(dh, 'close'):\n                yield dh.close()",
        "begin_line": 68,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw._isiterable#12",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw",
        "signature": "scrapy.core.spidermw._isiterable(possible_iterator)",
        "snippet": "def _isiterable(possible_iterator):\n    return hasattr(possible_iterator, '__iter__')",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager._get_mwlist_from_settings#20",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))",
        "begin_line": 20,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager._add_middleware#23",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n        if hasattr(mw, 'process_spider_input'):\n            self.methods['process_spider_input'].append(mw.process_spider_input)\n        if hasattr(mw, 'process_spider_output'):\n            self.methods['process_spider_output'].insert(0, mw.process_spider_output)\n        if hasattr(mw, 'process_spider_exception'):\n            self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)\n        if hasattr(mw, 'process_start_requests'):\n            self.methods['process_start_requests'].insert(0, mw.process_start_requests)",
        "begin_line": 23,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response#34",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response(self, scrape_func, response, request, spider)",
        "snippet": "    def scrape_response(self, scrape_func, response, request, spider):\n        fname = lambda f:'%s.%s' % (\n                six.get_method_self(f).__class__.__name__,\n                six.get_method_function(f).__name__)\n\n        def process_spider_input(response):\n            for method in self.methods['process_spider_input']:\n                try:\n                    result = method(response=response, spider=spider)\n                    assert result is None, \\\n                            'Middleware %s must returns None or ' \\\n                            'raise an exception, got %s ' \\\n                            % (fname(method), type(result))\n                except:\n                    return scrape_func(Failure(), request, spider)\n            return scrape_func(response, request, spider)\n\n        def process_spider_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_spider_exception']:\n                result = method(response=response, exception=exception, spider=spider)\n                assert result is None or _isiterable(result), \\\n                    'Middleware %s must returns None, or an iterable object, got %s ' % \\\n                    (fname(method), type(result))\n                if result is not None:\n                    return result\n            return _failure\n\n        def process_spider_output(result):\n            for method in self.methods['process_spider_output']:\n                result = method(response=response, result=result, spider=spider)\n                assert _isiterable(result), \\\n                    'Middleware %s must returns an iterable object, got %s ' % \\\n                    (fname(method), type(result))\n            return result\n\n        dfd = mustbe_deferred(process_spider_input, response)\n        dfd.addErrback(process_spider_exception)\n        dfd.addCallback(process_spider_output)\n        return dfd",
        "begin_line": 34,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_input#39",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_input(response)",
        "snippet": "        def process_spider_input(response):\n            for method in self.methods['process_spider_input']:\n                try:\n                    result = method(response=response, spider=spider)\n                    assert result is None, \\\n                            'Middleware %s must returns None or ' \\\n                            'raise an exception, got %s ' \\\n                            % (fname(method), type(result))\n                except:\n                    return scrape_func(Failure(), request, spider)\n            return scrape_func(response, request, spider)",
        "begin_line": 39,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_exception#51",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_exception(_failure)",
        "snippet": "        def process_spider_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_spider_exception']:\n                result = method(response=response, exception=exception, spider=spider)\n                assert result is None or _isiterable(result), \\\n                    'Middleware %s must returns None, or an iterable object, got %s ' % \\\n                    (fname(method), type(result))\n                if result is not None:\n                    return result\n            return _failure",
        "begin_line": 51,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_output#62",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_output(result)",
        "snippet": "        def process_spider_output(result):\n            for method in self.methods['process_spider_output']:\n                result = method(response=response, result=result, spider=spider)\n                assert _isiterable(result), \\\n                    'Middleware %s must returns an iterable object, got %s ' % \\\n                    (fname(method), type(result))\n            return result",
        "begin_line": 62,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_start_requests#75",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_start_requests(self, start_requests, spider)",
        "snippet": "    def process_start_requests(self, start_requests, spider):\n        return self._process_chain('process_start_requests', start_requests, spider)",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore.__init__#44",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore.__init__(self, basedir)",
        "snippet": "    def __init__(self, basedir):\n        if '://' in basedir:\n            basedir = basedir.split('://', 1)[1]\n        self.basedir = basedir\n        self._mkdir(self.basedir)\n        self.created_directories = defaultdict(set)",
        "begin_line": 44,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00023696682464454977,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore.persist_file#51",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore.persist_file(self, path, buf, info, meta=None, headers=None)",
        "snippet": "    def persist_file(self, path, buf, info, meta=None, headers=None):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(os.path.dirname(absolute_path), info)\n        with open(absolute_path, 'wb') as f:\n            f.write(buf.getvalue())",
        "begin_line": 51,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore._get_filesystem_path#69",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore._get_filesystem_path(self, path)",
        "snippet": "    def _get_filesystem_path(self, path):\n        path_comps = path.split('/')\n        return os.path.join(self.basedir, *path_comps)",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore._mkdir#73",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore._mkdir(self, dirname, domain=None)",
        "snippet": "    def _mkdir(self, dirname, domain=None):\n        seen = self.created_directories[domain] if domain else set()\n        if dirname not in seen:\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n            seen.add(dirname)",
        "begin_line": 73,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.S3FilesStore.__init__#92",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.S3FilesStore",
        "signature": "scrapy.pipelines.files.S3FilesStore.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.is_botocore = is_botocore()\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3', aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)\n        else:\n            from boto.s3.connection import S3Connection\n            self.S3Connection = S3Connection\n        assert uri.startswith('s3://')\n        self.bucket, self.prefix = uri[5:].split('/', 1)",
        "begin_line": 92,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.__init__#226",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.__init__(self, store_uri, download_func=None, settings=None)",
        "snippet": "    def __init__(self, store_uri, download_func=None, settings=None):\n        if not store_uri:\n            raise NotConfigured\n        \n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        cls_name = \"FilesPipeline\"\n        self.store = self._get_store(store_uri)\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=cls_name,\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve('FILES_EXPIRES'), self.EXPIRES\n        )\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field = settings.get(\n            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD\n        )\n        self.files_result_field = settings.get(\n            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n        )\n\n        super(FilesPipeline, self).__init__(download_func=download_func)",
        "begin_line": 226,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.from_settings#255",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)",
        "begin_line": 255,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline._get_store#264",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline._get_store(self, uri)",
        "snippet": "    def _get_store(self, uri):\n        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n            scheme = 'file'\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)",
        "begin_line": 264,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.media_to_download#272",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum}\n\n        path = self.file_path(request, info=info)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f:\n            logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n        )\n        return dfd",
        "begin_line": 272,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline._onsuccess#273",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline._onsuccess(result)",
        "snippet": "        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum}",
        "begin_line": 273,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.media_downloaded#323",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('download-error')\n\n        if not response.body:\n            logger.warning(\n                'File (empty-content): Empty file from %(request)s referred '\n                'in <%(referer)s>: no-content',\n                {'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('empty-content')\n\n        status = 'cached' if 'cached' in response.flags else 'downloaded'\n        logger.debug(\n            'File (%(status)s): Downloaded file from %(request)s referred in '\n            '<%(referer)s>',\n            {'status': status, 'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info)\n            checksum = self.file_downloaded(response, request, info)\n        except FileException as exc:\n            logger.warning(\n                'File (error): Error processing file from %(request)s '\n                'referred in <%(referer)s>: %(errormsg)s',\n                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n                extra={'spider': info.spider}, exc_info=True\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                'File (unknown-error): Error processing file from %(request)s '\n                'referred in <%(referer)s>',\n                {'request': request, 'referer': referer},\n                exc_info=True, extra={'spider': info.spider}\n            )\n            raise FileException(str(exc))\n\n        return {'url': request.url, 'path': path, 'checksum': checksum}",
        "begin_line": 323,
        "end_line": 374,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.get_media_requests#381",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.files_urls_field, [])]",
        "begin_line": 381,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.file_downloaded#384",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.file_downloaded(self, response, request, info)",
        "snippet": "    def file_downloaded(self, response, request, info):\n        path = self.file_path(request, response=response, info=info)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum",
        "begin_line": 384,
        "end_line": 390,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.item_completed#392",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.files_result_field in item.fields:\n            item[self.files_result_field] = [x for ok, x in results if ok]\n        return item",
        "begin_line": 392,
        "end_line": 395,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.file_path#397",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.file_path(self, request, response=None, info=None)",
        "snippet": "    def file_path(self, request, response=None, info=None):\n        ## start of deprecation warning block (can be removed in the future)\n        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n                          'file_path(request, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)\n\n        # check if called from file_key with url as first argument\n        if not isinstance(request, Request):\n            _warn()\n            url = request\n        else:\n            url = request.url\n\n        # detect if file_key() method has been overridden\n        if not hasattr(self.file_key, '_base'):\n            _warn()\n            return self.file_key(url)\n        ## end of deprecation warning block\n\n        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n        media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\n        return 'full/%s%s' % (media_guid, media_ext)",
        "begin_line": 397,
        "end_line": 421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline._warn#399",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline._warn()",
        "snippet": "        def _warn():\n            from scrapy.exceptions import ScrapyDeprecationWarning\n            import warnings\n            warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n                          'file_path(request, response=None, info=None) instead',\n                          category=ScrapyDeprecationWarning, stacklevel=1)",
        "begin_line": 399,
        "end_line": 404,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.file_key#424",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.file_key(self, url)",
        "snippet": "    def file_key(self, url):\n        return self.file_path(url)",
        "begin_line": 424,
        "end_line": 425,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.failure_to_exc_info#18",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.failure_to_exc_info(failure)",
        "snippet": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.TopLevelFormatter.__init__#36",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.TopLevelFormatter",
        "signature": "scrapy.utils.log.TopLevelFormatter.__init__(self, loggers=None)",
        "snippet": "    def __init__(self, loggers=None):\n        self.loggers = loggers or []",
        "begin_line": 36,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.TopLevelFormatter.filter#39",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.TopLevelFormatter",
        "signature": "scrapy.utils.log.TopLevelFormatter.filter(self, record)",
        "snippet": "    def filter(self, record):\n        if any(record.name.startswith(l + '.') for l in self.loggers):\n            record.name = record.name.split('.', 1)[0]\n        return True",
        "begin_line": 39,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.configure_logging#59",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.configure_logging(settings=None, install_root_handler=True)",
        "snippet": "def configure_logging(settings=None, install_root_handler=True):\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver('twisted')\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool('LOG_STDOUT'):\n        sys.stdout = StreamLogger(logging.getLogger('stdout'))\n\n    if install_root_handler:\n        logging.root.setLevel(logging.NOTSET)\n        handler = _get_handler(settings)\n        logging.root.addHandler(handler)",
        "begin_line": 59,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log._get_handler#104",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log._get_handler(settings)",
        "snippet": "def _get_handler(settings):\n    \"\"\" Return a log handler object according to settings \"\"\"\n    filename = settings.get('LOG_FILE')\n    if filename:\n        encoding = settings.get('LOG_ENCODING')\n        handler = logging.FileHandler(filename, encoding=encoding)\n    elif settings.getbool('LOG_ENABLED'):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get('LOG_FORMAT'),\n        datefmt=settings.get('LOG_DATEFORMAT')\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get('LOG_LEVEL'))\n    if settings.getbool('LOG_SHORT_NAMES'):\n        handler.addFilter(TopLevelFormatter(['scrapy']))\n    return handler",
        "begin_line": 104,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.log_scrapy_info#126",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.log_scrapy_info(settings)",
        "snippet": "def log_scrapy_info(settings):\n    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n\n    d = dict(overridden_settings(settings))\n    logger.info(\"Overridden settings: %(settings)r\", {'settings': d})",
        "begin_line": 126,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.StreamLogger.__init__#140",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.StreamLogger",
        "signature": "scrapy.utils.log.StreamLogger.__init__(self, logger, log_level=logging.INFO)",
        "snippet": "    def __init__(self, logger, log_level=logging.INFO):\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = ''",
        "begin_line": 140,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.StreamLogger.write#145",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.StreamLogger",
        "signature": "scrapy.utils.log.StreamLogger.write(self, buf)",
        "snippet": "    def write(self, buf):\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())",
        "begin_line": 145,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.LogCounterHandler.__init__#157",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.LogCounterHandler",
        "signature": "scrapy.utils.log.LogCounterHandler.__init__(self, crawler, *args, **kwargs)",
        "snippet": "    def __init__(self, crawler, *args, **kwargs):\n        super(LogCounterHandler, self).__init__(*args, **kwargs)\n        self.crawler = crawler",
        "begin_line": 157,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.LogCounterHandler.emit#161",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.LogCounterHandler",
        "signature": "scrapy.utils.log.LogCounterHandler.emit(self, record)",
        "snippet": "    def emit(self, record):\n        sname = 'log_count/{}'.format(record.levelname)\n        self.crawler.stats.inc_value(sname)",
        "begin_line": 161,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00019798059790140566,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.log.logformatter_adapter#166",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.logformatter_adapter(logkws)",
        "snippet": "def logformatter_adapter(logkws):\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {'level', 'msg', 'args'} <= set(logkws):\n        warnings.warn('Missing keys in LogFormatter method',\n                      ScrapyDeprecationWarning)\n\n    if 'format' in logkws:\n        warnings.warn('`format` key in LogFormatter methods has been '\n                      'deprecated, use `msg` instead',\n                      ScrapyDeprecationWarning)\n\n    level = logkws.get('level', logging.INFO)\n    message = logkws.get('format', logkws.get('msg'))\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = logkws if not logkws.get('args') else logkws['args']\n\n    return (level, message, args)",
        "begin_line": 166,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.__init__#35",
        "src_path": "scrapy/core/downloader/handlers/s3.py",
        "class_name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.__init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, httpdownloadhandler=HTTPDownloadHandler, **kw)",
        "snippet": "    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n            httpdownloadhandler=HTTPDownloadHandler, **kw):\n\n        if not aws_access_key_id:\n            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n        if not aws_secret_access_key:\n            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n\n        # If no credentials could be found anywhere,\n        # consider this an anonymous connection request by default;\n        # unless 'anon' was set explicitly (True/False).\n        anon = kw.get('anon')\n        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n            kw['anon'] = True\n        self.anon = kw.get('anon')\n\n        self._signer = None\n        if is_botocore():\n            import botocore.auth\n            import botocore.credentials\n            kw.pop('anon', None)\n            if kw:\n                raise TypeError('Unexpected keyword arguments: %s' % kw)\n            if not self.anon:\n                SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n                self._signer = SignerCls(botocore.credentials.Credentials(\n                    aws_access_key_id, aws_secret_access_key))\n        else:\n            _S3Connection = _get_boto_connection()\n            try:\n                self.conn = _S3Connection(\n                    aws_access_key_id, aws_secret_access_key, **kw)\n            except Exception as ex:\n                raise NotConfigured(str(ex))\n\n        self._download_http = httpdownloadhandler(settings).download_request",
        "begin_line": 35,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.download_request#72",
        "src_path": "scrapy/core/downloader/handlers/s3.py",
        "class_name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        p = urlparse_cached(request)\n        scheme = 'https' if request.meta.get('is_secure') else 'http'\n        bucket = p.hostname\n        path = p.path + '?' + p.query if p.query else p.path\n        url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n        if self.anon:\n            request = request.replace(url=url)\n        elif self._signer is not None:\n            import botocore.awsrequest\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),\n                headers=request.headers.to_unicode_dict(),\n                data=request.body)\n            self._signer.add_auth(awsrequest)\n            request = request.replace(\n                url=url, headers=awsrequest.headers.items())\n        else:\n            signed_headers = self.conn.make_request(\n                    method=request.method,\n                    bucket=bucket,\n                    key=unquote(p.path),\n                    query_args=unquote(p.query),\n                    headers=request.headers,\n                    data=request.body)\n            request = request.replace(url=url, headers=signed_headers)\n        return self._download_http(request, spider)",
        "begin_line": 72,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_any_domain#19",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_any_domain(url, domains)",
        "snippet": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)",
        "begin_line": 19,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_spider#28",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_spider(url, spider)",
        "snippet": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url,\n        [spider.name] + list(getattr(spider, 'allowed_domains', [])))",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.url_has_any_extension#34",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_has_any_extension(url, extensions)",
        "snippet": "def url_has_any_extension(url, extensions):\n    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00024348672997321646,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.parse_url#38",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.parse_url(url, encoding=None)",
        "snippet": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_unicode(url, encoding))",
        "begin_line": 38,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.escape_ajax#47",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.escape_ajax(url)",
        "snippet": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])",
        "begin_line": 47,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009433962264150943,
            "pseudo_dstar_susp": 0.009433962264150943,
            "pseudo_tarantula_susp": 0.009433962264150943,
            "pseudo_op2_susp": 0.009433962264150943,
            "pseudo_barinel_susp": 0.009433962264150943
        }
    },
    {
        "name": "scrapy.utils.url.add_http_if_no_scheme#76",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.add_http_if_no_scheme(url)",
        "snippet": "def add_http_if_no_scheme(url):\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url",
        "begin_line": 76,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002523977788995457,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.utils.url.guess_scheme#87",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.guess_scheme(url)",
        "snippet": "def guess_scheme(url):\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or http:// otherwise.\"\"\"\n    parts = urlparse(url)\n    if parts.scheme:\n        return url\n    # Note: this does not match Windows filepath\n    if re.match(r'''^                   # start with...\n                    (\n                        \\.              # ...a single dot,\n                        (\n                            \\. | [^/\\.]+  # optionally followed by\n                        )?                # either a second dot or some characters\n                    )?      # optional match of \".\", \"..\" or \".blabla\"\n                    /       # at least one \"/\" for a file path,\n                    .       # and something after the \"/\"\n                    ''', parts.path, flags=re.VERBOSE):\n        return any_to_uri(url)\n    else:\n        return add_http_if_no_scheme(url)",
        "begin_line": 87,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler.__init__#27",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.__init__(self, spidercls, settings=None)",
        "snippet": "    def __init__(self, spidercls, settings=None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls = spidercls\n        self.settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n\n        self.signals = SignalManager(self)\n        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n\n        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n        logging.root.addHandler(handler)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving __init__ scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n        self.logformatter = lf_cls.from_crawler(self)\n        self.extensions = ExtensionManager.from_crawler(self)\n\n        self.settings.freeze()\n        self.crawling = False\n        self.spider = None\n        self.engine = None",
        "begin_line": 27,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler.spiders#55",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.spiders(self)",
        "snippet": "    def spiders(self):\n        if not hasattr(self, '_spiders'):\n            warnings.warn(\"Crawler.spiders is deprecated, use \"\n                          \"CrawlerRunner.spider_loader or instantiate \"\n                          \"scrapy.spiderloader.SpiderLoader with your \"\n                          \"settings.\",\n                          category=ScrapyDeprecationWarning, stacklevel=2)\n            self._spiders = _get_spider_loader(self.settings.frozencopy())\n        return self._spiders",
        "begin_line": 55,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler.crawl#66",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.crawl(self, *args, **kwargs)",
        "snippet": "    def crawl(self, *args, **kwargs):\n        assert not self.crawling, \"Crawling already taking place\"\n        self.crawling = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield defer.maybeDeferred(self.engine.start)\n        except Exception:\n            # In Python 2 reraising an exception after yield discards\n            # the original traceback (see http://bugs.python.org/issue7563),\n            # so sys.exc_info() workaround is used.\n            # This workaround also works in Python 3, but it is not needed,\n            # and it is slower, so in Python 3 we use native `raise`.\n            if six.PY2:\n                exc_info = sys.exc_info()\n\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n\n            if six.PY2:\n                six.reraise(*exc_info)\n            raise",
        "begin_line": 66,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler._create_spider#93",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler._create_spider(self, *args, **kwargs)",
        "snippet": "    def _create_spider(self, *args, **kwargs):\n        return self.spidercls.from_crawler(self, *args, **kwargs)",
        "begin_line": 93,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020990764063811922,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler._create_engine#96",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler._create_engine(self)",
        "snippet": "    def _create_engine(self):\n        return ExecutionEngine(self, lambda _: self.stop())",
        "begin_line": 96,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.Crawler.stop#100",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.stop(self)",
        "snippet": "    def stop(self):\n        if self.crawling:\n            self.crawling = False\n            yield defer.maybeDeferred(self.engine.stop)",
        "begin_line": 100,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.__init__#125",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.__init__(self, settings=None)",
        "snippet": "    def __init__(self, settings=None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        self.settings = settings\n        self.spider_loader = _get_spider_loader(settings)\n        self._crawlers = set()\n        self._active = set()",
        "begin_line": 125,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020798668885191348,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.spiders#134",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.spiders(self)",
        "snippet": "    def spiders(self):\n        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n                      \"CrawlerRunner.spider_loader.\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        return self.spider_loader",
        "begin_line": 134,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.crawl#140",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.crawl(self, crawler_or_spidercls, *args, **kwargs)",
        "snippet": "    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If `crawler_or_spidercls` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param list args: arguments to initialize the spider\n\n        :param dict kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)",
        "begin_line": 140,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._crawl#165",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._crawl(self, crawler, *args, **kwargs)",
        "snippet": "    def _crawl(self, crawler, *args, **kwargs):\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            return result\n\n        return d.addBoth(_done)",
        "begin_line": 165,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._done#170",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._done(result)",
        "snippet": "        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            return result",
        "begin_line": 170,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.create_crawler#177",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.create_crawler(self, crawler_or_spidercls)",
        "snippet": "    def create_crawler(self, crawler_or_spidercls):\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If `crawler_or_spidercls` is a Crawler, it is returned as-is.\n        * If `crawler_or_spidercls` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If `crawler_or_spidercls` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)",
        "begin_line": 177,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._create_crawler#192",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._create_crawler(self, spidercls)",
        "snippet": "    def _create_crawler(self, spidercls):\n        if isinstance(spidercls, six.string_types):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)",
        "begin_line": 192,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler.CrawlerProcess.__init__#237",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerProcess",
        "signature": "scrapy.crawler.CrawlerProcess.__init__(self, settings=None)",
        "snippet": "    def __init__(self, settings=None):\n        super(CrawlerProcess, self).__init__(settings)\n        install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings)\n        log_scrapy_info(self.settings)",
        "begin_line": 237,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.crawler._get_spider_loader#305",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler",
        "signature": "scrapy.crawler._get_spider_loader(settings)",
        "snippet": "def _get_spider_loader(settings):\n    \"\"\" Get SpiderLoader instance from settings \"\"\"\n    if settings.get('SPIDER_MANAGER_CLASS'):\n        warnings.warn(\n            'SPIDER_MANAGER_CLASS option is deprecated. '\n            'Please use SPIDER_LOADER_CLASS.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n                            settings.get('SPIDER_LOADER_CLASS'))\n    loader_cls = load_object(cls_path)\n    try:\n        verifyClass(ISpiderLoader, loader_cls)\n    except DoesNotImplement:\n        warnings.warn(\n            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n            'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n            'Please add all missing methods to avoid unexpected runtime errors.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n    return loader_cls.from_settings(settings.frozencopy())",
        "begin_line": 305,
        "end_line": 325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__init__#25",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__init__(self, name=None, **kwargs)",
        "snippet": "    def __init__(self, name=None, **kwargs):\n        if name is not None:\n            self.name = name\n        elif not getattr(self, 'name', None):\n            raise ValueError(\"%s must have a name\" % type(self).__name__)\n        self.__dict__.update(kwargs)\n        if not hasattr(self, 'start_urls'):\n            self.start_urls = []",
        "begin_line": 25,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.logger#35",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.logger(self)",
        "snippet": "    def logger(self):\n        logger = logging.getLogger(self.name)\n        return logging.LoggerAdapter(logger, {'spider': self})",
        "begin_line": 35,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.log#39",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.log(self, message, level=logging.DEBUG, **kw)",
        "snippet": "    def log(self, message, level=logging.DEBUG, **kw):\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)",
        "begin_line": 39,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.from_crawler#49",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.from_crawler(cls, crawler, *args, **kwargs)",
        "snippet": "    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider",
        "begin_line": 49,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002094240837696335,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.set_crawler#54",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.set_crawler(self, crawler)",
        "snippet": "    def set_crawler(self, crawler):\n        warnings.warn(\"set_crawler is deprecated, instantiate and bound the \"\n                      \"spider to this crawler with from_crawler method \"\n                      \"instead.\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        assert not hasattr(self, 'crawler'), \"Spider already bounded to a \" \\\n                                             \"crawler\"\n        self._set_crawler(crawler)",
        "begin_line": 54,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider._set_crawler#63",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider._set_crawler(self, crawler)",
        "snippet": "    def _set_crawler(self, crawler):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)",
        "begin_line": 63,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002092487968194183,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.start_requests#68",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        for url in self.start_urls:\n            yield self.make_requests_from_url(url)",
        "begin_line": 68,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.make_requests_from_url#72",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.make_requests_from_url(self, url)",
        "snippet": "    def make_requests_from_url(self, url):\n        return Request(url, dont_filter=True)",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003866976024748647,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.update_settings#79",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.update_settings(cls, settings)",
        "snippet": "    def update_settings(cls, settings):\n        settings.setdict(cls.custom_settings or {}, priority='spider')",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020052135552436334,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.handles_request#83",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.handles_request(cls, request)",
        "snippet": "    def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.close#87",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.close(spider, reason)",
        "snippet": "    def close(spider, reason):\n        closed = getattr(spider, 'closed', None)\n        if callable(closed):\n            return closed(reason)",
        "begin_line": 87,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__str__#92",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.__init__#19",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop('formdata', None)\n        if formdata and kwargs.get('method') is None:\n            kwargs['method'] = 'POST'\n\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            querystr = _urlencode(items, self.encoding)\n            if self.method == 'POST':\n                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                self._set_body(querystr)\n            else:\n                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)",
        "begin_line": 19,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.021739130434782608,
            "pseudo_dstar_susp": 0.021739130434782608,
            "pseudo_tarantula_susp": 0.021739130434782608,
            "pseudo_op2_susp": 0.021739130434782608,
            "pseudo_barinel_susp": 0.021739130434782608
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.from_response#36",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None, clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs)",
        "snippet": "    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n\n        kwargs.setdefault('encoding', response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n        method = kwargs.pop('method', form.method)\n        return cls(url=url, method=method, formdata=formdata, **kwargs)",
        "begin_line": 36,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0625,
            "pseudo_dstar_susp": 0.0625,
            "pseudo_tarantula_susp": 0.07142857142857142,
            "pseudo_op2_susp": 0.0625,
            "pseudo_barinel_susp": 0.07142857142857142
        }
    },
    {
        "name": "scrapy.http.request.form._get_form_url#52",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form_url(form, url)",
        "snippet": "def _get_form_url(form, url):\n    if url is None:\n        return urljoin(form.base_url, form.action)\n    return urljoin(form.base_url, url)",
        "begin_line": 52,
        "end_line": 55,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0625,
            "pseudo_dstar_susp": 0.0625,
            "pseudo_tarantula_susp": 0.07142857142857142,
            "pseudo_op2_susp": 0.0625,
            "pseudo_barinel_susp": 0.07142857142857142
        }
    },
    {
        "name": "scrapy.http.request.form._urlencode#58",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._urlencode(seq, enc)",
        "snippet": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)",
        "begin_line": 58,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022109219544550078,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.form._get_form#65",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form(response, formname, formid, formnumber, formxpath)",
        "snippet": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(\"No <form> element found in %s\" % response)\n\n    if formname is not None:\n        f = root.xpath('//form[@name=\"%s\"]' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath('//form[@id=\"%s\"]' % formid)\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')\n        raise ValueError('No <form> element found with %s' % encoded)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(\"Form number %d not found in %s\" %\n                             (formnumber, response))\n        else:\n            return form",
        "begin_line": 65,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.2,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.5,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.5
        }
    },
    {
        "name": "scrapy.http.request.form._get_inputs#109",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_inputs(form, formdata, dont_click, clickdata, response)",
        "snippet": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[not(@type) or @type['\n                        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n                        ' and (../@checked or'\n                        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n                        namespaces={\n                            \"re\": \"http://exslt.org/regular-expressions\"})\n    values = [(k, u'' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values",
        "begin_line": 109,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.5,
            "pseudo_dstar_susp": 0.5,
            "pseudo_tarantula_susp": 0.3333333333333333,
            "pseudo_op2_susp": 0.5,
            "pseudo_barinel_susp": 0.3333333333333333
        }
    },
    {
        "name": "scrapy.http.request.form._value#136",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._value(ele)",
        "snippet": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v",
        "begin_line": 136,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.form._select_value#144",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._select_value(ele, n, v)",
        "snippet": "def _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags wihout options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath('.//option[@selected]')\n        v = [(o.get('value') or o.text or u'').strip() for o in selected_options]\n    return n, v",
        "begin_line": 144,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.http.request.form._get_clickable#159",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_clickable(clickdata, form)",
        "snippet": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = [\n        el for el in form.xpath(\n            'descendant::*[(self::input or self::button)'\n            ' and re:test(@type, \"^submit$\", \"i\")]'\n            '|descendant::button[not(@type)]',\n            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n        ]\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get('name'), el.get('value') or '')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get('name'), el.get('value') or '')\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u'.//*' + \\\n            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get('name'), el[0].get('value') or '')\n    elif len(el) > 1:\n        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n                         \"in clickdata: %r\" % (el, clickdata))\n    else:\n        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))",
        "begin_line": 159,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 1.0,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 1.0
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.__init__#14",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.__init__(self, stats, interval=60.0)",
        "snippet": "    def __init__(self, stats, interval=60.0):\n        self.stats = stats\n        self.interval = interval\n        self.multiplier = 60.0 / self.interval\n        self.task = None",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.from_crawler#21",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n        if not interval:\n            raise NotConfigured\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 21,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_opened#30",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.pagesprev = 0\n        self.itemsprev = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.log#37",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.log(self, spider)",
        "snippet": "    def log(self, spider):\n        items = self.stats.get_value('item_scraped_count', 0)\n        pages = self.stats.get_value('response_received_count', 0)\n        irate = (items - self.itemsprev) * self.multiplier\n        prate = (pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = pages, items\n\n        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n        log_args = {'pages': pages, 'pagerate': prate,\n                    'items': items, 'itemrate': irate}\n        logger.info(msg, log_args, extra={'spider': spider})",
        "begin_line": 37,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_closed#50",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        if self.task and self.task.running:\n            self.task.stop()",
        "begin_line": 50,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extension.ExtensionManager._get_mwlist_from_settings#14",
        "src_path": "scrapy/extension.py",
        "class_name": "scrapy.extension.ExtensionManager",
        "signature": "scrapy.extension.ExtensionManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('EXTENSIONS'))",
        "begin_line": 14,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.__init__#29",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.__init__(self, max_active_size=5000000)",
        "snippet": "    def __init__(self, max_active_size=5000000):\n        self.max_active_size = max_active_size\n        self.queue = deque()\n        self.active = set()\n        self.active_size = 0\n        self.itemproc_size = 0\n        self.closing = None",
        "begin_line": 29,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.add_response_request#37",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.add_response_request(self, response, request)",
        "snippet": "    def add_response_request(self, response, request):\n        deferred = defer.Deferred()\n        self.queue.append((response, request, deferred))\n        if isinstance(response, Response):\n            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred",
        "begin_line": 37,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.next_response_request_deferred#46",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.next_response_request_deferred(self)",
        "snippet": "    def next_response_request_deferred(self):\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred",
        "begin_line": 46,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.finish_response#51",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.finish_response(self, response, request)",
        "snippet": "    def finish_response(self, response, request):\n        self.active.remove(request)\n        if isinstance(response, Response):\n            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE",
        "begin_line": 51,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.is_idle#58",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.is_idle(self)",
        "snippet": "    def is_idle(self):\n        return not (self.queue or self.active)",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.needs_backout#61",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.needs_backout(self)",
        "snippet": "    def needs_backout(self):\n        return self.active_size > self.max_active_size",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.__init__#67",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.slot = None\n        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n        self.itemproc = itemproc_cls.from_crawler(crawler)\n        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n        self.crawler = crawler\n        self.signals = crawler.signals\n        self.logformatter = crawler.logformatter",
        "begin_line": 67,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.open_spider#78",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot()\n        yield self.itemproc.open_spider(spider)",
        "begin_line": 78,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.close_spider#83",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        slot = self.slot\n        slot.closing = defer.Deferred()\n        slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider, slot)\n        return slot.closing",
        "begin_line": 83,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._check_if_closing#95",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._check_if_closing(self, spider, slot)",
        "snippet": "    def _check_if_closing(self, spider, slot):\n        if slot.closing and slot.is_idle():\n            slot.closing.callback(spider)",
        "begin_line": 95,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.enqueue_scrape#99",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.enqueue_scrape(self, response, request, spider)",
        "snippet": "    def enqueue_scrape(self, response, request, spider):\n        slot = self.slot\n        dfd = slot.add_response_request(response, request)\n        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error('Scraper bug processing %(request)s',\n                                   {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n        self._scrape_next(spider, slot)\n        return dfd",
        "begin_line": 99,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.finish_scraping#102",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.finish_scraping(_)",
        "snippet": "        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _",
        "begin_line": 102,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape_next#116",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape_next(self, spider, slot)",
        "snippet": "    def _scrape_next(self, spider, slot):\n        while slot.queue:\n            response, request, deferred = slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)",
        "begin_line": 116,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape#121",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape(self, response, request, spider)",
        "snippet": "    def _scrape(self, response, request, spider):\n        \"\"\"Handle the downloaded response or failure through the spider\n        callback/errback\"\"\"\n        assert isinstance(response, (Response, Failure))\n\n        dfd = self._scrape2(response, request, spider) # returns spiders processed output\n        dfd.addErrback(self.handle_spider_error, request, response, spider)\n        dfd.addCallback(self.handle_spider_output, request, response, spider)\n        return dfd",
        "begin_line": 121,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape2#131",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape2(self, request_result, request, spider)",
        "snippet": "    def _scrape2(self, request_result, request, spider):\n        \"\"\"Handle the different cases of request's result been a Response or a\n        Failure\"\"\"\n        if not isinstance(request_result, Failure):\n            return self.spidermw.scrape_response(\n                self.call_spider, request_result, request, spider)\n        else:\n            # FIXME: don't ignore errors in spider middleware\n            dfd = self.call_spider(request_result, request, spider)\n            return dfd.addErrback(\n                self._log_download_errors, request_result, request, spider)",
        "begin_line": 131,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.call_spider#143",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.call_spider(self, result, request, spider)",
        "snippet": "    def call_spider(self, result, request, spider):\n        result.request = request\n        dfd = defer_result(result)\n        dfd.addCallbacks(request.callback or spider.parse, request.errback)\n        return dfd.addCallback(iterate_spider_output)",
        "begin_line": 143,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.handle_spider_output#170",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.handle_spider_output(self, result, request, response, spider)",
        "snippet": "    def handle_spider_output(self, result, request, response, spider):\n        if not result:\n            return defer_succeed(None)\n        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n        dfd = parallel(it, self.concurrent_items,\n            self._process_spidermw_output, request, response, spider)\n        return dfd",
        "begin_line": 170,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._process_spidermw_output#178",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._process_spidermw_output(self, output, request, response, spider)",
        "snippet": "    def _process_spidermw_output(self, output, request, response, spider):\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        if isinstance(output, Request):\n            self.crawler.engine.crawl(request=output, spider=spider)\n        elif isinstance(output, (BaseItem, dict)):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error('Spider must return Request, BaseItem, dict or None, '\n                         'got %(typename)r in %(request)s',\n                         {'request': request, 'typename': typename},\n                         extra={'spider': spider})",
        "begin_line": 178,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._log_download_errors#198",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._log_download_errors(self, spider_failure, download_failure, request, spider)",
        "snippet": "    def _log_download_errors(self, spider_failure, download_failure, request, spider):\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here)\n        \"\"\"\n        if (isinstance(download_failure, Failure) and\n                not download_failure.check(IgnoreRequest)):\n            if download_failure.frames:\n                logger.error('Error downloading %(request)s',\n                             {'request': request},\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logger.error('Error downloading %(request)s: %(errmsg)s',\n                                 {'request': request, 'errmsg': errmsg},\n                                 extra={'spider': spider})\n\n        if spider_failure is not download_failure:\n            return spider_failure",
        "begin_line": 198,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004037141703673799,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._itemproc_finished#219",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._itemproc_finished(self, output, item, response, spider)",
        "snippet": "    def _itemproc_finished(self, output, item, response, spider):\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n        \"\"\"\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped, item=item, response=response,\n                    spider=spider, exception=output.value)\n            else:\n                logger.error('Error processing %(item)s', {'item': item},\n                             exc_info=failure_to_exc_info(output),\n                             extra={'spider': spider})\n        else:\n            logkws = self.logformatter.scraped(output, response, spider)\n            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_scraped, item=output, response=response,\n                spider=spider)",
        "begin_line": 219,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.identity#16",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl",
        "signature": "scrapy.spiders.crawl.identity(x)",
        "snippet": "def identity(x):\n    return x",
        "begin_line": 16,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.Rule.__init__#22",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.Rule",
        "signature": "scrapy.spiders.crawl.Rule.__init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity)",
        "snippet": "    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=identity):\n        self.link_extractor = link_extractor\n        self.callback = callback\n        self.cb_kwargs = cb_kwargs or {}\n        self.process_links = process_links\n        self.process_request = process_request\n        if follow is None:\n            self.follow = False if callback else True\n        else:\n            self.follow = follow",
        "begin_line": 22,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.__init__#38",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.__init__(self, *a, **kw)",
        "snippet": "    def __init__(self, *a, **kw):\n        super(CrawlSpider, self).__init__(*a, **kw)\n        self._compile_rules()",
        "begin_line": 38,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider._requests_to_follow#51",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider._requests_to_follow(self, response)",
        "snippet": "    def _requests_to_follow(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        seen = set()\n        for n, rule in enumerate(self._rules):\n            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                     if lnk not in seen]\n            if links and rule.process_links:\n                links = rule.process_links(links)\n            for link in links:\n                seen.add(link)\n                r = Request(url=link.url, callback=self._response_downloaded)\n                r.meta.update(rule=n, link_text=link.text)\n                yield rule.process_request(r)",
        "begin_line": 51,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider._compile_rules#81",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider._compile_rules(self)",
        "snippet": "    def _compile_rules(self):\n        def get_method(method):\n            if callable(method):\n                return method\n            elif isinstance(method, six.string_types):\n                return getattr(self, method, None)\n\n        self._rules = [copy.copy(r) for r in self.rules]\n        for rule in self._rules:\n            rule.callback = get_method(rule.callback)\n            rule.process_links = get_method(rule.process_links)\n            rule.process_request = get_method(rule.process_request)",
        "begin_line": 81,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.get_method#82",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.get_method(method)",
        "snippet": "        def get_method(method):\n            if callable(method):\n                return method\n            elif isinstance(method, six.string_types):\n                return getattr(self, method, None)",
        "begin_line": 82,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0006042296072507553,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.from_crawler#95",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.from_crawler(cls, crawler, *args, **kwargs)",
        "snippet": "    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool(\n            'CRAWLSPIDER_FOLLOW_LINKS', True)\n        return spider",
        "begin_line": 95,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0005299417064122947,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.set_crawler#101",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.set_crawler(self, crawler)",
        "snippet": "    def set_crawler(self, crawler):\n        super(CrawlSpider, self).set_crawler(crawler)\n        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)",
        "begin_line": 101,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.__init__#19",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.__init__(self, settings, stats)",
        "snippet": "    def __init__(self, settings, stats):\n        if not settings.getbool('HTTPCACHE_ENABLED'):\n            raise NotConfigured\n        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n        self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n        self.stats = stats",
        "begin_line": 19,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.from_crawler#28",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 28,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_opened#34",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.storage.open_spider(spider)",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022696323195642307,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_closed#37",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        self.storage.close_spider(spider)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00022696323195642307,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_request#40",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_cache', False):\n            return\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta['_dont_cache'] = True  # flag as uncacheable\n            return\n\n        # Look for cached response and check if expired\n        cachedresponse = self.storage.retrieve_response(spider, request)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/miss', spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value('httpcache/ignore', spider=spider)\n                raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n            return  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append('cached')\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value('httpcache/hit', spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta['cached_response'] = cachedresponse",
        "begin_line": 40,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_response#68",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_cache', False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if 'cached' in response.flags or '_dont_cache' in request.meta:\n            request.meta.pop('_dont_cache', None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if 'Date' not in response.headers:\n            response.headers['Date'] = formatdate(usegmt=1)\n\n        # Do not validate first-hand responses\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/firsthand', spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value('httpcache/revalidate', spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value('httpcache/invalidate', spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response",
        "begin_line": 68,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_exception#97",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n            return cachedresponse",
        "begin_line": 97,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware._cache_response#103",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware._cache_response(self, spider, response, request, cachedresponse)",
        "snippet": "    def _cache_response(self, spider, response, request, cachedresponse):\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value('httpcache/store', spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value('httpcache/uncacheable', spider=spider)",
        "begin_line": 103,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.tls.set_tlsext_host_name#48",
        "src_path": "scrapy/core/downloader/tls.py",
        "class_name": "scrapy.core.downloader.tls",
        "signature": "scrapy.core.downloader.tls.set_tlsext_host_name(connection, hostNameBytes)",
        "snippet": "        def set_tlsext_host_name(connection, hostNameBytes):\n            connection.set_tlsext_host_name(hostNameBytes)",
        "begin_line": 48,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions._identityVerifyingInfoCallback#62",
        "src_path": "scrapy/core/downloader/tls.py",
        "class_name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions",
        "signature": "scrapy.core.downloader.tls.ScrapyClientTLSOptions._identityVerifyingInfoCallback(self, connection, where, ret)",
        "snippet": "        def _identityVerifyingInfoCallback(self, connection, where, ret):\n            if where & SSL_CB_HANDSHAKE_START:\n                set_tlsext_host_name(connection, self._hostnameBytes)\n            elif where & SSL_CB_HANDSHAKE_DONE:\n                try:\n                    verifyHostname(connection, self._hostnameASCII)\n                except VerificationError as e:\n                    logger.warning(\n                        'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n                            self._hostnameASCII, e))\n\n                except ValueError as e:\n                    logger.warning(\n                        'Ignoring error while verifying certificate '\n                        'from host \"{}\" (exception: {})'.format(\n                            self._hostnameASCII, repr(e)))",
        "begin_line": 62,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.crawled#34",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.crawled(self, request, response, spider)",
        "snippet": "    def crawled(self, request, response, spider):\n        flags = ' %s' % str(response.flags) if response.flags else ''\n        return {\n            'level': logging.DEBUG,\n            'msg': CRAWLEDMSG,\n            'args': {\n                'status': response.status,\n                'request': request,\n                'referer': referer_str(request),\n                'flags': flags,\n            }\n        }",
        "begin_line": 34,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.scraped#47",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.scraped(self, item, response, spider)",
        "snippet": "    def scraped(self, item, response, spider):\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            'level': logging.DEBUG,\n            'msg': SCRAPEDMSG,\n            'args': {\n                'src': src,\n                'item': item,\n            }\n        }",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.dropped#61",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.dropped(self, item, exception, response, spider)",
        "snippet": "    def dropped(self, item, exception, response, spider):\n        return {\n            'level': logging.WARNING,\n            'msg': DROPPEDMSG,\n            'args': {\n                'exception': exception,\n                'item': item,\n            }\n        }",
        "begin_line": 61,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.from_crawler#72",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls()",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020542317173377156,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.__init__#7",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 7,
        "end_line": 8,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002523977788995457,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.from_crawler#11",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n            raise NotConfigured\n        return cls(crawler.stats)",
        "begin_line": 11,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002588661661920787,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_request#16",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        self.stats.inc_value('downloader/request_count', spider=spider)\n        self.stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00028595939376608524,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_response#22",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        self.stats.inc_value('downloader/response_count', spider=spider)\n        self.stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n        reslen = len(response_httprepr(response))\n        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n        return response",
        "begin_line": 22,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_exception#29",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        ex_class = \"%s.%s\" % (exception.__class__.__module__, exception.__class__.__name__)\n        self.stats.inc_value('downloader/exception_count', spider=spider)\n        self.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)",
        "begin_line": 29,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.push#14",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.push(self, obj)",
        "snippet": "        def push(self, obj):\n            s = serialize(obj)\n            super(SerializableQueue, self).push(s)",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020885547201336674,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.pop#18",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.pop(self)",
        "snippet": "        def pop(self):\n            s = super(SerializableQueue, self).pop()\n            if s:\n                return deserialize(s)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00020885547201336674,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.squeues._pickle_serialize#25",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues",
        "signature": "scrapy.squeues._pickle_serialize(obj)",
        "snippet": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=2)\n    # Python>=3.5 raises AttributeError here while\n    # Python<=3.4 raises pickle.PicklingError\n    except (pickle.PicklingError, AttributeError) as e:\n        raise ValueError(str(e))",
        "begin_line": 25,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00045413260672116256,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.Slot.__init__#26",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.__init__(self, start_requests, close_if_idle, nextcall, scheduler)",
        "snippet": "    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n        self.closing = False\n        self.inprogress = set() # requests in progress\n        self.start_requests = iter(start_requests)\n        self.close_if_idle = close_if_idle\n        self.nextcall = nextcall\n        self.scheduler = scheduler\n        self.heartbeat = task.LoopingCall(nextcall.schedule)",
        "begin_line": 26,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.Slot.add_request#35",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.add_request(self, request)",
        "snippet": "    def add_request(self, request):\n        self.inprogress.add(request)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.Slot.remove_request#38",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.remove_request(self, request)",
        "snippet": "    def remove_request(self, request):\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()",
        "begin_line": 38,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.Slot.close#42",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.close(self)",
        "snippet": "    def close(self):\n        self.closing = defer.Deferred()\n        self._maybe_fire_closing()\n        return self.closing",
        "begin_line": 42,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.Slot._maybe_fire_closing#47",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot._maybe_fire_closing(self)",
        "snippet": "    def _maybe_fire_closing(self):\n        if self.closing and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)",
        "begin_line": 47,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.__init__#58",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.__init__(self, crawler, spider_closed_callback)",
        "snippet": "    def __init__(self, crawler, spider_closed_callback):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        self.signals = crawler.signals\n        self.logformatter = crawler.logformatter\n        self.slot = None\n        self.spider = None\n        self.running = False\n        self.paused = False\n        self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n        downloader_cls = load_object(self.settings['DOWNLOADER'])\n        self.downloader = downloader_cls(crawler)\n        self.scraper = Scraper(crawler)\n        self._spider_closed_callback = spider_closed_callback",
        "begin_line": 58,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.start#74",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.start(self)",
        "snippet": "    def start(self):\n        \"\"\"Start the execution engine\"\"\"\n        assert not self.running, \"Engine already running\"\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait = defer.Deferred()\n        yield self._closewait",
        "begin_line": 74,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.stop#83",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.stop(self)",
        "snippet": "    def stop(self):\n        \"\"\"Stop the execution engine gracefully\"\"\"\n        assert self.running, \"Engine not running\"\n        self.running = False\n        dfd = self._close_all_spiders()\n        return dfd.addBoth(lambda _: self._finish_stopping_engine())",
        "begin_line": 83,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.close#90",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.close(self)",
        "snippet": "    def close(self):\n        \"\"\"Close the execution engine gracefully.\n\n        If it has already been started, stop it. In all cases, close all spiders\n        and the downloader.\n        \"\"\"\n        if self.running:\n            # Will also close spiders and downloader\n            return self.stop()\n        elif self.open_spiders:\n            # Will also close downloader\n            return self._close_all_spiders()\n        else:\n            return defer.succeed(self.downloader.close())",
        "begin_line": 90,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._next_request#113",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._next_request(self, spider)",
        "snippet": "    def _next_request(self, spider):\n        slot = self.slot\n        if not slot:\n            return\n\n        if self.paused:\n            return\n\n        while not self._needs_backout(spider):\n            if not self._next_request_from_scheduler(spider):\n                break\n\n        if slot.start_requests and not self._needs_backout(spider):\n            try:\n                request = next(slot.start_requests)\n            except StopIteration:\n                slot.start_requests = None\n            except Exception:\n                slot.start_requests = None\n                logger.error('Error while obtaining start requests',\n                             exc_info=True, extra={'spider': spider})\n            else:\n                self.crawl(request, spider)\n\n        if self.spider_is_idle(spider) and slot.close_if_idle:\n            self._spider_idle(spider)",
        "begin_line": 113,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._needs_backout#140",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._needs_backout(self, spider)",
        "snippet": "    def _needs_backout(self, spider):\n        slot = self.slot\n        return not self.running \\\n            or slot.closing \\\n            or self.downloader.needs_backout() \\\n            or self.scraper.slot.needs_backout()",
        "begin_line": 140,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0003254149040026033,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._next_request_from_scheduler#147",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._next_request_from_scheduler(self, spider)",
        "snippet": "    def _next_request_from_scheduler(self, spider):\n        slot = self.slot\n        request = slot.scheduler.next_request()\n        if not request:\n            return\n        d = self._download(request, spider)\n        d.addBoth(self._handle_downloader_output, request, spider)\n        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.remove_request(request))\n        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.nextcall.schedule())\n        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        return d",
        "begin_line": 147,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._handle_downloader_output#167",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._handle_downloader_output(self, response, request, spider)",
        "snippet": "    def _handle_downloader_output(self, response, request, spider):\n        assert isinstance(response, (Request, Response, Failure)), response\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(response, Request):\n            self.crawl(response, spider)\n            return\n        # response is a Response or Failure\n        d = self.scraper.enqueue_scrape(response, request, spider)\n        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d",
        "begin_line": 167,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.spider_is_idle#180",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.spider_is_idle(self, spider)",
        "snippet": "    def spider_is_idle(self, spider):\n        if not self.scraper.slot.is_idle():\n            # scraper is not idle\n            return False\n\n        if self.downloader.active:\n            # downloader has pending requests\n            return False\n\n        if self.slot.start_requests is not None:\n            # not all start requests are handled\n            return False\n\n        if self.slot.scheduler.has_pending_requests():\n            # scheduler has pending requests\n            return False\n\n        return True",
        "begin_line": 180,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00036643459142543056,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.open_spiders#200",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.open_spiders(self)",
        "snippet": "    def open_spiders(self):\n        return [self.spider] if self.spider else []",
        "begin_line": 200,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002785515320334262,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.has_capacity#203",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.has_capacity(self)",
        "snippet": "    def has_capacity(self):\n        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n        return not bool(self.slot)",
        "begin_line": 203,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.crawl#207",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.crawl(self, request, spider)",
        "snippet": "    def crawl(self, request, spider):\n        assert spider in self.open_spiders, \\\n            \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n        self.schedule(request, spider)\n        self.slot.nextcall.schedule()",
        "begin_line": 207,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.schedule#213",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.schedule(self, request, spider)",
        "snippet": "    def schedule(self, request, spider):\n        self.signals.send_catch_log(signal=signals.request_scheduled,\n                request=request, spider=spider)\n        if not self.slot.scheduler.enqueue_request(request):\n            self.signals.send_catch_log(signal=signals.request_dropped,\n                                        request=request, spider=spider)",
        "begin_line": 213,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._download#232",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._download(self, request, spider)",
        "snippet": "    def _download(self, request, spider):\n        slot = self.slot\n        slot.add_request(request)\n        def _on_success(response):\n            assert isinstance(response, (Response, Request))\n            if isinstance(response, Response):\n                response.request = request # tie request to response received\n                logkws = self.logformatter.crawled(request, response, spider)\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(signal=signals.response_received, \\\n                    response=response, request=request, spider=spider)\n            return response\n\n        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _\n\n        dwld = self.downloader.fetch(request, spider)\n        dwld.addCallbacks(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld",
        "begin_line": 232,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._on_success#235",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._on_success(response)",
        "snippet": "        def _on_success(response):\n            assert isinstance(response, (Response, Request))\n            if isinstance(response, Response):\n                response.request = request # tie request to response received\n                logkws = self.logformatter.crawled(request, response, spider)\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(signal=signals.response_received, \\\n                    response=response, request=request, spider=spider)\n            return response",
        "begin_line": 235,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._on_complete#245",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._on_complete(_)",
        "snippet": "        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _",
        "begin_line": 245,
        "end_line": 247,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.open_spider#255",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.open_spider(self, spider, start_requests=(), close_if_idle=True)",
        "snippet": "    def open_spider(self, spider, start_requests=(), close_if_idle=True):\n        assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n            spider.name\n        logger.info(\"Spider opened\", extra={'spider': spider})\n        nextcall = CallLaterOnce(self._next_request, spider)\n        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.slot = slot\n        self.spider = spider\n        yield scheduler.open(spider)\n        yield self.scraper.open_spider(spider)\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        slot.nextcall.schedule()\n        slot.heartbeat.start(5)",
        "begin_line": 255,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00030721966205837174,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._spider_idle#272",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._spider_idle(self, spider)",
        "snippet": "    def _spider_idle(self, spider):\n        \"\"\"Called when a spider gets idle. This function is called when there\n        are no remaining pages to download or schedule. It can be called\n        multiple times. If some extension raises a DontCloseSpider exception\n        (in the spider_idle signal handler) the spider is not closed until the\n        next loop and this function is guaranteed to be called (at least) once\n        again for this spider.\n        \"\"\"\n        res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n            spider=spider, dont_log=DontCloseSpider)\n        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                for _, x in res):\n            return\n\n        if self.spider_is_idle(spider):\n            self.close_spider(spider, reason='finished')",
        "begin_line": 272,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00034281796366129587,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.close_spider#289",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.close_spider(self, spider, reason='cancelled')",
        "snippet": "    def close_spider(self, spider, reason='cancelled'):\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n        slot = self.slot\n        if slot.closing:\n            return slot.closing\n        logger.info(\"Closing spider (%(reason)s)\",\n                    {'reason': reason},\n                    extra={'spider': spider})\n\n        dfd = slot.close()\n\n        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure('Downloader close failure'))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure('Scraper close failure'))\n\n        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n        dfd.addErrback(log_failure('Scheduler close failure'))\n\n        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n            signal=signals.spider_closed, spider=spider, reason=reason))\n        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n        dfd.addErrback(log_failure('Stats close failure'))\n\n        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                          {'reason': reason},\n                                          extra={'spider': spider}))\n\n        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n        dfd.addErrback(log_failure('Error while unassigning slot'))\n\n        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n        dfd.addErrback(log_failure('Error while unassigning spider'))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd",
        "begin_line": 289,
        "end_line": 338,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.log_failure#301",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.log_failure(msg)",
        "snippet": "        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback",
        "begin_line": 301,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.errback#302",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.errback(failure)",
        "snippet": "            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )",
        "begin_line": 302,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._close_all_spiders#340",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._close_all_spiders(self)",
        "snippet": "    def _close_all_spiders(self):\n        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n        dlist = defer.DeferredList(dfds)\n        return dlist",
        "begin_line": 340,
        "end_line": 343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002960331557134399,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._finish_stopping_engine#346",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._finish_stopping_engine(self)",
        "snippet": "    def _finish_stopping_engine(self):\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n        self._closewait.callback(None)",
        "begin_line": 346,
        "end_line": 348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.000315059861373661,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "conftest.chdir#42",
        "src_path": "conftest.py",
        "class_name": "conftest",
        "signature": "conftest.chdir(tmpdir)",
        "snippet": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()",
        "begin_line": 42,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0072992700729927005,
            "pseudo_dstar_susp": 0.0072992700729927005,
            "pseudo_tarantula_susp": 0.0072992700729927005,
            "pseudo_op2_susp": 0.0072992700729927005,
            "pseudo_barinel_susp": 0.0072992700729927005
        }
    },
    {
        "name": "scrapy.selector.unified._st#17",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._st(response, st)",
        "snippet": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    return st",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.00021303792074989347,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified._response_from_text#23",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._response_from_text(text, st)",
        "snippet": "def _response_from_text(text, st):\n    rt = XmlResponse if st == 'xml' else HtmlResponse\n    return rt(url='about:blank', encoding='utf-8',\n              body=to_bytes(text, 'utf-8'))",
        "begin_line": 23,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0002549069589599796,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified.SelectorList.extract_unquoted#31",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SelectorList",
        "signature": "scrapy.selector.unified.SelectorList.extract_unquoted(self)",
        "snippet": "    def extract_unquoted(self):\n        return [x.extract_unquoted() for x in self]",
        "begin_line": 31,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified.SelectorList.select#39",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SelectorList",
        "signature": "scrapy.selector.unified.SelectorList.select(self, xpath)",
        "snippet": "    def select(self, xpath):\n        return self.xpath(xpath)",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.__init__#48",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.__init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs)",
        "snippet": "    def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n\n        st = _st(response, type or self._default_type)\n\n        if _root is not None:\n            warnings.warn(\"Argument `_root` is deprecated, use `root` instead\",\n                          ScrapyDeprecationWarning, stacklevel=2)\n            if root is None:\n                root = _root\n            else:\n                warnings.warn(\"Ignoring deprecated `_root` argument, using provided `root`\")\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            text = response.text\n            kwargs.setdefault('base_url', response.url)\n\n        self.response = response\n        super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)",
        "begin_line": 48,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.select#81",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.select(self, xpath)",
        "snippet": "    def select(self, xpath):\n        return self.xpath(xpath)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.extract_unquoted#85",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.extract_unquoted(self)",
        "snippet": "    def extract_unquoted(self):\n        return self.extract()",
        "begin_line": 85,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.BlockingFeedStorage.open#49",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.BlockingFeedStorage",
        "signature": "scrapy.extensions.feedexport.BlockingFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        path = spider.crawler.settings['FEED_TEMPDIR']\n        if path and not os.path.isdir(path):\n            raise OSError('Not a Directory: ' + str(path))\n\n        return NamedTemporaryFile(prefix='feed-', dir=path)",
        "begin_line": 49,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__#66",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__(self, uri, _stdout=None)",
        "snippet": "    def __init__(self, uri, _stdout=None):\n        if not _stdout:\n            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\n        self._stdout = _stdout",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.open#71",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        return self._stdout",
        "begin_line": 71,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.store#74",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        pass",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.__init__#81",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004224757076468103,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.open#84",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')",
        "begin_line": 84,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.store#90",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        file.close()",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0004914004914004914,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.SpiderSlot.__init__#149",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.SpiderSlot",
        "signature": "scrapy.extensions.feedexport.SpiderSlot.__init__(self, file, exporter, storage, uri)",
        "snippet": "    def __init__(self, file, exporter, storage, uri):\n        self.file = file\n        self.exporter = exporter\n        self.storage = storage\n        self.uri = uri\n        self.itemcount = 0",
        "begin_line": 149,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.__init__#159",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.settings = settings\n        self.urifmt = settings['FEED_URI']\n        if not self.urifmt:\n            raise NotConfigured\n        self.format = settings['FEED_FORMAT'].lower()\n        self.export_encoding = settings['FEED_EXPORT_ENCODING']\n        self.storages = self._load_components('FEED_STORAGES')\n        self.exporters = self._load_components('FEED_EXPORTERS')\n        if not self._storage_supported(self.urifmt):\n            raise NotConfigured\n        if not self._exporter_supported(self.format):\n            raise NotConfigured\n        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n        self._exporting = False\n        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n        uripar = settings['FEED_URI_PARAMS']\n        self._uripar = load_object(uripar) if uripar else lambda x, y: None",
        "begin_line": 159,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.from_crawler#179",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings)\n        crawler.signals.connect(o.open_spider, signals.spider_opened)\n        crawler.signals.connect(o.close_spider, signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n        return o",
        "begin_line": 179,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.open_spider#186",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        uri = self.urifmt % self._get_uri_params(spider)\n        storage = self._get_storage(uri)\n        file = storage.open(spider)\n        exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n            encoding=self.export_encoding)\n        if self.store_empty:\n            exporter.start_exporting()\n            self._exporting = True\n        self.slot = SpiderSlot(file, exporter, storage, uri)",
        "begin_line": 186,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.close_spider#197",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        slot = self.slot\n        if not slot.itemcount and not self.store_empty:\n            return\n        if self._exporting:\n            slot.exporter.finish_exporting()\n            self._exporting = False\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': self.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        d = defer.maybeDeferred(slot.storage.store, slot.file)\n        d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                            extra={'spider': spider}))\n        d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d",
        "begin_line": 197,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._load_components#225",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._load_components(self, setting_prefix)",
        "snippet": "    def _load_components(self, setting_prefix):\n        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d",
        "begin_line": 225,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._exporter_supported#235",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._exporter_supported(self, format)",
        "snippet": "    def _exporter_supported(self, format):\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})",
        "begin_line": 235,
        "end_line": 238,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._storage_supported#240",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._storage_supported(self, uri)",
        "snippet": "    def _storage_supported(self, uri):\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages:\n            try:\n                self._get_storage(uri)\n                return True\n            except NotConfigured:\n                logger.error(\"Disabled feed storage scheme: %(scheme)s\",\n                             {'scheme': scheme})\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})",
        "begin_line": 240,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_exporter#253",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_exporter(self, *args, **kwargs)",
        "snippet": "    def _get_exporter(self, *args, **kwargs):\n        return self.exporters[self.format](*args, **kwargs)",
        "begin_line": 253,
        "end_line": 254,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_storage#256",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_storage(self, uri)",
        "snippet": "    def _get_storage(self, uri):\n        return self.storages[urlparse(uri).scheme](uri)",
        "begin_line": 256,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_uri_params#259",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_uri_params(self, spider)",
        "snippet": "    def _get_uri_params(self, spider):\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n        params['time'] = ts\n        self._uripar(params, spider)\n        return params",
        "begin_line": 259,
        "end_line": 266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003756574004507889,
            "pseudo_dstar_susp": 0.0003756574004507889,
            "pseudo_tarantula_susp": 0.0003756574004507889,
            "pseudo_op2_susp": 0.0008,
            "pseudo_barinel_susp": 0.0003756574004507889
        }
    }
]