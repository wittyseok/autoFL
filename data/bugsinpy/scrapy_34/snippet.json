[
    {
        "name": "scrapy.utils.deprecate.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0019305019305019305,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__new__#55",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__new__(metacls, name, bases, clsdict_)",
        "snippet": "        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0019305019305019305,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__init__#61",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__init__(cls, name, bases, clsdict_)",
        "snippet": "        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002242152466367713,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__#77",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__(cls, inst)",
        "snippet": "        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__#81",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__(cls, sub)",
        "snippet": "        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)",
        "begin_line": 81,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__call__#95",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__call__(cls, *args, **kwargs)",
        "snippet": "        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.deprecate._clspath#120",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate._clspath(cls, forced=None)",
        "snippet": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "begin_line": 120,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.default.UrlContract.adjust_request_args#16",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.UrlContract",
        "signature": "scrapy.contracts.default.UrlContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.__init__#42",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ReturnsContract, self).__init__(*args, **kwargs)\n\n        assert len(self.args) in [1, 2, 3]\n        self.obj_name = self.args[0] or None\n        self.obj_type = self.objects[self.obj_name]\n\n        try:\n            self.min_bound = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound = int(self.args[2])\n        except IndexError:\n            self.max_bound = float('inf')",
        "begin_line": 42,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.post_process#59",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if isinstance(x, self.obj_type):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = '%s..%s' % (self.min_bound, self.max_bound)\n\n            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n                (occurrences, self.obj_name, expected))",
        "begin_line": 59,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.default.ScrapesContract.post_process#84",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ScrapesContract",
        "signature": "scrapy.contracts.default.ScrapesContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        for x in output:\n            if isinstance(x, (BaseItem, dict)):\n                for arg in self.args:\n                    if not arg in x:\n                        raise ContractFail(\"'%s' field is missing\" % arg)",
        "begin_line": 84,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.item.ItemMeta.__new__#26",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.ItemMeta",
        "signature": "scrapy.item.ItemMeta.__new__(mcs, class_name, bases, attrs)",
        "snippet": "    def __new__(mcs, class_name, bases, attrs):\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = {}\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)",
        "begin_line": 26,
        "end_line": 41,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.06666666666666667,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.06666666666666667
        }
    },
    {
        "name": "scrapy.item.DictItem.__init__#48",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._values = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in six.iteritems(dict(*args, **kwargs)):\n                self[k] = v",
        "begin_line": 48,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.16666666666666666,
            "pseudo_dstar_susp": 0.5,
            "pseudo_tarantula_susp": 0.2,
            "pseudo_op2_susp": 0.5,
            "pseudo_barinel_susp": 0.2
        }
    },
    {
        "name": "scrapy.item.DictItem.__getitem__#54",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._values[key]",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.03125,
            "pseudo_dstar_susp": 0.03125,
            "pseudo_tarantula_susp": 0.038461538461538464,
            "pseudo_op2_susp": 0.029411764705882353,
            "pseudo_barinel_susp": 0.038461538461538464
        }
    },
    {
        "name": "scrapy.item.DictItem.__setitem__#57",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(\"%s does not support field: %s\" %\n                (self.__class__.__name__, key))",
        "begin_line": 57,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.5,
            "pseudo_dstar_susp": 0.0625,
            "pseudo_tarantula_susp": 0.3333333333333333,
            "pseudo_op2_susp": 0.0625,
            "pseudo_barinel_susp": 0.3333333333333333
        }
    },
    {
        "name": "scrapy.item.DictItem.__getattr__#67",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(\"Use item[%r] to get field value\" % name)\n        raise AttributeError(name)",
        "begin_line": 67,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.item.DictItem.__setattr__#72",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                (name, value))\n        super(DictItem, self).__setattr__(name, value)",
        "begin_line": 72,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.04,
            "pseudo_dstar_susp": 0.05,
            "pseudo_tarantula_susp": 0.03333333333333333,
            "pseudo_op2_susp": 0.05,
            "pseudo_barinel_susp": 0.03333333333333333
        }
    },
    {
        "name": "scrapy.item.DictItem.__len__#78",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._values)",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.item.DictItem.__iter__#81",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self._values)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0022935779816513763,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.item.DictItem.keys#86",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.keys(self)",
        "snippet": "    def keys(self):\n        return self._values.keys()",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.034482758620689655,
            "pseudo_dstar_susp": 0.02631578947368421,
            "pseudo_tarantula_susp": 0.14285714285714285,
            "pseudo_op2_susp": 0.02631578947368421,
            "pseudo_barinel_susp": 0.14285714285714285
        }
    },
    {
        "name": "scrapy.item.DictItem.__repr__#89",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return pformat(dict(self))",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.027777777777777776,
            "pseudo_tarantula_susp": 1.0,
            "pseudo_op2_susp": 0.027777777777777776,
            "pseudo_barinel_susp": 1.0
        }
    },
    {
        "name": "scrapy.item.DictItem.copy#92",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.copy(self)",
        "snippet": "    def copy(self):\n        return self.__class__(self)",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.gz.gunzip#10",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gunzip(data)",
        "snippet": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = f.read(8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or f.extrabuf:\n                output += f.extrabuf\n                break\n            else:\n                raise\n    return output",
        "begin_line": 10,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.url.escape_ajax#79",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.escape_ajax(url)",
        "snippet": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])",
        "begin_line": 79,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002061855670103093,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.misc.arg_to_iter#17",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.arg_to_iter(arg)",
        "snippet": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]",
        "begin_line": 17,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.misc.load_object#31",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.load_object(path)",
        "snippet": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    object can be a class, function, variable o instance.\n    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n    \"\"\"\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n\n    module, name = path[:dot], path[dot+1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n\n    return obj",
        "begin_line": 31,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.misc.walk_modules#54",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.walk_modules(path)",
        "snippet": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from a the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods",
        "begin_line": 54,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__init__#18",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)",
        "snippet": "    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None",
        "begin_line": 18,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002061855670103093,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.meta#40",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.meta(self)",
        "snippet": "    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta",
        "begin_line": 40,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002398081534772182,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_url#45",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_url#48",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = escape_ajax(safe_url_string(url))\n        elif isinstance(url, six.text_type):\n            if self.encoding is None:\n                raise TypeError('Cannot convert unicode url - %s has no encoding' %\n                                type(self).__name__)\n            self._set_url(url.encode(self.encoding))\n        else:\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)",
        "begin_line": 48,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002061855670103093,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_body#66",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if isinstance(body, str):\n            self._body = body\n        elif isinstance(body, six.text_type):\n            if self.encoding is None:\n                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                                type(self).__name__)\n            self._body = body.encode(self.encoding)\n        elif body is None:\n            self._body = ''\n        else:\n            raise TypeError(\"Request body must either str or unicode. Got: '%s'\" % type(body).__name__)",
        "begin_line": 66,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002061855670103093,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.link.Link.__init__#15",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__init__(self, url, text='', fragment='', nofollow=False)",
        "snippet": "    def __init__(self, url, text='', fragment='', nofollow=False):\n        if isinstance(url, six.text_type):\n            import warnings\n            warnings.warn(\"Do not instantiate Link objects with unicode urls. \"\n                \"Assuming utf-8 encoding (which could be wrong)\")\n            url = url.encode('utf-8')\n        self.url = url\n        self.text = text\n        self.fragment = fragment\n        self.nofollow = nofollow",
        "begin_line": 15,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.link.Link.__eq__#26",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self.url == other.url and self.text == other.text and \\\n            self.fragment == other.fragment and self.nofollow == other.nofollow",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.link.Link.__hash__#30",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.link.Link.__repr__#33",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n            (self.url, self.text, self.fragment, self.nofollow)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__init__#14",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__init__(self, xmltext)",
        "snippet": "    def __init__(self, xmltext):\n        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002242152466367713,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__iter__#20",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__iter__(self)",
        "snippet": "    def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d",
        "begin_line": 20,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.sitemap.sitemap_urls_from_robots#37",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap",
        "signature": "scrapy.utils.sitemap.sitemap_urls_from_robots(robots_text)",
        "snippet": "def sitemap_urls_from_robots(robots_text):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().startswith('Sitemap:'):\n            yield line.split(':', 1)[1].strip()",
        "begin_line": 37,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.spider.iterate_spider_output#12",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iterate_spider_output(result)",
        "snippet": "def iterate_spider_output(result):\n    return arg_to_iter(result)",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.spider.iter_spider_classes#16",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iter_spider_classes(module)",
        "snippet": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (ie. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spider import Spider\n\n    for obj in six.itervalues(vars(module)):\n        if inspect.isclass(obj) and \\\n           issubclass(obj, Spider) and \\\n           obj.__module__ == module.__name__ and \\\n           getattr(obj, 'name', None):\n            yield obj",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.__init__#14",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.__init__(self, contracts)",
        "snippet": "    def __init__(self, contracts):\n        for contract in contracts:\n            self.contracts[contract.name] = contract",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.extract_contracts#27",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.extract_contracts(self, method)",
        "snippet": "    def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts",
        "begin_line": 27,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_method#48",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_method(self, method, results)",
        "snippet": "    def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            # calculate request args\n            args, kwargs = get_spec(Request.__init__)\n            kwargs['callback'] = method\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            # create and prepare request\n            args.remove('self')\n            if set(args).issubset(set(kwargs)):\n                request = Request(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request",
        "begin_line": 48,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager._clean_req#71",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager._clean_req(self, request, method, results)",
        "snippet": "    def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper",
        "begin_line": 71,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.cb_wrapper#77",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.cb_wrapper(response)",
        "snippet": "        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.eb_wrapper#85",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.eb_wrapper(failure)",
        "snippet": "        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)",
        "begin_line": 85,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.__init__#97",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.__init__(self, method, *args)",
        "snippet": "    def __init__(self, method, *args):\n        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n        self.args = args",
        "begin_line": 97,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_pre_hook#102",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_pre_hook(self, request, results)",
        "snippet": "    def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response)))\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 102,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_post_hook#125",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_post_hook(self, request, results)",
        "snippet": "    def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 125,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.wrapper#130",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.wrapper(response)",
        "snippet": "            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output",
        "begin_line": 130,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.adjust_request_args#149",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        return args",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__",
        "signature": "scrapy.contracts.__init__._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002347417840375587,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase.__str__#157",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase.__str__(_self)",
        "snippet": "        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__init__#18",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__init__(self, url, status=200, headers=None, body='', flags=None, request=None)",
        "snippet": "    def __init__(self, url, status=200, headers=None, body='', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 18,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_url#37",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__, \\\n                type(url).__name__))",
        "begin_line": 37,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_body#49",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if isinstance(body, str):\n            self._body = body\n        elif isinstance(body, unicode):\n            raise TypeError(\"Cannot assign a unicode body to a raw Response. \" \\\n                \"Use TextResponse, HtmlResponse, etc\")\n        elif body is None:\n            self._body = ''\n        else:\n            raise TypeError(\"Response body must either be str or unicode. Got: '%s'\" \\\n                % type(body).__name__)",
        "begin_line": 49,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.spider.Spider.__init__#25",
        "src_path": "scrapy/spider.py",
        "class_name": "scrapy.spider.Spider",
        "signature": "scrapy.spider.Spider.__init__(self, name=None, **kwargs)",
        "snippet": "    def __init__(self, name=None, **kwargs):\n        if name is not None:\n            self.name = name\n        elif not getattr(self, 'name', None):\n            raise ValueError(\"%s must have a name\" % type(self).__name__)\n        self.__dict__.update(kwargs)\n        if not hasattr(self, 'start_urls'):\n            self.start_urls = []",
        "begin_line": 25,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.python.get_spec#167",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_spec(func)",
        "snippet": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test(object):\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getargspec(func)\n    elif hasattr(func, '__call__'):\n        spec = inspect.getargspec(func.__call__)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs",
        "begin_line": 167,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.__init__#27",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.__init__(self, item=None, selector=None, response=None, **context)",
        "snippet": "    def __init__(self, item=None, selector=None, response=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n        self.selector = selector\n        context.update(selector=selector, response=response)\n        if item is None:\n            item = self.default_item_class()\n        self.item = context['item'] = item\n        self.context = context\n        self._values = defaultdict(list)",
        "begin_line": 27,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002398081534772182,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.httpobj.urlparse_cached#8",
        "src_path": "scrapy/utils/httpobj.py",
        "class_name": "scrapy.utils.httpobj",
        "signature": "scrapy.utils.httpobj.urlparse_cached(request_or_response)",
        "snippet": "def urlparse_cached(request_or_response):\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]",
        "begin_line": 8,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__init__#9",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__init__(self, seq=None, encoding='utf-8')",
        "snippet": "    def __init__(self, seq=None, encoding='utf-8'):\n        self.encoding = encoding\n        super(Headers, self).__init__(seq)",
        "begin_line": 9,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0018148820326678765,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normkey#13",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())",
        "begin_line": 13,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0018656716417910447,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normvalue#17",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (six.text_type, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]",
        "begin_line": 17,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers._tobytes#28",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers._tobytes(self, x)",
        "snippet": "    def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, six.text_type):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return six.text_type(x).encode(self.encoding)\n        else:\n            raise TypeError('Unsupported value type: {}'.format(type(x)))",
        "begin_line": 28,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__getitem__#38",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        try:\n            return super(Headers, self).__getitem__(key)[-1]\n        except IndexError:\n            return None",
        "begin_line": 38,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.get#44",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        try:\n            return super(Headers, self).get(key, def_val)[-1]\n        except IndexError:\n            return None",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.getlist#50",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.getlist(self, key, def_val=None)",
        "snippet": "    def getlist(self, key, def_val=None):\n        try:\n            return super(Headers, self).__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []",
        "begin_line": 50,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlist#58",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlist(self, key, list_)",
        "snippet": "    def setlist(self, key, list_):\n        self[key] = list_",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0029498525073746312,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlistdefault#61",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlistdefault(self, key, default_list=())",
        "snippet": "    def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.appendlist#64",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.appendlist(self, key, value)",
        "snippet": "    def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst",
        "begin_line": 64,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.items#69",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.items(self)",
        "snippet": "    def items(self):\n        return list(self.iteritems())",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.iteritems#72",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.iteritems(self)",
        "snippet": "    def iteritems(self):\n        return ((k, self.getlist(k)) for k in self.keys())",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.values#75",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.values(self)",
        "snippet": "    def values(self):\n        return [self[k] for k in self.keys()]",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__copy__#81",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__#17",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__(self, maxlength)",
        "snippet": "    def __init__(self, maxlength):\n        self.maxlength = maxlength",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output#27",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter#28",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.conf.build_component_list#9",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.build_component_list(base, custom)",
        "snippet": "def build_component_list(base, custom):\n    \"\"\"Compose a component list based on a custom and base dict of components\n    (typically middlewares or extensions), unless custom is already a list, in\n    which case it's returned.\n    \"\"\"\n    if isinstance(custom, (list, tuple)):\n        return custom\n    compdict = base.copy()\n    compdict.update(custom)\n    items = (x for x in six.iteritems(compdict) if x[1] is not None)\n    return [x[0] for x in sorted(items, key=itemgetter(1))]",
        "begin_line": 9,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.conf.arglist_to_dict#22",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.arglist_to_dict(arglist)",
        "snippet": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)",
        "begin_line": 22,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.__init__#10",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.__init__(self, jobdir=None)",
        "snippet": "    def __init__(self, jobdir=None):\n        self.jobdir = jobdir",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_closed#20",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=2)",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_opened#25",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}",
        "begin_line": 25,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.statefn#33",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.statefn(self)",
        "snippet": "    def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.http.decode_chunked_transfer#9",
        "src_path": "scrapy/utils/http.py",
        "class_name": "scrapy.utils.http",
        "signature": "scrapy.utils.http.decode_chunked_transfer(chunked_body)",
        "snippet": "def decode_chunked_transfer(chunked_body):\n    \"\"\"Parsed body received with chunked transfer encoding, and return the\n    decoded body.\n\n    For more info see:\n    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n    \"\"\"\n    body, h, t = '', '', chunked_body\n    while t:\n        h, t = t.split('\\r\\n', 1)\n        if h == '0':\n            break\n        size = int(h, 16)\n        body += t[:size]\n        t = t[size+2:]\n    return body",
        "begin_line": 9,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__init__#167",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__init__(self, seq=None)",
        "snippet": "    def __init__(self, seq=None):\n        super(CaselessDict, self).__init__()\n        if seq:\n            self.update(seq)",
        "begin_line": 167,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0018656716417910447,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__getitem__#172",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))",
        "begin_line": 172,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.001838235294117647,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__setitem__#175",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))",
        "begin_line": 175,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0018975332068311196,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__delitem__#178",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))",
        "begin_line": 178,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__contains__#181",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))",
        "begin_line": 181,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__copy__#185",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 185,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normkey#189",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()",
        "begin_line": 189,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0021691973969631237,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normvalue#193",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value",
        "begin_line": 193,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0021691973969631237,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.get#197",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 197,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0022935779816513763,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.setdefault#200",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.setdefault(self, key, def_val=None)",
        "snippet": "    def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 200,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.update#203",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.update(self, seq)",
        "snippet": "    def update(self, seq):\n        seq = seq.items() if isinstance(seq, dict) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super(CaselessDict, self).update(iseq)",
        "begin_line": 203,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.001838235294117647,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.fromkeys#209",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.fromkeys(cls, keys, value=None)",
        "snippet": "    def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)",
        "begin_line": 209,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0025252525252525255,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.pop#212",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)",
        "begin_line": 212,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.push#14",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.push(self, obj)",
        "snippet": "        def push(self, obj):\n            s = serialize(obj)\n            super(SerializableQueue, self).push(s)",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0017825311942959,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.pop#18",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.pop(self)",
        "snippet": "        def pop(self):\n            s = super(SerializableQueue, self).pop()\n            if s:\n                return deserialize(s)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0017825311942959,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.squeues._pickle_serialize#25",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues",
        "signature": "scrapy.squeues._pickle_serialize(obj)",
        "snippet": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=2)\n    except pickle.PicklingError as e:\n        raise ValueError(str(e))",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0017889087656529517,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "conftest.chdir#30",
        "src_path": "conftest.py",
        "class_name": "conftest",
        "signature": "conftest.chdir(tmpdir)",
        "snippet": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()",
        "begin_line": 30,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02631578947368421,
            "pseudo_dstar_susp": 0.029411764705882353,
            "pseudo_tarantula_susp": 0.02631578947368421,
            "pseudo_op2_susp": 0.03571428571428571,
            "pseudo_barinel_susp": 0.02631578947368421
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__#59",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__(self, uri, _stdout=sys.stdout)",
        "snippet": "    def __init__(self, uri, _stdout=sys.stdout):\n        self._stdout = _stdout",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.open#62",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        return self._stdout",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.store#65",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        pass",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.__init__#72",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.002398081534772182,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.open#75",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')",
        "begin_line": 75,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.007936507936507936,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.store#81",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        file.close()",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0025252525252525255,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "scrapy.utils.trackref.object_ref.__new__#28",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref.object_ref",
        "signature": "scrapy.utils.trackref.object_ref.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02857142857142857,
            "pseudo_dstar_susp": 0.04,
            "pseudo_tarantula_susp": 0.02857142857142857,
            "pseudo_op2_susp": 0.04,
            "pseudo_barinel_susp": 0.02857142857142857
        }
    }
]