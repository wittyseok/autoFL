[
    {
        "name": "keras.utils.data_utils._extract_archive#76",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils._extract_archive(file_path, path='.', archive_format='auto')",
        "snippet": "def _extract_archive(file_path, path='.', archive_format='auto'):\n    \"\"\"Extracts an archive if it matches tar, tar.gz, tar.bz, or zip formats.\n\n    # Arguments\n        file_path: path to the archive file\n        path: path to extract the archive file\n        archive_format: Archive format to try for extracting the file.\n            Options are 'auto', 'tar', 'zip', and None.\n            'tar' includes tar, tar.gz, and tar.bz files.\n            The default 'auto' is ['tar', 'zip'].\n            None or an empty list will return no matches found.\n\n    # Returns\n        True if a match was found and an archive extraction was completed,\n        False otherwise.\n    \"\"\"\n    if archive_format is None:\n        return False\n    if archive_format is 'auto':\n        archive_format = ['tar', 'zip']\n    if isinstance(archive_format, six.string_types):\n        archive_format = [archive_format]\n\n    for archive_type in archive_format:\n        if archive_type is 'tar':\n            open_fn = tarfile.open\n            is_match_fn = tarfile.is_tarfile\n        if archive_type is 'zip':\n            open_fn = zipfile.ZipFile\n            is_match_fn = zipfile.is_zipfile\n\n        if is_match_fn(file_path):\n            with open_fn(file_path) as archive:\n                try:\n                    archive.extractall(path)\n                except (tarfile.TarError, RuntimeError,\n                        KeyboardInterrupt):\n                    if os.path.exists(path):\n                        if os.path.isfile(path):\n                            os.remove(path)\n                        else:\n                            shutil.rmtree(path)\n                    raise\n            return True\n    return False",
        "begin_line": 76,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.get_file#123",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None)",
        "snippet": "def get_file(fname,\n             origin,\n             untar=False,\n             md5_hash=None,\n             file_hash=None,\n             cache_subdir='datasets',\n             hash_algorithm='auto',\n             extract=False,\n             archive_format='auto',\n             cache_dir=None):\n    \"\"\"Downloads a file from a URL if it not already in the cache.\n\n    By default the file at the url `origin` is downloaded to the\n    cache_dir `~/.keras`, placed in the cache_subdir `datasets`,\n    and given the filename `fname`. The final location of a file\n    `example.txt` would therefore be `~/.keras/datasets/example.txt`.\n\n    Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.\n    Passing a hash will verify the file after download. The command line\n    programs `shasum` and `sha256sum` can compute the hash.\n\n    # Arguments\n        fname: Name of the file. If an absolute path `/path/to/file.txt` is\n            specified the file will be saved at that location.\n        origin: Original URL of the file.\n        untar: Deprecated in favor of 'extract'.\n            boolean, whether the file should be decompressed\n        md5_hash: Deprecated in favor of 'file_hash'.\n            md5 hash of the file for verification\n        file_hash: The expected hash string of the file after download.\n            The sha256 and md5 hash algorithms are both supported.\n        cache_subdir: Subdirectory under the Keras cache dir where the file is\n            saved. If an absolute path `/path/to/folder` is\n            specified the file will be saved at that location.\n        hash_algorithm: Select the hash algorithm to verify the file.\n            options are 'md5', 'sha256', and 'auto'.\n            The default 'auto' detects the hash algorithm in use.\n        extract: True tries extracting the file as an Archive, like tar or zip.\n        archive_format: Archive format to try for extracting the file.\n            Options are 'auto', 'tar', 'zip', and None.\n            'tar' includes tar, tar.gz, and tar.bz files.\n            The default 'auto' is ['tar', 'zip'].\n            None or an empty list will return no matches found.\n        cache_dir: Location to store cached files, when None it\n            defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).\n\n    # Returns\n        Path to the downloaded file\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser('~'), '.keras')\n    if md5_hash is not None and file_hash is None:\n        file_hash = md5_hash\n        hash_algorithm = 'md5'\n    datadir_base = os.path.expanduser(cache_dir)\n    if not os.access(datadir_base, os.W_OK):\n        datadir_base = os.path.join('/tmp', '.keras')\n    datadir = os.path.join(datadir_base, cache_subdir)\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    if untar:\n        untar_fpath = os.path.join(datadir, fname)\n        fpath = untar_fpath + '.tar.gz'\n    else:\n        fpath = os.path.join(datadir, fname)\n\n    download = False\n    if os.path.exists(fpath):\n        # File found; verify integrity if a hash was provided.\n        if file_hash is not None:\n            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n                print('A local file was found, but it seems to be '\n                      'incomplete or outdated because the ' + hash_algorithm +\n                      ' file hash does not match the original value of ' +\n                      file_hash + ' so we will re-download the data.')\n                download = True\n    else:\n        download = True\n\n    if download:\n        print('Downloading data from', origin)\n\n        class ProgressTracker(object):\n            # Maintain progbar for the lifetime of download.\n            # This design was chosen for Python 2.7 compatibility.\n            progbar = None\n\n        def dl_progress(count, block_size, total_size):\n            if ProgressTracker.progbar is None:\n                if total_size is -1:\n                    total_size = None\n                ProgressTracker.progbar = Progbar(total_size)\n            else:\n                ProgressTracker.progbar.update(count * block_size)\n\n        error_msg = 'URL fetch failure on {}: {} -- {}'\n        try:\n            try:\n                urlretrieve(origin, fpath, dl_progress)\n            except URLError as e:\n                raise Exception(error_msg.format(origin, e.errno, e.reason))\n            except HTTPError as e:\n                raise Exception(error_msg.format(origin, e.code, e.msg))\n        except (Exception, KeyboardInterrupt):\n            if os.path.exists(fpath):\n                os.remove(fpath)\n            raise\n        ProgressTracker.progbar = None\n\n    if untar:\n        if not os.path.exists(untar_fpath):\n            _extract_archive(fpath, datadir, archive_format='tar')\n        return untar_fpath\n\n    if extract:\n        _extract_archive(fpath, datadir, archive_format)\n\n    return fpath",
        "begin_line": 123,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.ProgressTracker.get_file#123",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.ProgressTracker",
        "signature": "keras.utils.data_utils.ProgressTracker.get_file(fname, origin, untar=False, md5_hash=None, file_hash=None, cache_subdir='datasets', hash_algorithm='auto', extract=False, archive_format='auto', cache_dir=None)",
        "snippet": "def get_file(fname,\n             origin,\n             untar=False,\n             md5_hash=None,\n             file_hash=None,\n             cache_subdir='datasets',\n             hash_algorithm='auto',\n             extract=False,\n             archive_format='auto',\n             cache_dir=None):\n    \"\"\"Downloads a file from a URL if it not already in the cache.\n\n    By default the file at the url `origin` is downloaded to the\n    cache_dir `~/.keras`, placed in the cache_subdir `datasets`,\n    and given the filename `fname`. The final location of a file\n    `example.txt` would therefore be `~/.keras/datasets/example.txt`.\n\n    Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.\n    Passing a hash will verify the file after download. The command line\n    programs `shasum` and `sha256sum` can compute the hash.\n\n    # Arguments\n        fname: Name of the file. If an absolute path `/path/to/file.txt` is\n            specified the file will be saved at that location.\n        origin: Original URL of the file.\n        untar: Deprecated in favor of 'extract'.\n            boolean, whether the file should be decompressed\n        md5_hash: Deprecated in favor of 'file_hash'.\n            md5 hash of the file for verification\n        file_hash: The expected hash string of the file after download.\n            The sha256 and md5 hash algorithms are both supported.\n        cache_subdir: Subdirectory under the Keras cache dir where the file is\n            saved. If an absolute path `/path/to/folder` is\n            specified the file will be saved at that location.\n        hash_algorithm: Select the hash algorithm to verify the file.\n            options are 'md5', 'sha256', and 'auto'.\n            The default 'auto' detects the hash algorithm in use.\n        extract: True tries extracting the file as an Archive, like tar or zip.\n        archive_format: Archive format to try for extracting the file.\n            Options are 'auto', 'tar', 'zip', and None.\n            'tar' includes tar, tar.gz, and tar.bz files.\n            The default 'auto' is ['tar', 'zip'].\n            None or an empty list will return no matches found.\n        cache_dir: Location to store cached files, when None it\n            defaults to the [Keras Directory](/faq/#where-is-the-keras-configuration-filed-stored).\n\n    # Returns\n        Path to the downloaded file\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser('~'), '.keras')\n    if md5_hash is not None and file_hash is None:\n        file_hash = md5_hash\n        hash_algorithm = 'md5'\n    datadir_base = os.path.expanduser(cache_dir)\n    if not os.access(datadir_base, os.W_OK):\n        datadir_base = os.path.join('/tmp', '.keras')\n    datadir = os.path.join(datadir_base, cache_subdir)\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    if untar:\n        untar_fpath = os.path.join(datadir, fname)\n        fpath = untar_fpath + '.tar.gz'\n    else:\n        fpath = os.path.join(datadir, fname)\n\n    download = False\n    if os.path.exists(fpath):\n        # File found; verify integrity if a hash was provided.\n        if file_hash is not None:\n            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n                print('A local file was found, but it seems to be '\n                      'incomplete or outdated because the ' + hash_algorithm +\n                      ' file hash does not match the original value of ' +\n                      file_hash + ' so we will re-download the data.')\n                download = True\n    else:\n        download = True\n\n    if download:\n        print('Downloading data from', origin)\n\n        class ProgressTracker(object):\n            # Maintain progbar for the lifetime of download.\n            # This design was chosen for Python 2.7 compatibility.\n            progbar = None\n\n        def dl_progress(count, block_size, total_size):\n            if ProgressTracker.progbar is None:\n                if total_size is -1:\n                    total_size = None\n                ProgressTracker.progbar = Progbar(total_size)\n            else:\n                ProgressTracker.progbar.update(count * block_size)\n\n        error_msg = 'URL fetch failure on {}: {} -- {}'\n        try:\n            try:\n                urlretrieve(origin, fpath, dl_progress)\n            except URLError as e:\n                raise Exception(error_msg.format(origin, e.errno, e.reason))\n            except HTTPError as e:\n                raise Exception(error_msg.format(origin, e.code, e.msg))\n        except (Exception, KeyboardInterrupt):\n            if os.path.exists(fpath):\n                os.remove(fpath)\n            raise\n        ProgressTracker.progbar = None\n\n    if untar:\n        if not os.path.exists(untar_fpath):\n            _extract_archive(fpath, datadir, archive_format='tar')\n        return untar_fpath\n\n    if extract:\n        _extract_archive(fpath, datadir, archive_format)\n\n    return fpath",
        "begin_line": 123,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.dl_progress#211",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.dl_progress(count, block_size, total_size)",
        "snippet": "        def dl_progress(count, block_size, total_size):\n            if ProgressTracker.progbar is None:\n                if total_size is -1:\n                    total_size = None\n                ProgressTracker.progbar = Progbar(total_size)\n            else:\n                ProgressTracker.progbar.update(count * block_size)",
        "begin_line": 211,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils._hash_file#244",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils._hash_file(fpath, algorithm='sha256', chunk_size=65535)",
        "snippet": "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n    \"\"\"Calculates a file sha256 or md5 hash.\n\n    # Example\n\n    ```python\n        >>> from keras.data_utils import _hash_file\n        >>> _hash_file('/path/to/file.zip')\n        'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n    ```\n\n    # Arguments\n        fpath: path to the file being validated\n        algorithm: hash algorithm, one of 'auto', 'sha256', or 'md5'.\n            The default 'auto' detects the hash algorithm in use.\n        chunk_size: Bytes to read at a time, important for large files.\n\n    # Returns\n        The file hash\n    \"\"\"\n    if (algorithm is 'sha256') or (algorithm is 'auto' and len(hash) is 64):\n        hasher = hashlib.sha256()\n    else:\n        hasher = hashlib.md5()\n\n    with open(fpath, 'rb') as fpath_file:\n        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n            hasher.update(chunk)\n\n    return hasher.hexdigest()",
        "begin_line": 244,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.validate_file#276",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535)",
        "snippet": "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n    \"\"\"Validates a file against a sha256 or md5 hash.\n\n    # Arguments\n        fpath: path to the file being validated\n        file_hash:  The expected hash string of the file.\n            The sha256 and md5 hash algorithms are both supported.\n        algorithm: Hash algorithm, one of 'auto', 'sha256', or 'md5'.\n            The default 'auto' detects the hash algorithm in use.\n        chunk_size: Bytes to read at a time, important for large files.\n\n    # Returns\n        Whether the file is valid\n    \"\"\"\n    if ((algorithm is 'sha256') or\n            (algorithm is 'auto' and len(file_hash) is 64)):\n        hasher = 'sha256'\n    else:\n        hasher = 'md5'\n\n    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n        return True\n    else:\n        return False",
        "begin_line": 276,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.init_pool#382",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.init_pool(seqs)",
        "snippet": "def init_pool(seqs):\n    global _SHARED_SEQUENCES\n    _SHARED_SEQUENCES = seqs",
        "begin_line": 382,
        "end_line": 384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.058823529411764705,
            "pseudo_dstar_susp": 0.058823529411764705,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.058823529411764705,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.data_utils.get_index#387",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils",
        "signature": "keras.utils.data_utils.get_index(uid, i)",
        "snippet": "def get_index(uid, i):\n    \"\"\"Get the value from the Sequence `uid` at index `i`.\n\n    To allow multiple Sequences to be used at the same time, we use `uid` to\n    get a specific one. A single Sequence would cause the validation to\n    overwrite the training Sequence.\n\n    # Arguments\n        uid: int, Sequence identifier\n        i: index\n\n    # Returns\n        The value at index `i`.\n    \"\"\"\n    return _SHARED_SEQUENCES[uid][i]",
        "begin_line": 387,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009689922480620155,
            "pseudo_dstar_susp": 0.0009689922480620155,
            "pseudo_tarantula_susp": 0.0009689922480620155,
            "pseudo_op2_susp": 0.0009689922480620155,
            "pseudo_barinel_susp": 0.0009689922480620155
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.__init__#476",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.__init__(self, sequence, use_multiprocessing=False, shuffle=False)",
        "snippet": "    def __init__(self, sequence,\n                 use_multiprocessing=False,\n                 shuffle=False):\n        self.sequence = sequence\n        self.use_multiprocessing = use_multiprocessing\n\n        global _SEQUENCE_COUNTER\n        if _SEQUENCE_COUNTER is None:\n            try:\n                _SEQUENCE_COUNTER = multiprocessing.Value('i', 0)\n            except OSError:\n                # In this case the OS does not allow us to use\n                # multiprocessing. We resort to an int\n                # for enqueuer indexing.\n                _SEQUENCE_COUNTER = 0\n\n        if isinstance(_SEQUENCE_COUNTER, int):\n            self.uid = _SEQUENCE_COUNTER\n            _SEQUENCE_COUNTER += 1\n        else:\n            # Doing Multiprocessing.Value += x is not process-safe.\n            with _SEQUENCE_COUNTER.get_lock():\n                self.uid = _SEQUENCE_COUNTER.value\n                _SEQUENCE_COUNTER.value += 1\n\n        self.shuffle = shuffle\n        self.workers = 0\n        self.executor_fn = None\n        self.queue = None\n        self.run_thread = None\n        self.stop_signal = None",
        "begin_line": 476,
        "end_line": 506,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02127659574468085,
            "pseudo_dstar_susp": 0.02127659574468085,
            "pseudo_tarantula_susp": 0.00099601593625498,
            "pseudo_op2_susp": 0.02127659574468085,
            "pseudo_barinel_susp": 0.00099601593625498
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.is_running#508",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.is_running(self)",
        "snippet": "    def is_running(self):\n        return self.stop_signal is not None and not self.stop_signal.is_set()",
        "begin_line": 508,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02127659574468085,
            "pseudo_dstar_susp": 0.02127659574468085,
            "pseudo_tarantula_susp": 0.00099601593625498,
            "pseudo_op2_susp": 0.02127659574468085,
            "pseudo_barinel_susp": 0.00099601593625498
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.start#511",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.start(self, workers=1, max_queue_size=10)",
        "snippet": "    def start(self, workers=1, max_queue_size=10):\n        \"\"\"Start the handler's workers.\n\n        # Arguments\n            workers: number of worker threads\n            max_queue_size: queue size\n                (when full, workers could block on `put()`)\n        \"\"\"\n        if self.use_multiprocessing:\n            self.executor_fn = lambda seqs: multiprocessing.Pool(workers,\n                                                                 initializer=init_pool,\n                                                                 initargs=(seqs,))\n        else:\n            # We do not need the init since it's threads.\n            self.executor_fn = lambda _: ThreadPool(workers)\n        self.workers = workers\n        self.queue = queue.Queue(max_queue_size)\n        self.stop_signal = threading.Event()\n        self.run_thread = threading.Thread(target=self._run)\n        self.run_thread.daemon = True\n        self.run_thread.start()",
        "begin_line": 511,
        "end_line": 531,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.3333333333333333,
            "pseudo_dstar_susp": 0.3333333333333333,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.3333333333333333,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._wait_queue#533",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._wait_queue(self)",
        "snippet": "    def _wait_queue(self):\n        \"\"\"Wait for the queue to be empty.\"\"\"\n        while True:\n            time.sleep(0.1)\n            if self.queue.unfinished_tasks == 0 or self.stop_signal.is_set():\n                return",
        "begin_line": 533,
        "end_line": 538,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.00044583147570218456,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._run#540",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._run(self)",
        "snippet": "    def _run(self):\n        \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n        sequence = list(range(len(self.sequence)))\n        self._send_sequence()  # Share the initial sequence\n        while True:\n            if self.shuffle:\n                random.shuffle(sequence)\n\n            with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:\n                for i in sequence:\n                    if self.stop_signal.is_set():\n                        return\n                    self.queue.put(\n                        executor.apply_async(get_index, (self.uid, i)), block=True)\n\n                # Done with the current epoch, waiting for the final batches\n                self._wait_queue()\n\n                if self.stop_signal.is_set():\n                    # We're done\n                    return\n\n            # Call the internal on epoch end.\n            self.sequence.on_epoch_end()\n            self._send_sequence()  # Update the pool",
        "begin_line": 540,
        "end_line": 564,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.047619047619047616,
            "pseudo_dstar_susp": 0.047619047619047616,
            "pseudo_tarantula_susp": 0.004694835680751174,
            "pseudo_op2_susp": 0.047619047619047616,
            "pseudo_barinel_susp": 0.004694835680751174
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.get#566",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.get(self)",
        "snippet": "    def get(self):\n        \"\"\"Creates a generator to extract data from the queue.\n\n        Skip the data if it is `None`.\n\n        # Yields\n            The next element in the queue, i.e. a tuple\n            `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n        \"\"\"\n        try:\n            while self.is_running():\n                inputs = self.queue.get(block=True).get()\n                self.queue.task_done()\n                if inputs is not None:\n                    yield inputs\n        except Exception as e:\n            self.stop()\n            six.raise_from(StopIteration(e), e)",
        "begin_line": 566,
        "end_line": 584,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.047619047619047616,
            "pseudo_dstar_susp": 0.047619047619047616,
            "pseudo_tarantula_susp": 0.004694835680751174,
            "pseudo_op2_susp": 0.047619047619047616,
            "pseudo_barinel_susp": 0.004694835680751174
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer._send_sequence#586",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer._send_sequence(self)",
        "snippet": "    def _send_sequence(self):\n        \"\"\"Send current Sequence to all workers.\"\"\"\n        # For new processes that may spawn\n        _SHARED_SEQUENCES[self.uid] = self.sequence",
        "begin_line": 586,
        "end_line": 589,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02127659574468085,
            "pseudo_dstar_susp": 0.02127659574468085,
            "pseudo_tarantula_susp": 0.00099601593625498,
            "pseudo_op2_susp": 0.02127659574468085,
            "pseudo_barinel_susp": 0.00099601593625498
        }
    },
    {
        "name": "keras.utils.data_utils.OrderedEnqueuer.stop#591",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.OrderedEnqueuer",
        "signature": "keras.utils.data_utils.OrderedEnqueuer.stop(self, timeout=None)",
        "snippet": "    def stop(self, timeout=None):\n        \"\"\"Stops running threads and wait for them to exit, if necessary.\n\n        Should be called by the same thread which called `start()`.\n\n        # Arguments\n            timeout: maximum time to wait on `thread.join()`\n        \"\"\"\n        self.stop_signal.set()\n        with self.queue.mutex:\n            self.queue.queue.clear()\n            self.queue.unfinished_tasks = 0\n            self.queue.not_full.notify()\n        self.run_thread.join(timeout)\n        _SHARED_SEQUENCES[self.uid] = None",
        "begin_line": 591,
        "end_line": 605,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02127659574468085,
            "pseudo_dstar_susp": 0.02127659574468085,
            "pseudo_tarantula_susp": 0.00099601593625498,
            "pseudo_op2_susp": 0.02127659574468085,
            "pseudo_barinel_susp": 0.00099601593625498
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.__init__#624",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.__init__(self, generator, use_multiprocessing=False, wait_time=0.05, seed=None)",
        "snippet": "    def __init__(self, generator,\n                 use_multiprocessing=False,\n                 wait_time=0.05,\n                 seed=None):\n        self.wait_time = wait_time\n        self._generator = generator\n        if os.name is 'nt' and use_multiprocessing is True:\n            # On Windows, avoid **SYSTEMATIC** error in `multiprocessing`:\n            # `TypeError: can't pickle generator objects`\n            # => Suggest multithreading instead of multiprocessing on Windows\n            raise ValueError('Using a generator with `use_multiprocessing=True`'\n                             ' is not supported on Windows (no marshalling of'\n                             ' generators across process boundaries). Instead,'\n                             ' use single thread/process or multithreading.')\n        else:\n            self._use_multiprocessing = use_multiprocessing\n        self._threads = []\n        self._stop_event = None\n        self._manager = None\n        self.queue = None\n        self.seed = seed",
        "begin_line": 624,
        "end_line": 644,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0004399472063352398,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer._data_generator_task#646",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer._data_generator_task(self)",
        "snippet": "    def _data_generator_task(self):\n        if self._use_multiprocessing is False:\n            while not self._stop_event.is_set():\n                with self.genlock:\n                    try:\n                        if (self.queue is not None and\n                                self.queue.qsize() < self.max_queue_size):\n                            # On all OSes, avoid **SYSTEMATIC** error\n                            # in multithreading mode:\n                            # `ValueError: generator already executing`\n                            # => Serialize calls to\n                            # infinite iterator/generator's next() function\n                            generator_output = next(self._generator)\n                            self.queue.put((True, generator_output))\n                        else:\n                            time.sleep(self.wait_time)\n                    except StopIteration:\n                        break\n                    except Exception as e:\n                        # Can't pickle tracebacks.\n                        # As a compromise, print the traceback and pickle None instead.\n                        if not hasattr(e, '__traceback__'):\n                            setattr(e, '__traceback__', sys.exc_info()[2])\n                        self.queue.put((False, e))\n                        self._stop_event.set()\n                        break\n        else:\n            while not self._stop_event.is_set():\n                try:\n                    if (self.queue is not None and\n                            self.queue.qsize() < self.max_queue_size):\n                        generator_output = next(self._generator)\n                        self.queue.put((True, generator_output))\n                    else:\n                        time.sleep(self.wait_time)\n                except StopIteration:\n                    break\n                except Exception as e:\n                    # Can't pickle tracebacks.\n                    # As a compromise, print the traceback and pickle None instead.\n                    traceback.print_exc()\n                    setattr(e, '__traceback__', None)\n                    self.queue.put((False, e))\n                    self._stop_event.set()\n                    break",
        "begin_line": 646,
        "end_line": 690,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.start#692",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.start(self, workers=1, max_queue_size=10)",
        "snippet": "    def start(self, workers=1, max_queue_size=10):\n        \"\"\"Kicks off threads which add data from the generator into the queue.\n\n        # Arguments\n            workers: number of worker threads\n            max_queue_size: queue size\n                (when full, threads could block on `put()`)\n        \"\"\"\n        try:\n            self.max_queue_size = max_queue_size\n            if self._use_multiprocessing:\n                self._manager = multiprocessing.Manager()\n                self.queue = self._manager.Queue(maxsize=max_queue_size)\n                self._stop_event = multiprocessing.Event()\n            else:\n                # On all OSes, avoid **SYSTEMATIC** error in multithreading mode:\n                # `ValueError: generator already executing`\n                # => Serialize calls to infinite iterator/generator's next() function\n                self.genlock = threading.Lock()\n                self.queue = queue.Queue(maxsize=max_queue_size)\n                self._stop_event = threading.Event()\n\n            for _ in range(workers):\n                if self._use_multiprocessing:\n                    # Reset random seed else all children processes\n                    # share the same seed\n                    np.random.seed(self.seed)\n                    thread = multiprocessing.Process(target=self._data_generator_task)\n                    thread.daemon = True\n                    if self.seed is not None:\n                        self.seed += 1\n                else:\n                    thread = threading.Thread(target=self._data_generator_task)\n                self._threads.append(thread)\n                thread.start()\n        except:\n            self.stop()\n            raise",
        "begin_line": 692,
        "end_line": 729,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.00045433893684688776,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.is_running#731",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.is_running(self)",
        "snippet": "    def is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()",
        "begin_line": 731,
        "end_line": 732,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0004399472063352398,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.stop#734",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.stop(self, timeout=None)",
        "snippet": "    def stop(self, timeout=None):\n        \"\"\"Stops running threads and wait for them to exit, if necessary.\n\n        Should be called by the same thread which called `start()`.\n\n        # Arguments\n            timeout: maximum time to wait on `thread.join()`.\n        \"\"\"\n        if self.is_running():\n            self._stop_event.set()\n\n        for thread in self._threads:\n            if self._use_multiprocessing:\n                if thread.is_alive():\n                    thread.terminate()\n            else:\n                # The thread.is_alive() test is subject to a race condition:\n                # the thread could terminate right after the test and before the\n                # join, rendering this test meaningless -> Call thread.join()\n                # always, which is ok no matter what the status of the thread.\n                thread.join(timeout)\n\n        if self._manager:\n            self._manager.shutdown()\n\n        self._threads = []\n        self._stop_event = None\n        self.queue = None",
        "begin_line": 734,
        "end_line": 761,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.data_utils.GeneratorEnqueuer.get#763",
        "src_path": "keras/utils/data_utils.py",
        "class_name": "keras.utils.data_utils.GeneratorEnqueuer",
        "signature": "keras.utils.data_utils.GeneratorEnqueuer.get(self)",
        "snippet": "    def get(self):\n        \"\"\"Creates a generator to extract data from the queue.\n\n        Skip the data if it is `None`.\n\n        # Yields\n            The next element in the queue, i.e. a tuple\n            `(inputs, targets)` or\n            `(inputs, targets, sample_weights)`.\n        \"\"\"\n        while self.is_running():\n            if not self.queue.empty():\n                success, value = self.queue.get()\n                # Rethrow any exceptions found in the queue\n                if not success:\n                    six.reraise(value.__class__, value, value.__traceback__)\n                # Yield regular values\n                if value is not None:\n                    yield value\n            else:\n                all_finished = all([not thread.is_alive() for thread in self._threads])\n                if all_finished and self.queue.empty():\n                    raise StopIteration()\n                else:\n                    time.sleep(self.wait_time)\n\n        # Make sure to rethrow the first exception in the queue, if any\n        while not self.queue.empty():\n            success, value = self.queue.get()\n            if not success:\n                six.reraise(value.__class__, value, value.__traceback__)",
        "begin_line": 763,
        "end_line": 793,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.common.epsilon#13",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.epsilon()",
        "snippet": "def epsilon():\n    \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n\n    # Returns\n        A float.\n\n    # Example\n    ```python\n        >>> keras.backend.epsilon()\n        1e-07\n    ```\n    \"\"\"\n    return _EPSILON",
        "begin_line": 13,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.common.floatx#48",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.floatx()",
        "snippet": "def floatx():\n    \"\"\"Returns the default float type, as a string.\n    (e.g. 'float16', 'float32', 'float64').\n\n    # Returns\n        String, the current default float type.\n\n    # Example\n    ```python\n        >>> keras.backend.floatx()\n        'float32'\n    ```\n    \"\"\"\n    return _FLOATX",
        "begin_line": 48,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.common.image_data_format#113",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.image_data_format()",
        "snippet": "def image_data_format():\n    \"\"\"Returns the default image data format convention ('channels_first' or 'channels_last').\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> keras.backend.image_data_format()\n        'channels_first'\n    ```\n    \"\"\"\n    return _IMAGE_DATA_FORMAT",
        "begin_line": 113,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.common.normalize_data_format#150",
        "src_path": "keras/backend/common.py",
        "class_name": "keras.backend.common",
        "signature": "keras.backend.common.normalize_data_format(value)",
        "snippet": "def normalize_data_format(value):\n    \"\"\"Checks that the value correspond to a valid data format.\n\n    # Arguments\n        value: String or None. `'channels_first'` or `'channels_last'`.\n\n    # Returns\n        A string, either `'channels_first'` or `'channels_last'`\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> K.normalize_data_format(None)\n        'channels_first'\n        >>> K.normalize_data_format('channels_last')\n        'channels_last'\n    ```\n\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    \"\"\"\n    if value is None:\n        value = image_data_format()\n    data_format = value.lower()\n    if data_format not in {'channels_first', 'channels_last'}:\n        raise ValueError('The `data_format` argument must be one of '\n                         '\"channels_first\", \"channels_last\". Received: ' +\n                         str(value))\n    return data_format",
        "begin_line": 150,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__init__#44",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__init__(self, datapath, dataset, start=0, end=None, normalizer=None)",
        "snippet": "    def __init__(self, datapath, dataset, start=0, end=None, normalizer=None):\n        if h5py is None:\n            raise ImportError('The use of HDF5Matrix requires '\n                              'HDF5 and h5py installed.')\n\n        if datapath not in list(self.refs.keys()):\n            f = h5py.File(datapath)\n            self.refs[datapath] = f\n        else:\n            f = self.refs[datapath]\n        self.data = f[dataset]\n        self.start = start\n        if end is None:\n            self.end = self.data.shape[0]\n        else:\n            self.end = end\n        self.normalizer = normalizer\n        if self.normalizer is not None:\n            first_val = self.normalizer(self.data[0:1])\n        else:\n            first_val = self.data[0:1]\n        self._base_shape = first_val.shape[1:]\n        self._base_dtype = first_val.dtype",
        "begin_line": 44,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__len__#68",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__len__(self)",
        "snippet": "    def __len__(self):\n        return self.end - self.start",
        "begin_line": 68,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.__getitem__#71",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if isinstance(key, slice):\n            start, stop = key.start, key.stop\n            if start is None:\n                start = 0\n            if stop is None:\n                stop = self.shape[0]\n            if stop + self.start <= self.end:\n                idx = slice(start + self.start, stop + self.start)\n            else:\n                raise IndexError\n        elif isinstance(key, (int, np.integer)):\n            if key + self.start < self.end:\n                idx = key + self.start\n            else:\n                raise IndexError\n        elif isinstance(key, np.ndarray):\n            if np.max(key) + self.start < self.end:\n                idx = (self.start + key).tolist()\n            else:\n                raise IndexError\n        else:\n            # Assume list/iterable\n            if max(key) + self.start < self.end:\n                idx = [x + self.start for x in key]\n            else:\n                raise IndexError\n        if self.normalizer is not None:\n            return self.normalizer(self.data[idx])\n        else:\n            return self.data[idx]",
        "begin_line": 71,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.shape#104",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.shape(self)",
        "snippet": "    def shape(self):\n        \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n\n        # Returns\n            A numpy-style shape tuple.\n        \"\"\"\n        return (self.end - self.start,) + self._base_shape",
        "begin_line": 104,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.dtype#113",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.dtype(self)",
        "snippet": "    def dtype(self):\n        \"\"\"Gets the datatype of the dataset.\n\n        # Returns\n            A numpy dtype string.\n        \"\"\"\n        return self._base_dtype",
        "begin_line": 113,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.ndim#122",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.ndim(self)",
        "snippet": "    def ndim(self):\n        \"\"\"Gets the number of dimensions (rank) of the dataset.\n\n        # Returns\n            An integer denoting the number of dimensions (rank) of the dataset.\n        \"\"\"\n        return self.data.ndim",
        "begin_line": 122,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.HDF5Matrix.size#131",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils.HDF5Matrix",
        "signature": "keras.utils.io_utils.HDF5Matrix.size(self)",
        "snippet": "    def size(self):\n        \"\"\"Gets the total dataset size (number of elements).\n\n        # Returns\n            An integer denoting the number of elements in the dataset.\n        \"\"\"\n        return np.prod(self.shape)",
        "begin_line": 131,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.io_utils.ask_to_proceed_with_overwrite#140",
        "src_path": "keras/utils/io_utils.py",
        "class_name": "keras.utils.io_utils",
        "signature": "keras.utils.io_utils.ask_to_proceed_with_overwrite(filepath)",
        "snippet": "def ask_to_proceed_with_overwrite(filepath):\n    \"\"\"Produces a prompt asking about overwriting a file.\n\n    # Arguments\n        filepath: the path to the file to be overwritten.\n\n    # Returns\n        True if we can proceed with overwrite, False otherwise.\n    \"\"\"\n    overwrite = six.moves.input('[WARNING] %s already exists - overwrite? '\n                                '[y/n]' % (filepath)).strip().lower()\n    while overwrite not in ('y', 'n'):\n        overwrite = six.moves.input('Enter \"y\" (overwrite) or \"n\" '\n                                    '(cancel).').strip().lower()\n    if overwrite == 'n':\n        return False\n    print('[TIP] Next time specify overwrite=True!')\n    return True",
        "begin_line": 140,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.__init__#87",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        # Signature detection\n        if (len(args) == 2 or\n            len(args) == 1 and 'outputs' in kwargs or\n                'inputs' in kwargs and 'outputs' in kwargs):\n            # Graph network\n            self._init_graph_network(*args, **kwargs)\n        else:\n            # Subclassed network\n            self._init_subclassed_network(**kwargs)",
        "begin_line": 87,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.Network._base_init#98",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._base_init(self, name=None)",
        "snippet": "    def _base_init(self, name=None):\n        # The following are implemented as property functions:\n        # self.trainable_weights\n        # self.non_trainable_weights\n        # self.input_spec\n        # self.losses\n        # self.updates\n\n        # Handle `name` argument.\n        if not name:\n            prefix = self.__class__.__name__.lower()\n            name = prefix + '_' + str(K.get_uid(prefix))\n        self.name = name\n\n        # This acts just like the `trainable` attribute of any layer instance.\n        # It does not affect users of the underlying layers, only users of the\n        # Network instance.\n        self.trainable = True\n        self._is_compiled = False\n        self._expects_training_arg = False\n        self._initial_weights = None\n\n        self.supports_masking = False\n        if not hasattr(self, 'optimizer'):\n            # Don't reset optimizer if already set.\n            self.optimizer = None\n\n        # Private attributes to implement compatibility with Layer.\n        self._updates = []\n        self._losses = []\n        self._per_input_losses = {}\n        self._per_input_updates = {}\n\n        # All layers in order of horizontal graph traversal.\n        # Entries are unique. Includes input and output layers.\n        self._layers = []\n\n        # Used only in conjunction with graph-networks\n        self._outbound_nodes = []\n        self._inbound_nodes = []",
        "begin_line": 98,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.Network._init_graph_network#139",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._init_graph_network(self, inputs, outputs, name=None)",
        "snippet": "    def _init_graph_network(self, inputs, outputs, name=None):\n        self._uses_inputs_arg = True\n        # Normalize and set self.inputs, self.outputs.\n        if isinstance(inputs, (list, tuple)):\n            self.inputs = list(inputs)  # Tensor or list of tensors.\n        else:\n            self.inputs = [inputs]\n        if isinstance(outputs, (list, tuple)):\n            self.outputs = list(outputs)\n        else:\n            self.outputs = [outputs]\n\n        # User-provided argument validation.\n        # Check for redundancy in inputs.\n        if len(set(self.inputs)) != len(self.inputs):\n            raise ValueError('The list of inputs passed to the model '\n                             'is redundant. '\n                             'All inputs should only appear once.'\n                             ' Found: ' + str(self.inputs))\n        for x in self.inputs:\n            # Check that x has appropriate `_keras_history` metadata.\n            if not hasattr(x, '_keras_history'):\n                cls_name = self.__class__.__name__\n                raise ValueError('Input tensors to a ' + cls_name + ' ' +\n                                 'must come from `keras.layers.Input`. '\n                                 'Received: ' + str(x) +\n                                 ' (missing previous layer metadata).')\n            # Check that x is an input tensor.\n            layer, node_index, tensor_index = x._keras_history\n            if (len(layer._inbound_nodes) > 1 or\n                    (layer._inbound_nodes and\n                     layer._inbound_nodes[0].inbound_layers)):\n                cls_name = self.__class__.__name__\n                warnings.warn(cls_name + ' inputs must come from '\n                              '`keras.layers.Input` '\n                              '(thus holding past layer metadata), '\n                              'they cannot be the output of '\n                              'a previous non-Input layer. '\n                              'Here, a tensor specified as '\n                              'input to your model '\n                              'was not an Input tensor, '\n                              'it was generated by layer ' +\n                              layer.name + '.\\n'\n                              'Note that input tensors are '\n                              'instantiated via '\n                              '`tensor = keras.layers.Input(shape)`.\\n'\n                              'The tensor that caused the issue was: ' +\n                              str(x.name))\n        for x in self.outputs:\n            if not hasattr(x, '_keras_history'):\n                cls_name = self.__class__.__name__\n                raise ValueError('Output tensors to a ' + cls_name +\n                                 ' must be '\n                                 'the output of a Keras `Layer` '\n                                 '(thus holding past layer metadata). '\n                                 'Found: ' + str(x))\n        self._base_init(name=name)\n        self._compute_previous_mask = (\n            has_arg(self.call, 'mask') or\n            hasattr(self, 'compute_mask'))\n        # A Network does not create weights of its own,\n        # thus it is already built.\n        self.built = True\n        self._is_graph_network = True\n\n        self._input_layers = []\n        self._output_layers = []\n        self._input_coordinates = []\n        self._output_coordinates = []\n\n        # This is for performance optimization when calling the Network on new\n        # inputs. Every time the Network is called on a set on input tensors,\n        # we compute the output tensors,\n        # output masks and output shapes in one pass,\n        # then cache them here. When any of these outputs is queried later, we\n        # retrieve it from there instead of recomputing it.\n        self._output_mask_cache = {}\n        self._output_tensor_cache = {}\n        self._output_shape_cache = {}\n\n        # Build self._output_layers:\n        for x in self.outputs:\n            layer, node_index, tensor_index = x._keras_history\n            self._output_layers.append(layer)\n            self._output_coordinates.append((layer, node_index, tensor_index))\n\n        # Build self._input_layers:\n        for x in self.inputs:\n            layer, node_index, tensor_index = x._keras_history\n            # It's supposed to be an input layer, so only one node\n            # and one tensor output.\n            assert node_index == 0\n            assert tensor_index == 0\n            self._input_layers.append(layer)\n            self._input_coordinates.append((layer, node_index, tensor_index))\n\n        # Keep track of the network's nodes and layers.\n        nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n            self.inputs, self.outputs)\n        self._network_nodes = nodes\n        self._nodes_by_depth = nodes_by_depth\n        self._layers = layers\n        self._layers_by_depth = layers_by_depth\n\n        # Create the node linking internal inputs to internal outputs.\n        Node(outbound_layer=self,\n             inbound_layers=[],\n             node_indices=[],\n             tensor_indices=[],\n             input_tensors=self.inputs,\n             output_tensors=self.outputs,\n             # No network-level masking for now.\n             input_masks=[None for _ in self.inputs],\n             output_masks=[None for _ in self.outputs],\n             input_shapes=[x._keras_shape for x in self.inputs],\n             output_shapes=[x._keras_shape for x in self.outputs])\n\n        # Fill in the output mask cache.\n        masks = []\n        for x in self.inputs:\n            layer, node_index, tensor_index = x._keras_history\n            node = layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        mask_cache_key = object_list_uid(inputs)\n        mask_cache_key += '_' + object_list_uid(masks)\n        masks = []\n        for x in self.outputs:\n            layer, node_index, tensor_index = x._keras_history\n            node = layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        mask = unpack_singleton(masks)\n        self._output_mask_cache[mask_cache_key] = mask\n\n        # Build self.input_names and self.output_names.\n        self.input_names = []\n        self.output_names = []\n        self._feed_input_names = []\n        self._feed_inputs = []\n        self._feed_input_shapes = []\n        for i, layer in enumerate(self._input_layers):\n            # Check that layer is an InputLayer.\n            if not isinstance(layer, InputLayer):\n                raise TypeError(\n                    'Input layers to a `Model` must be `InputLayer` objects. '\n                    'Received inputs: {}. '\n                    'Input {} (0-based) originates '\n                    'from layer type `{}`.'.format(inputs,\n                                                   i,\n                                                   layer.__class__.__name__))\n            self.input_names.append(layer.name)\n            if layer.is_placeholder:\n                self._feed_inputs.append(layer.input)\n                self._feed_input_names.append(layer.name)\n                self._feed_input_shapes.append(self.inputs[i]._keras_shape)\n\n        for layer in self._output_layers:\n            self.output_names.append(layer.name)",
        "begin_line": 139,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.Network._init_subclassed_network#299",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network._init_subclassed_network(self, name=None)",
        "snippet": "    def _init_subclassed_network(self, name=None):\n        self._base_init(name=name)\n        self._is_graph_network = False\n        self._expects_training_arg = has_arg(self.call, 'training')\n        self._uses_inputs_arg = has_arg(self.call, 'inputs')\n        self.outputs = None\n        self.inputs = None\n        self.built = False",
        "begin_line": 299,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.Network.__setattr__#308",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        # Automatically track layers set as Model\n        # attributes for subclassed Models.\n        if isinstance(value, (Layer, Network)):\n            try:\n                is_graph_network = self._is_graph_network\n            except AttributeError:\n                raise RuntimeError(\n                    'It looks like you are subclassing `Model` and you '\n                    'forgot to call `super(YourClass, self).__init__()`.'\n                    ' Always start with this line.')\n            if not is_graph_network:\n                if value not in self._layers:\n                    self._layers.append(value)\n        super(Network, self).__setattr__(name, value)",
        "begin_line": 308,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.Network.updates#367",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.updates(self)",
        "snippet": "    def updates(self):\n        \"\"\"Retrieves the model's updates.\n\n        Will only include updates that are either\n        unconditional, or conditional on inputs to this model\n        (e.g. will not include updates that depend on tensors\n        that aren't inputs to this model).\n\n        # Returns\n            A list of update ops.\n        \"\"\"\n        if not self.trainable and not self.stateful:\n            return []\n        updates = []\n        for layer in self.layers:\n            if hasattr(layer, 'updates'):\n                if self._is_graph_network:\n                    # Collect updates that are dependent on inputs\n                    # that are part of the model.\n                    for node_index, node in enumerate(layer._inbound_nodes):\n                        node_key = self._node_key(layer, node_index)\n                        if node_key in self._network_nodes:\n                            # The model owns this layer node.\n                            inputs = node.input_tensors\n                            updates += layer.get_updates_for(inputs)\n                    # Collect unconditional updates.\n                    updates += layer.get_updates_for(None)\n                else:\n                    updates += layer.updates\n        return updates",
        "begin_line": 367,
        "end_line": 396,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.losses#399",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.losses(self)",
        "snippet": "    def losses(self):\n        \"\"\"Retrieves the model's losses.\n\n        Will only include losses that are either\n        unconditional, or conditional on inputs to this model\n        (e.g. will not include losses that depend on tensors\n        that aren't inputs to this model).\n\n        # Returns\n            A list of loss tensors.\n        \"\"\"\n        losses = []\n        for layer in self.layers:\n            if hasattr(layer, 'losses'):\n                if self._is_graph_network:\n                    # Collect losses that are dependent on inputs\n                    # that are part of the model.\n                    for node_index, node in enumerate(layer._inbound_nodes):\n                        node_key = self._node_key(layer, node_index)\n                        if node_key in self._network_nodes:\n                            # The model owns this layer node.\n                            inputs = node.input_tensors\n                            losses += layer.get_losses_for(inputs)\n                    # Collect unconditional losses.\n                    losses += layer.get_losses_for(None)\n                else:\n                    losses += layer.losses\n\n        # Add any potential unconditional model-level loss.\n        losses += self.get_losses_for(None)\n\n        unique_tensors = list(\n            set(x for x in losses if not isinstance(x, (float, int))))\n        non_tensors = [x for x in losses if isinstance(x, (float, int))]\n        return unique_tensors + non_tensors",
        "begin_line": 399,
        "end_line": 433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.uses_learning_phase#436",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.uses_learning_phase(self)",
        "snippet": "    def uses_learning_phase(self):\n        if not self.outputs:\n            return False\n        return any([x._uses_learning_phase for x in self.outputs])",
        "begin_line": 436,
        "end_line": 439,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.stateful#442",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.stateful(self)",
        "snippet": "    def stateful(self):\n        return any([(hasattr(layer, 'stateful') and\n                    layer.stateful) for layer in self.layers])",
        "begin_line": 442,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.state_updates#452",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.state_updates(self)",
        "snippet": "    def state_updates(self):\n        \"\"\"Returns the `updates` from all layers that are stateful.\n\n        This is useful for separating training updates and\n        state updates, e.g. when we need to update a layer's internal state\n        during prediction.\n\n        # Returns\n            A list of update ops.\n        \"\"\"\n        state_updates = []\n        for layer in self.layers:\n            if layer.stateful:\n                state_updates += layer.updates\n        return state_updates",
        "begin_line": 452,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.trainable_weights#469",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        if not self.trainable:\n            return []\n        weights = []\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights",
        "begin_line": 469,
        "end_line": 475,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network.Network.compute_mask#573",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network.Network",
        "signature": "keras.engine.network.Network.compute_mask(self, inputs, mask)",
        "snippet": "    def compute_mask(self, inputs, mask):\n        if not self._is_graph_network:\n            return None\n\n        inputs = to_list(inputs)\n        if mask is None:\n            masks = [None for _ in range(len(inputs))]\n        else:\n            masks = to_list(mask)\n        cache_key = object_list_uid(inputs)\n        cache_key += '_' + object_list_uid(masks)\n        if cache_key in self._output_mask_cache:\n            return self._output_mask_cache[cache_key]\n        else:\n            _, output_masks, _ = self.run_internal_graph(inputs, masks)\n            return output_masks",
        "begin_line": 573,
        "end_line": 588,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.network._make_node_key#1257",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network._make_node_key(layer_name, node_index)",
        "snippet": "def _make_node_key(layer_name, node_index):\n    return layer_name + '_ib-' + str(node_index)",
        "begin_line": 1257,
        "end_line": 1258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network._map_graph_network#1261",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network._map_graph_network(inputs, outputs)",
        "snippet": "def _map_graph_network(inputs, outputs):\n    \"\"\"Validates a network's topology and gather its layers and nodes.\n\n    # Arguments\n        inputs: List of input tensors.\n        outputs: List of outputs tensors.\n\n    # Returns\n        A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.\n        - nodes: list of Node instances.\n        - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n        - layers: list of Layer instances.\n        - layers_by_depth: dict mapping ints (depth)\n            to lists of layer instances.\n\n    # Raises\n        ValueError: In case the network is not valid (e.g. disconnected graph).\n    \"\"\"\n    # Network_nodes: set of nodes included in the graph of layers\n    # (not all nodes included in the layers are relevant to the current graph).\n    network_nodes = set()  # ids of all nodes relevant to the Network\n    nodes_depths = {}  # dict {node: depth value}\n    layers_depths = {}  # dict {layer: depth value}\n    layer_indices = {}  # dict {layer: index in traversal}\n    nodes_in_decreasing_depth = []\n\n    def build_map(tensor,\n                  finished_nodes,\n                  nodes_in_progress,\n                  layer,\n                  node_index,\n                  tensor_index):\n        \"\"\"Builds a map of the graph of layers.\n\n        This recursively updates the map `layer_indices`,\n        the list `nodes_in_decreasing_depth` and the set `network_nodes`.\n\n        # Arguments:\n            tensor: Some tensor in a graph.\n            finished_nodes: Set of nodes whose subgraphs have been traversed\n                completely. Useful to prevent duplicated work.\n            nodes_in_progress: Set of nodes that are currently active on the\n                recursion stack. Useful to detect cycles.\n            layer: Layer from which `tensor` comes from. If not provided,\n                will be obtained from `tensor._keras_history`.\n            node_index: Node index from which `tensor` comes from.\n            tensor_index: Tensor_index from which `tensor` comes from.\n\n        # Raises:\n            ValueError: if a cycle is detected.\n        \"\"\"\n        node = layer._inbound_nodes[node_index]\n\n        # Prevent cycles.\n        if node in nodes_in_progress:\n            raise ValueError('The tensor ' + str(tensor) + ' at layer \"' +\n                             layer.name + '\" is part of a cycle.')\n\n        # Don't repeat work for shared subgraphs\n        if node in finished_nodes:\n            return\n\n        node_key = _make_node_key(layer.name, node_index)\n        # Update network_nodes.\n        network_nodes.add(node_key)\n\n        # Store the traversal order for layer sorting.\n        if layer not in layer_indices:\n            layer_indices[layer] = len(layer_indices)\n\n        nodes_in_progress.add(node)\n\n        # Propagate to all previous tensors connected to this node.\n        for i in range(len(node.inbound_layers)):\n            x = node.input_tensors[i]\n            layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            tensor_index = node.tensor_indices[i]\n            build_map(x, finished_nodes, nodes_in_progress, layer,\n                      node_index, tensor_index)\n\n        finished_nodes.add(node)\n        nodes_in_progress.remove(node)\n        nodes_in_decreasing_depth.append(node)\n\n    finished_nodes = set()\n    nodes_in_progress = set()\n    for x in outputs:\n        layer, node_index, tensor_index = x._keras_history\n        build_map(x, finished_nodes, nodes_in_progress,\n                  layer=layer,\n                  node_index=node_index,\n                  tensor_index=tensor_index)\n\n    for node in reversed(nodes_in_decreasing_depth):\n        # If the depth is not set, the node has no outbound nodes (depth 0).\n        depth = nodes_depths.setdefault(node, 0)\n\n        # Update the depth of the corresponding layer\n        previous_depth = layers_depths.get(node.outbound_layer, 0)\n        # If we've seen this layer before at a higher depth,\n        # we should use that depth instead of the node depth.\n        # This is necessary for shared layers that have inputs at different\n        # depth levels in the graph.\n        depth = max(depth, previous_depth)\n        layers_depths[node.outbound_layer] = depth\n        nodes_depths[node] = depth\n\n        # Update the depth of inbound nodes.\n        # The \"depth\" of a node is the max of the depths\n        # of all layers it is connected to.\n        for i in range(len(node.inbound_layers)):\n            inbound_layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            inbound_node = inbound_layer._inbound_nodes[node_index]\n            previous_depth = nodes_depths.get(inbound_node, 0)\n            nodes_depths[inbound_node] = max(depth + 1, previous_depth)\n\n    # Build a dict {depth: list of nodes with this depth}\n    nodes_by_depth = {}\n    for node, depth in nodes_depths.items():\n        if depth not in nodes_by_depth:\n            nodes_by_depth[depth] = []\n        nodes_by_depth[depth].append(node)\n\n    # Build a dict {depth: list of layers with this depth}\n    layers_by_depth = {}\n    for layer, depth in layers_depths.items():\n        if depth not in layers_by_depth:\n            layers_by_depth[depth] = []\n        layers_by_depth[depth].append(layer)\n\n    # Get sorted list of layer depths.\n    depth_keys = list(layers_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    # Set self.layers and self._layers_by_depth.\n    layers = []\n    for depth in depth_keys:\n        layers_for_depth = layers_by_depth[depth]\n        # Network.layers needs to have a deterministic order:\n        # here we order them by traversal order.\n        layers_for_depth.sort(key=lambda x: layer_indices[x])\n        layers.extend(layers_for_depth)\n\n    # Get sorted list of node depths.\n    depth_keys = list(nodes_by_depth.keys())\n    depth_keys.sort(reverse=True)\n\n    # Check that all tensors required are computable.\n    # computable_tensors: all tensors in the graph\n    # that can be computed from the inputs provided.\n    computable_tensors = []\n    for x in inputs:\n        computable_tensors.append(x)\n\n    layers_with_complete_input = []  # To provide a better error msg.\n    for depth in depth_keys:\n        for node in nodes_by_depth[depth]:\n            layer = node.outbound_layer\n            if layer:\n                for x in node.input_tensors:\n                    if x not in computable_tensors:\n                        raise ValueError('Graph disconnected: '\n                                         'cannot obtain value for tensor ' +\n                                         str(x) + ' at layer \"' +\n                                         layer.name + '\". '\n                                         'The following previous layers '\n                                         'were accessed without issue: ' +\n                                         str(layers_with_complete_input))\n                for x in node.output_tensors:\n                    computable_tensors.append(x)\n                layers_with_complete_input.append(layer.name)\n\n    # Ensure name unicity, which will be crucial for serialization\n    # (since serialized nodes refer to layers by their name).\n    all_names = [layer.name for layer in layers]\n    for name in all_names:\n        if all_names.count(name) != 1:\n            raise ValueError('The name \"' + name + '\" is used ' +\n                             str(all_names.count(name)) +\n                             ' times in the model. '\n                             'All layer names should be unique.')\n    return network_nodes, nodes_by_depth, layers, layers_by_depth",
        "begin_line": 1261,
        "end_line": 1444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.network.build_map#1287",
        "src_path": "keras/engine/network.py",
        "class_name": "keras.engine.network",
        "signature": "keras.engine.network.build_map(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)",
        "snippet": "    def build_map(tensor,\n                  finished_nodes,\n                  nodes_in_progress,\n                  layer,\n                  node_index,\n                  tensor_index):\n        \"\"\"Builds a map of the graph of layers.\n\n        This recursively updates the map `layer_indices`,\n        the list `nodes_in_decreasing_depth` and the set `network_nodes`.\n\n        # Arguments:\n            tensor: Some tensor in a graph.\n            finished_nodes: Set of nodes whose subgraphs have been traversed\n                completely. Useful to prevent duplicated work.\n            nodes_in_progress: Set of nodes that are currently active on the\n                recursion stack. Useful to detect cycles.\n            layer: Layer from which `tensor` comes from. If not provided,\n                will be obtained from `tensor._keras_history`.\n            node_index: Node index from which `tensor` comes from.\n            tensor_index: Tensor_index from which `tensor` comes from.\n\n        # Raises:\n            ValueError: if a cycle is detected.\n        \"\"\"\n        node = layer._inbound_nodes[node_index]\n\n        # Prevent cycles.\n        if node in nodes_in_progress:\n            raise ValueError('The tensor ' + str(tensor) + ' at layer \"' +\n                             layer.name + '\" is part of a cycle.')\n\n        # Don't repeat work for shared subgraphs\n        if node in finished_nodes:\n            return\n\n        node_key = _make_node_key(layer.name, node_index)\n        # Update network_nodes.\n        network_nodes.add(node_key)\n\n        # Store the traversal order for layer sorting.\n        if layer not in layer_indices:\n            layer_indices[layer] = len(layer_indices)\n\n        nodes_in_progress.add(node)\n\n        # Propagate to all previous tensors connected to this node.\n        for i in range(len(node.inbound_layers)):\n            x = node.input_tensors[i]\n            layer = node.inbound_layers[i]\n            node_index = node.node_indices[i]\n            tensor_index = node.tensor_indices[i]\n            build_map(x, finished_nodes, nodes_in_progress, layer,\n                      node_index, tensor_index)\n\n        finished_nodes.add(node)\n        nodes_in_progress.remove(node)\n        nodes_in_decreasing_depth.append(node)",
        "begin_line": 1287,
        "end_line": 1344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.__init__#86",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.__init__(self, layers=None, name=None)",
        "snippet": "    def __init__(self, layers=None, name=None):\n        super(Sequential, self).__init__(name=name)\n\n        # Add to the model any layers passed to the constructor.\n        if layers:\n            for layer in layers:\n                self.add(layer)",
        "begin_line": 86,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.layers#95",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.layers(self)",
        "snippet": "    def layers(self):\n        # Historically, `sequential.layers` only returns layers that were added\n        # via `add`, and omits the auto-generated `InputLayer`\n        # that comes at the bottom of the stack.\n        if self._layers and isinstance(self._layers[0], InputLayer):\n            return self._layers[1:]\n        return self._layers",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.add#114",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.add(self, layer)",
        "snippet": "    def add(self, layer):\n        \"\"\"Adds a layer instance on top of the layer stack.\n\n        # Arguments\n            layer: layer instance.\n\n        # Raises\n            TypeError: If `layer` is not a layer instance.\n            ValueError: In case the `layer` argument does not\n                know its input shape.\n            ValueError: In case the `layer` argument has\n                multiple output tensors, or is already connected\n                somewhere else (forbidden in `Sequential` models).\n        \"\"\"\n        if not isinstance(layer, Layer):\n            raise TypeError('The added layer must be '\n                            'an instance of class Layer. '\n                            'Found: ' + str(layer))\n        self.built = False\n        if not self._layers:\n            set_inputs = False\n            # First layer in model: check that it is an input layer.\n            if not isinstance(layer, InputLayer):\n                # Create an input tensor and call `layer` on the input tensor.\n                # First, we need to infer the expected input shape and dtype.\n                first_layer = layer\n                if isinstance(layer, (Model, Sequential)):\n                    # We were passed a model as first layer.\n                    # This requires a specific way to figure out the\n                    # input shape and dtype.\n                    if not layer.layers:\n                        raise ValueError('Cannot add an empty model '\n                                         'to a `Sequential` model.')\n                    # In case of nested models: recover the first layer\n                    # of the deepest model to infer input shape and dtype.\n                    first_layer = layer.layers[0]\n                    while isinstance(first_layer, (Model, Sequential)):\n                        first_layer = first_layer.layers[0]\n\n                if hasattr(first_layer, 'batch_input_shape'):\n                    batch_shape = first_layer.batch_input_shape\n                    dtype = first_layer.dtype\n                    # Instantiate the input layer.\n                    x = Input(\n                        batch_shape=batch_shape,\n                        dtype=dtype,\n                        name=layer.name + '_input')\n                    # This will build the current layer\n                    # and create the node connecting the current layer\n                    # to the input layer we just created.\n                    layer(x)\n                    set_inputs = True\n                else:\n                    # The layer doesn't know about its expected shape.\n                    # We will have to\n                    # build the model lazily on `fit`/etc.\n                    batch_shape = None\n            else:\n                # Corner case where the user passes an InputLayer via `add`.\n                assert len(layer._inbound_nodes[-1].output_tensors) == 1\n                set_inputs = True\n\n            if set_inputs:\n                if len(layer._inbound_nodes[-1].output_tensors) != 1:\n                    raise ValueError('All layers in a Sequential model '\n                                     'should have a single output tensor. '\n                                     'For multi-output layers, '\n                                     'use the functional API.')\n                self.outputs = [layer._inbound_nodes[-1].output_tensors[0]]\n                self.inputs = network.get_source_inputs(self.outputs[0])\n        elif self.outputs:\n            output_tensor = layer(self.outputs[0])\n            if isinstance(output_tensor, list):\n                raise TypeError('All layers in a Sequential model '\n                                'should have a single output tensor. '\n                                'For multi-output layers, '\n                                'use the functional API.')\n            self.outputs = [output_tensor]\n        if self.inputs:\n            self.build()\n        else:\n            self._layers.append(layer)",
        "begin_line": 114,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.sequential.Sequential.build#216",
        "src_path": "keras/engine/sequential.py",
        "class_name": "keras.engine.sequential.Sequential",
        "signature": "keras.engine.sequential.Sequential.build(self, input_shape=None)",
        "snippet": "    def build(self, input_shape=None):\n        if input_shape and not self.inputs:\n            batch_shape = tuple(input_shape)\n            dtype = K.floatx()\n            x = Input(batch_shape=batch_shape,\n                      dtype=dtype,\n                      name=self.name + '_input')\n            self.inputs = [x]\n            for layer in self._layers:\n                x = layer(x)\n            self.outputs = [x]\n            if self._layers:\n                self._layers[0].batch_input_shape = batch_shape\n\n        if self.inputs:\n            self._init_graph_network(self.inputs,\n                                     self.outputs,\n                                     name=self.name)\n            self.built = True",
        "begin_line": 216,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.np_utils.to_categorical#9",
        "src_path": "keras/utils/np_utils.py",
        "class_name": "keras.utils.np_utils",
        "signature": "keras.utils.np_utils.to_categorical(y, num_classes=None)",
        "snippet": "def to_categorical(y, num_classes=None):\n    \"\"\"Converts a class vector (integers) to binary class matrix.\n\n    E.g. for use with categorical_crossentropy.\n\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n\n    # Returns\n        A binary matrix representation of the input. The classes axis\n        is placed last.\n    \"\"\"\n    y = np.array(y, dtype='int')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype=np.float32)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical",
        "begin_line": 9,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.__init__#87",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.__init__(self, rank, filters, kernel_size, strides=1, padding='valid', data_format=None, dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, rank,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=1,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(_Conv, self).__init__(**kwargs)\n        self.rank = rank\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = K.normalize_data_format(data_format)\n        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=self.rank + 2)",
        "begin_line": 87,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.build#123",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True",
        "begin_line": 123,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.call#152",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.rank == 1:\n            outputs = K.conv1d(\n                inputs,\n                self.kernel,\n                strides=self.strides[0],\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate[0])\n        if self.rank == 2:\n            outputs = K.conv2d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.rank == 3:\n            outputs = K.conv3d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs",
        "begin_line": 152,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.convolutional._Conv.compute_output_shape#188",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional._Conv",
        "signature": "keras.layers.convolutional._Conv.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            space = input_shape[1:-1]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n        if self.data_format == 'channels_first':\n            space = input_shape[2:]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0], self.filters) + tuple(new_space)",
        "begin_line": 188,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.convolutional.Conv2D.__init__#449",
        "src_path": "keras/layers/convolutional.py",
        "class_name": "keras.layers.convolutional.Conv2D",
        "signature": "keras.layers.convolutional.Conv2D.__init__(self, filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        super(Conv2D, self).__init__(\n            rank=2,\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs)",
        "begin_line": 449,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.initializers.Initializer.from_config#25",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Initializer",
        "signature": "keras.initializers.Initializer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        if 'dtype' in config:\n            # Initializers saved from `tf.keras`\n            # may contain an unused `dtype` argument.\n            config.pop('dtype')\n        return cls(**config)",
        "begin_line": 25,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers.Zeros.__call__#37",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.Zeros",
        "signature": "keras.initializers.Zeros.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        return K.constant(0, shape=shape, dtype=dtype)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers.VarianceScaling.__init__#180",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.VarianceScaling",
        "signature": "keras.initializers.VarianceScaling.__init__(self, scale=1.0, mode='fan_in', distribution='normal', seed=None)",
        "snippet": "    def __init__(self, scale=1.0,\n                 mode='fan_in',\n                 distribution='normal',\n                 seed=None):\n        if scale <= 0.:\n            raise ValueError('`scale` must be a positive float. Got:', scale)\n        mode = mode.lower()\n        if mode not in {'fan_in', 'fan_out', 'fan_avg'}:\n            raise ValueError('Invalid `mode` argument: '\n                             'expected on of {\"fan_in\", \"fan_out\", \"fan_avg\"} '\n                             'but got', mode)\n        distribution = distribution.lower()\n        if distribution not in {'normal', 'uniform'}:\n            raise ValueError('Invalid `distribution` argument: '\n                             'expected one of {\"normal\", \"uniform\"} '\n                             'but got', distribution)\n        self.scale = scale\n        self.mode = mode\n        self.distribution = distribution\n        self.seed = seed",
        "begin_line": 180,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers.VarianceScaling.__call__#201",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers.VarianceScaling",
        "signature": "keras.initializers.VarianceScaling.__call__(self, shape, dtype=None)",
        "snippet": "    def __call__(self, shape, dtype=None):\n        fan_in, fan_out = _compute_fans(shape)\n        scale = self.scale\n        if self.mode == 'fan_in':\n            scale /= max(1., fan_in)\n        elif self.mode == 'fan_out':\n            scale /= max(1., fan_out)\n        else:\n            scale /= max(1., float(fan_in + fan_out) / 2)\n        if self.distribution == 'normal':\n            # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n            stddev = np.sqrt(scale) / .87962566103423978\n            return K.truncated_normal(shape, 0., stddev,\n                                      dtype=dtype, seed=self.seed)\n        else:\n            limit = np.sqrt(3. * scale)\n            return K.random_uniform(shape, -limit, limit,\n                                    dtype=dtype, seed=self.seed)",
        "begin_line": 201,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers.glorot_uniform#338",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.glorot_uniform(seed=None)",
        "snippet": "def glorot_uniform(seed=None):\n    \"\"\"Glorot uniform initializer, also called Xavier uniform initializer.\n\n    It draws samples from a uniform distribution within [-limit, limit]\n    where `limit` is `sqrt(6 / (fan_in + fan_out))`\n    where `fan_in` is the number of input units in the weight tensor\n    and `fan_out` is the number of output units in the weight tensor.\n\n    # Arguments\n        seed: A Python integer. Used to seed the random generator.\n\n    # Returns\n        An initializer.\n\n    # References\n        Glorot & Bengio, AISTATS 2010\n        http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n    \"\"\"\n    return VarianceScaling(scale=1.,\n                           mode='fan_avg',\n                           distribution='uniform',\n                           seed=seed)",
        "begin_line": 338,
        "end_line": 359,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers._compute_fans#443",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers._compute_fans(shape, data_format='channels_last')",
        "snippet": "def _compute_fans(shape, data_format='channels_last'):\n    \"\"\"Computes the number of input and output units for a weight shape.\n\n    # Arguments\n        shape: Integer shape tuple.\n        data_format: Image data format to use for convolution kernels.\n            Note that all kernels in Keras are standardized on the\n            `channels_last` ordering (even when inputs are set\n            to `channels_first`).\n\n    # Returns\n        A tuple of scalars, `(fan_in, fan_out)`.\n\n    # Raises\n        ValueError: in case of invalid `data_format` argument.\n    \"\"\"\n    if len(shape) == 2:\n        fan_in = shape[0]\n        fan_out = shape[1]\n    elif len(shape) in {3, 4, 5}:\n        # Assuming convolution kernels (1D, 2D or 3D).\n        # TH kernel shape: (depth, input_depth, ...)\n        # TF kernel shape: (..., input_depth, depth)\n        if data_format == 'channels_first':\n            receptive_field_size = np.prod(shape[2:])\n            fan_in = shape[1] * receptive_field_size\n            fan_out = shape[0] * receptive_field_size\n        elif data_format == 'channels_last':\n            receptive_field_size = np.prod(shape[:-2])\n            fan_in = shape[-2] * receptive_field_size\n            fan_out = shape[-1] * receptive_field_size\n        else:\n            raise ValueError('Invalid data_format: ' + data_format)\n    else:\n        # No specific assumptions.\n        fan_in = np.sqrt(np.prod(shape))\n        fan_out = np.sqrt(np.prod(shape))\n    return fan_in, fan_out",
        "begin_line": 443,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.initializers.deserialize#487",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    return deserialize_keras_object(config,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='initializer')",
        "begin_line": 487,
        "end_line": 491,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.initializers.get#494",
        "src_path": "keras/initializers.py",
        "class_name": "keras.initializers",
        "signature": "keras.initializers.get(identifier)",
        "snippet": "def get(identifier):\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret initializer identifier: ' +\n                         str(identifier))",
        "begin_line": 494,
        "end_line": 504,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.training.Model.compile#37",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.compile(self, optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs)",
        "snippet": "    def compile(self, optimizer,\n                loss=None,\n                metrics=None,\n                loss_weights=None,\n                sample_weight_mode=None,\n                weighted_metrics=None,\n                target_tensors=None,\n                **kwargs):\n        \"\"\"Configures the model for training.\n\n        # Arguments\n            optimizer: String (name of optimizer) or optimizer instance.\n                See [optimizers](/optimizers).\n            loss: String (name of objective function) or objective function.\n                See [losses](/losses).\n                If the model has multiple outputs, you can use a different loss\n                on each output by passing a dictionary or a list of losses.\n                The loss value that will be minimized by the model\n                will then be the sum of all individual losses.\n            metrics: List of metrics to be evaluated by the model\n                during training and testing.\n                Typically you will use `metrics=['accuracy']`.\n                To specify different metrics for different outputs of a\n                multi-output model, you could also pass a dictionary,\n                such as `metrics={'output_a': 'accuracy'}`.\n            loss_weights: Optional list or dictionary specifying scalar\n                coefficients (Python floats) to weight the loss contributions\n                of different model outputs.\n                The loss value that will be minimized by the model\n                will then be the *weighted sum* of all individual losses,\n                weighted by the `loss_weights` coefficients.\n                If a list, it is expected to have a 1:1 mapping\n                to the model's outputs. If a tensor, it is expected to map\n                output names (strings) to scalar coefficients.\n            sample_weight_mode: If you need to do timestep-wise\n                sample weighting (2D weights), set this to `\"temporal\"`.\n                `None` defaults to sample-wise weights (1D).\n                If the model has multiple outputs, you can use a different\n                `sample_weight_mode` on each output by passing a\n                dictionary or a list of modes.\n            weighted_metrics: List of metrics to be evaluated and weighted\n                by sample_weight or class_weight during training and testing.\n            target_tensors: By default, Keras will create placeholders for the\n                model's target, which will be fed with the target data during\n                training. If instead you would like to use your own\n                target tensors (in turn, Keras will not expect external\n                Numpy data for these targets at training time), you\n                can specify them via the `target_tensors` argument. It can be\n                a single tensor (for a single-output model), a list of tensors,\n                or a dict mapping output names to target tensors.\n            **kwargs: When using the Theano/CNTK backends, these arguments\n                are passed into `K.function`.\n                When using the TensorFlow backend,\n                these arguments are passed into `tf.Session.run`.\n\n        # Raises\n            ValueError: In case of invalid arguments for\n                `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n        \"\"\"\n        self.optimizer = optimizers.get(optimizer)\n        self.loss = loss or []\n        self.metrics = metrics or []\n        self.loss_weights = loss_weights\n        self.sample_weight_mode = sample_weight_mode\n        self.weighted_metrics = weighted_metrics\n\n        if not self.built:\n            # Model is not compilable because\n            # it does not know its number of inputs\n            # and outputs, nor their shapes and names.\n            # We will compile after the first\n            # time the model gets called on training data.\n            return\n        self._is_compiled = True\n\n        # Prepare loss functions.\n        if isinstance(loss, dict):\n            for name in loss:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in loss '\n                                     'dictionary: \"' + name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            loss_functions = []\n            for name in self.output_names:\n                if name not in loss:\n                    warnings.warn('Output \"' + name +\n                                  '\" missing from loss dictionary. '\n                                  'We assume this was done on purpose, '\n                                  'and we will not be expecting '\n                                  'any data to be passed to \"' + name +\n                                  '\" during training.', stacklevel=2)\n                loss_functions.append(losses.get(loss.get(name)))\n        elif isinstance(loss, list):\n            if len(loss) != len(self.outputs):\n                raise ValueError('When passing a list as loss, '\n                                 'it should have one entry per model outputs. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed loss=' +\n                                 str(loss))\n            loss_functions = [losses.get(l) for l in loss]\n        else:\n            loss_function = losses.get(loss)\n            loss_functions = [loss_function for _ in range(len(self.outputs))]\n        self.loss_functions = loss_functions\n        weighted_losses = [\n            weighted_masked_objective(fn) for fn in loss_functions]\n        skip_target_indices = []\n        skip_target_weighing_indices = []\n        self._feed_outputs = []\n        self._feed_output_names = []\n        self._feed_output_shapes = []\n        self._feed_loss_fns = []\n        for i in range(len(weighted_losses)):\n            if weighted_losses[i] is None:\n                skip_target_indices.append(i)\n                skip_target_weighing_indices.append(i)\n\n        # Prepare output masks.\n        masks = self.compute_mask(self.inputs, mask=None)\n        if masks is None:\n            masks = [None for _ in self.outputs]\n        masks = to_list(masks)\n\n        # Prepare loss weights.\n        if loss_weights is None:\n            loss_weights_list = [1. for _ in range(len(self.outputs))]\n        elif isinstance(loss_weights, dict):\n            for name in loss_weights:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in loss_weights '\n                                     'dictionary: \"' + name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            loss_weights_list = []\n            for name in self.output_names:\n                loss_weights_list.append(loss_weights.get(name, 1.))\n        elif isinstance(loss_weights, list):\n            if len(loss_weights) != len(self.outputs):\n                raise ValueError('When passing a list as loss_weights, '\n                                 'it should have one entry per model output. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed loss_weights=' +\n                                 str(loss_weights))\n            loss_weights_list = loss_weights\n        else:\n            raise TypeError('Could not interpret loss_weights argument: ' +\n                            str(loss_weights) +\n                            ' - expected a list of dicts.')\n\n        # Prepare targets of model.\n        self.targets = []\n        self._feed_targets = []\n        if target_tensors is not None:\n            if isinstance(target_tensors, list):\n                if len(target_tensors) != len(self.outputs):\n                    raise ValueError(\n                        'When passing a list as `target_tensors`, '\n                        'it should have one entry per model output. '\n                        'The model has ' + str(len(self.outputs)) +\n                        ' outputs, but you passed target_tensors=' +\n                        str(target_tensors))\n            elif isinstance(target_tensors, dict):\n                for name in target_tensors:\n                    if name not in self.output_names:\n                        raise ValueError('Unknown entry in `target_tensors` '\n                                         'dictionary: \"' + name + '\". '\n                                         'Only expected the following keys: ' +\n                                         str(self.output_names))\n                tmp_target_tensors = []\n                for name in self.output_names:\n                    tmp_target_tensors.append(target_tensors.get(name, None))\n                target_tensors = tmp_target_tensors\n            else:\n                raise TypeError('Expected `target_tensors` to be '\n                                'a list or dict, but got:', target_tensors)\n        for i in range(len(self.outputs)):\n            if i in skip_target_indices:\n                self.targets.append(None)\n            else:\n                shape = K.int_shape(self.outputs[i])\n                name = self.output_names[i]\n                if target_tensors is not None:\n                    target = target_tensors[i]\n                else:\n                    target = None\n                if target is None or K.is_placeholder(target):\n                    if target is None:\n                        target = K.placeholder(\n                            ndim=len(shape),\n                            name=name + '_target',\n                            sparse=K.is_sparse(self.outputs[i]),\n                            dtype=K.dtype(self.outputs[i]))\n                    self._feed_targets.append(target)\n                    self._feed_outputs.append(self.outputs[i])\n                    self._feed_output_names.append(name)\n                    self._feed_output_shapes.append(shape)\n                    self._feed_loss_fns.append(self.loss_functions[i])\n                else:\n                    skip_target_weighing_indices.append(i)\n                self.targets.append(target)\n\n        # Prepare sample weights.\n        sample_weights = []\n        sample_weight_modes = []\n        if isinstance(sample_weight_mode, dict):\n            for name in sample_weight_mode:\n                if name not in self.output_names:\n                    raise ValueError('Unknown entry in '\n                                     'sample_weight_mode dictionary: \"' +\n                                     name + '\". '\n                                     'Only expected the following keys: ' +\n                                     str(self.output_names))\n            for i, name in enumerate(self.output_names):\n                if i in skip_target_weighing_indices:\n                    weight = None\n                    sample_weight_modes.append(None)\n                else:\n                    if name not in sample_weight_mode:\n                        raise ValueError('Output \"' + name +\n                                         '\" missing from sample_weight_modes '\n                                         'dictionary')\n                    if sample_weight_mode.get(name) == 'temporal':\n                        weight = K.placeholder(ndim=2,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append('temporal')\n                    else:\n                        weight = K.placeholder(ndim=1,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append(None)\n                sample_weights.append(weight)\n        elif isinstance(sample_weight_mode, list):\n            if len(sample_weight_mode) != len(self.outputs):\n                raise ValueError('When passing a list as sample_weight_mode, '\n                                 'it should have one entry per model output. '\n                                 'The model has ' + str(len(self.outputs)) +\n                                 ' outputs, but you passed '\n                                 'sample_weight_mode=' +\n                                 str(sample_weight_mode))\n            for i in range(len(self.output_names)):\n                if i in skip_target_weighing_indices:\n                    weight = None\n                    sample_weight_modes.append(None)\n                else:\n                    mode = sample_weight_mode[i]\n                    name = self.output_names[i]\n                    if mode == 'temporal':\n                        weight = K.placeholder(ndim=2,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append('temporal')\n                    else:\n                        weight = K.placeholder(ndim=1,\n                                               name=name + '_sample_weights')\n                        sample_weight_modes.append(None)\n                sample_weights.append(weight)\n        else:\n            for i, name in enumerate(self.output_names):\n                if i in skip_target_weighing_indices:\n                    sample_weight_modes.append(None)\n                    sample_weights.append(None)\n                else:\n                    if sample_weight_mode == 'temporal':\n                        sample_weights.append(\n                            K.placeholder(ndim=2,\n                                          name=name + '_sample_weights'))\n                        sample_weight_modes.append('temporal')\n                    else:\n                        sample_weights.append(\n                            K.placeholder(ndim=1,\n                                          name=name + '_sample_weights'))\n                        sample_weight_modes.append(None)\n        self.sample_weight_modes = sample_weight_modes\n        self._feed_sample_weight_modes = []\n        for i in range(len(self.outputs)):\n            if i not in skip_target_weighing_indices:\n                self._feed_sample_weight_modes.append(\n                    self.sample_weight_modes[i])\n\n        # Prepare metrics.\n        self.metrics_names = ['loss']\n        self.metrics_tensors = []\n\n        # Compute total loss.\n        total_loss = None\n        with K.name_scope('loss'):\n            for i in range(len(self.outputs)):\n                if i in skip_target_indices:\n                    continue\n                y_true = self.targets[i]\n                y_pred = self.outputs[i]\n                weighted_loss = weighted_losses[i]\n                sample_weight = sample_weights[i]\n                mask = masks[i]\n                loss_weight = loss_weights_list[i]\n                with K.name_scope(self.output_names[i] + '_loss'):\n                    output_loss = weighted_loss(y_true, y_pred,\n                                                sample_weight, mask)\n                if len(self.outputs) > 1:\n                    self.metrics_tensors.append(output_loss)\n                    self.metrics_names.append(self.output_names[i] + '_loss')\n                if total_loss is None:\n                    total_loss = loss_weight * output_loss\n                else:\n                    total_loss += loss_weight * output_loss\n            if total_loss is None:\n                if not self.losses:\n                    raise ValueError('The model cannot be compiled '\n                                     'because it has no loss to optimize.')\n                else:\n                    total_loss = 0.\n\n            # Add regularization penalties\n            # and other layer-specific losses.\n            for loss_tensor in self.losses:\n                total_loss += loss_tensor\n\n        # List of same size as output_names.\n        # contains tuples (metrics for output, names of metrics).\n        nested_metrics = collect_metrics(metrics, self.output_names)\n        nested_weighted_metrics = collect_metrics(weighted_metrics,\n                                                  self.output_names)\n        self.metrics_updates = []\n        self.stateful_metric_names = []\n        self.stateful_metric_functions = []\n\n        def handle_metrics(metrics, weights=None):\n            metric_name_prefix = 'weighted_' if weights is not None else ''\n\n            for metric in metrics:\n                if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n                    # custom handling of accuracy/crossentropy\n                    # (because of class mode duality)\n                    output_shape = K.int_shape(self.outputs[i])\n                    if (output_shape[-1] == 1 or\n                       self.loss_functions[i] == losses.binary_crossentropy):\n                        # case: binary accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.binary_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.binary_crossentropy\n                    elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n                        # case: categorical accuracy/crossentropy\n                        # with sparse targets\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.sparse_categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.sparse_categorical_crossentropy\n                    else:\n                        # case: categorical accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.categorical_crossentropy\n                    if metric in ('accuracy', 'acc'):\n                            suffix = 'acc'\n                    elif metric in ('crossentropy', 'ce'):\n                            suffix = 'ce'\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    metric_name = metric_name_prefix + suffix\n                else:\n                    metric_fn = metrics_module.get(metric)\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    # Get metric name as string\n                    if hasattr(metric_fn, 'name'):\n                        metric_name = metric_fn.name\n                    else:\n                        metric_name = metric_fn.__name__\n                    metric_name = metric_name_prefix + metric_name\n\n                with K.name_scope(metric_name):\n                    metric_result = weighted_metric_fn(y_true, y_pred,\n                                                       weights=weights,\n                                                       mask=masks[i])\n\n                # Append to self.metrics_names, self.metric_tensors,\n                # self.stateful_metric_names\n                if len(self.output_names) > 1:\n                    metric_name = self.output_names[i] + '_' + metric_name\n                # Dedupe name\n                j = 1\n                base_metric_name = metric_name\n                while metric_name in self.metrics_names:\n                    metric_name = base_metric_name + '_' + str(j)\n                    j += 1\n                self.metrics_names.append(metric_name)\n                self.metrics_tensors.append(metric_result)\n\n                # Keep track of state updates created by\n                # stateful metrics (i.e. metrics layers).\n                if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                    self.stateful_metric_names.append(metric_name)\n                    self.stateful_metric_functions.append(metric_fn)\n                    self.metrics_updates += metric_fn.updates\n        with K.name_scope('metrics'):\n            for i in range(len(self.outputs)):\n                if i in skip_target_indices:\n                    continue\n\n                y_true = self.targets[i]\n                y_pred = self.outputs[i]\n                weights = sample_weights[i]\n                output_metrics = nested_metrics[i]\n                output_weighted_metrics = nested_weighted_metrics[i]\n                handle_metrics(output_metrics)\n                handle_metrics(output_weighted_metrics, weights=weights)\n\n        # Prepare gradient updates and state updates.\n        self.total_loss = total_loss\n        self.sample_weights = sample_weights\n        self._feed_sample_weights = []\n        for i in range(len(self.sample_weights)):\n            if i not in skip_target_weighing_indices:\n                self._feed_sample_weights.append(sample_weights[i])\n\n        # Functions for train, test and predict will\n        # be compiled lazily when required.\n        # This saves time when the user is not using all functions.\n        self._function_kwargs = kwargs\n\n        self.train_function = None\n        self.test_function = None\n        self.predict_function = None\n\n        # Collected trainable weights, sorted in topological order.\n        trainable_weights = self.trainable_weights\n        self._collected_trainable_weights = trainable_weights",
        "begin_line": 37,
        "end_line": 462,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model.handle_metrics#362",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.handle_metrics(metrics, weights=None)",
        "snippet": "        def handle_metrics(metrics, weights=None):\n            metric_name_prefix = 'weighted_' if weights is not None else ''\n\n            for metric in metrics:\n                if metric in ('accuracy', 'acc', 'crossentropy', 'ce'):\n                    # custom handling of accuracy/crossentropy\n                    # (because of class mode duality)\n                    output_shape = K.int_shape(self.outputs[i])\n                    if (output_shape[-1] == 1 or\n                       self.loss_functions[i] == losses.binary_crossentropy):\n                        # case: binary accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.binary_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.binary_crossentropy\n                    elif self.loss_functions[i] == losses.sparse_categorical_crossentropy:\n                        # case: categorical accuracy/crossentropy\n                        # with sparse targets\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.sparse_categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.sparse_categorical_crossentropy\n                    else:\n                        # case: categorical accuracy/crossentropy\n                        if metric in ('accuracy', 'acc'):\n                            metric_fn = metrics_module.categorical_accuracy\n                        elif metric in ('crossentropy', 'ce'):\n                            metric_fn = metrics_module.categorical_crossentropy\n                    if metric in ('accuracy', 'acc'):\n                            suffix = 'acc'\n                    elif metric in ('crossentropy', 'ce'):\n                            suffix = 'ce'\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    metric_name = metric_name_prefix + suffix\n                else:\n                    metric_fn = metrics_module.get(metric)\n                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n                    # Get metric name as string\n                    if hasattr(metric_fn, 'name'):\n                        metric_name = metric_fn.name\n                    else:\n                        metric_name = metric_fn.__name__\n                    metric_name = metric_name_prefix + metric_name\n\n                with K.name_scope(metric_name):\n                    metric_result = weighted_metric_fn(y_true, y_pred,\n                                                       weights=weights,\n                                                       mask=masks[i])\n\n                # Append to self.metrics_names, self.metric_tensors,\n                # self.stateful_metric_names\n                if len(self.output_names) > 1:\n                    metric_name = self.output_names[i] + '_' + metric_name\n                # Dedupe name\n                j = 1\n                base_metric_name = metric_name\n                while metric_name in self.metrics_names:\n                    metric_name = base_metric_name + '_' + str(j)\n                    j += 1\n                self.metrics_names.append(metric_name)\n                self.metrics_tensors.append(metric_result)\n\n                # Keep track of state updates created by\n                # stateful metrics (i.e. metrics layers).\n                if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                    self.stateful_metric_names.append(metric_name)\n                    self.stateful_metric_functions.append(metric_fn)\n                    self.metrics_updates += metric_fn.updates",
        "begin_line": 362,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._check_trainable_weights_consistency#464",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._check_trainable_weights_consistency(self)",
        "snippet": "    def _check_trainable_weights_consistency(self):\n        \"\"\"Check trainable weights count consistency.\n\n        This will raise a warning if `trainable_weights` and\n        `_collected_trainable_weights` are inconsistent (i.e. have different\n        number of parameters).\n        Inconsistency will typically arise when one modifies `model.trainable`\n        without calling `model.compile` again.\n        \"\"\"\n        if not hasattr(self, '_collected_trainable_weights'):\n            return\n\n        if (len(self.trainable_weights) !=\n                len(self._collected_trainable_weights)):\n            warnings.warn(UserWarning(\n                'Discrepancy between trainable weights and collected trainable'\n                ' weights, did you set `model.trainable` without calling'\n                ' `model.compile` after ?'))",
        "begin_line": 464,
        "end_line": 481,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._make_train_function#483",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_train_function(self)",
        "snippet": "    def _make_train_function(self):\n        if not hasattr(self, 'train_function'):\n            raise RuntimeError('You must compile your model before using it.')\n        self._check_trainable_weights_consistency()\n        if self.train_function is None:\n            inputs = (self._feed_inputs +\n                      self._feed_targets +\n                      self._feed_sample_weights)\n            if self._uses_dynamic_learning_phase():\n                inputs += [K.learning_phase()]\n\n            with K.name_scope('training'):\n                with K.name_scope(self.optimizer.__class__.__name__):\n                    training_updates = self.optimizer.get_updates(\n                        params=self._collected_trainable_weights,\n                        loss=self.total_loss)\n                updates = (self.updates +\n                           training_updates +\n                           self.metrics_updates)\n                # Gets loss and metrics. Updates weights at each call.\n                self.train_function = K.function(\n                    inputs,\n                    [self.total_loss] + self.metrics_tensors,\n                    updates=updates,\n                    name='train_function',\n                    **self._function_kwargs)",
        "begin_line": 483,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._make_test_function#510",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_test_function(self)",
        "snippet": "    def _make_test_function(self):\n        if not hasattr(self, 'test_function'):\n            raise RuntimeError('You must compile your model before using it.')\n        if self.test_function is None:\n            inputs = (self._feed_inputs +\n                      self._feed_targets +\n                      self._feed_sample_weights)\n            if self._uses_dynamic_learning_phase():\n                inputs += [K.learning_phase()]\n            # Return loss and metrics, no gradient updates.\n            # Does update the network states.\n            self.test_function = K.function(\n                inputs,\n                [self.total_loss] + self.metrics_tensors,\n                updates=self.state_updates + self.metrics_updates,\n                name='test_function',\n                **self._function_kwargs)",
        "begin_line": 510,
        "end_line": 526,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._make_predict_function#528",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._make_predict_function(self)",
        "snippet": "    def _make_predict_function(self):\n        if not hasattr(self, 'predict_function'):\n            self.predict_function = None\n        if self.predict_function is None:\n            if self._uses_dynamic_learning_phase():\n                inputs = self._feed_inputs + [K.learning_phase()]\n            else:\n                inputs = self._feed_inputs\n            # Gets network outputs. Does not update weights.\n            # Does update the network states.\n            kwargs = getattr(self, '_function_kwargs', {})\n            self.predict_function = K.function(inputs,\n                                               self.outputs,\n                                               updates=self.state_updates,\n                                               name='predict_function',\n                                               **kwargs)",
        "begin_line": 528,
        "end_line": 543,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._uses_dynamic_learning_phase#545",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._uses_dynamic_learning_phase(self)",
        "snippet": "    def _uses_dynamic_learning_phase(self):\n        return (self.uses_learning_phase and\n                not isinstance(K.learning_phase(), int))",
        "begin_line": 545,
        "end_line": 547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model._standardize_user_data#637",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model._standardize_user_data(self, x, y=None, sample_weight=None, class_weight=None, check_array_lengths=True, batch_size=None)",
        "snippet": "    def _standardize_user_data(self, x,\n                               y=None,\n                               sample_weight=None,\n                               class_weight=None,\n                               check_array_lengths=True,\n                               batch_size=None):\n        all_inputs = []\n        if not self.built:\n            # We need to use `x` to set the model inputs.\n            # We type-check that `x` and `y` are either single arrays\n            # or lists of arrays.\n            if isinstance(x, (list, tuple)):\n                if not all(isinstance(v, np.ndarray) or\n                           K.is_tensor(v) for v in x):\n                    raise ValueError('Please provide as model inputs '\n                                     'either a single '\n                                     'array or a list of arrays. '\n                                     'You passed: x=' + str(x))\n                all_inputs += list(x)\n            elif isinstance(x, dict):\n                raise ValueError('Please do not pass a dictionary '\n                                 'as model inputs.')\n            else:\n                if not isinstance(x, np.ndarray) and not K.is_tensor(x):\n                    raise ValueError('Please provide as model inputs '\n                                     'either a single '\n                                     'array or a list of arrays. '\n                                     'You passed: x=' + str(x))\n                all_inputs.append(x)\n\n            # Build the model using the retrieved inputs (value or symbolic).\n            # If values, then in symbolic-mode placeholders will be created\n            # to match the value shapes.\n            if not self.inputs:\n                self._set_inputs(x)\n\n        if y is not None:\n            if not self.optimizer:\n                raise RuntimeError('You must compile a model before '\n                                   'training/testing. '\n                                   'Use `model.compile(optimizer, loss)`.')\n            if not self._is_compiled:\n                # On-the-fly compilation of the model.\n                # We need to use `y` to set the model targets.\n                if isinstance(y, (list, tuple)):\n                    if not all(isinstance(v, np.ndarray) or\n                               K.is_tensor(v) for v in y):\n                        raise ValueError('Please provide as model targets '\n                                         'either a single '\n                                         'array or a list of arrays. '\n                                         'You passed: y=' + str(y))\n                elif isinstance(y, dict):\n                    raise ValueError('Please do not pass a dictionary '\n                                     'as model targets.')\n                else:\n                    if not isinstance(y, np.ndarray) and not K.is_tensor(y):\n                        raise ValueError('Please provide as model targets '\n                                         'either a single '\n                                         'array or a list of arrays. '\n                                         'You passed: y=' + str(y))\n                # Typecheck that all inputs are *either* value *or* symbolic.\n                if y is not None:\n                    if isinstance(y, (list, tuple)):\n                        all_inputs += list(y)\n                    else:\n                        all_inputs.append(y)\n                if any(K.is_tensor(v) for v in all_inputs):\n                    if not all(K.is_tensor(v) for v in all_inputs):\n                        raise ValueError('Do not pass inputs that mix Numpy '\n                                         'arrays and symbolic tensors. '\n                                         'You passed: x=' + str(x) +\n                                         '; y=' + str(y))\n\n                # Handle target tensors if any passed.\n                if not isinstance(y, (list, tuple)):\n                    y = [y]\n                target_tensors = [v for v in y if K.is_tensor(v)]\n                if not target_tensors:\n                    target_tensors = None\n                self.compile(optimizer=self.optimizer,\n                             loss=self.loss,\n                             metrics=self.metrics,\n                             loss_weights=self.loss_weights,\n                             target_tensors=target_tensors)\n\n        # If `x` and `y` were all symbolic,\n        # then the model should not be fed any inputs and targets.\n        # Note: in this case, `any` and `all` are equivalent since we disallow\n        # mixed symbolic/value inputs.\n        if any(K.is_tensor(v) for v in all_inputs):\n            return [], [], []\n\n        # What follows is input validation and standardization to list format,\n        # in the case where all inputs are value arrays.\n\n        if not self._is_graph_network:\n            # Case: symbolic-mode subclassed network.\n            # Do not do shape validation.\n            feed_input_names = self._feed_input_names\n            feed_input_shapes = None\n        else:\n            # Case: symbolic-mode graph network.\n            # In this case, we run extensive shape validation checks.\n            feed_input_names = self._feed_input_names\n            feed_input_shapes = self._feed_input_shapes\n\n        # Standardize the inputs.\n        x = standardize_input_data(\n            x,\n            feed_input_names,\n            feed_input_shapes,\n            check_batch_axis=False,  # Don't enforce the batch size.\n            exception_prefix='input')\n\n        if y is not None:\n            if not self._is_graph_network:\n                feed_output_names = self._feed_output_names\n                feed_output_shapes = None\n                # Sample weighting not supported in this case.\n                # TODO: consider supporting it.\n                feed_sample_weight_modes = [None for _ in self.outputs]\n            else:\n                feed_output_names = self._feed_output_names\n                feed_sample_weight_modes = self._feed_sample_weight_modes\n                feed_output_shapes = []\n                for output_shape, loss_fn in zip(self._feed_output_shapes,\n                                                 self._feed_loss_fns):\n                    if loss_fn is losses.sparse_categorical_crossentropy:\n                        if K.image_data_format() == 'channels_first' and len(\n                                output_shape) in [4, 5]:\n                            feed_output_shapes.append(\n                                (output_shape[0], 1) + output_shape[2:])\n                        else:\n                            feed_output_shapes.append(output_shape[:-1] + (1,))\n                    elif (not hasattr(loss_fn, '__name__') or\n                            getattr(losses, loss_fn.__name__, None) is None):\n                        # If `loss_fn` is not a function (e.g. callable class)\n                        # or if it not in the `losses` module, then\n                        # it is a user-defined loss and we make no assumptions\n                        # about it.\n                        feed_output_shapes.append(None)\n                    else:\n                        feed_output_shapes.append(output_shape)\n\n            # Standardize the outputs.\n            y = standardize_input_data(\n                y,\n                feed_output_names,\n                feed_output_shapes,\n                check_batch_axis=False,  # Don't enforce the batch size.\n                exception_prefix='target')\n\n            # Generate sample-wise weight values given the `sample_weight` and\n            # `class_weight` arguments.\n            sample_weights = standardize_sample_weights(\n                sample_weight, feed_output_names)\n            class_weights = standardize_class_weights(\n                class_weight, feed_output_names)\n            sample_weights = [\n                standardize_weights(ref, sw, cw, mode)\n                for (ref, sw, cw, mode) in\n                zip(y, sample_weights, class_weights,\n                    feed_sample_weight_modes)\n            ]\n            # Check that all arrays have the same length.\n            check_array_length_consistency(x, y, sample_weights)\n            if self._is_graph_network:\n                # Additional checks to avoid users mistakenly\n                # using improper loss fns.\n                check_loss_and_target_compatibility(\n                    y, self._feed_loss_fns, feed_output_shapes)\n        else:\n            y = []\n            sample_weights = []\n\n        if self.stateful and batch_size:\n            # Check that for stateful networks, number of samples is a multiple\n            # of the static batch size.\n            if x[0].shape[0] % batch_size != 0:\n                raise ValueError('In a stateful network, '\n                                 'you should only pass inputs with '\n                                 'a number of samples that can be '\n                                 'divided by the batch size. Found: ' +\n                                 str(x[0].shape[0]) + ' samples')\n        return x, y, sample_weights",
        "begin_line": 637,
        "end_line": 821,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model.fit#823",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, **kwargs)",
        "snippet": "    def fit(self,\n            x=None,\n            y=None,\n            batch_size=None,\n            epochs=1,\n            verbose=1,\n            callbacks=None,\n            validation_split=0.,\n            validation_data=None,\n            shuffle=True,\n            class_weight=None,\n            sample_weight=None,\n            initial_epoch=0,\n            steps_per_epoch=None,\n            validation_steps=None,\n            **kwargs):\n        \"\"\"Trains the model for a given number of epochs (iterations on a dataset).\n\n        # Arguments\n            x: Numpy array of training data (if the model has a single input),\n                or list of Numpy arrays (if the model has multiple inputs).\n                If input layers in the model are named, you can also pass a\n                dictionary mapping input names to Numpy arrays.\n                `x` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            y: Numpy array of target (label) data\n                (if the model has a single output),\n                or list of Numpy arrays (if the model has multiple outputs).\n                If output layers in the model are named, you can also pass a\n                dictionary mapping output names to Numpy arrays.\n                `y` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            batch_size: Integer or `None`.\n                Number of samples per gradient update.\n                If unspecified, `batch_size` will default to 32.\n            epochs: Integer. Number of epochs to train the model.\n                An epoch is an iteration over the entire `x` and `y`\n                data provided.\n                Note that in conjunction with `initial_epoch`,\n                `epochs` is to be understood as \"final epoch\".\n                The model is not trained for a number of iterations\n                given by `epochs`, but merely until the epoch\n                of index `epochs` is reached.\n            verbose: Integer. 0, 1, or 2. Verbosity mode.\n                0 = silent, 1 = progress bar, 2 = one line per epoch.\n            callbacks: List of `keras.callbacks.Callback` instances.\n                List of callbacks to apply during training.\n                See [callbacks](/callbacks).\n            validation_split: Float between 0 and 1.\n                Fraction of the training data to be used as validation data.\n                The model will set apart this fraction of the training data,\n                will not train on it, and will evaluate\n                the loss and any model metrics\n                on this data at the end of each epoch.\n                The validation data is selected from the last samples\n                in the `x` and `y` data provided, before shuffling.\n            validation_data: tuple `(x_val, y_val)` or tuple\n                `(x_val, y_val, val_sample_weights)` on which to evaluate\n                the loss and any model metrics at the end of each epoch.\n                The model will not be trained on this data.\n                `validation_data` will override `validation_split`.\n            shuffle: Boolean (whether to shuffle the training data\n                before each epoch) or str (for 'batch').\n                'batch' is a special option for dealing with the\n                limitations of HDF5 data; it shuffles in batch-sized chunks.\n                Has no effect when `steps_per_epoch` is not `None`.\n            class_weight: Optional dictionary mapping class indices (integers)\n                to a weight (float) value, used for weighting the loss function\n                (during training only).\n                This can be useful to tell the model to\n                \"pay more attention\" to samples from\n                an under-represented class.\n            sample_weight: Optional Numpy array of weights for\n                the training samples, used for weighting the loss function\n                (during training only). You can either pass a flat (1D)\n                Numpy array with the same length as the input samples\n                (1:1 mapping between weights and samples),\n                or in the case of temporal data,\n                you can pass a 2D array with shape\n                `(samples, sequence_length)`,\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                `sample_weight_mode=\"temporal\"` in `compile()`.\n            initial_epoch: Integer.\n                Epoch at which to start training\n                (useful for resuming a previous training run).\n            steps_per_epoch: Integer or `None`.\n                Total number of steps (batches of samples)\n                before declaring one epoch finished and starting the\n                next epoch. When training with input tensors such as\n                TensorFlow data tensors, the default `None` is equal to\n                the number of samples in your dataset divided by\n                the batch size, or 1 if that cannot be determined.\n            validation_steps: Only relevant if `steps_per_epoch`\n                is specified. Total number of steps (batches of samples)\n                to validate before stopping.\n\n        # Returns\n            A `History` object. Its `History.history` attribute is\n            a record of training loss values and metrics values\n            at successive epochs, as well as validation loss values\n            and validation metrics values (if applicable).\n\n        # Raises\n            RuntimeError: If the model was never compiled.\n            ValueError: In case of mismatch between the provided input data\n                and what the model expects.\n        \"\"\"\n        # Backwards compatibility\n        if batch_size is None and steps_per_epoch is None:\n            batch_size = 32\n        # Legacy support\n        if 'nb_epoch' in kwargs:\n            warnings.warn('The `nb_epoch` argument in `fit` '\n                          'has been renamed `epochs`.', stacklevel=2)\n            epochs = kwargs.pop('nb_epoch')\n        if kwargs:\n            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n        if x is None and y is None and steps_per_epoch is None:\n            raise ValueError('If fitting from data tensors, '\n                             'you should specify the `steps_per_epoch` '\n                             'argument.')\n        # Validate user data.\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight,\n            class_weight=class_weight,\n            batch_size=batch_size)\n        # Prepare validation data.\n        do_validation = False\n        if validation_data:\n            do_validation = True\n            if len(validation_data) == 2:\n                val_x, val_y = validation_data\n                val_sample_weight = None\n            elif len(validation_data) == 3:\n                val_x, val_y, val_sample_weight = validation_data\n            else:\n                raise ValueError('When passing validation_data, '\n                                 'it must contain 2 (x_val, y_val) '\n                                 'or 3 (x_val, y_val, val_sample_weights) '\n                                 'items, however it contains %d items' %\n                                 len(validation_data))\n\n            val_x, val_y, val_sample_weights = self._standardize_user_data(\n                val_x, val_y,\n                sample_weight=val_sample_weight,\n                batch_size=batch_size)\n            if self._uses_dynamic_learning_phase():\n                val_ins = val_x + val_y + val_sample_weights + [0.]\n            else:\n                val_ins = val_x + val_y + val_sample_weights\n\n        elif validation_split and 0. < validation_split < 1.:\n            if any(K.is_tensor(t) for t in x):\n                raise ValueError(\n                    'If your data is in the form of symbolic tensors, '\n                    'you cannot use `validation_split`.')\n            do_validation = True\n            if hasattr(x[0], 'shape'):\n                split_at = int(int(x[0].shape[0]) * (1. - validation_split))\n            else:\n                split_at = int(len(x[0]) * (1. - validation_split))\n            x, val_x = (slice_arrays(x, 0, split_at),\n                        slice_arrays(x, split_at))\n            y, val_y = (slice_arrays(y, 0, split_at),\n                        slice_arrays(y, split_at))\n            sample_weights, val_sample_weights = (\n                slice_arrays(sample_weights, 0, split_at),\n                slice_arrays(sample_weights, split_at))\n            if self._uses_dynamic_learning_phase():\n                val_ins = val_x + val_y + val_sample_weights + [0.]\n            else:\n                val_ins = val_x + val_y + val_sample_weights\n\n        elif validation_steps:\n            do_validation = True\n            if self._uses_dynamic_learning_phase():\n                val_ins = [0.]\n\n        # Prepare input arrays and training function.\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [1.]\n        else:\n            ins = x + y + sample_weights\n        self._make_train_function()\n        f = self.train_function\n\n        # Prepare display labels.\n        out_labels = self.metrics_names\n\n        if do_validation:\n            self._make_test_function()\n            val_f = self.test_function\n            callback_metrics = copy.copy(out_labels) + [\n                'val_' + n for n in out_labels]\n        else:\n            callback_metrics = copy.copy(out_labels)\n            val_f = None\n            val_ins = []\n\n        # Delegate logic to `fit_loop`.\n        return training_arrays.fit_loop(self, f, ins,\n                                        out_labels=out_labels,\n                                        batch_size=batch_size,\n                                        epochs=epochs,\n                                        verbose=verbose,\n                                        callbacks=callbacks,\n                                        val_f=val_f,\n                                        val_ins=val_ins,\n                                        shuffle=shuffle,\n                                        callback_metrics=callback_metrics,\n                                        initial_epoch=initial_epoch,\n                                        steps_per_epoch=steps_per_epoch,\n                                        validation_steps=validation_steps)",
        "begin_line": 823,
        "end_line": 1037,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model.evaluate#1039",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.evaluate(self, x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None)",
        "snippet": "    def evaluate(self, x=None, y=None,\n                 batch_size=None,\n                 verbose=1,\n                 sample_weight=None,\n                 steps=None):\n        \"\"\"Returns the loss value & metrics values for the model in test mode.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: Numpy array of test data (if the model has a single input),\n                or list of Numpy arrays (if the model has multiple inputs).\n                If input layers in the model are named, you can also pass a\n                dictionary mapping input names to Numpy arrays.\n                `x` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            y: Numpy array of target (label) data\n                (if the model has a single output),\n                or list of Numpy arrays (if the model has multiple outputs).\n                If output layers in the model are named, you can also pass a\n                dictionary mapping output names to Numpy arrays.\n                `y` can be `None` (default) if feeding from\n                framework-native tensors (e.g. TensorFlow data tensors).\n            batch_size: Integer or `None`.\n                Number of samples per evaluation step.\n                If unspecified, `batch_size` will default to 32.\n            verbose: 0 or 1. Verbosity mode.\n                0 = silent, 1 = progress bar.\n            sample_weight: Optional Numpy array of weights for\n                the test samples, used for weighting the loss function.\n                You can either pass a flat (1D)\n                Numpy array with the same length as the input samples\n                (1:1 mapping between weights and samples),\n                or in the case of temporal data,\n                you can pass a 2D array with shape\n                `(samples, sequence_length)`,\n                to apply a different weight to every timestep of every sample.\n                In this case you should make sure to specify\n                `sample_weight_mode=\"temporal\"` in `compile()`.\n            steps: Integer or `None`.\n                Total number of steps (batches of samples)\n                before declaring the evaluation round finished.\n                Ignored with the default value of `None`.\n\n        # Returns\n            Scalar test loss (if the model has a single output and no metrics)\n            or list of scalars (if the model has multiple outputs\n            and/or metrics). The attribute `model.metrics_names` will give you\n            the display labels for the scalar outputs.\n        \"\"\"\n        # Backwards compatibility.\n        if batch_size is None and steps is None:\n            batch_size = 32\n        if x is None and y is None and steps is None:\n            raise ValueError('If evaluating from data tensors, '\n                             'you should specify the `steps` '\n                             'argument.')\n        # Validate user data.\n        x, y, sample_weights = self._standardize_user_data(\n            x, y,\n            sample_weight=sample_weight,\n            batch_size=batch_size)\n        # Prepare inputs, delegate logic to `test_loop`.\n        if self._uses_dynamic_learning_phase():\n            ins = x + y + sample_weights + [0.]\n        else:\n            ins = x + y + sample_weights\n        self._make_test_function()\n        f = self.test_function\n        return training_arrays.test_loop(self, f, ins,\n                                         batch_size=batch_size,\n                                         verbose=verbose,\n                                         steps=steps)",
        "begin_line": 1039,
        "end_line": 1111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training.Model.predict#1113",
        "src_path": "keras/engine/training.py",
        "class_name": "keras.engine.training.Model",
        "signature": "keras.engine.training.Model.predict(self, x, batch_size=None, verbose=0, steps=None)",
        "snippet": "    def predict(self, x,\n                batch_size=None,\n                verbose=0,\n                steps=None):\n        \"\"\"Generates output predictions for the input samples.\n\n        Computation is done in batches.\n\n        # Arguments\n            x: The input data, as a Numpy array\n                (or list of Numpy arrays if the model has multiple inputs).\n            batch_size: Integer. If unspecified, it will default to 32.\n            verbose: Verbosity mode, 0 or 1.\n            steps: Total number of steps (batches of samples)\n                before declaring the prediction round finished.\n                Ignored with the default value of `None`.\n\n        # Returns\n            Numpy array(s) of predictions.\n\n        # Raises\n            ValueError: In case of mismatch between the provided\n                input data and the model's expectations,\n                or in case a stateful model receives a number of samples\n                that is not a multiple of the batch size.\n        \"\"\"\n        # Backwards compatibility.\n        if batch_size is None and steps is None:\n            batch_size = 32\n        if x is None and steps is None:\n            raise ValueError('If predicting from data tensors, '\n                             'you should specify the `steps` '\n                             'argument.')\n        # Validate user data.\n        x, _, _ = self._standardize_user_data(x)\n        if self.stateful:\n            if x[0].shape[0] > batch_size and x[0].shape[0] % batch_size != 0:\n                raise ValueError('In a stateful network, '\n                                 'you should only pass inputs with '\n                                 'a number of samples that can be '\n                                 'divided by the batch size. Found: ' +\n                                 str(x[0].shape[0]) + ' samples. '\n                                 'Batch size: ' + str(batch_size) + '.')\n\n        # Prepare inputs, delegate logic to `predict_loop`.\n        if self._uses_dynamic_learning_phase():\n            ins = x + [0.]\n        else:\n            ins = x\n        self._make_predict_function()\n        f = self.predict_function\n        return training_arrays.predict_loop(self, f, ins,\n                                            batch_size=batch_size,\n                                            verbose=verbose,\n                                            steps=steps)",
        "begin_line": 1113,
        "end_line": 1167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_single_array#15",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_single_array(x)",
        "snippet": "def standardize_single_array(x):\n    if x is None:\n        return None\n    elif K.is_tensor(x):\n        shape = K.int_shape(x)\n        if shape is None or shape[0] is None:\n            raise ValueError(\n                'When feeding symbolic tensors to a model, we expect the'\n                'tensors to have a static batch size. '\n                'Got tensor with shape: %s' % str(shape))\n        return x\n    elif x.ndim == 1:\n        x = np.expand_dims(x, 1)\n    return x",
        "begin_line": 15,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_input_data#31",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_input_data(data, names, shapes=None, check_batch_axis=True, exception_prefix='')",
        "snippet": "def standardize_input_data(data,\n                           names,\n                           shapes=None,\n                           check_batch_axis=True,\n                           exception_prefix=''):\n    \"\"\"Normalizes inputs and targets provided by users.\n\n    Users may pass data as a list of arrays, dictionary of arrays,\n    or as a single array. We normalize this to an ordered list of\n    arrays (same order as `names`), while checking that the provided\n    arrays have shapes that match the network's expectations.\n\n    # Arguments\n        data: User-provided input data (polymorphic).\n        names: List of expected array names.\n        shapes: Optional list of expected array shapes.\n        check_batch_axis: Boolean; whether to check that\n            the batch axis of the arrays matches the expected\n            value found in `shapes`.\n        exception_prefix: String prefix used for exception formatting.\n\n    # Returns\n        List of standardized input arrays (one array per model input).\n\n    # Raises\n        ValueError: in case of improperly formatted user-provided data.\n    \"\"\"\n    if not names:\n        if data is not None and hasattr(data, '__len__') and len(data):\n            raise ValueError('Error when checking model ' +\n                             exception_prefix + ': '\n                             'expected no data, but got:', data)\n        return []\n    if data is None:\n        return [None for _ in range(len(names))]\n\n    if isinstance(data, dict):\n        try:\n            data = [\n                data[x].values\n                if data[x].__class__.__name__ == 'DataFrame' else data[x]\n                for x in names\n            ]\n        except KeyError as e:\n            raise ValueError('No data provided for \"' + e.args[0] +\n                             '\". Need data '\n                             'for each key in: ' + str(names))\n    elif isinstance(data, list):\n        if isinstance(data[0], list):\n            data = [np.asarray(d) for d in data]\n        elif len(names) == 1 and isinstance(data[0], (float, int)):\n            data = [np.asarray(data)]\n        else:\n            data = [\n                x.values if x.__class__.__name__ == 'DataFrame'\n                else x for x in data\n            ]\n    else:\n        data = data.values if data.__class__.__name__ == 'DataFrame' else data\n        data = [data]\n    data = [standardize_single_array(x) for x in data]\n\n    if len(data) != len(names):\n        if data and hasattr(data[0], 'shape'):\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': the list of Numpy arrays that you are passing to '\n                'your model is not the size the model expected. '\n                'Expected to see ' + str(len(names)) + ' array(s), '\n                'but instead got the following list of ' +\n                str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n        elif len(names) > 1:\n            raise ValueError(\n                'Error when checking model ' + exception_prefix +\n                ': you are passing a list as input to your model, '\n                'but the model expects a list of ' + str(len(names)) +\n                ' Numpy arrays instead. '\n                'The list you passed was: ' + str(data)[:200])\n        elif len(data) == 1 and not hasattr(data[0], 'shape'):\n            raise TypeError('Error when checking model ' + exception_prefix +\n                            ': data should be a Numpy array, or list/dict of '\n                            'Numpy arrays. Found: ' + str(data)[:200] + '...')\n        elif len(names) == 1:\n            data = [np.asarray(data)]\n\n    # Check shapes compatibility.\n    if shapes:\n        for i in range(len(names)):\n            if shapes[i] is not None and not K.is_tensor(data[i]):\n                data_shape = data[i].shape\n                shape = shapes[i]\n                if data[i].ndim != len(shape):\n                    raise ValueError(\n                        'Error when checking ' + exception_prefix +\n                        ': expected ' + names[i] + ' to have ' +\n                        str(len(shape)) + ' dimensions, but got array '\n                        'with shape ' + str(data_shape))\n                if not check_batch_axis:\n                    data_shape = data_shape[1:]\n                    shape = shape[1:]\n                for dim, ref_dim in zip(data_shape, shape):\n                    if ref_dim != dim and ref_dim:\n                        raise ValueError(\n                            'Error when checking ' + exception_prefix +\n                            ': expected ' + names[i] + ' to have shape ' +\n                            str(shape) + ' but got array with shape ' +\n                            str(data_shape))\n    return data",
        "begin_line": 31,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_sample_or_class_weights#141",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_sample_or_class_weights(x_weight, output_names, weight_type)",
        "snippet": "def standardize_sample_or_class_weights(x_weight,\n                                        output_names,\n                                        weight_type):\n    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n\n    # Arguments\n        x_weight: User-provided `sample_weight` or `class_weight` argument.\n        output_names: List of output names (strings) in the model.\n        weight_type: A string used purely for exception printing.\n\n    # Returns\n        A list of `sample_weight` or `class_weight` where there are exactly\n            one element per model output.\n\n    # Raises\n        ValueError: In case of invalid user-provided argument.\n    \"\"\"\n    if x_weight is None or len(x_weight) == 0:\n        return [None for _ in output_names]\n    if len(output_names) == 1:\n        if isinstance(x_weight, list) and len(x_weight) == 1:\n            return x_weight\n        if isinstance(x_weight, dict) and output_names[0] in x_weight:\n            return [x_weight[output_names[0]]]\n        else:\n            return [x_weight]\n    if isinstance(x_weight, list):\n        if len(x_weight) != len(output_names):\n            raise ValueError('Provided `' + weight_type + '` was a list of ' +\n                             str(len(x_weight)) +\n                             ' elements, but the model has ' +\n                             str(len(output_names)) + ' outputs. '\n                             'You should provide one `' + weight_type + '`'\n                             'array per model output.')\n        return x_weight\n    if isinstance(x_weight, dict):\n        x_weights = []\n        for name in output_names:\n            x_weights.append(x_weight.get(name))\n        return x_weights\n    else:\n        raise TypeError('The model has multiple outputs, so `' +\n                        weight_type + '` '\n                        'should be either a list or a dict. '\n                        'Provided `' + weight_type +\n                        '` type not understood: ' +\n                        str(x_weight))",
        "begin_line": 141,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_class_weights#190",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_class_weights(class_weight, output_names)",
        "snippet": "def standardize_class_weights(class_weight, output_names):\n    return standardize_sample_or_class_weights(class_weight,\n                                               output_names,\n                                               'class_weight')",
        "begin_line": 190,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_sample_weights#196",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_sample_weights(sample_weight, output_names)",
        "snippet": "def standardize_sample_weights(sample_weight, output_names):\n    return standardize_sample_or_class_weights(sample_weight,\n                                               output_names,\n                                               'sample_weight')",
        "begin_line": 196,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.check_array_length_consistency#202",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_array_length_consistency(inputs, targets, weights=None)",
        "snippet": "def check_array_length_consistency(inputs, targets, weights=None):\n    \"\"\"Checks if batch axes are the same for numpy arrays.\n\n    # Arguments\n        inputs: list of Numpy arrays of inputs.\n        targets: list of Numpy arrays of targets.\n        weights: list of Numpy arrays of sample weights.\n\n    # Raises\n        ValueError: in case of incorrectly formatted data.\n    \"\"\"\n    def set_of_lengths(x):\n        # return a set with the variation between\n        # different shapes, with None => 0\n        if x is None:\n            return {0}\n        else:\n            return set([0 if y is None else int(y.shape[0]) for y in x])\n\n    set_x = set_of_lengths(inputs)\n    set_y = set_of_lengths(targets)\n    set_w = set_of_lengths(weights)\n    if len(set_x) > 1:\n        raise ValueError('All input arrays (x) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([x.shape for x in inputs]))\n    if len(set_y) > 1:\n        raise ValueError('All target arrays (y) should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([y.shape for y in targets]))\n    if set_x and set_y and list(set_x)[0] != list(set_y)[0]:\n        raise ValueError('Input arrays should have '\n                         'the same number of samples as target arrays. '\n                         'Found ' + str(list(set_x)[0]) + ' input samples '\n                         'and ' + str(list(set_y)[0]) + ' target samples.')\n    if len(set_w) > 1:\n        raise ValueError('All sample_weight arrays should have '\n                         'the same number of samples. Got array shapes: ' +\n                         str([w.shape for w in weights]))\n    if set_y and set_w and list(set_y)[0] != list(set_w)[0]:\n        raise ValueError('Sample_weight arrays should have '\n                         'the same number of samples as target arrays. Got ' +\n                         str(list(set_y)[0]) + ' input samples and ' +\n                         str(list(set_w)[0]) + ' target samples.')",
        "begin_line": 202,
        "end_line": 245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.set_of_lengths#213",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.set_of_lengths(x)",
        "snippet": "    def set_of_lengths(x):\n        # return a set with the variation between\n        # different shapes, with None => 0\n        if x is None:\n            return {0}\n        else:\n            return set([0 if y is None else int(y.shape[0]) for y in x])",
        "begin_line": 213,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.check_loss_and_target_compatibility#248",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_loss_and_target_compatibility(targets, loss_fns, output_shapes)",
        "snippet": "def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n    \"\"\"Does validation on the compatibility of targets and loss functions.\n\n    This helps prevent users from using loss functions incorrectly.\n\n    # Arguments\n        targets: list of Numpy arrays of targets.\n        loss_fns: list of loss functions.\n        output_shapes: list of shapes of model outputs.\n\n    # Raises\n        ValueError: if a loss function or target array\n            is incompatible with an output.\n    \"\"\"\n    key_losses = {losses.mean_squared_error,\n                  losses.binary_crossentropy,\n                  losses.categorical_crossentropy}\n    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n        if y is None or loss is None:\n            continue\n        if loss is losses.categorical_crossentropy:\n            if y.shape[-1] == 1:\n                raise ValueError(\n                    'You are passing a target array of shape ' + str(y.shape) +\n                    ' while using as loss `categorical_crossentropy`. '\n                    '`categorical_crossentropy` expects '\n                    'targets to be binary matrices (1s and 0s) '\n                    'of shape (samples, classes). '\n                    'If your targets are integer classes, '\n                    'you can convert them to the expected format via:\\n'\n                    '```\\n'\n                    'from keras.utils import to_categorical\\n'\n                    'y_binary = to_categorical(y_int)\\n'\n                    '```\\n'\n                    '\\n'\n                    'Alternatively, you can use the loss function '\n                    '`sparse_categorical_crossentropy` instead, '\n                    'which does expect integer targets.')\n        if loss in key_losses:\n            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n                if out_dim is not None and target_dim != out_dim:\n                    raise ValueError(\n                        'A target array with shape ' + str(y.shape) +\n                        ' was passed for an output of shape ' + str(shape) +\n                        ' while using as loss `' + loss.__name__ + '`. '\n                        'This loss expects '\n                        'targets to have the same shape '\n                        'as the output.')",
        "begin_line": 248,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.collect_metrics#298",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.collect_metrics(metrics, output_names)",
        "snippet": "def collect_metrics(metrics, output_names):\n    \"\"\"Maps metric functions to model outputs.\n\n    # Arguments\n        metrics: a list or dict of metric functions.\n        output_names: a list of the names (strings) of model outputs.\n\n    # Returns\n        A list (one entry per model output) of lists of metric functions.\n        For instance, if the model has 2 outputs, and for the first output\n        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n        and just \"binary_accuracy\" for the second output,\n        the list would look like:\n            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n\n    # Raises\n        TypeError: if an incorrect type is passed for the `metrics` argument.\n    \"\"\"\n    if not metrics:\n        return [[] for _ in output_names]\n    if isinstance(metrics, list):\n        # we then apply all metrics to all outputs.\n        return [copy.copy(metrics) for _ in output_names]\n    elif isinstance(metrics, dict):\n        nested_metrics = []\n        for name in output_names:\n            output_metrics = metrics.get(name, [])\n            output_metrics = to_list(output_metrics)\n            nested_metrics.append(output_metrics)\n        return nested_metrics\n    else:\n        raise TypeError('Type of `metrics` argument not understood. '\n                        'Expected a list or dictionary, found: ' +\n                        str(metrics))",
        "begin_line": 298,
        "end_line": 331,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.batch_shuffle#334",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.batch_shuffle(index_array, batch_size)",
        "snippet": "def batch_shuffle(index_array, batch_size):\n    \"\"\"Shuffles an array in a batch-wise fashion.\n\n    Useful for shuffling HDF5 arrays\n    (where one cannot access arbitrary indices).\n\n    # Arguments\n        index_array: array of indices to be shuffled.\n        batch_size: integer.\n\n    # Returns\n        The `index_array` array, shuffled in a batch-wise fashion.\n    \"\"\"\n    batch_count = int(len(index_array) / batch_size)\n    # to reshape we need to be cleanly divisible by batch size\n    # we stash extra items and reappend them after shuffling\n    last_batch = index_array[batch_count * batch_size:]\n    index_array = index_array[:batch_count * batch_size]\n    index_array = index_array.reshape((batch_count, batch_size))\n    np.random.shuffle(index_array)\n    index_array = index_array.flatten()\n    return np.append(index_array, last_batch)",
        "begin_line": 334,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.make_batches#358",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.make_batches(size, batch_size)",
        "snippet": "def make_batches(size, batch_size):\n    \"\"\"Returns a list of batch indices (tuples of indices).\n\n    # Arguments\n        size: Integer, total size of the data to slice into batches.\n        batch_size: Integer, batch size.\n\n    # Returns\n        A list of tuples of array indices.\n    \"\"\"\n    num_batches = (size + batch_size - 1) // batch_size  # round up\n    return [(i * batch_size, min(size, (i + 1) * batch_size))\n            for i in range(num_batches)]",
        "begin_line": 358,
        "end_line": 370,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.weighted_masked_objective#373",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.weighted_masked_objective(fn)",
        "snippet": "def weighted_masked_objective(fn):\n    \"\"\"Adds support for masking and sample-weighting to an objective function.\n\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    \"\"\"\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        # score_array has ndim >= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\n    return weighted",
        "begin_line": 373,
        "end_line": 423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.weighted#390",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.weighted(y_true, y_pred, weights, mask=None)",
        "snippet": "    def weighted(y_true, y_pred, weights, mask=None):\n        \"\"\"Wrapper function.\n\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n\n        # Returns\n            Scalar tensor.\n        \"\"\"\n        # score_array has ndim >= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)",
        "begin_line": 390,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.standardize_weights#426",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.standardize_weights(y, sample_weight=None, class_weight=None, sample_weight_mode=None)",
        "snippet": "def standardize_weights(y,\n                        sample_weight=None,\n                        class_weight=None,\n                        sample_weight_mode=None):\n    \"\"\"Performs sample weight validation and standardization.\n\n    Everything gets normalized to a single sample-wise (or timestep-wise)\n    weight array.\n\n    # Arguments\n        y: Numpy array of model targets to be weighted.\n        sample_weight: User-provided `sample_weight` argument.\n        class_weight: User-provided `class_weight` argument.\n        sample_weight_mode: One of `None` or `\"temporal\"`.\n            `\"temporal\"` indicated that we expect 2D weight data\n            that will be applied to the last 2 dimensions of\n            the targets (i.e. we are weighting timesteps, not samples).\n\n    # Returns\n        A numpy array of target weights, one entry per sample to weight.\n\n    # Raises\n        ValueError: In case of invalid user-provided arguments.\n    \"\"\"\n    if sample_weight_mode is not None:\n        if sample_weight_mode != 'temporal':\n            raise ValueError('\"sample_weight_mode '\n                             'should be None or \"temporal\". '\n                             'Found: ' + str(sample_weight_mode))\n        if len(y.shape) < 3:\n            raise ValueError('Found a sample_weight array for '\n                             'an input with shape ' +\n                             str(y.shape) + '. '\n                             'Timestep-wise sample weighting (use of '\n                             'sample_weight_mode=\"temporal\") is restricted to '\n                             'outputs that are at least 3D, i.e. that have '\n                             'a time dimension.')\n        if sample_weight is not None and len(sample_weight.shape) != 2:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weighting, '\n                             'you should pass a 2D sample_weight array.')\n    else:\n        if sample_weight is not None and len(sample_weight.shape) != 1:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) + '. '\n                             'In order to use timestep-wise sample weights, '\n                             'you should specify '\n                             'sample_weight_mode=\"temporal\" '\n                             'in compile(). If you just mean to use '\n                             'sample-wise weights, make sure your '\n                             'sample_weight array is 1D.')\n\n    if sample_weight is not None:\n        if len(sample_weight.shape) > len(y.shape):\n            raise ValueError('Found a sample_weight with shape' +\n                             str(sample_weight.shape) + '.'\n                             'Expected sample_weight with rank '\n                             'less than or equal to ' + str(len(y.shape)))\n\n        if y.shape[:sample_weight.ndim] != sample_weight.shape:\n            raise ValueError('Found a sample_weight array with shape ' +\n                             str(sample_weight.shape) +\n                             ' for an input with shape ' +\n                             str(y.shape) + '. '\n                             'sample_weight cannot be broadcast.')\n        return sample_weight\n    elif isinstance(class_weight, dict):\n        if len(y.shape) > 2:\n            raise ValueError('`class_weight` not supported for '\n                             '3+ dimensional targets.')\n        if y.shape[1] > 1:\n            y_classes = np.argmax(y, axis=1)\n        elif y.shape[1] == 1:\n            y_classes = np.reshape(y, y.shape[0])\n        else:\n            y_classes = y\n\n        weights = np.asarray([class_weight[cls] for cls in y_classes\n                              if cls in class_weight])\n\n        if len(weights) != len(y_classes):\n            # subtract the sets to pick all missing classes\n            existing_classes = set(y_classes)\n            existing_class_weight = set(class_weight.keys())\n            raise ValueError('`class_weight` must contain '\n                             'all classes in the data.'\n                             ' The classes %s exist in the data but not in '\n                             '`class_weight`.'\n                             % (existing_classes - existing_class_weight))\n        return weights\n    else:\n        if sample_weight_mode is None:\n            return np.ones((y.shape[0],), dtype=K.floatx())\n        else:\n            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())",
        "begin_line": 426,
        "end_line": 521,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_utils.check_num_samples#524",
        "src_path": "keras/engine/training_utils.py",
        "class_name": "keras.engine.training_utils",
        "signature": "keras.engine.training_utils.check_num_samples(ins, batch_size=None, steps=None, steps_name='steps')",
        "snippet": "def check_num_samples(ins,\n                      batch_size=None,\n                      steps=None,\n                      steps_name='steps'):\n    \"\"\"Checks the number of samples provided for training and evaluation.\n\n    The number of samples is not defined when running with `steps`,\n    in which case the number of samples is set to `None`.\n\n    # Arguments\n        ins: List of tensors to be fed to the Keras function.\n        batch_size: Integer batch size or `None` if not defined.\n        steps: Total number of steps (batches of samples)\n            before declaring `predict_loop` finished.\n            Ignored with the default value of `None`.\n        steps_name: The public API's parameter name for `steps`.\n\n    # Raises\n        ValueError: when `steps` is `None` and the attribute `ins.shape`\n        does not exist. Also raises ValueError when `steps` is not `None`\n        and `batch_size` is not `None` because they are mutually\n        exclusive.\n\n    # Returns\n        When steps is `None`, returns the number of samples to be\n        processed based on the size of the first dimension of the\n        first input numpy array. When steps is not `None` and\n        `batch_size` is `None`, returns `None`.\n\n    # Raises\n        ValueError: In case of invalid arguments.\n    \"\"\"\n    if steps is not None and batch_size is not None:\n        raise ValueError(\n            'If ' + steps_name + ' is set, the `batch_size` must be None.')\n\n    if not ins or any(K.is_tensor(x) for x in ins):\n        if steps is None:\n            raise ValueError(\n                'If your data is in the form of symbolic tensors, '\n                'you should specify the `' + steps_name + '` argument '\n                '(instead of the `batch_size` argument, '\n                'because symbolic tensors are expected to produce '\n                'batches of input data).')\n        return None\n\n    if hasattr(ins[0], 'shape'):\n        return int(ins[0].shape[0])\n    return None  # Edge case where ins == [static_learning_phase]",
        "begin_line": 524,
        "end_line": 572,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.layer_utils.get_source_inputs#251",
        "src_path": "keras/utils/layer_utils.py",
        "class_name": "keras.utils.layer_utils",
        "signature": "keras.utils.layer_utils.get_source_inputs(tensor, layer=None, node_index=None)",
        "snippet": "def get_source_inputs(tensor, layer=None, node_index=None):\n    \"\"\"Returns the list of input tensors necessary to compute `tensor`.\n\n    Output will always be a list of tensors\n    (potentially with 1 element).\n\n    # Arguments\n        tensor: The tensor to start from.\n        layer: Origin layer of the tensor. Will be\n            determined via tensor._keras_history if not provided.\n        node_index: Origin node index of the tensor.\n\n    # Returns\n        List of input tensors.\n    \"\"\"\n    if not hasattr(tensor, '_keras_history'):\n        return tensor\n\n    if layer is None or node_index:\n        layer, node_index, _ = tensor._keras_history\n    if not layer._inbound_nodes:\n        return [tensor]\n    else:\n        node = layer._inbound_nodes[node_index]\n        if not node.inbound_layers:\n            # Reached an Input layer, stop recursion.\n            return node.input_tensors\n        else:\n            source_tensors = []\n            for i in range(len(node.inbound_layers)):\n                x = node.input_tensors[i]\n                layer = node.inbound_layers[i]\n                node_index = node.node_indices[i]\n                previous_sources = get_source_inputs(x,\n                                                     layer,\n                                                     node_index)\n                # Avoid input redundancy.\n                for x in previous_sources:\n                    if x not in source_tensors:\n                        source_tensors.append(x)\n            return source_tensors",
        "begin_line": 251,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.__init__.backend#111",
        "src_path": "keras/backend/__init__.py",
        "class_name": "keras.backend.__init__",
        "signature": "keras.backend.__init__.backend()",
        "snippet": "def backend():\n    \"\"\"Publicly accessible method\n    for determining the current backend.\n\n    # Returns\n        String, the name of the backend Keras is currently using.\n\n    # Example\n    ```python\n        >>> keras.backend.backend()\n        'tensorflow'\n    ```\n    \"\"\"\n    return _BACKEND",
        "begin_line": 111,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.optimizers.Optimizer.__init__#74",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        allowed_kwargs = {'clipnorm', 'clipvalue'}\n        for k in kwargs:\n            if k not in allowed_kwargs:\n                raise TypeError('Unexpected keyword argument '\n                                'passed to optimizer: ' + str(k))\n        self.__dict__.update(kwargs)\n        self.updates = []\n        self.weights = []",
        "begin_line": 74,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.Optimizer.get_gradients#88",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.get_gradients(self, loss, params)",
        "snippet": "    def get_gradients(self, loss, params):\n        grads = K.gradients(loss, params)\n        if None in grads:\n            raise ValueError('An operation has `None` for gradient. '\n                             'Please make sure that all of your ops have a '\n                             'gradient defined (i.e. are differentiable). '\n                             'Common ops without gradient: '\n                             'K.argmax, K.round, K.eval.')\n        if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n            norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n            grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n        if hasattr(self, 'clipvalue') and self.clipvalue > 0:\n            grads = [K.clip(g, -self.clipvalue, self.clipvalue) for g in grads]\n        return grads",
        "begin_line": 88,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.Optimizer.from_config#153",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.Optimizer",
        "signature": "keras.optimizers.Optimizer.from_config(cls, config)",
        "snippet": "    def from_config(cls, config):\n        return cls(**config)",
        "begin_line": 153,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.SGD.__init__#171",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.SGD",
        "signature": "keras.optimizers.SGD.__init__(self, lr=0.01, momentum=0.0, decay=0.0, nesterov=False, **kwargs)",
        "snippet": "    def __init__(self, lr=0.01, momentum=0., decay=0.,\n                 nesterov=False, **kwargs):\n        super(SGD, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.momentum = K.variable(momentum, name='momentum')\n            self.decay = K.variable(decay, name='decay')\n        self.initial_decay = decay\n        self.nesterov = nesterov",
        "begin_line": 171,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.SGD.get_updates#183",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers.SGD",
        "signature": "keras.optimizers.SGD.get_updates(self, loss, params)",
        "snippet": "    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n                                                      K.dtype(self.decay))))\n        # momentum\n        shapes = [K.int_shape(p) for p in params]\n        moments = [K.zeros(shape) for shape in shapes]\n        self.weights = [self.iterations] + moments\n        for p, g, m in zip(params, grads, moments):\n            v = self.momentum * m - lr * g  # velocity\n            self.updates.append(K.update(m, v))\n\n            if self.nesterov:\n                new_p = p + self.momentum * v - lr * g\n            else:\n                new_p = p + v\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, new_p))\n        return self.updates",
        "begin_line": 183,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.deserialize#731",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.deserialize(config, custom_objects=None)",
        "snippet": "def deserialize(config, custom_objects=None):\n    \"\"\"Inverse of the `serialize` function.\n\n    # Arguments\n        config: Optimizer configuration dictionary.\n        custom_objects: Optional dictionary mapping\n            names (strings) to custom objects\n            (classes and functions)\n            to be considered during deserialization.\n\n    # Returns\n        A Keras Optimizer instance.\n    \"\"\"\n    all_classes = {\n        'sgd': SGD,\n        'rmsprop': RMSprop,\n        'adagrad': Adagrad,\n        'adadelta': Adadelta,\n        'adam': Adam,\n        'adamax': Adamax,\n        'nadam': Nadam,\n        'tfoptimizer': TFOptimizer,\n    }\n    # Make deserialization case-insensitive for built-in optimizers.\n    if config['class_name'].lower() in all_classes:\n        config['class_name'] = config['class_name'].lower()\n    return deserialize_keras_object(config,\n                                    module_objects=all_classes,\n                                    custom_objects=custom_objects,\n                                    printable_module_name='optimizer')",
        "begin_line": 731,
        "end_line": 760,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.optimizers.get#763",
        "src_path": "keras/optimizers.py",
        "class_name": "keras.optimizers",
        "signature": "keras.optimizers.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Retrieves a Keras Optimizer instance.\n\n    # Arguments\n        identifier: Optimizer identifier, one of\n            - String: name of an optimizer\n            - Dictionary: configuration dictionary.\n            - Keras Optimizer instance (it will be returned unchanged).\n            - TensorFlow Optimizer instance\n                (it will be wrapped as a Keras Optimizer).\n\n    # Returns\n        A Keras Optimizer instance.\n\n    # Raises\n        ValueError: If `identifier` cannot be interpreted.\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        # Wrap TF optimizer instances\n        if isinstance(identifier, tf.train.Optimizer):\n            return TFOptimizer(identifier)\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    if isinstance(identifier, Optimizer):\n        return identifier\n    else:\n        raise ValueError('Could not interpret optimizer identifier: ' +\n                         str(identifier))",
        "begin_line": 763,
        "end_line": 793,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.losses.binary_crossentropy#76",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.binary_crossentropy(y_true, y_pred)",
        "snippet": "def binary_crossentropy(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.losses.deserialize#110",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.deserialize(name, custom_objects=None)",
        "snippet": "def deserialize(name, custom_objects=None):\n    return deserialize_keras_object(name,\n                                    module_objects=globals(),\n                                    custom_objects=custom_objects,\n                                    printable_module_name='loss function')",
        "begin_line": 110,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.losses.get#117",
        "src_path": "keras/losses.py",
        "class_name": "keras.losses",
        "signature": "keras.losses.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Get the `identifier` loss function.\n\n    # Arguments\n        identifier: None or str, name of the function.\n\n    # Returns\n        The loss function or None if `identifier` is None.\n\n    # Raises\n        ValueError if unknown identifier.\n    \"\"\"\n    if identifier is None:\n        return None\n    if isinstance(identifier, six.string_types):\n        identifier = str(identifier)\n        return deserialize(identifier)\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret '\n                         'loss function identifier:', identifier)",
        "begin_line": 117,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__init__#41",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__init__(self, *args)",
        "snippet": "    def __init__(self, *args):\n        self.custom_objects = args\n        self.backup = None",
        "begin_line": 41,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__enter__#45",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__enter__(self)",
        "snippet": "    def __enter__(self):\n        self.backup = _GLOBAL_CUSTOM_OBJECTS.copy()\n        for objects in self.custom_objects:\n            _GLOBAL_CUSTOM_OBJECTS.update(objects)\n        return self",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.CustomObjectScope.__exit__#51",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.CustomObjectScope",
        "signature": "keras.utils.generic_utils.CustomObjectScope.__exit__(self, *args, **kwargs)",
        "snippet": "    def __exit__(self, *args, **kwargs):\n        _GLOBAL_CUSTOM_OBJECTS.clear()\n        _GLOBAL_CUSTOM_OBJECTS.update(self.backup)",
        "begin_line": 51,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.deserialize_keras_object#120",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.deserialize_keras_object(identifier, module_objects=None, custom_objects=None, printable_module_name='object')",
        "snippet": "def deserialize_keras_object(identifier, module_objects=None,\n                             custom_objects=None,\n                             printable_module_name='object'):\n    if isinstance(identifier, dict):\n        # In this case we are dealing with a Keras config dictionary.\n        config = identifier\n        if 'class_name' not in config or 'config' not in config:\n            raise ValueError('Improper config format: ' + str(config))\n        class_name = config['class_name']\n        if custom_objects and class_name in custom_objects:\n            cls = custom_objects[class_name]\n        elif class_name in _GLOBAL_CUSTOM_OBJECTS:\n            cls = _GLOBAL_CUSTOM_OBJECTS[class_name]\n        else:\n            module_objects = module_objects or {}\n            cls = module_objects.get(class_name)\n            if cls is None:\n                raise ValueError('Unknown ' + printable_module_name +\n                                 ': ' + class_name)\n        if hasattr(cls, 'from_config'):\n            custom_objects = custom_objects or {}\n            if has_arg(cls.from_config, 'custom_objects'):\n                return cls.from_config(\n                    config['config'],\n                    custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                                        list(custom_objects.items())))\n            with CustomObjectScope(custom_objects):\n                return cls.from_config(config['config'])\n        else:\n            # Then `cls` may be a function returning a class.\n            # in this case by convention `config` holds\n            # the kwargs of the function.\n            custom_objects = custom_objects or {}\n            with CustomObjectScope(custom_objects):\n                return cls(**config['config'])\n    elif isinstance(identifier, six.string_types):\n        function_name = identifier\n        if custom_objects and function_name in custom_objects:\n            fn = custom_objects.get(function_name)\n        elif function_name in _GLOBAL_CUSTOM_OBJECTS:\n            fn = _GLOBAL_CUSTOM_OBJECTS[function_name]\n        else:\n            fn = module_objects.get(function_name)\n            if fn is None:\n                raise ValueError('Unknown ' + printable_module_name +\n                                 ':' + function_name)\n        return fn\n    else:\n        raise ValueError('Could not interpret serialized ' +\n                         printable_module_name + ': ' + identifier)",
        "begin_line": 120,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.has_arg#244",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.has_arg(fn, name, accept_all=False)",
        "snippet": "def has_arg(fn, name, accept_all=False):\n    \"\"\"Checks if a callable accepts a given keyword argument.\n\n    For Python 2, checks if there is an argument with the given name.\n\n    For Python 3, checks if there is an argument with the given name, and\n    also whether this argument can be called with a keyword (i.e. if it is\n    not a positional-only argument).\n\n    # Arguments\n        fn: Callable to inspect.\n        name: Check if `fn` can be called with `name` as a keyword argument.\n        accept_all: What to return if there is no parameter called `name`\n                    but the function accepts a `**kwargs` argument.\n\n    # Returns\n        bool, whether `fn` accepts a `name` keyword argument.\n    \"\"\"\n    if sys.version_info < (3,):\n        arg_spec = inspect.getargspec(fn)\n        if accept_all and arg_spec.keywords is not None:\n            return True\n        return (name in arg_spec.args)\n    elif sys.version_info < (3, 3):\n        arg_spec = inspect.getfullargspec(fn)\n        if accept_all and arg_spec.varkw is not None:\n            return True\n        return (name in arg_spec.args or\n                name in arg_spec.kwonlyargs)\n    else:\n        signature = inspect.signature(fn)\n        parameter = signature.parameters.get(name)\n        if parameter is None:\n            if accept_all:\n                for param in signature.parameters.values():\n                    if param.kind == inspect.Parameter.VAR_KEYWORD:\n                        return True\n            return False\n        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                                   inspect.Parameter.KEYWORD_ONLY))",
        "begin_line": 244,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.Progbar.__init__#300",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.Progbar",
        "signature": "keras.utils.generic_utils.Progbar.__init__(self, target, width=30, verbose=1, interval=0.05, stateful_metrics=None)",
        "snippet": "    def __init__(self, target, width=30, verbose=1, interval=0.05,\n                 stateful_metrics=None):\n        self.target = target\n        self.width = width\n        self.verbose = verbose\n        self.interval = interval\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()\n\n        self._dynamic_display = ((hasattr(sys.stdout, 'isatty') and\n                                  sys.stdout.isatty()) or\n                                 'ipykernel' in sys.modules)\n        self._total_width = 0\n        self._seen_so_far = 0\n        self._values = collections.OrderedDict()\n        self._start = time.time()\n        self._last_update = 0",
        "begin_line": 300,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.generic_utils.Progbar.update#320",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils.Progbar",
        "signature": "keras.utils.generic_utils.Progbar.update(self, current, values=None)",
        "snippet": "    def update(self, current, values=None):\n        \"\"\"Updates the progress bar.\n\n        # Arguments\n            current: Index of current step.\n            values: List of tuples:\n                `(name, value_for_last_step)`.\n                If `name` is in `stateful_metrics`,\n                `value_for_last_step` will be displayed as-is.\n                Else, an average of the metric over time will be displayed.\n        \"\"\"\n        values = values or []\n        for k, v in values:\n            if k not in self.stateful_metrics:\n                if k not in self._values:\n                    self._values[k] = [v * (current - self._seen_so_far),\n                                       current - self._seen_so_far]\n                else:\n                    self._values[k][0] += v * (current - self._seen_so_far)\n                    self._values[k][1] += (current - self._seen_so_far)\n            else:\n                # Stateful metrics output a numeric value.  This representation\n                # means \"take an average from a single value\" but keeps the\n                # numeric formatting.\n                self._values[k] = [v, 1]\n        self._seen_so_far = current\n\n        now = time.time()\n        info = ' - %.0fs' % (now - self._start)\n        if self.verbose == 1:\n            if (now - self._last_update < self.interval and\n                    self.target is not None and current < self.target):\n                return\n\n            prev_total_width = self._total_width\n            if self._dynamic_display:\n                sys.stdout.write('\\b' * prev_total_width)\n                sys.stdout.write('\\r')\n            else:\n                sys.stdout.write('\\n')\n\n            if self.target is not None:\n                numdigits = int(np.floor(np.log10(self.target))) + 1\n                barstr = '%%%dd/%d [' % (numdigits, self.target)\n                bar = barstr % current\n                prog = float(current) / self.target\n                prog_width = int(self.width * prog)\n                if prog_width > 0:\n                    bar += ('=' * (prog_width - 1))\n                    if current < self.target:\n                        bar += '>'\n                    else:\n                        bar += '='\n                bar += ('.' * (self.width - prog_width))\n                bar += ']'\n            else:\n                bar = '%7d/Unknown' % current\n\n            self._total_width = len(bar)\n            sys.stdout.write(bar)\n\n            if current:\n                time_per_unit = (now - self._start) / current\n            else:\n                time_per_unit = 0\n            if self.target is not None and current < self.target:\n                eta = time_per_unit * (self.target - current)\n                if eta > 3600:\n                    eta_format = ('%d:%02d:%02d' %\n                                  (eta // 3600, (eta % 3600) // 60, eta % 60))\n                elif eta > 60:\n                    eta_format = '%d:%02d' % (eta // 60, eta % 60)\n                else:\n                    eta_format = '%ds' % eta\n\n                info = ' - ETA: %s' % eta_format\n            else:\n                if time_per_unit >= 1:\n                    info += ' %.0fs/step' % time_per_unit\n                elif time_per_unit >= 1e-3:\n                    info += ' %.0fms/step' % (time_per_unit * 1e3)\n                else:\n                    info += ' %.0fus/step' % (time_per_unit * 1e6)\n\n            for k in self._values:\n                info += ' - %s:' % k\n                if isinstance(self._values[k], list):\n                    avg = np.mean(\n                        self._values[k][0] / max(1, self._values[k][1]))\n                    if abs(avg) > 1e-3:\n                        info += ' %.4f' % avg\n                    else:\n                        info += ' %.4e' % avg\n                else:\n                    info += ' %s' % self._values[k]\n\n            self._total_width += len(info)\n            if prev_total_width > self._total_width:\n                info += (' ' * (prev_total_width - self._total_width))\n\n            if self.target is not None and current >= self.target:\n                info += '\\n'\n\n            sys.stdout.write(info)\n            sys.stdout.flush()\n\n        elif self.verbose == 2:\n            if self.target is None or current >= self.target:\n                for k in self._values:\n                    info += ' - %s:' % k\n                    avg = np.mean(\n                        self._values[k][0] / max(1, self._values[k][1]))\n                    if avg > 1e-3:\n                        info += ' %.4f' % avg\n                    else:\n                        info += ' %.4e' % avg\n                info += '\\n'\n\n                sys.stdout.write(info)\n                sys.stdout.flush()\n\n        self._last_update = now",
        "begin_line": 320,
        "end_line": 441,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.utils.generic_utils.to_list#447",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.to_list(x)",
        "snippet": "def to_list(x):\n    \"\"\"Normalizes a list/tensor into a list.\n\n    If a tensor is passed, we return\n    a list of size 1 containing the tensor.\n\n    # Arguments\n        x: target object to be normalized.\n\n    # Returns\n        A list.\n    \"\"\"\n    if isinstance(x, list):\n        return x\n    return [x]",
        "begin_line": 447,
        "end_line": 461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.unpack_singleton#464",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.unpack_singleton(x)",
        "snippet": "def unpack_singleton(x):\n    \"\"\"Gets the first element if the iterable has only one value.\n\n    Otherwise return the iterable.\n\n    # Argument:\n        x: A list or tuple.\n\n    # Returns:\n        The same iterable or the first element.\n    \"\"\"\n    if len(x) == 1:\n        return x[0]\n    return x",
        "begin_line": 464,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.object_list_uid#480",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.object_list_uid(object_list)",
        "snippet": "def object_list_uid(object_list):\n    object_list = to_list(object_list)\n    return ', '.join([str(abs(id(x))) for x in object_list])",
        "begin_line": 480,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.is_all_none#485",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.is_all_none(iterable_or_element)",
        "snippet": "def is_all_none(iterable_or_element):\n    if not isinstance(iterable_or_element, (list, tuple)):\n        iterable = [iterable_or_element]\n    else:\n        iterable = iterable_or_element\n    for element in iterable:\n        if element is not None:\n            return False\n    return True",
        "begin_line": 485,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.generic_utils.slice_arrays#496",
        "src_path": "keras/utils/generic_utils.py",
        "class_name": "keras.utils.generic_utils",
        "signature": "keras.utils.generic_utils.slice_arrays(arrays, start=None, stop=None)",
        "snippet": "def slice_arrays(arrays, start=None, stop=None):\n    \"\"\"Slices an array or list of arrays.\n\n    This takes an array-like, or a list of\n    array-likes, and outputs:\n        - arrays[start:stop] if `arrays` is an array-like\n        - [x[start:stop] for x in arrays] if `arrays` is a list\n\n    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n\n    # Arguments\n        arrays: Single array or list of arrays.\n        start: can be an integer index (start index)\n            or a list/array of indices\n        stop: integer (stop index); should be None if\n            `start` was a list.\n\n    # Returns\n        A slice of the array(s).\n    \"\"\"\n    if arrays is None:\n        return [None]\n    elif isinstance(arrays, list):\n        if hasattr(start, '__len__'):\n            # hdf5 datasets only support list objects as indices\n            if hasattr(start, 'shape'):\n                start = start.tolist()\n            return [None if x is None else x[start] for x in arrays]\n        else:\n            return [None if x is None else x[start:stop] for x in arrays]\n    else:\n        if hasattr(start, '__len__'):\n            if hasattr(start, 'shape'):\n                start = start.tolist()\n            return arrays[start]\n        elif hasattr(start, '__getitem__'):\n            return arrays[start:stop]\n        else:\n            return [None]",
        "begin_line": 496,
        "end_line": 534,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_arrays.fit_loop#21",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.fit_loop(model, f, ins, out_labels=None, batch_size=None, epochs=100, verbose=1, callbacks=None, val_f=None, val_ins=None, shuffle=True, callback_metrics=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)",
        "snippet": "def fit_loop(model, f, ins,\n             out_labels=None,\n             batch_size=None,\n             epochs=100,\n             verbose=1,\n             callbacks=None,\n             val_f=None,\n             val_ins=None,\n             shuffle=True,\n             callback_metrics=None,\n             initial_epoch=0,\n             steps_per_epoch=None,\n             validation_steps=None):\n    \"\"\"Abstract fit function for `f(ins)`.\n\n    Assumes that f returns a list, labeled by out_labels.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors\n        ins: List of tensors to be fed to `f`\n        out_labels: List of strings, display names of\n            the outputs of `f`\n        batch_size: Integer batch size or None if unknown.\n        epochs: Number of times to iterate over the data\n        verbose: Verbosity mode, 0, 1 or 2\n        callbacks: List of callbacks to be called during training\n        val_f: Keras function to call for validation\n        val_ins: List of tensors to be fed to `val_f`\n        shuffle: Whether to shuffle the data at the beginning of each epoch\n        callback_metrics: List of strings, the display names of the metrics\n            passed to the callbacks. They should be the\n            concatenation of list the display names of the outputs of\n             `f` and the list of display names of the outputs of `f_val`.\n        initial_epoch: Epoch at which to start training\n            (useful for resuming a previous training run)\n        steps_per_epoch: Total number of steps (batches of samples)\n            before declaring one epoch finished and starting the\n            next epoch. Ignored with the default value of `None`.\n        validation_steps: Number of steps to run validation for\n            (only if doing validation from data tensors).\n            Ignored with the default value of `None`.\n\n    # Returns\n        `History` object.\n    \"\"\"\n    do_validation = False\n    if val_f and val_ins:\n        do_validation = True\n        if (verbose and ins and\n           hasattr(ins[0], 'shape') and hasattr(val_ins[0], 'shape')):\n            print('Train on %d samples, validate on %d samples' %\n                  (ins[0].shape[0], val_ins[0].shape[0]))\n    if validation_steps:\n        do_validation = True\n        if steps_per_epoch is None:\n            raise ValueError('Can only use `validation_steps` '\n                             'when doing step-wise '\n                             'training, i.e. `steps_per_epoch` '\n                             'must be set.')\n    elif do_validation:\n        if steps_per_epoch:\n            raise ValueError('Must specify `validation_steps` '\n                             'to perform validation '\n                             'when doing step-wise training.')\n\n    num_train_samples = check_num_samples(ins,\n                                          batch_size=batch_size,\n                                          steps=steps_per_epoch,\n                                          steps_name='steps_per_epoch')\n    if num_train_samples is not None:\n        index_array = np.arange(num_train_samples)\n\n    model.history = cbks.History()\n    _callbacks = [cbks.BaseLogger(\n        stateful_metrics=model.stateful_metric_names)]\n    if verbose:\n        if steps_per_epoch is not None:\n            count_mode = 'steps'\n        else:\n            count_mode = 'samples'\n        _callbacks.append(\n            cbks.ProgbarLogger(\n                count_mode,\n                stateful_metrics=model.stateful_metric_names))\n    _callbacks += (callbacks or []) + [model.history]\n    callbacks = cbks.CallbackList(_callbacks)\n    out_labels = out_labels or []\n\n    # it's possible to callback a different model than itself\n    # (used by Sequential models)\n    if hasattr(model, 'callback_model') and model.callback_model:\n        callback_model = model.callback_model\n    else:\n        callback_model = model\n\n    callbacks.set_model(callback_model)\n    callbacks.set_params({\n        'batch_size': batch_size,\n        'epochs': epochs,\n        'steps': steps_per_epoch,\n        'samples': num_train_samples,\n        'verbose': verbose,\n        'do_validation': do_validation,\n        'metrics': callback_metrics or [],\n    })\n    callbacks.on_train_begin()\n    callback_model.stop_training = False\n    for cbk in callbacks:\n        cbk.validation_data = val_ins\n\n    # To prevent a slowdown,\n    # we find beforehand the arrays that need conversion.\n    feed = (model._feed_inputs +\n            model._feed_targets +\n            model._feed_sample_weights)\n    indices_for_conversion_to_dense = []\n    for i in range(len(feed)):\n        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    for epoch in range(initial_epoch, epochs):\n        # Reset stateful metrics\n        for m in model.stateful_metric_functions:\n            m.reset_states()\n        callbacks.on_epoch_begin(epoch)\n        epoch_logs = {}\n        if steps_per_epoch is not None:\n            for step_index in range(steps_per_epoch):\n                batch_logs = {}\n                batch_logs['batch'] = step_index\n                batch_logs['size'] = 1\n                callbacks.on_batch_begin(step_index, batch_logs)\n                outs = f(ins)\n\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(step_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n            if do_validation:\n                val_outs = test_loop(model, val_f, val_ins,\n                                     steps=validation_steps,\n                                     verbose=0)\n                val_outs = to_list(val_outs)\n                # Same labels assumed.\n                for l, o in zip(out_labels, val_outs):\n                    epoch_logs['val_' + l] = o\n        else:\n            if shuffle == 'batch':\n                index_array = batch_shuffle(index_array, batch_size)\n            elif shuffle:\n                np.random.shuffle(index_array)\n\n            batches = make_batches(num_train_samples, batch_size)\n            for batch_index, (batch_start, batch_end) in enumerate(batches):\n                batch_ids = index_array[batch_start:batch_end]\n                try:\n                    if isinstance(ins[-1], float):\n                        # Do not slice the training phase flag.\n                        ins_batch = slice_arrays(\n                            ins[:-1], batch_ids) + [ins[-1]]\n                    else:\n                        ins_batch = slice_arrays(ins, batch_ids)\n                except TypeError:\n                    raise TypeError('TypeError while preparing batch. '\n                                    'If using HDF5 input data, '\n                                    'pass shuffle=\"batch\".')\n                batch_logs = {}\n                batch_logs['batch'] = batch_index\n                batch_logs['size'] = len(batch_ids)\n                callbacks.on_batch_begin(batch_index, batch_logs)\n                for i in indices_for_conversion_to_dense:\n                    ins_batch[i] = ins_batch[i].toarray()\n\n                outs = f(ins_batch)\n                outs = to_list(outs)\n                for l, o in zip(out_labels, outs):\n                    batch_logs[l] = o\n\n                callbacks.on_batch_end(batch_index, batch_logs)\n                if callback_model.stop_training:\n                    break\n\n                if batch_index == len(batches) - 1:  # Last batch.\n                    if do_validation:\n                        val_outs = test_loop(model, val_f, val_ins,\n                                             batch_size=batch_size,\n                                             verbose=0)\n                        val_outs = to_list(val_outs)\n                        # Same labels assumed.\n                        for l, o in zip(out_labels, val_outs):\n                            epoch_logs['val_' + l] = o\n        callbacks.on_epoch_end(epoch, epoch_logs)\n        if callback_model.stop_training:\n            break\n    callbacks.on_train_end()\n    return model.history",
        "begin_line": 21,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_arrays.predict_loop#224",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None)",
        "snippet": "def predict_loop(model, f, ins, batch_size=32, verbose=0, steps=None):\n    \"\"\"Abstract method to loop over some data in batches.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors.\n        ins: list of tensors to be fed to `f`.\n        batch_size: integer batch size.\n        verbose: verbosity mode.\n        steps: Total number of steps (batches of samples)\n            before declaring `predict_loop` finished.\n            Ignored with the default value of `None`.\n\n    # Returns\n        Array of predictions (if the model has a single output)\n        or list of arrays of predictions\n        (if the model has multiple outputs).\n    \"\"\"\n    num_samples = check_num_samples(ins,\n                                    batch_size=batch_size,\n                                    steps=steps,\n                                    steps_name='steps')\n    if verbose == 1:\n        if steps is not None:\n            progbar = Progbar(target=steps)\n        else:\n            progbar = Progbar(target=num_samples)\n\n    indices_for_conversion_to_dense = []\n    for i in range(len(model._feed_inputs)):\n        if issparse(ins[i]) and not K.is_sparse(model._feed_inputs[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    if steps is not None:\n        # Step-based predictions.\n        # Since we do not know how many samples\n        # we will see, we cannot pre-allocate\n        # the returned Numpy arrays.\n        # Instead, we store one array per batch seen\n        # and concatenate them upon returning.\n        unconcatenated_outs = []\n        for step in range(steps):\n            batch_outs = f(ins)\n            batch_outs = to_list(batch_outs)\n            if step == 0:\n                for batch_out in batch_outs:\n                    unconcatenated_outs.append([])\n            for i, batch_out in enumerate(batch_outs):\n                unconcatenated_outs[i].append(batch_out)\n            if verbose == 1:\n                progbar.update(step + 1)\n        if len(unconcatenated_outs) == 1:\n            return np.concatenate(unconcatenated_outs[0], axis=0)\n        return [np.concatenate(unconcatenated_outs[i], axis=0)\n                for i in range(len(unconcatenated_outs))]\n    else:\n        # Sample-based predictions.\n        outs = []\n        batches = make_batches(num_samples, batch_size)\n        index_array = np.arange(num_samples)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            if ins and isinstance(ins[-1], float):\n                # Do not slice the training phase flag.\n                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n            else:\n                ins_batch = slice_arrays(ins, batch_ids)\n            for i in indices_for_conversion_to_dense:\n                ins_batch[i] = ins_batch[i].toarray()\n\n            batch_outs = f(ins_batch)\n            batch_outs = to_list(batch_outs)\n            if batch_index == 0:\n                # Pre-allocate the results arrays.\n                for batch_out in batch_outs:\n                    shape = (num_samples,) + batch_out.shape[1:]\n                    outs.append(np.zeros(shape, dtype=batch_out.dtype))\n            for i, batch_out in enumerate(batch_outs):\n                outs[i][batch_start:batch_end] = batch_out\n            if verbose == 1:\n                progbar.update(batch_end)\n        return unpack_singleton(outs)",
        "begin_line": 224,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.training_arrays.test_loop#308",
        "src_path": "keras/engine/training_arrays.py",
        "class_name": "keras.engine.training_arrays",
        "signature": "keras.engine.training_arrays.test_loop(model, f, ins, batch_size=None, verbose=0, steps=None)",
        "snippet": "def test_loop(model, f, ins, batch_size=None, verbose=0, steps=None):\n    \"\"\"Abstract method to loop over some data in batches.\n\n    # Arguments\n        model: Keras model instance.\n        f: Keras function returning a list of tensors.\n        ins: list of tensors to be fed to `f`.\n        batch_size: integer batch size or `None`.\n        verbose: verbosity mode.\n        steps: Total number of steps (batches of samples)\n            before declaring predictions finished.\n            Ignored with the default value of `None`.\n\n    # Returns\n        Scalar loss (if the model has a single output and no metrics)\n        or list of scalars (if the model has multiple outputs\n        and/or metrics). The attribute `model.metrics_names` will give you\n        the display labels for the scalar outputs.\n    \"\"\"\n\n    if hasattr(model, 'metrics'):\n        for m in model.stateful_metric_functions:\n            m.reset_states()\n        stateful_metric_indices = [\n            i for i, name in enumerate(model.metrics_names)\n            if str(name) in model.stateful_metric_names]\n    else:\n        stateful_metric_indices = []\n\n    num_samples = check_num_samples(ins,\n                                    batch_size=batch_size,\n                                    steps=steps,\n                                    steps_name='steps')\n    outs = []\n    if verbose == 1:\n        if steps is not None:\n            progbar = Progbar(target=steps)\n        else:\n            progbar = Progbar(target=num_samples)\n\n    # To prevent a slowdown,\n    # we find beforehand the arrays that need conversion.\n    feed = (model._feed_inputs +\n            model._feed_targets +\n            model._feed_sample_weights)\n    indices_for_conversion_to_dense = []\n    for i in range(len(feed)):\n        if issparse(ins[i]) and not K.is_sparse(feed[i]):\n            indices_for_conversion_to_dense.append(i)\n\n    if steps is not None:\n        for step in range(steps):\n            batch_outs = f(ins)\n            if isinstance(batch_outs, list):\n                if step == 0:\n                    for _ in enumerate(batch_outs):\n                        outs.append(0.)\n                for i, batch_out in enumerate(batch_outs):\n                    if i in stateful_metric_indices:\n                        outs[i] = float(batch_out)\n                    else:\n                        outs[i] += batch_out\n            else:\n                if step == 0:\n                    outs.append(0.)\n                outs[0] += batch_outs\n            if verbose == 1:\n                progbar.update(step + 1)\n        for i in range(len(outs)):\n            if i not in stateful_metric_indices:\n                outs[i] /= steps\n    else:\n        batches = make_batches(num_samples, batch_size)\n        index_array = np.arange(num_samples)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            if isinstance(ins[-1], float):\n                # Do not slice the training phase flag.\n                ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n            else:\n                ins_batch = slice_arrays(ins, batch_ids)\n            for i in indices_for_conversion_to_dense:\n                ins_batch[i] = ins_batch[i].toarray()\n\n            batch_outs = f(ins_batch)\n            if isinstance(batch_outs, list):\n                if batch_index == 0:\n                    for batch_out in enumerate(batch_outs):\n                        outs.append(0.)\n                for i, batch_out in enumerate(batch_outs):\n                    if i in stateful_metric_indices:\n                        outs[i] = batch_out\n                    else:\n                        outs[i] += batch_out * len(batch_ids)\n            else:\n                if batch_index == 0:\n                    outs.append(0.)\n                outs[0] += batch_outs * len(batch_ids)\n\n            if verbose == 1:\n                progbar.update(batch_end)\n        for i in range(len(outs)):\n            if i not in stateful_metric_indices:\n                outs[i] /= num_samples\n    return unpack_singleton(outs)",
        "begin_line": 308,
        "end_line": 412,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.constraints.get#173",
        "src_path": "keras/constraints.py",
        "class_name": "keras.constraints",
        "signature": "keras.constraints.get(identifier)",
        "snippet": "def get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret constraint identifier: ' +\n                         str(identifier))",
        "begin_line": 173,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.legacy.interfaces.legacy_support#26",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.legacy_support(func)",
        "snippet": "    def legacy_support(func):\n        @six.wraps(func)\n        def wrapper(*args, **kwargs):\n            if object_type == 'class':\n                object_name = args[0].__class__.__name__\n            else:\n                object_name = func.__name__\n            if preprocessor:\n                args, kwargs, converted = preprocessor(args, kwargs)\n            else:\n                converted = []\n            if check_positional_args:\n                if len(args) > len(allowed_positional_args) + 1:\n                    raise TypeError('`' + object_name +\n                                    '` can accept only ' +\n                                    str(len(allowed_positional_args)) +\n                                    ' positional arguments ' +\n                                    str(tuple(allowed_positional_args)) +\n                                    ', but you passed the following '\n                                    'positional arguments: ' +\n                                    str(list(args[1:])))\n            for key in value_conversions:\n                if key in kwargs:\n                    old_value = kwargs[key]\n                    if old_value in value_conversions[key]:\n                        kwargs[key] = value_conversions[key][old_value]\n            for old_name, new_name in conversions:\n                if old_name in kwargs:\n                    value = kwargs.pop(old_name)\n                    if new_name in kwargs:\n                        raise_duplicate_arg_error(old_name, new_name)\n                    kwargs[new_name] = value\n                    converted.append((new_name, old_name))\n            if converted:\n                signature = '`' + object_name + '('\n                for i, value in enumerate(args[1:]):\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(args[1:]) - 1 or kwargs:\n                        signature += ', '\n                for i, (name, value) in enumerate(kwargs.items()):\n                    signature += name + '='\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(kwargs) - 1:\n                        signature += ', '\n                signature += ')`'\n                warnings.warn('Update your `' + object_name +\n                              '` call to the Keras 2 API: ' + signature, stacklevel=2)\n            return func(*args, **kwargs)\n        wrapper._original_function = func\n        return wrapper",
        "begin_line": 26,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.legacy.interfaces.wrapper#28",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.wrapper(*args, **kwargs)",
        "snippet": "        def wrapper(*args, **kwargs):\n            if object_type == 'class':\n                object_name = args[0].__class__.__name__\n            else:\n                object_name = func.__name__\n            if preprocessor:\n                args, kwargs, converted = preprocessor(args, kwargs)\n            else:\n                converted = []\n            if check_positional_args:\n                if len(args) > len(allowed_positional_args) + 1:\n                    raise TypeError('`' + object_name +\n                                    '` can accept only ' +\n                                    str(len(allowed_positional_args)) +\n                                    ' positional arguments ' +\n                                    str(tuple(allowed_positional_args)) +\n                                    ', but you passed the following '\n                                    'positional arguments: ' +\n                                    str(list(args[1:])))\n            for key in value_conversions:\n                if key in kwargs:\n                    old_value = kwargs[key]\n                    if old_value in value_conversions[key]:\n                        kwargs[key] = value_conversions[key][old_value]\n            for old_name, new_name in conversions:\n                if old_name in kwargs:\n                    value = kwargs.pop(old_name)\n                    if new_name in kwargs:\n                        raise_duplicate_arg_error(old_name, new_name)\n                    kwargs[new_name] = value\n                    converted.append((new_name, old_name))\n            if converted:\n                signature = '`' + object_name + '('\n                for i, value in enumerate(args[1:]):\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(args[1:]) - 1 or kwargs:\n                        signature += ', '\n                for i, (name, value) in enumerate(kwargs.items()):\n                    signature += name + '='\n                    if isinstance(value, six.string_types):\n                        signature += '\"' + value + '\"'\n                    else:\n                        if isinstance(value, np.ndarray):\n                            str_val = 'array'\n                        else:\n                            str_val = str(value)\n                        if len(str_val) > 10:\n                            str_val = str_val[:10] + '...'\n                        signature += str_val\n                    if i < len(kwargs) - 1:\n                        signature += ', '\n                signature += ')`'\n                warnings.warn('Update your `' + object_name +\n                              '` call to the Keras 2 API: ' + signature, stacklevel=2)\n            return func(*args, **kwargs)",
        "begin_line": 28,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.legacy.interfaces.conv2d_args_preprocessor#268",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.conv2d_args_preprocessor(args, kwargs)",
        "snippet": "def conv2d_args_preprocessor(args, kwargs):\n    converted = []\n    if len(args) > 4:\n        raise TypeError('Layer can receive at most 3 positional arguments.')\n    if len(args) == 4:\n        if isinstance(args[2], int) and isinstance(args[3], int):\n            new_keywords = ['padding', 'strides', 'data_format']\n            for kwd in new_keywords:\n                if kwd in kwargs:\n                    raise ValueError(\n                        'It seems that you are using the Keras 2 '\n                        'and you are passing both `kernel_size` and `strides` '\n                        'as integer positional arguments. For safety reasons, '\n                        'this is disallowed. Pass `strides` '\n                        'as a keyword argument instead.')\n            kernel_size = (args[2], args[3])\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 3 and isinstance(args[2], int):\n        if 'nb_col' in kwargs:\n            kernel_size = (args[2], kwargs.pop('nb_col'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 2:\n        if 'nb_row' in kwargs and 'nb_col' in kwargs:\n            kernel_size = (kwargs.pop('nb_row'), kwargs.pop('nb_col'))\n            args = [args[0], args[1], kernel_size]\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    elif len(args) == 1:\n        if 'nb_row' in kwargs and 'nb_col' in kwargs:\n            kernel_size = (kwargs.pop('nb_row'), kwargs.pop('nb_col'))\n            kwargs['kernel_size'] = kernel_size\n            converted.append(('kernel_size', 'nb_row/nb_col'))\n    return args, kwargs, converted",
        "begin_line": 268,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.legacy.interfaces.add_weight_args_preprocessing#623",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.add_weight_args_preprocessing(args, kwargs)",
        "snippet": "def add_weight_args_preprocessing(args, kwargs):\n    if len(args) > 1:\n        if isinstance(args[1], (tuple, list)):\n            kwargs['shape'] = args[1]\n            args = (args[0],) + args[2:]\n            if len(args) > 1:\n                if isinstance(args[1], six.string_types):\n                    kwargs['name'] = args[1]\n                    args = (args[0],) + args[2:]\n    return args, kwargs, []",
        "begin_line": 623,
        "end_line": 632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.legacy.interfaces.get_updates_arg_preprocessing#640",
        "src_path": "keras/legacy/interfaces.py",
        "class_name": "keras.legacy.interfaces",
        "signature": "keras.legacy.interfaces.get_updates_arg_preprocessing(args, kwargs)",
        "snippet": "def get_updates_arg_preprocessing(args, kwargs):\n    # Old interface: (params, constraints, loss)\n    # New interface: (loss, params)\n    if len(args) > 4:\n        raise TypeError('`get_update` call received more arguments '\n                        'than expected.')\n    elif len(args) == 4:\n        # Assuming old interface.\n        opt, params, _, loss = args\n        kwargs['loss'] = loss\n        kwargs['params'] = params\n        return [opt], kwargs, []\n    elif len(args) == 3:\n        if isinstance(args[1], (list, tuple)):\n            assert isinstance(args[2], dict)\n            assert 'loss' in kwargs\n            opt, params, _ = args\n            kwargs['params'] = params\n            return [opt], kwargs, []\n    return args, kwargs, []",
        "begin_line": 640,
        "end_line": 659,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.__init__#94",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        self.input_spec = None\n        self.supports_masking = False\n        self.stateful = False\n\n        # These properties will be set upon call of self.build()\n        self._trainable_weights = []\n        self._non_trainable_weights = []\n        self._losses = []\n        self._updates = []\n        self._per_input_losses = {}\n        self._per_input_updates = {}\n        self._built = False\n\n        # These lists will be filled via successive calls\n        # to self._add_inbound_node().\n        self._inbound_nodes = []\n        self._outbound_nodes = []\n\n        # These properties should be set by the user via keyword arguments.\n        # note that 'dtype', 'input_shape' and 'batch_input_shape'\n        # are only applicable to input layers: do not pass these keywords\n        # to non-input layers.\n        allowed_kwargs = {'input_shape',\n                          'batch_input_shape',\n                          'batch_size',\n                          'dtype',\n                          'name',\n                          'trainable',\n                          'weights',\n                          'input_dtype',  # legacy\n                          }\n        for kwarg in kwargs:\n            if kwarg not in allowed_kwargs:\n                raise TypeError('Keyword argument not understood:', kwarg)\n        name = kwargs.get('name')\n        if not name:\n            prefix = self.__class__.__name__\n            name = _to_snake_case(prefix) + '_' + str(K.get_uid(prefix))\n        self.name = name\n\n        self.trainable = kwargs.get('trainable', True)\n        if 'input_shape' in kwargs or 'batch_input_shape' in kwargs:\n            # In this case we will later create an input layer\n            # to insert before the current layer\n            if 'batch_input_shape' in kwargs:\n                batch_input_shape = tuple(kwargs['batch_input_shape'])\n            elif 'input_shape' in kwargs:\n                if 'batch_size' in kwargs:\n                    batch_size = kwargs['batch_size']\n                else:\n                    batch_size = None\n                batch_input_shape = (\n                    batch_size,) + tuple(kwargs['input_shape'])\n            self.batch_input_shape = batch_input_shape\n\n            # Set dtype.\n            dtype = kwargs.get('dtype')\n            if dtype is None:\n                dtype = kwargs.get('input_dtype')\n            if dtype is None:\n                dtype = K.floatx()\n            self.dtype = dtype\n\n        if 'weights' in kwargs:\n            self._initial_weights = kwargs['weights']\n        else:\n            self._initial_weights = None",
        "begin_line": 94,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._node_key#164",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._node_key(layer, node_index)",
        "snippet": "    def _node_key(layer, node_index):\n        \"\"\"Converts a layer and its index to a unique (immutable type) name.\n\n        This function is used internally with `self._network_nodes`.\n\n        # Arguments\n            layer: The layer.\n            node_index: The layer's position (e.g. via enumerate) in a list of\n                nodes.\n\n        # Returns\n            The unique name.\n        \"\"\"\n        return layer.name + '_ib-' + str(node_index)",
        "begin_line": 164,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.losses#180",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.losses(self)",
        "snippet": "    def losses(self):\n        return self._losses",
        "begin_line": 180,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.updates#184",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.updates(self)",
        "snippet": "    def updates(self):\n        if not self.trainable and not self.stateful:\n            return []\n        return self._updates",
        "begin_line": 184,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.built#190",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.built(self)",
        "snippet": "    def built(self):\n        return self._built",
        "begin_line": 190,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.built#194",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.built(self, value)",
        "snippet": "    def built(self, value):\n        self._built = value",
        "begin_line": 194,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.trainable_weights#198",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.trainable_weights(self)",
        "snippet": "    def trainable_weights(self):\n        trainable = getattr(self, 'trainable', True)\n        if trainable:\n            return self._trainable_weights\n        else:\n            return []",
        "begin_line": 198,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.add_weight#222",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.add_weight(self, name, shape, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)",
        "snippet": "    def add_weight(self,\n                   name,\n                   shape,\n                   dtype=None,\n                   initializer=None,\n                   regularizer=None,\n                   trainable=True,\n                   constraint=None):\n        \"\"\"Adds a weight variable to the layer.\n\n        # Arguments\n            name: String, the name for the weight variable.\n            shape: The shape tuple of the weight.\n            dtype: The dtype of the weight.\n            initializer: An Initializer instance (callable).\n            regularizer: An optional Regularizer instance.\n            trainable: A boolean, whether the weight should\n                be trained via backprop or not (assuming\n                that the layer itself is also trainable).\n            constraint: An optional Constraint instance.\n\n        # Returns\n            The created weight variable.\n        \"\"\"\n        initializer = initializers.get(initializer)\n        if dtype is None:\n            dtype = K.floatx()\n        weight = K.variable(initializer(shape),\n                            dtype=dtype,\n                            name=name,\n                            constraint=constraint)\n        if regularizer is not None:\n            with K.name_scope('weight_regularizer'):\n                self.add_loss(regularizer(weight))\n        if trainable:\n            self._trainable_weights.append(weight)\n        else:\n            self._non_trainable_weights.append(weight)\n        return weight",
        "begin_line": 222,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.assert_input_compatibility#262",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.assert_input_compatibility(self, inputs)",
        "snippet": "    def assert_input_compatibility(self, inputs):\n        \"\"\"Checks compatibility between the layer and provided inputs.\n\n        This checks that the tensor(s) `input`\n        verify the input assumptions of the layer\n        (if any). If not, exceptions are raised.\n\n        # Arguments\n            inputs: input tensor or list of input tensors.\n\n        # Raises\n            ValueError: in case of mismatch between\n                the provided inputs and the expectations of the layer.\n        \"\"\"\n        inputs = to_list(inputs)\n        for x in inputs:\n            try:\n                K.is_keras_tensor(x)\n            except ValueError:\n                raise ValueError('Layer ' + self.name + ' was called with '\n                                 'an input that isn\\'t a symbolic tensor. '\n                                 'Received type: ' +\n                                 str(type(x)) + '. Full input: ' +\n                                 str(inputs) + '. All inputs to the layer '\n                                 'should be tensors.')\n\n        if not self.input_spec:\n            return\n        if not isinstance(self.input_spec, (list, tuple)):\n            input_spec = to_list(self.input_spec)\n        else:\n            input_spec = self.input_spec\n        if len(inputs) != len(input_spec):\n            raise ValueError('Layer ' + self.name + ' expects ' +\n                             str(len(input_spec)) + ' inputs, '\n                             'but it received ' + str(len(inputs)) +\n                             ' input tensors. Input received: ' +\n                             str(inputs))\n        for input_index, (x, spec) in enumerate(zip(inputs, input_spec)):\n            if spec is None:\n                continue\n\n            # Check ndim.\n            if spec.ndim is not None:\n                if K.ndim(x) != spec.ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected ndim=' +\n                                     str(spec.ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            if spec.max_ndim is not None:\n                ndim = K.ndim(x)\n                if ndim is not None and ndim > spec.max_ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected max_ndim=' +\n                                     str(spec.max_ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            if spec.min_ndim is not None:\n                ndim = K.ndim(x)\n                if ndim is not None and ndim < spec.min_ndim:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected min_ndim=' +\n                                     str(spec.min_ndim) + ', found ndim=' +\n                                     str(K.ndim(x)))\n            # Check dtype.\n            if spec.dtype is not None:\n                if K.dtype(x) != spec.dtype:\n                    raise ValueError('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected dtype=' +\n                                     str(spec.dtype) + ', found dtype=' +\n                                     str(K.dtype(x)))\n            # Check specific shape axes.\n            if spec.axes:\n                try:\n                    x_shape = K.int_shape(x)\n                except TypeError:\n                    x_shape = None\n                if x_shape is not None:\n                    for axis, value in spec.axes.items():\n                        if (value is not None and\n                                x_shape[int(axis)] not in {value, None}):\n                            raise ValueError(\n                                'Input ' + str(input_index) +\n                                ' is incompatible with layer ' +\n                                self.name + ': expected axis ' +\n                                str(axis) + ' of input shape to have '\n                                'value ' + str(value) +\n                                ' but got shape ' + str(x_shape))\n            # Check shape.\n            if spec.shape is not None:\n                try:\n                    x_shape = K.int_shape(x)\n                except TypeError:\n                    x_shape = None\n                if x_shape is not None:\n                    for spec_dim, dim in zip(spec.shape, x_shape):\n                        if spec_dim is not None and dim is not None:\n                            if spec_dim != dim:\n                                raise ValueError(\n                                    'Input ' + str(input_index) +\n                                    ' is incompatible with layer ' +\n                                    self.name + ': expected shape=' +\n                                    str(spec.shape) + ', found shape=' +\n                                    str(x_shape))",
        "begin_line": 262,
        "end_line": 368,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.__call__#382",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.__call__(self, inputs, **kwargs)",
        "snippet": "    def __call__(self, inputs, **kwargs):\n        \"\"\"Wrapper around self.call(), for handling internal references.\n\n        If a Keras tensor is passed:\n            - We call self._add_inbound_node().\n            - If necessary, we `build` the layer to match\n                the _keras_shape of the input(s).\n            - We update the _keras_shape of every input tensor with\n                its new shape (obtained via self.compute_output_shape).\n                This is done as part of _add_inbound_node().\n            - We update the _keras_history of the output tensor(s)\n                with the current layer.\n                This is done as part of _add_inbound_node().\n\n        # Arguments\n            inputs: Can be a tensor or list/tuple of tensors.\n            **kwargs: Additional keyword arguments to be passed to `call()`.\n\n        # Returns\n            Output of the layer's `call` method.\n\n        # Raises\n            ValueError: in case the layer is missing shape information\n                for its `build` call.\n        \"\"\"\n        if isinstance(inputs, list):\n            inputs = inputs[:]\n        with K.name_scope(self.name):\n            # Handle laying building (weight creating, input spec locking).\n            if not self.built:\n                # Raise exceptions in case the input is not compatible\n                # with the input_spec specified in the layer constructor.\n                self.assert_input_compatibility(inputs)\n\n                # Collect input shapes to build layer.\n                input_shapes = []\n                for x_elem in to_list(inputs):\n                    if hasattr(x_elem, '_keras_shape'):\n                        input_shapes.append(x_elem._keras_shape)\n                    elif hasattr(K, 'int_shape'):\n                        input_shapes.append(K.int_shape(x_elem))\n                    else:\n                        raise ValueError('You tried to call layer \"' +\n                                         self.name +\n                                         '\". This layer has no information'\n                                         ' about its expected input shape, '\n                                         'and thus cannot be built. '\n                                         'You can build it manually via: '\n                                         '`layer.build(batch_input_shape)`')\n                self.build(unpack_singleton(input_shapes))\n                self.built = True\n\n                # Load weights that were specified at layer instantiation.\n                if self._initial_weights is not None:\n                    self.set_weights(self._initial_weights)\n\n            # Raise exceptions in case the input is not compatible\n            # with the input_spec set at build time.\n            self.assert_input_compatibility(inputs)\n\n            # Handle mask propagation.\n            previous_mask = _collect_previous_mask(inputs)\n            user_kwargs = copy.copy(kwargs)\n            if not is_all_none(previous_mask):\n                # The previous layer generated a mask.\n                if has_arg(self.call, 'mask'):\n                    if 'mask' not in kwargs:\n                        # If mask is explicitly passed to __call__,\n                        # we should override the default mask.\n                        kwargs['mask'] = previous_mask\n            # Handle automatic shape inference (only useful for Theano).\n            input_shape = _collect_input_shape(inputs)\n\n            # Actually call the layer,\n            # collecting output(s), mask(s), and shape(s).\n            output = self.call(inputs, **kwargs)\n            output_mask = self.compute_mask(inputs, previous_mask)\n\n            # If the layer returns tensors from its inputs, unmodified,\n            # we copy them to avoid loss of tensor metadata.\n            output_ls = to_list(output)\n            inputs_ls = to_list(inputs)\n            output_ls_copy = []\n            for x in output_ls:\n                if x in inputs_ls:\n                    x = K.identity(x)\n                output_ls_copy.append(x)\n            output = unpack_singleton(output_ls_copy)\n\n            # Inferring the output shape is only relevant for Theano.\n            if all([s is not None\n                    for s in to_list(input_shape)]):\n                output_shape = self.compute_output_shape(input_shape)\n            else:\n                if isinstance(input_shape, list):\n                    output_shape = [None for _ in input_shape]\n                else:\n                    output_shape = None\n\n            if (not isinstance(output_mask, (list, tuple)) and\n                    len(output_ls) > 1):\n                # Augment the mask to match the length of the output.\n                output_mask = [output_mask] * len(output_ls)\n\n            # Add an inbound node to the layer, so that it keeps track\n            # of the call and of all new variables created during the call.\n            # This also updates the layer history of the output tensor(s).\n            # If the input tensor(s) had not previous Keras history,\n            # this does nothing.\n            self._add_inbound_node(input_tensors=inputs,\n                                   output_tensors=output,\n                                   input_masks=previous_mask,\n                                   output_masks=output_mask,\n                                   input_shapes=input_shape,\n                                   output_shapes=output_shape,\n                                   arguments=user_kwargs)\n\n            # Apply activity regularizer if any:\n            if (hasattr(self, 'activity_regularizer') and\n                    self.activity_regularizer is not None):\n                with K.name_scope('activity_regularizer'):\n                    regularization_losses = [\n                        self.activity_regularizer(x)\n                        for x in to_list(output)]\n                self.add_loss(regularization_losses,\n                              inputs=to_list(inputs))\n        return output",
        "begin_line": 382,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._add_inbound_node#510",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._add_inbound_node(self, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments=None)",
        "snippet": "    def _add_inbound_node(self, input_tensors, output_tensors,\n                          input_masks, output_masks,\n                          input_shapes, output_shapes, arguments=None):\n        \"\"\"Internal method to create an inbound node for the layer.\n\n        # Arguments\n            input_tensors: list of input tensors.\n            output_tensors: list of output tensors.\n            input_masks: list of input masks (a mask can be a tensor, or None).\n            output_masks: list of output masks\n                (a mask can be a tensor, or None).\n            input_shapes: list of input shape tuples.\n            output_shapes: list of output shape tuples.\n            arguments: dictionary of keyword arguments that were passed to the\n                `call` method of the layer at the call that created the node.\n        \"\"\"\n        input_tensors = to_list(input_tensors)\n        output_tensors = to_list(output_tensors)\n        input_masks = to_list(input_masks)\n        output_masks = to_list(output_masks)\n        input_shapes = to_list(input_shapes)\n        output_shapes = to_list(output_shapes)\n\n        # Collect input tensor(s) coordinates.\n        inbound_layers = []\n        node_indices = []\n        tensor_indices = []\n        for x in input_tensors:\n            if hasattr(x, '_keras_history'):\n                inbound_layer, node_index, tensor_index = x._keras_history\n                inbound_layers.append(inbound_layer)\n                node_indices.append(node_index)\n                tensor_indices.append(tensor_index)\n            else:\n                inbound_layers.append(None)\n                node_indices.append(None)\n                tensor_indices.append(None)\n\n        # Create node, add it to inbound nodes.\n        Node(\n            self,\n            inbound_layers=inbound_layers,\n            node_indices=node_indices,\n            tensor_indices=tensor_indices,\n            input_tensors=input_tensors,\n            output_tensors=output_tensors,\n            input_masks=input_masks,\n            output_masks=output_masks,\n            input_shapes=input_shapes,\n            output_shapes=output_shapes,\n            arguments=arguments\n        )\n\n        # Update tensor history, _keras_shape and _uses_learning_phase.\n        for i in range(len(output_tensors)):\n            output_tensors[i]._keras_shape = output_shapes[i]\n            uses_lp = any(\n                [getattr(x, '_uses_learning_phase', False)\n                 for x in input_tensors])\n            uses_lp = getattr(self, 'uses_learning_phase', False) or uses_lp\n            output_tensors[i]._uses_learning_phase = getattr(\n                output_tensors[i], '_uses_learning_phase', False) or uses_lp\n            output_tensors[i]._keras_history = (self,\n                                                len(self._inbound_nodes) - 1,\n                                                i)",
        "begin_line": 510,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.013157894736842105,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.013157894736842105,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.compute_mask#593",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.compute_mask(self, inputs, mask=None)",
        "snippet": "    def compute_mask(self, inputs, mask=None):\n        \"\"\"Computes an output mask tensor.\n\n        # Arguments\n            inputs: Tensor or list of tensors.\n            mask: Tensor or list of tensors.\n\n        # Returns\n            None or a tensor (or list of tensors,\n                one per output tensor of the layer).\n        \"\"\"\n        if not self.supports_masking:\n            if mask is not None:\n                if isinstance(mask, list):\n                    if any(m is not None for m in mask):\n                        raise TypeError('Layer ' + self.name +\n                                        ' does not support masking, '\n                                        'but was passed an input_mask: ' +\n                                        str(mask))\n                else:\n                    raise TypeError('Layer ' + self.name +\n                                    ' does not support masking, '\n                                    'but was passed an input_mask: ' +\n                                    str(mask))\n            # masking not explicitly supported: return None as mask\n            return None\n        # if masking is explicitly supported, by default\n        # carry over the input mask\n        return mask",
        "begin_line": 593,
        "end_line": 621,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.build#623",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        \"\"\"Creates the layer weights.\n\n        Must be implemented on all layers that have weights.\n\n        # Arguments\n            input_shape: Keras tensor (future input to layer)\n                or list/tuple of Keras tensors to reference\n                for weight shape computations.\n        \"\"\"\n        self.built = True",
        "begin_line": 623,
        "end_line": 633,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.engine.base_layer.Layer._get_node_attribute_at_index#635",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer._get_node_attribute_at_index(self, node_index, attr, attr_name)",
        "snippet": "    def _get_node_attribute_at_index(self, node_index, attr, attr_name):\n        \"\"\"Retrieves an attribute (e.g. input_tensors) from a node.\n\n        This is used to implement the methods:\n            - get_input_shape_at\n            - get_output_shape_at\n            - get_input_at\n            etc...\n\n        # Arguments\n            node_index: Integer index of the node from which\n                to retrieve the attribute.\n            attr: Exact node attribute name.\n            attr_name: Human-readable attribute name, for error messages.\n\n        # Returns\n            The layer's attribute `attr` at the node of index `node_index`.\n\n        # Raises\n            RuntimeError: If the layer has no inbound nodes.\n            ValueError: If the index is does not match any node.\n        \"\"\"\n        if not self._inbound_nodes:\n            raise RuntimeError('The layer has never been called '\n                               'and thus has no defined ' + attr_name + '.')\n        if not len(self._inbound_nodes) > node_index:\n            raise ValueError('Asked to get ' + attr_name +\n                             ' at node ' + str(node_index) +\n                             ', but the layer has only ' +\n                             str(len(self._inbound_nodes)) + ' inbound nodes.')\n        values = getattr(self._inbound_nodes[node_index], attr)\n        return unpack_singleton(values)",
        "begin_line": 635,
        "end_line": 666,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.input#769",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.input(self)",
        "snippet": "    def input(self):\n        \"\"\"Retrieves the input tensor(s) of a layer.\n\n        Only applicable if the layer has exactly one inbound node,\n        i.e. if it is connected to one incoming layer.\n\n        # Returns\n            Input tensor or list of input tensors.\n\n        # Raises\n            AttributeError: if the layer is connected to\n            more than one incoming layers.\n        \"\"\"\n        if len(self._inbound_nodes) > 1:\n            raise AttributeError('Layer ' + self.name +\n                                 ' has multiple inbound nodes, '\n                                 'hence the notion of \"layer input\" '\n                                 'is ill-defined. '\n                                 'Use `get_input_at(node_index)` instead.')\n        elif not self._inbound_nodes:\n            raise AttributeError('Layer ' + self.name +\n                                 ' is not connected, no input to return.')\n        return self._get_node_attribute_at_index(0, 'input_tensors',\n                                                 'input')",
        "begin_line": 769,
        "end_line": 792,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_updates_for#1000",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_updates_for(self, inputs)",
        "snippet": "    def get_updates_for(self, inputs):\n        if not self.trainable and not self.stateful:\n            return []\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            inputs_hash = None\n        if inputs_hash in self._per_input_updates:\n            return self._per_input_updates[inputs_hash]\n        return []",
        "begin_line": 1000,
        "end_line": 1009,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.Layer.get_losses_for#1011",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Layer",
        "signature": "keras.engine.base_layer.Layer.get_losses_for(self, inputs)",
        "snippet": "    def get_losses_for(self, inputs):\n        if inputs is not None:\n            inputs_hash = object_list_uid(inputs)\n        else:\n            inputs_hash = None\n        if inputs_hash in self._per_input_losses:\n            return self._per_input_losses[inputs_hash]\n        return []",
        "begin_line": 1011,
        "end_line": 1018,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer.InputSpec.__init__#1152",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.InputSpec",
        "signature": "keras.engine.base_layer.InputSpec.__init__(self, dtype=None, shape=None, ndim=None, max_ndim=None, min_ndim=None, axes=None)",
        "snippet": "    def __init__(self, dtype=None,\n                 shape=None,\n                 ndim=None,\n                 max_ndim=None,\n                 min_ndim=None,\n                 axes=None):\n        self.dtype = dtype\n        self.shape = shape\n        if shape is not None:\n            self.ndim = len(shape)\n        else:\n            self.ndim = ndim\n        self.max_ndim = max_ndim\n        self.min_ndim = min_ndim\n        self.axes = axes or {}",
        "begin_line": 1152,
        "end_line": 1166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer.Node.__init__#1224",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer.Node",
        "signature": "keras.engine.base_layer.Node.__init__(self, outbound_layer, inbound_layers, node_indices, tensor_indices, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments=None)",
        "snippet": "    def __init__(self, outbound_layer,\n                 inbound_layers, node_indices, tensor_indices,\n                 input_tensors, output_tensors,\n                 input_masks, output_masks,\n                 input_shapes, output_shapes,\n                 arguments=None):\n        # Layer instance (NOT a list).\n        # this is the layer that takes a list of input tensors\n        # and turns them into a list of output tensors.\n        # the current node will be added to\n        # the inbound_nodes of outbound_layer.\n        self.outbound_layer = outbound_layer\n\n        # The following 3 properties describe where\n        # the input tensors come from: which layers,\n        # and for each layer, which node and which\n        # tensor output of each node.\n\n        # List of layer instances.\n        self.inbound_layers = inbound_layers\n        # List of integers, 1:1 mapping with inbound_layers.\n        self.node_indices = node_indices\n        # List of integers, 1:1 mapping with inbound_layers.\n        self.tensor_indices = tensor_indices\n\n        # Following 2 properties:\n        # tensor inputs and outputs of outbound_layer.\n\n        # List of tensors. 1:1 mapping with inbound_layers.\n        self.input_tensors = input_tensors\n        # List of tensors, created by outbound_layer.call().\n        self.output_tensors = output_tensors\n\n        # Following 2 properties: input and output masks.\n        # List of tensors, 1:1 mapping with input_tensor.\n        self.input_masks = input_masks\n        # List of tensors, created by outbound_layer.compute_mask().\n        self.output_masks = output_masks\n\n        # Following 2 properties: input and output shapes.\n\n        # List of shape tuples, shapes of input_tensors.\n        self.input_shapes = input_shapes\n        # List of shape tuples, shapes of output_tensors.\n        self.output_shapes = output_shapes\n\n        # Optional keyword arguments to layer's `call`.\n        self.arguments = arguments\n\n        # Add nodes to all layers involved.\n        for layer in inbound_layers:\n            if layer is not None:\n                layer._outbound_nodes.append(self)\n        outbound_layer._inbound_nodes.append(self)",
        "begin_line": 1224,
        "end_line": 1277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer._collect_previous_mask#1296",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._collect_previous_mask(input_tensors)",
        "snippet": "def _collect_previous_mask(input_tensors):\n    \"\"\"Retrieves the output mask(s) of the previous node.\n\n    # Arguments\n        input_tensors: A tensor or list of tensors.\n\n    # Returns\n        A mask tensor or list of mask tensors.\n    \"\"\"\n    input_tensors = to_list(input_tensors)\n    masks = []\n    for x in input_tensors:\n        if hasattr(x, '_keras_history'):\n            inbound_layer, node_index, tensor_index = x._keras_history\n            node = inbound_layer._inbound_nodes[node_index]\n            mask = node.output_masks[tensor_index]\n            masks.append(mask)\n        else:\n            masks.append(None)\n    return unpack_singleton(masks)",
        "begin_line": 1296,
        "end_line": 1315,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.base_layer._to_snake_case#1318",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._to_snake_case(name)",
        "snippet": "def _to_snake_case(name):\n    intermediate = re.sub('(.)([A-Z][a-z0-9]+)', r'\\1_\\2', name)\n    insecure = re.sub('([a-z])([A-Z])', r'\\1_\\2', intermediate).lower()\n    # If the class is private the name starts with \"_\" which is not secure\n    # for creating scopes. We prefix the name with \"private\" in this case.\n    if insecure[0] != '_':\n        return insecure\n    return 'private' + insecure",
        "begin_line": 1318,
        "end_line": 1325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.engine.base_layer._collect_input_shape#1328",
        "src_path": "keras/engine/base_layer.py",
        "class_name": "keras.engine.base_layer",
        "signature": "keras.engine.base_layer._collect_input_shape(input_tensors)",
        "snippet": "def _collect_input_shape(input_tensors):\n    \"\"\"Collects the output shape(s) of a list of Keras tensors.\n\n    # Arguments\n        input_tensors: list of input tensors (or single input tensor).\n\n    # Returns\n        List of shape tuples (or single tuple), one tuple per input.\n    \"\"\"\n    input_tensors = to_list(input_tensors)\n    shapes = []\n    for x in input_tensors:\n        try:\n            shapes.append(K.int_shape(x))\n        except TypeError:\n            shapes.append(None)\n    return unpack_singleton(shapes)",
        "begin_line": 1328,
        "end_line": 1344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.vis_utils._check_pydot#16",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils._check_pydot()",
        "snippet": "def _check_pydot():\n    \"\"\"Raise errors if `pydot` or GraphViz unavailable.\"\"\"\n    if pydot is None:\n        raise ImportError(\n            'Failed to import `pydot`. '\n            'Please install `pydot`. '\n            'For example with `pip install pydot`.')\n    try:\n        # Attempt to create an image of a blank graph\n        # to check the pydot/graphviz installation.\n        pydot.Dot.create(pydot.Dot())\n    except OSError:\n        raise OSError(\n            '`pydot` failed to call GraphViz.'\n            'Please install GraphViz (https://www.graphviz.org/) '\n            'and ensure that its executables are in the $PATH.')",
        "begin_line": 16,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.vis_utils.model_to_dot#34",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils.model_to_dot(model, show_shapes=False, show_layer_names=True, rankdir='TB')",
        "snippet": "def model_to_dot(model,\n                 show_shapes=False,\n                 show_layer_names=True,\n                 rankdir='TB'):\n    \"\"\"Convert a Keras model to dot format.\n\n    # Arguments\n        model: A Keras model instance.\n        show_shapes: whether to display shape information.\n        show_layer_names: whether to display layer names.\n        rankdir: `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot:\n            'TB' creates a vertical plot;\n            'LR' creates a horizontal plot.\n\n    # Returns\n        A `pydot.Dot` instance representing the Keras model.\n    \"\"\"\n    from ..layers.wrappers import Wrapper\n    from ..models import Sequential\n\n    _check_pydot()\n    dot = pydot.Dot()\n    dot.set('rankdir', rankdir)\n    dot.set('concentrate', True)\n    dot.set_node_defaults(shape='record')\n\n    if isinstance(model, Sequential):\n        if not model.built:\n            model.build()\n    layers = model.layers\n\n    # Create graph nodes.\n    for layer in layers:\n        layer_id = str(id(layer))\n\n        # Append a wrapped layer's label to node's label, if it exists.\n        layer_name = layer.name\n        class_name = layer.__class__.__name__\n        if isinstance(layer, Wrapper):\n            layer_name = '{}({})'.format(layer_name, layer.layer.name)\n            child_class_name = layer.layer.__class__.__name__\n            class_name = '{}({})'.format(class_name, child_class_name)\n\n        # Create node's label.\n        if show_layer_names:\n            label = '{}: {}'.format(layer_name, class_name)\n        else:\n            label = class_name\n\n        # Rebuild the label as a table including input/output shapes.\n        if show_shapes:\n            try:\n                outputlabels = str(layer.output_shape)\n            except AttributeError:\n                outputlabels = 'multiple'\n            if hasattr(layer, 'input_shape'):\n                inputlabels = str(layer.input_shape)\n            elif hasattr(layer, 'input_shapes'):\n                inputlabels = ', '.join(\n                    [str(ishape) for ishape in layer.input_shapes])\n            else:\n                inputlabels = 'multiple'\n            label = '%s\\n|{input:|output:}|{{%s}|{%s}}' % (label,\n                                                           inputlabels,\n                                                           outputlabels)\n        node = pydot.Node(layer_id, label=label)\n        dot.add_node(node)\n\n    # Connect nodes with edges.\n    for layer in layers:\n        layer_id = str(id(layer))\n        for i, node in enumerate(layer._inbound_nodes):\n            node_key = layer.name + '_ib-' + str(i)\n            if node_key in model._network_nodes:\n                for inbound_layer in node.inbound_layers:\n                    inbound_layer_id = str(id(inbound_layer))\n                    dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n    return dot",
        "begin_line": 34,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.vis_utils.plot_model#115",
        "src_path": "keras/utils/vis_utils.py",
        "class_name": "keras.utils.vis_utils",
        "signature": "keras.utils.vis_utils.plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')",
        "snippet": "def plot_model(model,\n               to_file='model.png',\n               show_shapes=False,\n               show_layer_names=True,\n               rankdir='TB'):\n    \"\"\"Converts a Keras model to dot format and save to a file.\n\n    # Arguments\n        model: A Keras model instance\n        to_file: File name of the plot image.\n        show_shapes: whether to display shape information.\n        show_layer_names: whether to display layer names.\n        rankdir: `rankdir` argument passed to PyDot,\n            a string specifying the format of the plot:\n            'TB' creates a vertical plot;\n            'LR' creates a horizontal plot.\n    \"\"\"\n    dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)\n    _, extension = os.path.splitext(to_file)\n    if not extension:\n        extension = 'png'\n    else:\n        extension = extension[1:]\n    dot.write(to_file, format=extension)",
        "begin_line": 115,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_uid#62",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_uid(prefix='')",
        "snippet": "def get_uid(prefix=''):\n    \"\"\"Get the uid for the default graph.\n\n    # Arguments\n        prefix: An optional prefix of the graph.\n\n    # Returns\n        A unique identifier for the graph.\n    \"\"\"\n    global _GRAPH_UID_DICTS\n    graph = tf.get_default_graph()\n    if graph not in _GRAPH_UID_DICTS:\n        _GRAPH_UID_DICTS[graph] = defaultdict(int)\n    _GRAPH_UID_DICTS[graph][prefix] += 1\n    return _GRAPH_UID_DICTS[graph][prefix]",
        "begin_line": 62,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.get_session#154",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.get_session()",
        "snippet": "def get_session():\n    \"\"\"Returns the TF session to be used by the backend.\n\n    If a default TensorFlow session is available, we will return it.\n\n    Else, we will return the global Keras session.\n\n    If no global Keras session exists at this point:\n    we will create a new global session.\n\n    Note that you can manually set the global session\n    via `K.set_session(sess)`.\n\n    # Returns\n        A TensorFlow session.\n    \"\"\"\n    global _SESSION\n\n    default_session = tf.get_default_session()\n\n    if default_session is not None:\n        session = default_session\n    else:\n        if _SESSION is None:\n            if not os.environ.get('OMP_NUM_THREADS'):\n                config = tf.ConfigProto(allow_soft_placement=True)\n            else:\n                num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n                config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n                                        allow_soft_placement=True)\n            _SESSION = tf.Session(config=config)\n        session = _SESSION\n    if not _MANUAL_VAR_INIT:\n        with session.graph.as_default():\n            variables = tf.global_variables()\n            candidate_vars = []\n            for v in variables:\n                if not getattr(v, '_keras_initialized', False):\n                    candidate_vars.append(v)\n            if candidate_vars:\n                # This step is expensive, so we only run it on variables\n                # not already marked as initialized.\n                is_initialized = session.run(\n                    [tf.is_variable_initialized(v) for v in candidate_vars])\n                uninitialized_vars = []\n                for flag, v in zip(is_initialized, candidate_vars):\n                    if not flag:\n                        uninitialized_vars.append(v)\n                    v._keras_initialized = True\n                if uninitialized_vars:\n                    session.run(tf.variables_initializer(uninitialized_vars))\n    # hack for list_devices() function.\n    # list_devices() function is not available under tensorflow r1.3.\n    if not hasattr(session, 'list_devices'):\n        session.list_devices = lambda: device_lib.list_local_devices()\n    return session",
        "begin_line": 154,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._to_tensor#296",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._to_tensor(x, dtype)",
        "snippet": "def _to_tensor(x, dtype):\n    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n\n    # Arguments\n        x: An object to be converted (numpy array, list, tensors).\n        dtype: The destination type.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.convert_to_tensor(x, dtype=dtype)",
        "begin_line": 296,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_sparse#309",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_sparse(tensor)",
        "snippet": "def is_sparse(tensor):\n    \"\"\"Returns whether a tensor is a sparse tensor.\n\n    # Arguments\n        tensor: A tensor instance.\n\n    # Returns\n        A boolean.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> a = K.placeholder((2, 2), sparse=False)\n        >>> print(K.is_sparse(a))\n        False\n        >>> b = K.placeholder((2, 2), sparse=True)\n        >>> print(K.is_sparse(b))\n        True\n    ```\n    \"\"\"\n    return isinstance(tensor, tf.SparseTensor)",
        "begin_line": 309,
        "end_line": 329,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.variable#361",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.variable(value, dtype=None, name=None, constraint=None)",
        "snippet": "def variable(value, dtype=None, name=None, constraint=None):\n    \"\"\"Instantiates a variable and returns it.\n\n    # Arguments\n        value: Numpy array, initial value of the tensor.\n        dtype: Tensor type.\n        name: Optional name string for the tensor.\n        constraint: Optional projection function to be\n            applied to the variable after an optimizer update.\n\n    # Returns\n        A variable instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n        >>> K.dtype(kvar)\n        'float64'\n        >>> print(kvar)\n        example_var\n        >>> K.eval(kvar)\n        array([[ 1.,  2.],\n               [ 3.,  4.]])\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if hasattr(value, 'tocoo'):\n        sparse_coo = value.tocoo()\n        indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n                                  np.expand_dims(sparse_coo.col, 1)), 1)\n        v = tf.SparseTensor(indices=indices,\n                            values=sparse_coo.data,\n                            dense_shape=sparse_coo.shape)\n        v._keras_shape = sparse_coo.shape\n        v._uses_learning_phase = False\n        return v\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\n    if isinstance(value, np.ndarray):\n        v._keras_shape = value.shape\n    elif hasattr(value, 'get_shape'):\n        v._keras_shape = int_shape(value)\n    v._uses_learning_phase = False\n    # TODO: move to Variable constructor when supported in public release.\n    try:\n        v.constraint = constraint\n    except AttributeError:\n        v._constraint = constraint\n    return v",
        "begin_line": 361,
        "end_line": 411,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.constant#414",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.constant(value, dtype=None, shape=None, name=None)",
        "snippet": "def constant(value, dtype=None, shape=None, name=None):\n    \"\"\"Creates a constant tensor.\n\n    # Arguments\n        value: A constant value (or list)\n        dtype: The type of the elements of the resulting tensor.\n        shape: Optional dimensions of resulting tensor.\n        name: Optional name for the tensor.\n\n    # Returns\n        A Constant Tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    return tf.constant(value, dtype=dtype, shape=shape, name=name)",
        "begin_line": 414,
        "end_line": 428,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_keras_tensor#431",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_keras_tensor(x)",
        "snippet": "def is_keras_tensor(x):\n    \"\"\"Returns whether `x` is a Keras tensor.\n\n    A \"Keras tensor\" is a tensor that was returned by a Keras layer,\n    (`Layer` class) or by `Input`.\n\n    # Arguments\n        x: A candidate tensor.\n\n    # Returns\n        A boolean: Whether the argument is a Keras tensor.\n\n    # Raises\n        ValueError: In case `x` is not a symbolic tensor.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> from keras.layers import Input, Dense\n        >>> np_var = numpy.array([1, 2])\n        >>> K.is_keras_tensor(np_var) # A numpy array is not a symbolic tensor.\n        ValueError\n        >>> k_var = tf.placeholder('float32', shape=(1,1))\n        >>> K.is_keras_tensor(k_var) # A variable indirectly created outside of keras is not a Keras tensor.\n        False\n        >>> keras_var = K.variable(np_var)\n        >>> K.is_keras_tensor(keras_var)  # A variable created with the keras backend is not a Keras tensor.\n        False\n        >>> keras_placeholder = K.placeholder(shape=(2, 4, 5))\n        >>> K.is_keras_tensor(keras_placeholder)  # A placeholder is not a Keras tensor.\n        False\n        >>> keras_input = Input([10])\n        >>> K.is_keras_tensor(keras_input) # An Input is a Keras tensor.\n        True\n        >>> keras_layer_output = Dense(10)(keras_input)\n        >>> K.is_keras_tensor(keras_layer_output) # Any Keras layer output is a Keras tensor.\n        True\n    ```\n    \"\"\"\n    if not is_tensor(x):\n        raise ValueError('Unexpectedly found an instance of type `' +\n                         str(type(x)) + '`. '\n                         'Expected a symbolic tensor instance.')\n    return hasattr(x, '_keras_history')",
        "begin_line": 431,
        "end_line": 474,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.is_tensor#477",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.is_tensor(x)",
        "snippet": "def is_tensor(x):\n    return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)",
        "begin_line": 477,
        "end_line": 478,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.placeholder#481",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)",
        "snippet": "def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n    \"\"\"Instantiates a placeholder tensor and returns it.\n\n    # Arguments\n        shape: Shape of the placeholder\n            (integer tuple, may include `None` entries).\n        ndim: Number of axes of the tensor.\n            At least one of {`shape`, `ndim`} must be specified.\n            If both are specified, `shape` is used.\n        dtype: Placeholder type.\n        sparse: Boolean, whether the placeholder should have a sparse type.\n        name: Optional name string for the placeholder.\n\n    # Returns\n        Tensor instance (with Keras metadata included).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> input_ph = K.placeholder(shape=(2, 4, 5))\n        >>> input_ph._keras_shape\n        (2, 4, 5)\n        >>> input_ph\n        <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if not shape:\n        if ndim:\n            shape = tuple([None for _ in range(ndim)])\n    if sparse:\n        x = tf.sparse_placeholder(dtype, shape=shape, name=name)\n    else:\n        x = tf.placeholder(dtype, shape=shape, name=name)\n    x._keras_shape = shape\n    x._uses_learning_phase = False\n    return x",
        "begin_line": 481,
        "end_line": 518,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.shape#536",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.shape(x)",
        "snippet": "def shape(x):\n    \"\"\"Returns the symbolic shape of a tensor or variable.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A symbolic shape (which is itself a tensor).\n\n    # Examples\n    ```python\n        # TensorFlow example\n        >>> from keras import backend as K\n        >>> tf_session = K.get_session()\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> inputs = keras.backend.placeholder(shape=(2, 4, 5))\n        >>> K.shape(kvar)\n        <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n        >>> K.shape(inputs)\n        <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n        # To get integer shape (Instead, you can use K.int_shape(x))\n        >>> K.shape(kvar).eval(session=tf_session)\n        array([2, 2], dtype=int32)\n        >>> K.shape(inputs).eval(session=tf_session)\n        array([2, 4, 5], dtype=int32)\n    ```\n    \"\"\"\n    return tf.shape(x)",
        "begin_line": 536,
        "end_line": 564,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.int_shape#567",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.int_shape(x)",
        "snippet": "def int_shape(x):\n    \"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        A tuple of integers (or None entries).\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> K.int_shape(inputs)\n        (2, 4, 5)\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.int_shape(kvar)\n        (2, 2)\n    ```\n    \"\"\"\n    if hasattr(x, '_keras_shape'):\n        return x._keras_shape\n    try:\n        return tuple(x.get_shape().as_list())\n    except ValueError:\n        return None",
        "begin_line": 567,
        "end_line": 593,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.ndim#596",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.ndim(x)",
        "snippet": "def ndim(x):\n    \"\"\"Returns the number of axes in a tensor, as an integer.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        Integer (scalar), number of axes.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> inputs = K.placeholder(shape=(2, 4, 5))\n        >>> val = np.array([[1, 2], [3, 4]])\n        >>> kvar = K.variable(value=val)\n        >>> K.ndim(inputs)\n        3\n        >>> K.ndim(kvar)\n        2\n    ```\n    \"\"\"\n    dims = x.get_shape()._dims\n    if dims is not None:\n        return len(dims)\n    return None",
        "begin_line": 596,
        "end_line": 620,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.dtype#623",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.dtype(x)",
        "snippet": "def dtype(x):\n    \"\"\"Returns the dtype of a Keras tensor or variable, as a string.\n\n    # Arguments\n        x: Tensor or variable.\n\n    # Returns\n        String, dtype of `x`.\n\n    # Examples\n    ```python\n        >>> from keras import backend as K\n        >>> K.dtype(K.placeholder(shape=(2,4,5)))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n        'float32'\n        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n        'float64'\n        # Keras variable\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n        >>> K.dtype(kvar)\n        'float32_ref'\n        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n        >>> K.dtype(kvar)\n        'float32_ref'\n    ```\n    \"\"\"\n    return x.dtype.base_dtype.name",
        "begin_line": 623,
        "end_line": 650,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.zeros#674",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.zeros(shape, dtype=None, name=None)",
        "snippet": "def zeros(shape, dtype=None, name=None):\n    \"\"\"Instantiates an all-zeros variable and returns it.\n\n    # Arguments\n        shape: Tuple of integers, shape of returned Keras variable\n        dtype: String, data type of returned Keras variable\n        name: String, name of returned Keras variable\n\n    # Returns\n        A variable (including Keras metadata), filled with `0.0`.\n        Note that if `shape` was symbolic, we cannot return a variable,\n        and will return a dynamically-shaped tensor instead.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> kvar = K.zeros((3,4))\n        >>> K.eval(kvar)\n        array([[ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\n    ```\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    tf_dtype = tf.as_dtype(dtype)\n    v = tf.zeros(shape=shape, dtype=tf_dtype, name=name)\n    if py_all(v.get_shape().as_list()):\n        return variable(v, dtype=dtype, name=name)\n    return v",
        "begin_line": 674,
        "end_line": 703,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.cast#926",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.cast(x, dtype)",
        "snippet": "def cast(x, dtype):\n    \"\"\"Casts a tensor to a different dtype and returns it.\n\n    You can cast a Keras variable but it still returns a Keras tensor.\n\n    # Arguments\n        x: Keras tensor (or variable).\n        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n\n    # Returns\n        Keras tensor with dtype `dtype`.\n\n    # Example\n    ```python\n        >>> from keras import backend as K\n        >>> input = K.placeholder((2, 3), dtype='float32')\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # It doesn't work in-place as below.\n        >>> K.cast(input, dtype='float16')\n        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>\n        >>> input\n        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n        # you need to assign it.\n        >>> input = K.cast(input, dtype='float16')\n        >>> input\n        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>\n    ```\n    \"\"\"\n    return tf.cast(x, dtype)",
        "begin_line": 926,
        "end_line": 955,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.update#961",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.update(x, new_x)",
        "snippet": "def update(x, new_x):\n    \"\"\"Update the value of `x` to `new_x`.\n\n    # Arguments\n        x: A `Variable`.\n        new_x: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf.assign(x, new_x)",
        "begin_line": 961,
        "end_line": 971,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.update_add#974",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.update_add(x, increment)",
        "snippet": "def update_add(x, increment):\n    \"\"\"Update the value of `x` by adding `increment`.\n\n    # Arguments\n        x: A `Variable`.\n        increment: A tensor of same shape as `x`.\n\n    # Returns\n        The variable `x` updated.\n    \"\"\"\n    return tf.assign_add(x, increment)",
        "begin_line": 974,
        "end_line": 984,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.dot#1017",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.dot(x, y)",
        "snippet": "def dot(x, y):\n    \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n\n    When attempting to multiply a nD tensor\n    with a nD tensor, it reproduces the Theano behavior.\n    (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`)\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A tensor, dot product of `x` and `y`.\n\n    # Examples\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(2, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n    ```\n\n    ```python\n        # dot product between tensors\n        >>> x = K.placeholder(shape=(32, 28, 3))\n        >>> y = K.placeholder(shape=(3, 4))\n        >>> xy = K.dot(x, y)\n        >>> xy\n        <tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32>\n    ```\n\n    ```python\n        # Theano-like behavior example\n        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n        >>> y = K.ones((4, 3, 5))\n        >>> xy = K.dot(x, y)\n        >>> K.int_shape(xy)\n        (2, 4, 5)\n    ```\n    \"\"\"\n    if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n        x_shape = []\n        for i, s in zip(int_shape(x), tf.unstack(tf.shape(x))):\n            if i is not None:\n                x_shape.append(i)\n            else:\n                x_shape.append(s)\n        x_shape = tuple(x_shape)\n        y_shape = []\n        for i, s in zip(int_shape(y), tf.unstack(tf.shape(y))):\n            if i is not None:\n                y_shape.append(i)\n            else:\n                y_shape.append(s)\n        y_shape = tuple(y_shape)\n        y_permute_dim = list(range(ndim(y)))\n        y_permute_dim = [y_permute_dim.pop(-2)] + y_permute_dim\n        xt = tf.reshape(x, [-1, x_shape[-1]])\n        yt = tf.reshape(tf.transpose(y, perm=y_permute_dim), [y_shape[-2], -1])\n        return tf.reshape(tf.matmul(xt, yt),\n                          x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n    if is_sparse(x):\n        out = tf.sparse_tensor_dense_matmul(x, y)\n    else:\n        out = tf.matmul(x, y)\n    return out",
        "begin_line": 1017,
        "end_line": 1084,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.prod#1283",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.prod(x, axis=None, keepdims=False)",
        "snippet": "def prod(x, axis=None, keepdims=False):\n    \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the product.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the product of elements of `x`.\n    \"\"\"\n    return tf.reduce_prod(x, axis, keepdims)",
        "begin_line": 1283,
        "end_line": 1297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.mean#1366",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.mean(x, axis=None, keepdims=False)",
        "snippet": "def mean(x, axis=None, keepdims=False):\n    \"\"\"Mean of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: A list of integer. Axes to compute the mean.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1 for each entry in `axis`. If `keepdims` is `True`,\n            the reduced dimensions are retained with length 1.\n\n    # Returns\n        A tensor with the mean of elements of `x`.\n    \"\"\"\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\n    return tf.reduce_mean(x, axis, keepdims)",
        "begin_line": 1366,
        "end_line": 1382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.not_equal#1597",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.not_equal(x, y)",
        "snippet": "def not_equal(x, y):\n    \"\"\"Element-wise inequality between two tensors.\n\n    # Arguments\n        x: Tensor or variable.\n        y: Tensor or variable.\n\n    # Returns\n        A bool tensor.\n    \"\"\"\n    return tf.not_equal(x, y)",
        "begin_line": 1597,
        "end_line": 1607,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.batch_flatten#2165",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.batch_flatten(x)",
        "snippet": "def batch_flatten(x):\n    \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n\n    In other words, it flattens each data samples of a batch.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n    return x",
        "begin_line": 2165,
        "end_line": 2177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function.__init__#2500",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function.__init__(self, inputs, outputs, updates=None, name=None, **session_kwargs)",
        "snippet": "    def __init__(self, inputs, outputs,\n                 updates=None,\n                 name=None,\n                 **session_kwargs):\n        updates = updates or []\n        if not isinstance(inputs, (list, tuple)):\n            raise TypeError('`inputs` to a TensorFlow backend function '\n                            'should be a list or tuple.')\n        if not isinstance(outputs, (list, tuple)):\n            raise TypeError('`outputs` of a TensorFlow backend function '\n                            'should be a list or tuple.')\n        if not isinstance(updates, (list, tuple)):\n            raise TypeError('`updates` in a TensorFlow backend function '\n                            'should be a list or tuple.')\n        self.inputs = list(inputs)\n        self.outputs = list(outputs)\n        with tf.control_dependencies(self.outputs):\n            updates_ops = []\n            for update in updates:\n                if isinstance(update, tuple):\n                    p, new_p = update\n                    updates_ops.append(tf.assign(p, new_p))\n                else:\n                    # assumed already an op\n                    updates_ops.append(update)\n            self.updates_op = tf.group(*updates_ops)\n        self.name = name\n        # additional tensor substitutions\n        self.feed_dict = session_kwargs.pop('feed_dict', {})\n        # additional operations\n        self.fetches = session_kwargs.pop('fetches', [])\n        if not isinstance(self.fetches, list):\n            self.fetches = [self.fetches]\n        # The main use case of `fetches` being passed to a model is the ability\n        # to run custom updates\n        # (since the outputs of fetches are never returned).\n        # This requires us to wrap fetches in `identity` ops.\n        self.fetches = [tf.identity(x) for x in self.fetches]\n        self.session_kwargs = session_kwargs\n        if session_kwargs:\n            raise ValueError('Some keys in session_kwargs are not '\n                             'supported at this '\n                             'time: %s', session_kwargs.keys())\n        self._callable_fn = None\n        self._feed_arrays = None\n        self._feed_symbols = None\n        self._symbol_vals = None\n        self._session = None",
        "begin_line": 2500,
        "end_line": 2547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function._make_callable#2549",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function._make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)",
        "snippet": "    def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):\n        \"\"\"Generates a callable that runs the graph.\n\n        # Arguments\n            feed_arrays: List of input tensors to be fed\n                Numpy arrays at runtime.\n            feed_symbols: List of input tensors to be fed\n                symbolic tensors at runtime.\n            symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n            session: Session to use to generate the callable.\n\n        # Returns\n            Function that runs the graph according to the above options.\n        \"\"\"\n        # Prepare callable options.\n        callable_opts = config_pb2.CallableOptions()\n        # Handle external-data feed.\n        for x in feed_arrays:\n            callable_opts.feed.append(x.name)\n        if self.feed_dict:\n            for key in sorted(self.feed_dict.keys()):\n                callable_opts.feed.append(key.name)\n        # Handle symbolic feed.\n        for x, y in zip(feed_symbols, symbol_vals):\n            connection = callable_opts.tensor_connection.add()\n            if x.dtype != y.dtype:\n                y = tf.cast(y, dtype=x.dtype)\n            from_tensor = tf_ops._as_graph_element(y)\n            if from_tensor is None:\n                from_tensor = y\n            connection.from_tensor = from_tensor.name  # Data tensor\n            connection.to_tensor = x.name  # Placeholder\n        # Handle fetches.\n        for x in self.outputs + self.fetches:\n            callable_opts.fetch.append(x.name)\n        # Handle updates.\n        callable_opts.target.append(self.updates_op.name)\n        # Create callable.\n        callable_fn = session._make_callable_from_options(callable_opts)\n        # Cache parameters corresponding to the generated callable, so that\n        # we can detect future mismatches and refresh the callable.\n        self._callable_fn = callable_fn\n        self._feed_arrays = feed_arrays\n        self._feed_symbols = feed_symbols\n        self._symbol_vals = symbol_vals\n        self._session = session",
        "begin_line": 2549,
        "end_line": 2594,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function._call#2596",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function._call(self, inputs)",
        "snippet": "    def _call(self, inputs):\n        if not isinstance(inputs, (list, tuple)):\n            raise TypeError('`inputs` should be a list or tuple.')\n\n        session = get_session()\n        feed_arrays = []\n        array_vals = []\n        feed_symbols = []\n        symbol_vals = []\n        for tensor, value in zip(self.inputs, inputs):\n            if value is None:\n                continue\n            if is_tensor(value):\n                # Case: feeding symbolic tensor.\n                feed_symbols.append(tensor)\n                symbol_vals.append(value)\n            else:\n                feed_arrays.append(tensor)\n                # We need to do array conversion and type casting\n                # at this level, since\n                # `callable_fn` only supports exact matches.\n                array_vals.append(\n                    np.asarray(value,\n                               dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n        if self.feed_dict:\n            for key in sorted(self.feed_dict.keys()):\n                array_vals.append(\n                    np.asarray(self.feed_dict[key],\n                               dtype=tf.as_dtype(key.dtype).as_numpy_dtype))\n\n        # Refresh callable if anything has changed.\n        if (self._callable_fn is None or\n                feed_arrays != self._feed_arrays or\n                symbol_vals != self._symbol_vals or\n                feed_symbols != self._feed_symbols or\n                session != self._session):\n            self._make_callable(feed_arrays,\n                                feed_symbols,\n                                symbol_vals,\n                                session)\n        fetched = self._callable_fn(*array_vals)\n        return fetched[:len(self.outputs)]",
        "begin_line": 2596,
        "end_line": 2637,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.Function.__call__#2657",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend.Function",
        "signature": "keras.backend.tensorflow_backend.Function.__call__(self, inputs)",
        "snippet": "    def __call__(self, inputs):\n        if hasattr(get_session(), '_make_callable_from_options'):\n            if py_any(is_sparse(x) for x in self.inputs):\n                if py_any(is_tensor(x) for x in inputs):\n                    raise ValueError(\n                        'Feeding from symbolic tensors is not '\n                        'supported with sparse inputs.')\n                return self._legacy_call(inputs)\n\n            return self._call(inputs)\n        else:\n            if py_any(is_tensor(x) for x in inputs):\n                raise ValueError(\n                    'In order to feed symbolic tensors to a Keras model '\n                    'in TensorFlow, you need tensorflow 1.8 or higher.')\n            return self._legacy_call(inputs)",
        "begin_line": 2657,
        "end_line": 2672,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.function#2675",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.function(inputs, outputs, updates=None, **kwargs)",
        "snippet": "def function(inputs, outputs, updates=None, **kwargs):\n    \"\"\"Instantiates a Keras function.\n\n    # Arguments\n        inputs: List of placeholder tensors.\n        outputs: List of output tensors.\n        updates: List of update ops.\n        **kwargs: Passed to `tf.Session.run`.\n\n    # Returns\n        Output values as Numpy arrays.\n\n    # Raises\n        ValueError: if invalid kwargs are passed in.\n    \"\"\"\n    if kwargs:\n        for key in kwargs:\n            if not (has_arg(tf.Session.run, key, True) or has_arg(Function.__init__, key, True)):\n                msg = 'Invalid argument \"%s\" passed to K.function with TensorFlow backend' % key\n                raise ValueError(msg)\n    return Function(inputs, outputs, updates=updates, **kwargs)",
        "begin_line": 2675,
        "end_line": 2695,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.gradients#2698",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.gradients(loss, variables)",
        "snippet": "def gradients(loss, variables):\n    \"\"\"Returns the gradients of `loss` w.r.t. `variables`.\n\n    # Arguments\n        loss: Scalar tensor to minimize.\n        variables: List of variables.\n\n    # Returns\n        A gradients tensor.\n    \"\"\"\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)",
        "begin_line": 2698,
        "end_line": 2708,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.relu#3102",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.relu(x, alpha=0.0, max_value=None)",
        "snippet": "def relu(x, alpha=0., max_value=None):\n    \"\"\"Rectified linear unit.\n\n    With default values, it returns element-wise `max(x, 0)`.\n\n    # Arguments\n        x: A tensor or variable.\n        alpha: A scalar, slope of negative section (default=`0.`).\n        max_value: Saturation threshold.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if alpha != 0.:\n        x = tf.nn.leaky_relu(x, alpha)\n    else:\n        x = tf.nn.relu(x)\n\n    if max_value is not None:\n        max_value = _to_tensor(max_value, x.dtype.base_dtype)\n        x = tf.minimum(x, max_value)\n    return x",
        "begin_line": 3102,
        "end_line": 3123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.binary_crossentropy#3280",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.binary_crossentropy(target, output, from_logits=False)",
        "snippet": "def binary_crossentropy(target, output, from_logits=False):\n    \"\"\"Binary crossentropy between an output tensor and a target tensor.\n\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # transform back to logits\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output / (1 - output))\n\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                   logits=output)",
        "begin_line": 3280,
        "end_line": 3302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.sigmoid#3305",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.sigmoid(x)",
        "snippet": "def sigmoid(x):\n    \"\"\"Element-wise sigmoid.\n\n    # Arguments\n        x: A tensor or variable.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    return tf.nn.sigmoid(x)",
        "begin_line": 3305,
        "end_line": 3314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_conv2d_input#3424",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_conv2d_input(x, data_format)",
        "snippet": "def _preprocess_conv2d_input(x, data_format):\n    \"\"\"Transpose and cast the input before the conv2d.\n\n    # Arguments\n        x: input tensor.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype(x) == 'float64':\n        x = tf.cast(x, 'float32')\n    tf_data_format = 'NHWC'\n    if data_format == 'channels_first':\n        if not _has_nchw_support():\n            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n        else:\n            tf_data_format = 'NCHW'\n    return x, tf_data_format",
        "begin_line": 3424,
        "end_line": 3442,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend._preprocess_padding#3466",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend._preprocess_padding(padding)",
        "snippet": "def _preprocess_padding(padding):\n    \"\"\"Convert keras' padding to tensorflow's padding.\n\n    # Arguments\n        padding: string, `\"same\"` or `\"valid\"`.\n\n    # Returns\n        a string, `\"SAME\"` or `\"VALID\"`.\n\n    # Raises\n        ValueError: if `padding` is invalid.\n    \"\"\"\n    if padding == 'same':\n        padding = 'SAME'\n    elif padding == 'valid':\n        padding = 'VALID'\n    else:\n        raise ValueError('Invalid padding: ' + str(padding))\n    return padding",
        "begin_line": 3466,
        "end_line": 3484,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.conv2d#3533",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.conv2d(x, kernel, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1))",
        "snippet": "def conv2d(x, kernel, strides=(1, 1), padding='valid',\n           data_format=None, dilation_rate=(1, 1)):\n    \"\"\"2D convolution.\n\n    # Arguments\n        x: Tensor or variable.\n        kernel: kernel tensor.\n        strides: strides tuple.\n        padding: string, `\"same\"` or `\"valid\"`.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n            Whether to use Theano or TensorFlow/CNTK data format\n            for inputs/kernels/outputs.\n        dilation_rate: tuple of 2 integers.\n\n    # Returns\n        A tensor, result of 2D convolution.\n\n    # Raises\n        ValueError: If `data_format` is neither\n            `\"channels_last\"` nor `\"channels_first\"`.\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n\n    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n\n    padding = _preprocess_padding(padding)\n    x = tf.nn.convolution(\n        input=x,\n        filter=kernel,\n        dilation_rate=dilation_rate,\n        strides=strides,\n        padding=padding,\n        data_format=tf_data_format)\n\n    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n    return x",
        "begin_line": 3533,
        "end_line": 3569,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.bias_add#3939",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.bias_add(x, bias, data_format=None)",
        "snippet": "def bias_add(x, bias, data_format=None):\n    \"\"\"Adds a bias vector to a tensor.\n\n    # Arguments\n        x: Tensor or variable.\n        bias: Bias tensor to add.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n\n    # Returns\n        Output tensor.\n\n    # Raises\n        ValueError: In one of the two cases below:\n                    1. invalid `data_format` argument.\n                    2. invalid bias shape.\n                       the bias should be either a vector or\n                       a tensor with ndim(x) - 1 dimension\n    \"\"\"\n    data_format = normalize_data_format(data_format)\n    bias_shape = int_shape(bias)\n    if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n        raise ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions'\n                         % (len(bias_shape), ndim(x)))\n    if ndim(x) == 5:\n        if data_format == 'channels_first':\n            if len(bias_shape) == 1:\n                x += reshape(bias, (1, bias_shape[0], 1, 1, 1))\n            else:\n                x += reshape(bias, (1, bias_shape[3]) + bias_shape[:3])\n        elif data_format == 'channels_last':\n            if len(bias_shape) == 1:\n                x += reshape(bias, (1, 1, 1, bias_shape[0]))\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 4:\n        if data_format == 'channels_first':\n            if len(bias_shape) == 1:\n                if _has_nchw_support():\n                    x = tf.nn.bias_add(x, bias,\n                                       data_format='NCHW')\n                else:\n                    x += reshape(bias, (1, bias_shape[0], 1, 1))\n            else:\n                x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n        elif data_format == 'channels_last':\n            if len(bias_shape) == 1:\n                x = tf.nn.bias_add(x, bias,\n                                   data_format='NHWC')\n            else:\n                x += reshape(bias, (1,) + bias_shape)\n    elif ndim(x) == 3:\n        if data_format == 'channels_first':\n            if len(bias_shape) == 1:\n                x += reshape(bias, (1, bias_shape[0], 1))\n            else:\n                x += reshape(bias, (1, bias_shape[1], bias_shape[0]))\n        elif data_format == 'channels_last':\n            if len(bias_shape) == 1:\n                x += reshape(bias, (1, 1, bias_shape[0]))\n            else:\n                x += reshape(bias, (1, ) + bias_shape)\n    else:\n        x = tf.nn.bias_add(x, bias)\n    return x",
        "begin_line": 3939,
        "end_line": 4002,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.backend.tensorflow_backend.random_uniform#4029",
        "src_path": "keras/backend/tensorflow_backend.py",
        "class_name": "keras.backend.tensorflow_backend",
        "signature": "keras.backend.tensorflow_backend.random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None)",
        "snippet": "def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n    \"\"\"Returns a tensor with uniform distribution of values.\n\n    # Arguments\n        shape: A tuple of integers, the shape of tensor to create.\n        minval: A float, lower boundary of the uniform distribution\n            to draw samples.\n        maxval: A float, upper boundary of the uniform distribution\n            to draw samples.\n        dtype: String, dtype of returned tensor.\n        seed: Integer, random seed.\n\n    # Returns\n        A tensor.\n    \"\"\"\n    if dtype is None:\n        dtype = floatx()\n    if seed is None:\n        seed = np.random.randint(10e6)\n    return tf.random_uniform(shape, minval=minval, maxval=maxval,\n                             dtype=dtype, seed=seed)",
        "begin_line": 4029,
        "end_line": 4049,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.input_layer.InputLayer.__init__#34",
        "src_path": "keras/engine/input_layer.py",
        "class_name": "keras.engine.input_layer.InputLayer",
        "signature": "keras.engine.input_layer.InputLayer.__init__(self, input_shape=None, batch_size=None, batch_input_shape=None, dtype=None, input_tensor=None, sparse=False, name=None)",
        "snippet": "    def __init__(self, input_shape=None, batch_size=None,\n                 batch_input_shape=None,\n                 dtype=None, input_tensor=None, sparse=False, name=None):\n        if not name:\n            prefix = 'input'\n            name = prefix + '_' + str(K.get_uid(prefix))\n        super(InputLayer, self).__init__(dtype=dtype, name=name)\n\n        self.trainable = False\n        self.built = True\n        self.sparse = sparse\n\n        if input_shape and batch_input_shape:\n            raise ValueError('Only provide the input_shape OR '\n                             'batch_input_shape argument to '\n                             'InputLayer, not both at the same time.')\n        if input_tensor is not None and batch_input_shape is None:\n            # If input_tensor is set, and batch_input_shape is not set:\n            # Attempt automatic input shape inference.\n            try:\n                batch_input_shape = K.int_shape(input_tensor)\n            except TypeError:\n                if not input_shape and not batch_input_shape:\n                    raise ValueError('InputLayer was provided '\n                                     'an input_tensor argument, '\n                                     'but its input shape cannot be '\n                                     'automatically inferred. '\n                                     'You should pass an input_shape or '\n                                     'batch_input_shape argument.')\n        if not batch_input_shape:\n            if not input_shape:\n                raise ValueError('An Input layer should be passed either '\n                                 'a `batch_input_shape` or an `input_shape`.')\n            else:\n                batch_input_shape = (batch_size,) + tuple(input_shape)\n        else:\n            batch_input_shape = tuple(batch_input_shape)\n\n        if not dtype:\n            if input_tensor is None:\n                dtype = K.floatx()\n            else:\n                dtype = K.dtype(input_tensor)\n\n        self.batch_input_shape = batch_input_shape\n        self.dtype = dtype\n\n        if input_tensor is None:\n            self.is_placeholder = True\n            input_tensor = K.placeholder(shape=batch_input_shape,\n                                         dtype=dtype,\n                                         sparse=self.sparse,\n                                         name=self.name)\n        else:\n            self.is_placeholder = False\n            input_tensor._keras_shape = batch_input_shape\n        # Create an input node to add to self.outbound_node\n        # and set output_tensors' _keras_history.\n        input_tensor._uses_learning_phase = False\n        input_tensor._keras_history = (self, 0, 0)\n        Node(self,\n             inbound_layers=[],\n             node_indices=[],\n             tensor_indices=[],\n             input_tensors=[input_tensor],\n             output_tensors=[input_tensor],\n             input_masks=[None],\n             output_masks=[None],\n             input_shapes=[batch_input_shape],\n             output_shapes=[batch_input_shape])",
        "begin_line": 34,
        "end_line": 103,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.engine.input_layer.Input#113",
        "src_path": "keras/engine/input_layer.py",
        "class_name": "keras.engine.input_layer",
        "signature": "keras.engine.input_layer.Input(shape=None, batch_shape=None, name=None, dtype=None, sparse=False, tensor=None)",
        "snippet": "def Input(shape=None, batch_shape=None,\n          name=None, dtype=None, sparse=False,\n          tensor=None):\n    \"\"\"`Input()` is used to instantiate a Keras tensor.\n\n    A Keras tensor is a tensor object from the underlying backend\n    (Theano, TensorFlow or CNTK), which we augment with certain\n    attributes that allow us to build a Keras model\n    just by knowing the inputs and outputs of the model.\n\n    For instance, if a, b and c are Keras tensors,\n    it becomes possible to do:\n    `model = Model(input=[a, b], output=c)`\n\n    The added Keras attributes are:\n        `_keras_shape`: Integer shape tuple propagated\n            via Keras-side shape inference.\n        `_keras_history`: Last layer applied to the tensor.\n            the entire layer graph is retrievable from that layer,\n            recursively.\n\n    # Arguments\n        shape: A shape tuple (integer), not including the batch size.\n            For instance, `shape=(32,)` indicates that the expected input\n            will be batches of 32-dimensional vectors.\n        batch_shape: A shape tuple (integer), including the batch size.\n            For instance, `batch_shape=(10, 32)` indicates that\n            the expected input will be batches of 10 32-dimensional vectors.\n            `batch_shape=(None, 32)` indicates batches of an arbitrary number\n            of 32-dimensional vectors.\n        name: An optional name string for the layer.\n            Should be unique in a model (do not reuse the same name twice).\n            It will be autogenerated if it isn't provided.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n        sparse: A boolean specifying whether the placeholder\n            to be created is sparse.\n        tensor: Optional existing tensor to wrap into the `Input` layer.\n            If set, the layer will not create a placeholder tensor.\n\n    # Returns\n        A tensor.\n\n    # Example\n\n    ```python\n    # this is a logistic regression in Keras\n    x = Input(shape=(32,))\n    y = Dense(16, activation='softmax')(x)\n    model = Model(x, y)\n    ```\n    \"\"\"\n    if not batch_shape and tensor is None:\n        assert shape is not None, ('Please provide to Input either a `shape`'\n                                   ' or a `batch_shape` argument. Note that '\n                                   '`shape` does not include the batch '\n                                   'dimension.')\n    if shape is not None and not batch_shape:\n        batch_shape = (None,) + tuple(shape)\n    if not dtype:\n        dtype = K.floatx()\n    input_layer = InputLayer(batch_input_shape=batch_shape,\n                             name=name, dtype=dtype,\n                             sparse=sparse,\n                             input_tensor=tensor)\n    # Return tensor including _keras_shape and _keras_history.\n    # Note that in this case train_output and test_output are the same pointer.\n    outputs = input_layer._inbound_nodes[0].output_tensors\n    return unpack_singleton(outputs)",
        "begin_line": 113,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.activations.relu#109",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.relu(x, alpha=0.0, max_value=None)",
        "snippet": "def relu(x, alpha=0., max_value=None):\n    \"\"\"Rectified Linear Unit.\n\n    # Arguments\n        x: Input tensor.\n        alpha: Slope of the negative part. Defaults to zero.\n        max_value: Maximum value for the output.\n\n    # Returns\n        The (leaky) rectified linear unit activation: `x` if `x > 0`,\n        `alpha * x` if `x < 0`. If `max_value` is defined, the result\n        is truncated to this value.\n    \"\"\"\n    return K.relu(x, alpha=alpha, max_value=max_value)",
        "begin_line": 109,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.activations.sigmoid#131",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.sigmoid(x)",
        "snippet": "def sigmoid(x):\n    \"\"\"Sigmoid activation function.\n    \"\"\"\n    return K.sigmoid(x)",
        "begin_line": 131,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.activations.linear#155",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.linear(x)",
        "snippet": "def linear(x):\n    \"\"\"Linear (i.e. identity) activation function.\n    \"\"\"\n    return x",
        "begin_line": 155,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.activations.deserialize#165",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.deserialize(name, custom_objects=None)",
        "snippet": "def deserialize(name, custom_objects=None):\n    return deserialize_keras_object(\n        name,\n        module_objects=globals(),\n        custom_objects=custom_objects,\n        printable_module_name='activation function')",
        "begin_line": 165,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.activations.get#173",
        "src_path": "keras/activations.py",
        "class_name": "keras.activations",
        "signature": "keras.activations.get(identifier)",
        "snippet": "def get(identifier):\n    \"\"\"Get the `identifier` activation function.\n\n    # Arguments\n        identifier: None or str, name of the function.\n\n    # Returns\n        The activation function, `linear` if `identifier` is None.\n\n    # Raises\n        ValueError if unknown identifier\n    \"\"\"\n    if identifier is None:\n        return linear\n    if isinstance(identifier, six.string_types):\n        identifier = str(identifier)\n        return deserialize(identifier)\n    elif callable(identifier):\n        if isinstance(identifier, Layer):\n            warnings.warn(\n                'Do not pass a layer instance (such as {identifier}) as the '\n                'activation argument of another layer. Instead, advanced '\n                'activation layers should be used just like any other '\n                'layer in a model.'.format(\n                    identifier=identifier.__class__.__name__))\n        return identifier\n    else:\n        raise ValueError('Could not interpret '\n                         'activation function identifier:', identifier)",
        "begin_line": 173,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.regularizers.get#76",
        "src_path": "keras/regularizers.py",
        "class_name": "keras.regularizers",
        "signature": "keras.regularizers.get(identifier)",
        "snippet": "def get(identifier):\n    if identifier is None:\n        return None\n    if isinstance(identifier, dict):\n        return deserialize(identifier)\n    elif isinstance(identifier, six.string_types):\n        config = {'class_name': str(identifier), 'config': {}}\n        return deserialize(config)\n    elif callable(identifier):\n        return identifier\n    else:\n        raise ValueError('Could not interpret regularizer identifier: ' +\n                         str(identifier))",
        "begin_line": 76,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.layers.core.Flatten.__init__#487",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.__init__(self, data_format=None, **kwargs)",
        "snippet": "    def __init__(self, data_format=None, **kwargs):\n        super(Flatten, self).__init__(**kwargs)\n        self.input_spec = InputSpec(min_ndim=3)\n        self.data_format = K.normalize_data_format(data_format)",
        "begin_line": 487,
        "end_line": 490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.core.Flatten.compute_output_shape#492",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        if not all(input_shape[1:]):\n            raise ValueError('The shape of the input to \"Flatten\" '\n                             'is not fully defined '\n                             '(got ' + str(input_shape[1:]) + '. '\n                             'Make sure to pass a complete \"input_shape\" '\n                             'or \"batch_input_shape\" argument to the first '\n                             'layer in your model.')\n        return (input_shape[0], np.prod(input_shape[1:]))",
        "begin_line": 492,
        "end_line": 500,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.core.Flatten.call#502",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Flatten",
        "signature": "keras.layers.core.Flatten.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        if self.data_format == 'channels_first':\n            # Ensure works for any dim\n            permutation = [0]\n            permutation.extend([i for i in\n                                range(2, K.ndim(inputs))])\n            permutation.append(1)\n            inputs = K.permute_dimensions(inputs, permutation)\n\n        return K.batch_flatten(inputs)",
        "begin_line": 502,
        "end_line": 511,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.layers.core.Dense.__init__#826",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.__init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)",
        "snippet": "    def __init__(self, units,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super(Dense, self).__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True",
        "begin_line": 826,
        "end_line": 851,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.layers.core.Dense.build#853",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.build(self, input_shape)",
        "snippet": "    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True",
        "begin_line": 853,
        "end_line": 871,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.layers.core.Dense.call#873",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.call(self, inputs)",
        "snippet": "    def call(self, inputs):\n        output = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output",
        "begin_line": 873,
        "end_line": 879,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.layers.core.Dense.compute_output_shape#881",
        "src_path": "keras/layers/core.py",
        "class_name": "keras.layers.core.Dense",
        "signature": "keras.layers.core.Dense.compute_output_shape(self, input_shape)",
        "snippet": "    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)",
        "begin_line": 881,
        "end_line": 886,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015337423312883436,
            "pseudo_dstar_susp": 0.0015337423312883436,
            "pseudo_tarantula_susp": 0.0016722408026755853,
            "pseudo_op2_susp": 0.0015337423312883436,
            "pseudo_barinel_susp": 0.0016722408026755853
        }
    },
    {
        "name": "keras.utils.conv_utils.normalize_tuple#12",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.normalize_tuple(value, n, name)",
        "snippet": "def normalize_tuple(value, n, name):\n    \"\"\"Transforms a single int or iterable of ints into an int tuple.\n\n    # Arguments\n        value: The value to validate and convert. Could an int, or any iterable\n          of ints.\n        n: The size of the tuple to be returned.\n        name: The name of the argument being validated, e.g. \"strides\" or\n          \"kernel_size\". This is only used to format error messages.\n\n    # Returns\n        A tuple of n integers.\n\n    # Raises\n        ValueError: If something else than an int/long or iterable thereof was\n        passed.\n    \"\"\"\n    if isinstance(value, int):\n        return (value,) * n\n    else:\n        try:\n            value_tuple = tuple(value)\n        except TypeError:\n            raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                             str(n) + ' integers. Received: ' + str(value))\n        if len(value_tuple) != n:\n            raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                             str(n) + ' integers. Received: ' + str(value))\n        for single_value in value_tuple:\n            try:\n                int(single_value)\n            except ValueError:\n                raise ValueError('The `' + name + '` argument must be a tuple of ' +\n                                 str(n) + ' integers. Received: ' + str(value) + ' '\n                                 'including element ' + str(single_value) + ' of type' +\n                                 ' ' + str(type(single_value)))\n    return value_tuple",
        "begin_line": 12,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.conv_utils.normalize_padding#51",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.normalize_padding(value)",
        "snippet": "def normalize_padding(value):\n    padding = value.lower()\n    allowed = {'valid', 'same', 'causal'}\n    if K.backend() == 'theano':\n        allowed.add('full')\n    if padding not in allowed:\n        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" (or \"causal\" for Conv1D). '\n                         'Received: ' + str(padding))\n    return padding",
        "begin_line": 51,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.utils.conv_utils.conv_output_length#85",
        "src_path": "keras/utils/conv_utils.py",
        "class_name": "keras.utils.conv_utils",
        "signature": "keras.utils.conv_utils.conv_output_length(input_length, filter_size, padding, stride, dilation=1)",
        "snippet": "def conv_output_length(input_length, filter_size,\n                       padding, stride, dilation=1):\n    \"\"\"Determines output length of a convolution given input length.\n\n    # Arguments\n        input_length: integer.\n        filter_size: integer.\n        padding: one of \"same\", \"valid\", \"full\".\n        stride: integer.\n        dilation: dilation rate, integer.\n\n    # Returns\n        The output length (integer).\n    \"\"\"\n    if input_length is None:\n        return None\n    assert padding in {'same', 'valid', 'full', 'causal'}\n    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n    if padding == 'same':\n        output_length = input_length\n    elif padding == 'valid':\n        output_length = input_length - dilated_filter_size + 1\n    elif padding == 'causal':\n        output_length = input_length\n    elif padding == 'full':\n        output_length = input_length + dilated_filter_size - 1\n    return (output_length + stride - 1) // stride",
        "begin_line": 85,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.009523809523809525,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.009523809523809525
        }
    },
    {
        "name": "keras.callbacks.CallbackList.__init__#38",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.__init__(self, callbacks=None, queue_length=10)",
        "snippet": "    def __init__(self, callbacks=None, queue_length=10):\n        callbacks = callbacks or []\n        self.callbacks = [c for c in callbacks]\n        self.queue_length = queue_length",
        "begin_line": 38,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.set_params#46",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.set_params(self, params)",
        "snippet": "    def set_params(self, params):\n        for callback in self.callbacks:\n            callback.set_params(params)",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.set_model#50",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.set_model(self, model)",
        "snippet": "    def set_model(self, model):\n        for callback in self.callbacks:\n            callback.set_model(model)",
        "begin_line": 50,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_epoch_begin#54",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        \"\"\"Called at the start of an epoch.\n\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_begin(epoch, logs)\n        self._delta_t_batch = 0.\n        self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n        self._delta_ts_batch_end = deque([], maxlen=self.queue_length)",
        "begin_line": 54,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_epoch_end#68",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        \"\"\"Called at the end of an epoch.\n\n        # Arguments\n            epoch: integer, index of epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_epoch_end(epoch, logs)",
        "begin_line": 68,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_batch_begin#79",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_batch_begin(self, batch, logs=None)",
        "snippet": "    def on_batch_begin(self, batch, logs=None):\n        \"\"\"Called right before processing a batch.\n\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        t_before_callbacks = time.time()\n        for callback in self.callbacks:\n            callback.on_batch_begin(batch, logs)\n        self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n        delta_t_median = np.median(self._delta_ts_batch_begin)\n        if (self._delta_t_batch > 0. and\n           delta_t_median > 0.95 * self._delta_t_batch and\n           delta_t_median > 0.1):\n            warnings.warn('Method on_batch_begin() is slow compared '\n                          'to the batch update (%f). Check your callbacks.'\n                          % delta_t_median)\n        self._t_enter_batch = time.time()",
        "begin_line": 79,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_batch_end#100",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        \"\"\"Called at the end of a batch.\n\n        # Arguments\n            batch: integer, index of batch within the current epoch.\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        if not hasattr(self, '_t_enter_batch'):\n            self._t_enter_batch = time.time()\n        self._delta_t_batch = time.time() - self._t_enter_batch\n        t_before_callbacks = time.time()\n        for callback in self.callbacks:\n            callback.on_batch_end(batch, logs)\n        self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n        delta_t_median = np.median(self._delta_ts_batch_end)\n        if (self._delta_t_batch > 0. and\n           (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n            warnings.warn('Method on_batch_end() is slow compared '\n                          'to the batch update (%f). Check your callbacks.'\n                          % delta_t_median)",
        "begin_line": 100,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_train_begin#122",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        \"\"\"Called at the beginning of training.\n\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)",
        "begin_line": 122,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.on_train_end#132",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        \"\"\"Called at the end of training.\n\n        # Arguments\n            logs: dictionary of logs.\n        \"\"\"\n        logs = logs or {}\n        for callback in self.callbacks:\n            callback.on_train_end(logs)",
        "begin_line": 132,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.CallbackList.__iter__#142",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.CallbackList",
        "signature": "keras.callbacks.CallbackList.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self.callbacks)",
        "begin_line": 142,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.__init__#173",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.__init__(self)",
        "snippet": "    def __init__(self):\n        self.validation_data = None\n        self.model = None",
        "begin_line": 173,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.set_params#177",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.set_params(self, params)",
        "snippet": "    def set_params(self, params):\n        self.params = params",
        "begin_line": 177,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.set_model#180",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.set_model(self, model)",
        "snippet": "    def set_model(self, model):\n        self.model = model",
        "begin_line": 180,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.on_epoch_begin#183",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        pass",
        "begin_line": 183,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.on_batch_begin#189",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_batch_begin(self, batch, logs=None)",
        "snippet": "    def on_batch_begin(self, batch, logs=None):\n        pass",
        "begin_line": 189,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.on_batch_end#192",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        pass",
        "begin_line": 192,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.on_train_begin#195",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        pass",
        "begin_line": 195,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.Callback.on_train_end#198",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.Callback",
        "signature": "keras.callbacks.Callback.on_train_end(self, logs=None)",
        "snippet": "    def on_train_end(self, logs=None):\n        pass",
        "begin_line": 198,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.__init__#214",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.__init__(self, stateful_metrics=None)",
        "snippet": "    def __init__(self, stateful_metrics=None):\n        if stateful_metrics:\n            self.stateful_metrics = set(stateful_metrics)\n        else:\n            self.stateful_metrics = set()",
        "begin_line": 214,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_epoch_begin#220",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_epoch_begin(self, epoch, logs=None)",
        "snippet": "    def on_epoch_begin(self, epoch, logs=None):\n        self.seen = 0\n        self.totals = {}",
        "begin_line": 220,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_batch_end#224",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_batch_end(self, batch, logs=None)",
        "snippet": "    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        batch_size = logs.get('size', 0)\n        self.seen += batch_size\n\n        for k, v in logs.items():\n            if k in self.stateful_metrics:\n                self.totals[k] = v\n            else:\n                if k in self.totals:\n                    self.totals[k] += v * batch_size\n                else:\n                    self.totals[k] = v * batch_size",
        "begin_line": 224,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.BaseLogger.on_epoch_end#238",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.BaseLogger",
        "signature": "keras.callbacks.BaseLogger.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        if logs is not None:\n            for k in self.params['metrics']:\n                if k in self.totals:\n                    # Make value available to next callbacks.\n                    if k in self.stateful_metrics:\n                        logs[k] = self.totals[k]\n                    else:\n                        logs[k] = self.totals[k] / self.seen",
        "begin_line": 238,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.History.on_train_begin#347",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.History",
        "signature": "keras.callbacks.History.on_train_begin(self, logs=None)",
        "snippet": "    def on_train_begin(self, logs=None):\n        self.epoch = []\n        self.history = {}",
        "begin_line": 347,
        "end_line": 349,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    },
    {
        "name": "keras.callbacks.History.on_epoch_end#351",
        "src_path": "keras/callbacks.py",
        "class_name": "keras.callbacks.History",
        "signature": "keras.callbacks.History.on_epoch_end(self, epoch, logs=None)",
        "snippet": "    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.epoch.append(epoch)\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)",
        "begin_line": 351,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006024096385542169,
            "pseudo_dstar_susp": 0.0006024096385542169,
            "pseudo_tarantula_susp": 0.0006024096385542169,
            "pseudo_op2_susp": 0.0006230529595015577,
            "pseudo_barinel_susp": 0.0006024096385542169
        }
    }
]