[
    {
        "name": "lib.ansible.parsing.metadata._seek_end_of_dict#40",
        "src_path": "lib/ansible/parsing/metadata.py",
        "class_name": "lib.ansible.parsing.metadata",
        "signature": "lib.ansible.parsing.metadata._seek_end_of_dict(module_data, start_line, start_col, next_node_line, next_node_col)",
        "snippet": "def _seek_end_of_dict(module_data, start_line, start_col, next_node_line, next_node_col):\n    \"\"\"Look for the end of a dict in a set of lines\n\n    We know the starting position of the dict and we know the start of the\n    next code node but in between there may be multiple newlines and comments.\n    There may also be multiple python statements on the same line (separated\n    by semicolons)\n\n    Examples::\n        ANSIBLE_METADATA = {[..]}\n        DOCUMENTATION = [..]\n\n        ANSIBLE_METADATA = {[..]} # Optional comments with confusing junk => {}\n        # Optional comments {}\n        DOCUMENTATION = [..]\n\n        ANSIBLE_METADATA = {\n            [..]\n            }\n        # Optional comments {}\n        DOCUMENTATION = [..]\n\n        ANSIBLE_METADATA = {[..]} ; DOCUMENTATION = [..]\n\n        ANSIBLE_METADATA = {}EOF\n    \"\"\"\n    if next_node_line is None:\n        # The dict is the last statement in the file\n        snippet = module_data.splitlines()[start_line:]\n        next_node_col = 0\n        # Include the last line in the file\n        last_line_offset = 0\n    else:\n        # It's somewhere in the middle so we need to separate it from the rest\n        snippet = module_data.splitlines()[start_line:next_node_line]\n        # Do not include the last line because that's where the next node\n        # starts\n        last_line_offset = 1\n\n    if next_node_col == 0:\n        # This handles all variants where there are only comments and blank\n        # lines between the dict and the next code node\n\n        # Step backwards through all the lines in the snippet\n        for line_idx, line in tuple(reversed(tuple(enumerate(snippet))))[last_line_offset:]:\n            end_col = None\n            # Step backwards through all the characters in the line\n            for col_idx, char in reversed(tuple(enumerate(c for c in line))):\n                if not isinstance(char, bytes):\n                    # Python3 wart.  slicing a byte string yields integers\n                    char = bytes((char,))\n                if char == b'}' and end_col is None:\n                    # Potentially found the end of the dict\n                    end_col = col_idx\n\n                elif char == b'#' and end_col is not None:\n                    # The previous '}' was part of a comment.  Keep trying\n                    end_col = None\n\n            if end_col is not None:\n                # Found the end!\n                end_line = start_line + line_idx\n                break\n        else:\n            raise ParseError('Unable to find the end of dictionary')\n    else:\n        # Harder cases involving multiple statements on one line\n        # Good Ansible Module style doesn't do this so we're just going to\n        # treat this as an error for now:\n        raise ParseError('Multiple statements per line confuses the module metadata parser.')\n\n    return end_line, end_col",
        "begin_line": 40,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.metadata._seek_end_of_string#114",
        "src_path": "lib/ansible/parsing/metadata.py",
        "class_name": "lib.ansible.parsing.metadata",
        "signature": "lib.ansible.parsing.metadata._seek_end_of_string(module_data, start_line, start_col, next_node_line, next_node_col)",
        "snippet": "def _seek_end_of_string(module_data, start_line, start_col, next_node_line, next_node_col):\n    \"\"\"\n    This is much trickier than finding the end of a dict.  A dict has only one\n    ending character, \"}\".  Strings have four potential ending characters.  We\n    have to parse the beginning of the string to determine what the ending\n    character will be.\n\n    Examples:\n        ANSIBLE_METADATA = '''[..]''' # Optional comment with confusing chars '''\n        # Optional comment with confusing chars '''\n        DOCUMENTATION = [..]\n\n        ANSIBLE_METADATA = '''\n            [..]\n            '''\n        DOCUMENTATIONS = [..]\n\n        ANSIBLE_METADATA = '''[..]''' ; DOCUMENTATION = [..]\n\n        SHORT_NAME = ANSIBLE_METADATA = '''[..]''' ; DOCUMENTATION = [..]\n\n    String marker variants:\n        * '[..]'\n        * \"[..]\"\n        * '''[..]'''\n        * \\\"\\\"\\\"[..]\\\"\\\"\\\"\n\n    Each of these come in u, r, and b variants:\n        * '[..]'\n        * u'[..]'\n        * b'[..]'\n        * r'[..]'\n        * ur'[..]'\n        * ru'[..]'\n        * br'[..]'\n        * b'[..]'\n        * rb'[..]'\n    \"\"\"\n    raise NotImplementedError('Finding end of string not yet implemented')",
        "begin_line": 114,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.metadata.extract_metadata#155",
        "src_path": "lib/ansible/parsing/metadata.py",
        "class_name": "lib.ansible.parsing.metadata",
        "signature": "lib.ansible.parsing.metadata.extract_metadata(module_ast=None, module_data=None, offsets=False)",
        "snippet": "def extract_metadata(module_ast=None, module_data=None, offsets=False):\n    \"\"\"Extract the metadata from a module\n\n    :kwarg module_ast: ast representation of the module.  At least one of this\n        or ``module_data`` must be given.  If the code calling\n        :func:`extract_metadata` has already parsed the module_data into an ast,\n        giving the ast here will save reparsing it.\n    :kwarg module_data: Byte string containing a module's code.  At least one\n        of this or ``module_ast`` must be given.\n    :kwarg offsets: If set to True, offests into the source code will be\n        returned.  This requires that ``module_data`` be set.\n    :returns: a tuple of metadata (a dict), line the metadata starts on,\n        column the metadata starts on, line the metadata ends on, column the\n        metadata ends on, and the names the metadata is assigned to.  One of\n        the names the metadata is assigned to will be ANSIBLE_METADATA.  If no\n        metadata is found, the tuple will be (None, -1, -1, -1, -1, None).\n        If ``offsets`` is False then the tuple will consist of\n        (metadata, -1, -1, -1, -1, None).\n    :raises ansible.parsing.metadata.ParseError: if ``module_data`` does not parse\n    :raises SyntaxError: if ``module_data`` is needed but does not parse correctly\n    \"\"\"\n    if offsets and module_data is None:\n        raise TypeError('If offsets is True then module_data must also be given')\n\n    if module_ast is None and module_data is None:\n        raise TypeError('One of module_ast or module_data must be given')\n\n    metadata = None\n    start_line = -1\n    start_col = -1\n    end_line = -1\n    end_col = -1\n    targets = None\n    if module_ast is None:\n        module_ast = ast.parse(module_data)\n\n    for root_idx, child in reversed(list(enumerate(module_ast.body))):\n        if isinstance(child, ast.Assign):\n            for target in child.targets:\n                if isinstance(target, ast.Name) and target.id == 'ANSIBLE_METADATA':\n                    metadata = ast.literal_eval(child.value)\n                    if not offsets:\n                        continue\n\n                    try:\n                        # Determine where the next node starts\n                        next_node = module_ast.body[root_idx + 1]\n                        next_lineno = next_node.lineno\n                        next_col_offset = next_node.col_offset\n                    except IndexError:\n                        # Metadata is defined in the last node of the file\n                        next_lineno = None\n                        next_col_offset = None\n\n                    if isinstance(child.value, ast.Dict):\n                        # Determine where the current metadata ends\n                        end_line, end_col = _seek_end_of_dict(module_data,\n                                                              child.lineno - 1,\n                                                              child.col_offset,\n                                                              next_lineno,\n                                                              next_col_offset)\n\n                    elif isinstance(child.value, ast.Str):\n                        metadata = yaml.safe_load(child.value.s)\n                        end_line, end_col = _seek_end_of_string(module_data,\n                                                                child.lineno - 1,\n                                                                child.col_offset,\n                                                                next_lineno,\n                                                                next_col_offset)\n                    elif isinstance(child.value, ast.Bytes):\n                        metadata = yaml.safe_load(to_text(child.value.s, errors='surrogate_or_strict'))\n                        end_line, end_col = _seek_end_of_string(module_data,\n                                                                child.lineno - 1,\n                                                                child.col_offset,\n                                                                next_lineno,\n                                                                next_col_offset)\n                    else:\n                        raise ParseError('Ansible plugin metadata must be a dict')\n\n                    # Do these after the if-else so we don't pollute them in\n                    # case this was a false positive\n                    start_line = child.lineno - 1\n                    start_col = child.col_offset\n                    targets = [t.id for t in child.targets]\n                    break\n\n        if metadata is not None:\n            # Once we've found the metadata we're done\n            break\n\n    return metadata, start_line, start_col, end_line, end_col, targets",
        "begin_line": 155,
        "end_line": 245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.dns.DnsFactCollector.collect#29",
        "src_path": "lib/ansible/module_utils/facts/system/dns.py",
        "class_name": "lib.ansible.module_utils.facts.system.dns.DnsFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.dns.DnsFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        dns_facts = {}\n\n        # TODO: flatten\n        dns_facts['dns'] = {}\n\n        for line in get_file_content('/etc/resolv.conf', '').splitlines():\n            if line.startswith('#') or line.startswith(';') or line.strip() == '':\n                continue\n            tokens = line.split()\n            if len(tokens) == 0:\n                continue\n            if tokens[0] == 'nameserver':\n                if 'nameservers' not in dns_facts['dns']:\n                    dns_facts['dns']['nameservers'] = []\n                for nameserver in tokens[1:]:\n                    dns_facts['dns']['nameservers'].append(nameserver)\n            elif tokens[0] == 'domain':\n                if len(tokens) > 1:\n                    dns_facts['dns']['domain'] = tokens[1]\n            elif tokens[0] == 'search':\n                dns_facts['dns']['search'] = []\n                for suffix in tokens[1:]:\n                    dns_facts['dns']['search'].append(suffix)\n            elif tokens[0] == 'sortlist':\n                dns_facts['dns']['sortlist'] = []\n                for address in tokens[1:]:\n                    dns_facts['dns']['sortlist'].append(address)\n            elif tokens[0] == 'options':\n                dns_facts['dns']['options'] = {}\n                if len(tokens) > 1:\n                    for option in tokens[1:]:\n                        option_tokens = option.split(':', 1)\n                        if len(option_tokens) == 0:\n                            continue\n                        val = len(option_tokens) == 2 and option_tokens[1] or True\n                        dns_facts['dns']['options'][option_tokens[0]] = val\n\n        return dns_facts",
        "begin_line": 29,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.ssh_pub_keys.SshPubKeyFactCollector.collect#32",
        "src_path": "lib/ansible/module_utils/facts/system/ssh_pub_keys.py",
        "class_name": "lib.ansible.module_utils.facts.system.ssh_pub_keys.SshPubKeyFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.ssh_pub_keys.SshPubKeyFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        ssh_pub_key_facts = {}\n        algos = ('dsa', 'rsa', 'ecdsa', 'ed25519')\n\n        # list of directories to check for ssh keys\n        # used in the order listed here, the first one with keys is used\n        keydirs = ['/etc/ssh', '/etc/openssh', '/etc']\n\n        for keydir in keydirs:\n            for algo in algos:\n                factname = 'ssh_host_key_%s_public' % algo\n                if factname in ssh_pub_key_facts:\n                    # a previous keydir was already successful, stop looking\n                    # for keys\n                    return ssh_pub_key_facts\n                key_filename = '%s/ssh_host_%s_key.pub' % (keydir, algo)\n                keydata = get_file_content(key_filename)\n                if keydata is not None:\n                    (keytype, key) = keydata.split()[0:2]\n                    ssh_pub_key_facts[factname] = key\n                    ssh_pub_key_facts[factname + '_keytype'] = keytype\n\n        return ssh_pub_key_facts",
        "begin_line": 32,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.cmd.ShellModule.quote#38",
        "src_path": "lib/ansible/plugins/shell/cmd.py",
        "class_name": "lib.ansible.plugins.shell.cmd.ShellModule",
        "signature": "lib.ansible.plugins.shell.cmd.ShellModule.quote(self, s)",
        "snippet": "    def quote(self, s):\n        # cmd does not support single quotes that the shlex_quote uses. We need to override the quoting behaviour to\n        # better match cmd.exe.\n        # https://blogs.msdn.microsoft.com/twistylittlepassagesallalike/2011/04/23/everyone-quotes-command-line-arguments-the-wrong-way/\n\n        # Return an empty argument\n        if not s:\n            return '\"\"'\n\n        if _find_unsafe(s) is None:\n            return s\n\n        # Escape the metachars as we are quoting the string to stop cmd from interpreting that metachar. For example\n        # 'file &whoami.exe' would result in 'file $(whoami.exe)' instead of the literal string\n        # https://stackoverflow.com/questions/3411771/multiple-character-replace-with-python\n        for c in '^()%!\"<>&|':  # '^' must be the first char that we scan and replace\n            if c in s:\n                # I can't find any docs that explicitly say this but to escape \", it needs to be prefixed with \\^.\n                s = s.replace(c, (\"\\\\^\" if c == '\"' else \"^\") + c)\n\n        return '^\"' + s + '^\"'",
        "begin_line": 38,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.handler_task_include.HandlerTaskInclude.load#32",
        "src_path": "lib/ansible/playbook/handler_task_include.py",
        "class_name": "lib.ansible.playbook.handler_task_include.HandlerTaskInclude",
        "signature": "lib.ansible.playbook.handler_task_include.HandlerTaskInclude.load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None)",
        "snippet": "    def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n        t = HandlerTaskInclude(block=block, role=role, task_include=task_include)\n        return t.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 32,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.__init__#53",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.__init__(self)",
        "snippet": "    def __init__(self):\n\n        self._basedir = '.'\n\n        # NOTE: not effective with forks as the main copy does not get updated.\n        # avoids rereading files\n        self._FILE_CACHE = dict()\n\n        # NOTE: not thread safe, also issues with forks not returning data to main proc\n        #       so they need to be cleaned independantly. See WorkerProcess for example.\n        # used to keep track of temp files for cleaning\n        self._tempfiles = set()\n\n        # initialize the vault stuff with an empty password\n        # TODO: replace with a ref to something that can get the password\n        #       a creds/auth provider\n        # self.set_vault_password(None)\n        self._vaults = {}\n        self._vault = VaultLib()\n        self.set_vault_secrets(None)",
        "begin_line": 53,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005307855626326964,
            "pseudo_dstar_susp": 0.0005307855626326964,
            "pseudo_tarantula_susp": 0.0005307855626326964,
            "pseudo_op2_susp": 0.0005307855626326964,
            "pseudo_barinel_susp": 0.0005307855626326964
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.set_vault_secrets#75",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.set_vault_secrets(self, vault_secrets)",
        "snippet": "    def set_vault_secrets(self, vault_secrets):\n        self._vault.secrets = vault_secrets",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005561735261401557,
            "pseudo_dstar_susp": 0.0005561735261401557,
            "pseudo_tarantula_susp": 0.0005561735261401557,
            "pseudo_op2_susp": 0.0005561735261401557,
            "pseudo_barinel_susp": 0.0005561735261401557
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.load#78",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.load(self, data, file_name='<string>', show_content=True, json_only=False)",
        "snippet": "    def load(self, data, file_name='<string>', show_content=True, json_only=False):\n        '''Backwards compat for now'''\n        return from_yaml(data, file_name, show_content, self._vault.secrets, json_only=json_only)",
        "begin_line": 78,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.load_from_file#82",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.load_from_file(self, file_name, cache=True, unsafe=False, json_only=False)",
        "snippet": "    def load_from_file(self, file_name, cache=True, unsafe=False, json_only=False):\n        ''' Loads data from a file, which can contain either JSON or YAML.  '''\n\n        file_name = self.path_dwim(file_name)\n        display.debug(\"Loading data from %s\" % file_name)\n\n        # if the file has already been read in and cached, we'll\n        # return those results to avoid more file/vault operations\n        if cache and file_name in self._FILE_CACHE:\n            parsed_data = self._FILE_CACHE[file_name]\n        else:\n            # read the file contents and load the data structure from them\n            (b_file_data, show_content) = self._get_file_contents(file_name)\n\n            file_data = to_text(b_file_data, errors='surrogate_or_strict')\n            parsed_data = self.load(data=file_data, file_name=file_name, show_content=show_content, json_only=json_only)\n\n            # cache the file contents for next time\n            self._FILE_CACHE[file_name] = parsed_data\n\n        if unsafe:\n            return parsed_data\n        else:\n            # return a deep copy here, so the cache is not affected\n            return copy.deepcopy(parsed_data)",
        "begin_line": 82,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.path_exists#108",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.path_exists(self, path)",
        "snippet": "    def path_exists(self, path):\n        path = self.path_dwim(path)\n        return os.path.exists(to_bytes(path, errors='surrogate_or_strict'))",
        "begin_line": 108,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000749063670411985,
            "pseudo_dstar_susp": 0.0007485029940119761,
            "pseudo_tarantula_susp": 0.0007776049766718507,
            "pseudo_op2_susp": 0.0007485029940119761,
            "pseudo_barinel_susp": 0.0007776049766718507
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.is_file#112",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.is_file(self, path)",
        "snippet": "    def is_file(self, path):\n        path = self.path_dwim(path)\n        return os.path.isfile(to_bytes(path, errors='surrogate_or_strict')) or path == os.devnull",
        "begin_line": 112,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.is_directory#116",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.is_directory(self, path)",
        "snippet": "    def is_directory(self, path):\n        path = self.path_dwim(path)\n        return os.path.isdir(to_bytes(path, errors='surrogate_or_strict'))",
        "begin_line": 116,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.is_executable#124",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.is_executable(self, path)",
        "snippet": "    def is_executable(self, path):\n        '''is the given path executable?'''\n        path = self.path_dwim(path)\n        return is_executable(path)",
        "begin_line": 124,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader._decrypt_if_vault_data#129",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader._decrypt_if_vault_data(self, b_vault_data, b_file_name=None)",
        "snippet": "    def _decrypt_if_vault_data(self, b_vault_data, b_file_name=None):\n        '''Decrypt b_vault_data if encrypted and return b_data and the show_content flag'''\n\n        if not is_encrypted(b_vault_data):\n            show_content = True\n            return b_vault_data, show_content\n\n        b_ciphertext, b_version, cipher_name, vault_id = parse_vaulttext_envelope(b_vault_data)\n        b_data = self._vault.decrypt(b_vault_data, filename=b_file_name)\n\n        show_content = False\n        return b_data, show_content",
        "begin_line": 129,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007434944237918215,
            "pseudo_dstar_susp": 0.0007429420505200594,
            "pseudo_tarantula_susp": 0.0007651109410864575,
            "pseudo_op2_susp": 0.0007429420505200594,
            "pseudo_barinel_susp": 0.0007651109410864575
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader._get_file_contents#142",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader._get_file_contents(self, file_name)",
        "snippet": "    def _get_file_contents(self, file_name):\n        '''\n        Reads the file contents from the given file name\n\n        If the contents are vault-encrypted, it will decrypt them and return\n        the decrypted data\n\n        :arg file_name: The name of the file to read.  If this is a relative\n            path, it will be expanded relative to the basedir\n        :raises AnsibleFileNotFound: if the file_name does not refer to a file\n        :raises AnsibleParserError: if we were unable to read the file\n        :return: Returns a byte string of the file contents\n        '''\n        if not file_name or not isinstance(file_name, (binary_type, text_type)):\n            raise AnsibleParserError(\"Invalid filename: '%s'\" % to_native(file_name))\n\n        b_file_name = to_bytes(self.path_dwim(file_name))\n        # This is what we really want but have to fix unittests to make it pass\n        # if not os.path.exists(b_file_name) or not os.path.isfile(b_file_name):\n        if not self.path_exists(b_file_name):\n            raise AnsibleFileNotFound(\"Unable to retrieve file contents\", file_name=file_name)\n\n        try:\n            with open(b_file_name, 'rb') as f:\n                data = f.read()\n                return self._decrypt_if_vault_data(data, b_file_name)\n        except (IOError, OSError) as e:\n            raise AnsibleParserError(\"an error occurred while trying to read the file '%s': %s\" % (file_name, to_native(e)), orig_exc=e)",
        "begin_line": 142,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000777000777000777,
            "pseudo_dstar_susp": 0.0007751937984496124,
            "pseudo_tarantula_susp": 0.0008090614886731392,
            "pseudo_op2_susp": 0.0007751937984496124,
            "pseudo_barinel_susp": 0.0008090614886731392
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.get_basedir#171",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.get_basedir(self)",
        "snippet": "    def get_basedir(self):\n        ''' returns the current basedir '''\n        return self._basedir",
        "begin_line": 171,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000757002271006813,
            "pseudo_dstar_susp": 0.0007558578987150416,
            "pseudo_tarantula_susp": 0.0007855459544383347,
            "pseudo_op2_susp": 0.0007558578987150416,
            "pseudo_barinel_susp": 0.0007855459544383347
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.set_basedir#175",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.set_basedir(self, basedir)",
        "snippet": "    def set_basedir(self, basedir):\n        ''' sets the base directory, used to find files when a relative path is given '''\n\n        if basedir is not None:\n            self._basedir = to_text(basedir)",
        "begin_line": 175,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.path_dwim#181",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.path_dwim(self, given)",
        "snippet": "    def path_dwim(self, given):\n        '''\n        make relative paths work like folks expect.\n        '''\n\n        given = unquote(given)\n        given = to_text(given, errors='surrogate_or_strict')\n\n        if given.startswith(to_text(os.path.sep)) or given.startswith(u'~'):\n            path = given\n        else:\n            basedir = to_text(self._basedir, errors='surrogate_or_strict')\n            path = os.path.join(basedir, given)\n\n        return unfrackpath(path, follow=False)",
        "begin_line": 181,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000700770847932726,
            "pseudo_dstar_susp": 0.000700770847932726,
            "pseudo_tarantula_susp": 0.0007062146892655367,
            "pseudo_op2_susp": 0.000700770847932726,
            "pseudo_barinel_susp": 0.0007062146892655367
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader._is_role#197",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader._is_role(self, path)",
        "snippet": "    def _is_role(self, path):\n        ''' imperfect role detection, roles are still valid w/o tasks|meta/main.yml|yaml|etc '''\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        b_upath = to_bytes(unfrackpath(path, follow=False), errors='surrogate_or_strict')\n\n        for b_finddir in (b'meta', b'tasks'):\n            for b_suffix in (b'.yml', b'.yaml', b''):\n                b_main = b'main%s' % (b_suffix)\n                b_tasked = os.path.join(b_finddir, b_main)\n\n                if (\n                    RE_TASKS.search(path) and\n                    os.path.exists(os.path.join(b_path, b_main)) or\n                    os.path.exists(os.path.join(b_upath, b_tasked)) or\n                    os.path.exists(os.path.join(os.path.dirname(b_path), b_tasked))\n                ):\n                    return True\n        return False",
        "begin_line": 197,
        "end_line": 215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.path_dwim_relative#217",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.path_dwim_relative(self, path, dirname, source, is_role=False)",
        "snippet": "    def path_dwim_relative(self, path, dirname, source, is_role=False):\n        '''\n        find one file in either a role or playbook dir with or without\n        explicitly named dirname subdirs\n\n        Used in action plugins and lookups to find supplemental files that\n        could be in either place.\n        '''\n\n        search = []\n        source = to_text(source, errors='surrogate_or_strict')\n\n        # I have full path, nothing else needs to be looked at\n        if source.startswith(to_text(os.path.sep)) or source.startswith(u'~'):\n            search.append(unfrackpath(source, follow=False))\n        else:\n            # base role/play path + templates/files/vars + relative filename\n            search.append(os.path.join(path, dirname, source))\n            basedir = unfrackpath(path, follow=False)\n\n            # not told if role, but detect if it is a role and if so make sure you get correct base path\n            if not is_role:\n                is_role = self._is_role(path)\n\n            if is_role and RE_TASKS.search(path):\n                basedir = unfrackpath(os.path.dirname(path), follow=False)\n\n            cur_basedir = self._basedir\n            self.set_basedir(basedir)\n            # resolved base role/play path + templates/files/vars + relative filename\n            search.append(unfrackpath(os.path.join(basedir, dirname, source), follow=False))\n            self.set_basedir(cur_basedir)\n\n            if is_role and not source.endswith(dirname):\n                # look in role's tasks dir w/o dirname\n                search.append(unfrackpath(os.path.join(basedir, 'tasks', source), follow=False))\n\n            # try to create absolute path for loader basedir + templates/files/vars + filename\n            search.append(unfrackpath(os.path.join(dirname, source), follow=False))\n\n            # try to create absolute path for loader basedir\n            search.append(unfrackpath(os.path.join(basedir, source), follow=False))\n\n            # try to create absolute path for  dirname + filename\n            search.append(self.path_dwim(os.path.join(dirname, source)))\n\n            # try to create absolute path for filename\n            search.append(self.path_dwim(source))\n\n        for candidate in search:\n            if os.path.exists(to_bytes(candidate, errors='surrogate_or_strict')):\n                break\n\n        return candidate",
        "begin_line": 217,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.path_dwim_relative_stack#272",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.path_dwim_relative_stack(self, paths, dirname, source, is_role=False)",
        "snippet": "    def path_dwim_relative_stack(self, paths, dirname, source, is_role=False):\n        '''\n        find one file in first path in stack taking roles into account and adding play basedir as fallback\n\n        :arg paths: A list of text strings which are the paths to look for the filename in.\n        :arg dirname: A text string representing a directory.  The directory\n            is prepended to the source to form the path to search for.\n        :arg source: A text string which is the filename to search for\n        :rtype: A text string\n        :returns: An absolute path to the filename ``source`` if found\n        :raises: An AnsibleFileNotFound Exception if the file is found to exist in the search paths\n        '''\n        b_dirname = to_bytes(dirname, errors='surrogate_or_strict')\n        b_source = to_bytes(source, errors='surrogate_or_strict')\n\n        result = None\n        search = []\n        if source is None:\n            display.warning('Invalid request to find a file that matches a \"null\" value')\n        elif source and (source.startswith('~') or source.startswith(os.path.sep)):\n            # path is absolute, no relative needed, check existence and return source\n            test_path = unfrackpath(b_source, follow=False)\n            if os.path.exists(to_bytes(test_path, errors='surrogate_or_strict')):\n                result = test_path\n        else:\n            display.debug(u'evaluation_path:\\n\\t%s' % '\\n\\t'.join(paths))\n            for path in paths:\n                upath = unfrackpath(path, follow=False)\n                b_upath = to_bytes(upath, errors='surrogate_or_strict')\n                b_pb_base_dir = os.path.dirname(b_upath)\n\n                # if path is in role and 'tasks' not there already, add it into the search\n                if (is_role or self._is_role(path)) and b_pb_base_dir.endswith(b'/tasks'):\n                    search.append(os.path.join(os.path.dirname(b_pb_base_dir), b_dirname, b_source))\n                    search.append(os.path.join(b_pb_base_dir, b_source))\n                else:\n                    # don't add dirname if user already is using it in source\n                    if b_source.split(b'/')[0] != dirname:\n                        search.append(os.path.join(b_upath, b_dirname, b_source))\n                    search.append(os.path.join(b_upath, b_source))\n\n            # always append basedir as last resort\n            # don't add dirname if user already is using it in source\n            if b_source.split(b'/')[0] != dirname:\n                search.append(os.path.join(to_bytes(self.get_basedir(), errors='surrogate_or_strict'), b_dirname, b_source))\n            search.append(os.path.join(to_bytes(self.get_basedir(), errors='surrogate_or_strict'), b_source))\n\n            display.debug(u'search_path:\\n\\t%s' % to_text(b'\\n\\t'.join(search)))\n            for b_candidate in search:\n                display.vvvvv(u'looking for \"%s\" at \"%s\"' % (source, to_text(b_candidate)))\n                if os.path.exists(b_candidate):\n                    result = to_text(b_candidate)\n                    break\n\n        if result is None:\n            raise AnsibleFileNotFound(file_name=source, paths=[to_native(p) for p in search])\n\n        return result",
        "begin_line": 272,
        "end_line": 329,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader._create_content_tempfile#331",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader._create_content_tempfile(self, content)",
        "snippet": "    def _create_content_tempfile(self, content):\n        ''' Create a tempfile containing defined content '''\n        fd, content_tempfile = tempfile.mkstemp(dir=C.DEFAULT_LOCAL_TMP)\n        f = os.fdopen(fd, 'wb')\n        content = to_bytes(content)\n        try:\n            f.write(content)\n        except Exception as err:\n            os.remove(content_tempfile)\n            raise Exception(err)\n        finally:\n            f.close()\n        return content_tempfile",
        "begin_line": 331,
        "end_line": 343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.get_real_file#345",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.get_real_file(self, file_path, decrypt=True)",
        "snippet": "    def get_real_file(self, file_path, decrypt=True):\n        \"\"\"\n        If the file is vault encrypted return a path to a temporary decrypted file\n        If the file is not encrypted then the path is returned\n        Temporary files are cleanup in the destructor\n        \"\"\"\n\n        if not file_path or not isinstance(file_path, (binary_type, text_type)):\n            raise AnsibleParserError(\"Invalid filename: '%s'\" % to_native(file_path))\n\n        b_file_path = to_bytes(file_path, errors='surrogate_or_strict')\n        if not self.path_exists(b_file_path) or not self.is_file(b_file_path):\n            raise AnsibleFileNotFound(file_name=file_path)\n\n        real_path = self.path_dwim(file_path)\n\n        try:\n            if decrypt:\n                with open(to_bytes(real_path), 'rb') as f:\n                    # Limit how much of the file is read since we do not know\n                    # whether this is a vault file and therefore it could be very\n                    # large.\n                    if is_encrypted_file(f, count=len(b_HEADER)):\n                        # if the file is encrypted and no password was specified,\n                        # the decrypt call would throw an error, but we check first\n                        # since the decrypt function doesn't know the file name\n                        data = f.read()\n                        if not self._vault.secrets:\n                            raise AnsibleParserError(\"A vault password or secret must be specified to decrypt %s\" % to_native(file_path))\n\n                        data = self._vault.decrypt(data, filename=real_path)\n                        # Make a temp file\n                        real_path = self._create_content_tempfile(data)\n                        self._tempfiles.add(real_path)\n\n            return real_path\n\n        except (IOError, OSError) as e:\n            raise AnsibleParserError(\"an error occurred while trying to read the file '%s': %s\" % (to_native(real_path), to_native(e)), orig_exc=e)",
        "begin_line": 345,
        "end_line": 383,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader.find_vars_files#406",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader.find_vars_files(self, path, name, extensions=None, allow_dir=True)",
        "snippet": "    def find_vars_files(self, path, name, extensions=None, allow_dir=True):\n        \"\"\"\n        Find vars files in a given path with specified name. This will find\n        files in a dir named <name>/ or a file called <name> ending in known\n        extensions.\n        \"\"\"\n\n        b_path = to_bytes(os.path.join(path, name))\n        found = []\n\n        if extensions is None:\n            # Look for file with no extension first to find dir before file\n            extensions = [''] + C.YAML_FILENAME_EXTENSIONS\n        # add valid extensions to name\n        for ext in extensions:\n\n            if '.' in ext:\n                full_path = b_path + to_bytes(ext)\n            elif ext:\n                full_path = b'.'.join([b_path, to_bytes(ext)])\n            else:\n                full_path = b_path\n\n            if self.path_exists(full_path):\n                if self.is_directory(full_path):\n                    if allow_dir:\n                        found.extend(self._get_dir_vars_files(to_text(full_path), extensions))\n                    else:\n                        continue\n                else:\n                    found.append(full_path)\n                break\n        return found",
        "begin_line": 406,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.dataloader.DataLoader._get_dir_vars_files#440",
        "src_path": "lib/ansible/parsing/dataloader.py",
        "class_name": "lib.ansible.parsing.dataloader.DataLoader",
        "signature": "lib.ansible.parsing.dataloader.DataLoader._get_dir_vars_files(self, path, extensions)",
        "snippet": "    def _get_dir_vars_files(self, path, extensions):\n        found = []\n        for spath in sorted(self.list_directory(path)):\n            if not spath.startswith(u'.') and not spath.endswith(u'~'):  # skip hidden and backups\n\n                ext = os.path.splitext(spath)[-1]\n                full_spath = os.path.join(path, spath)\n\n                if self.is_directory(full_spath) and not ext:  # recursive search if dir\n                    found.extend(self._get_dir_vars_files(full_spath, extensions))\n                elif self.is_file(full_spath) and (not ext or to_text(ext) in extensions):\n                    # only consider files with valid extensions or no extension\n                    found.append(full_spath)\n\n        return found",
        "begin_line": 440,
        "end_line": 454,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.get_unique_id#44",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.get_unique_id()",
        "snippet": "def get_unique_id():\n    global cur_id\n    cur_id += 1\n    return \"-\".join([\n        node_mac[0:8],\n        node_mac[8:12],\n        random_int[0:4],\n        random_int[4:8],\n        (\"%012x\" % cur_id)[:12],\n    ])",
        "begin_line": 44,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.860592755214051e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars._validate_mutable_mappings#56",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars._validate_mutable_mappings(a, b)",
        "snippet": "def _validate_mutable_mappings(a, b):\n    \"\"\"\n    Internal convenience function to ensure arguments are MutableMappings\n\n    This checks that all arguments are MutableMappings or raises an error\n\n    :raises AnsibleError: if one of the arguments is not a MutableMapping\n    \"\"\"\n\n    # If this becomes generally needed, change the signature to operate on\n    # a variable number of arguments instead.\n\n    if not (isinstance(a, MutableMapping) and isinstance(b, MutableMapping)):\n        myvars = []\n        for x in [a, b]:\n            try:\n                myvars.append(dumps(x))\n            except Exception:\n                myvars.append(to_native(x))\n        raise AnsibleError(\"failed to combine variables, expected dicts but got a '{0}' and a '{1}': \\n{2}\\n{3}\".format(\n            a.__class__.__name__, b.__class__.__name__, myvars[0], myvars[1])\n        )",
        "begin_line": 56,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.combine_vars#80",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.combine_vars(a, b)",
        "snippet": "def combine_vars(a, b):\n    \"\"\"\n    Return a copy of dictionaries of variables based on configured hash behavior\n    \"\"\"\n\n    if C.DEFAULT_HASH_BEHAVIOUR == \"merge\":\n        return merge_hash(a, b)\n    else:\n        # HASH_BEHAVIOUR == 'replace'\n        _validate_mutable_mappings(a, b)\n        result = a.copy()\n        result.update(b)\n        return result",
        "begin_line": 80,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.merge_hash#95",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.merge_hash(x, y, recursive=True, list_merge='replace')",
        "snippet": "def merge_hash(x, y, recursive=True, list_merge='replace'):\n    \"\"\"\n    Return a new dictionary result of the merges of y into x,\n    so that keys from y take precedence over keys from x.\n    (x and y aren't modified)\n    \"\"\"\n    if list_merge not in ('replace', 'keep', 'append', 'prepend', 'append_rp', 'prepend_rp'):\n        raise AnsibleError(\"merge_hash: 'list_merge' argument can only be equal to 'replace', 'keep', 'append', 'prepend', 'append_rp' or 'prepend_rp'\")\n\n    # verify x & y are dicts\n    _validate_mutable_mappings(x, y)\n\n    # to speed things up: if x is empty or equal to y, return y\n    # (this `if` can be remove without impact on the function\n    #  except performance)\n    if x == {} or x == y:\n        return y.copy()\n\n    # in the following we will copy elements from y to x, but\n    # we don't want to modify x, so we create a copy of it\n    x = x.copy()\n\n    # to speed things up: use dict.update if possible\n    # (this `if` can be remove without impact on the function\n    #  except performance)\n    if not recursive and list_merge == 'replace':\n        x.update(y)\n        return x\n\n    # insert each element of y in x, overriding the one in x\n    # (as y has higher priority)\n    # we copy elements from y to x instead of x to y because\n    # there is a high probability x will be the \"default\" dict the user\n    # want to \"patch\" with y\n    # therefore x will have much more elements than y\n    for key, y_value in iteritems(y):\n        # if `key` isn't in x\n        # update x and move on to the next element of y\n        if key not in x:\n            x[key] = y_value\n            continue\n        # from this point we know `key` is in x\n\n        x_value = x[key]\n\n        # if both x's element and y's element are dicts\n        # recursively \"combine\" them or override x's with y's element\n        # depending on the `recursive` argument\n        # and move on to the next element of y\n        if isinstance(x_value, MutableMapping) and isinstance(y_value, MutableMapping):\n            if recursive:\n                x[key] = merge_hash(x_value, y_value, recursive, list_merge)\n            else:\n                x[key] = y_value\n            continue\n\n        # if both x's element and y's element are lists\n        # \"merge\" them depending on the `list_merge` argument\n        # and move on to the next element of y\n        if isinstance(x_value, MutableSequence) and isinstance(y_value, MutableSequence):\n            if list_merge == 'replace':\n                # replace x value by y's one as it has higher priority\n                x[key] = y_value\n            elif list_merge == 'append':\n                x[key] = x_value + y_value\n            elif list_merge == 'prepend':\n                x[key] = y_value + x_value\n            elif list_merge == 'append_rp':\n                # append all elements from y_value (high prio) to x_value (low prio)\n                # and remove x_value elements that are also in y_value\n                # we don't remove elements from x_value nor y_value that were already in double\n                # (we assume that there is a reason if there where such double elements)\n                # _rp stands for \"remove present\"\n                x[key] = [z for z in x_value if z not in y_value] + y_value\n            elif list_merge == 'prepend_rp':\n                # same as 'append_rp' but y_value elements are prepend\n                x[key] = y_value + [z for z in x_value if z not in y_value]\n            # else 'keep'\n            #   keep x value even if y it's of higher priority\n            #   it's done by not changing x[key]\n            continue\n\n        # else just override x's element with y's one\n        x[key] = y_value\n\n    return x",
        "begin_line": 95,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.load_extra_vars#183",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.load_extra_vars(loader)",
        "snippet": "def load_extra_vars(loader):\n    extra_vars = {}\n    for extra_vars_opt in context.CLIARGS.get('extra_vars', tuple()):\n        data = None\n        extra_vars_opt = to_text(extra_vars_opt, errors='surrogate_or_strict')\n        if extra_vars_opt is None or not extra_vars_opt:\n            continue\n\n        if extra_vars_opt.startswith(u\"@\"):\n            # Argument is a YAML file (JSON is a subset of YAML)\n            data = loader.load_from_file(extra_vars_opt[1:])\n        elif extra_vars_opt[0] in [u'/', u'.']:\n            raise AnsibleOptionsError(\"Please prepend extra_vars filename '%s' with '@'\" % extra_vars_opt)\n        elif extra_vars_opt[0] in [u'[', u'{']:\n            # Arguments as YAML\n            data = loader.load(extra_vars_opt)\n        else:\n            # Arguments as Key-value\n            data = parse_kv(extra_vars_opt)\n\n        if isinstance(data, MutableMapping):\n            extra_vars = combine_vars(extra_vars, data)\n        else:\n            raise AnsibleOptionsError(\"Invalid extra vars data supplied. '%s' could not be made into a dictionary\" % extra_vars_opt)\n\n    return extra_vars",
        "begin_line": 183,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.load_options_vars#211",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.load_options_vars(version)",
        "snippet": "def load_options_vars(version):\n\n    if version is None:\n        version = 'Unknown'\n    options_vars = {'ansible_version': version}\n    attrs = {'check': 'check_mode',\n             'diff': 'diff_mode',\n             'forks': 'forks',\n             'inventory': 'inventory_sources',\n             'skip_tags': 'skip_tags',\n             'subset': 'limit',\n             'tags': 'run_tags',\n             'verbosity': 'verbosity'}\n\n    for attr, alias in attrs.items():\n        opt = context.CLIARGS.get(attr)\n        if opt is not None:\n            options_vars['ansible_%s' % alias] = opt\n\n    return options_vars",
        "begin_line": 211,
        "end_line": 230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.vars.isidentifier#233",
        "src_path": "lib/ansible/utils/vars.py",
        "class_name": "lib.ansible.utils.vars",
        "signature": "lib.ansible.utils.vars.isidentifier(ident)",
        "snippet": "def isidentifier(ident):\n    \"\"\"\n    Determines, if string is valid Python identifier using the ast module.\n    Originally posted at: http://stackoverflow.com/a/29586366\n    \"\"\"\n\n    if not isinstance(ident, string_types):\n        return False\n\n    try:\n        root = ast.parse(ident)\n    except SyntaxError:\n        return False\n\n    if not isinstance(root, ast.Module):\n        return False\n\n    if len(root.body) != 1:\n        return False\n\n    if not isinstance(root.body[0], ast.Expr):\n        return False\n\n    if not isinstance(root.body[0].value, ast.Name):\n        return False\n\n    if root.body[0].value.id != ident:\n        return False\n\n    return True",
        "begin_line": 233,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.__init__#22",
        "src_path": "lib/ansible/parsing/ajson.py",
        "class_name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder",
        "signature": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        kwargs['object_hook'] = self.object_hook\n        super(AnsibleJSONDecoder, self).__init__(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.158196134574088e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.set_secrets#27",
        "src_path": "lib/ansible/parsing/ajson.py",
        "class_name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder",
        "signature": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.set_secrets(cls, secrets)",
        "snippet": "    def set_secrets(cls, secrets):\n        cls._vaults['default'] = VaultLib(secrets=secrets)",
        "begin_line": 27,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.18752246100769e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.object_hook#30",
        "src_path": "lib/ansible/parsing/ajson.py",
        "class_name": "lib.ansible.parsing.ajson.AnsibleJSONDecoder",
        "signature": "lib.ansible.parsing.ajson.AnsibleJSONDecoder.object_hook(self, pairs)",
        "snippet": "    def object_hook(self, pairs):\n        for key in pairs:\n            value = pairs[key]\n\n            if key == '__ansible_vault':\n                value = AnsibleVaultEncryptedUnicode(value)\n                if self._vaults:\n                    value.vault = self._vaults['default']\n                return value\n            elif key == '__ansible_unsafe':\n                return wrap_var(value)\n\n        return pairs",
        "begin_line": 30,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.json_utils._filter_non_json_lines#32",
        "src_path": "lib/ansible/module_utils/json_utils.py",
        "class_name": "lib.ansible.module_utils.json_utils",
        "signature": "lib.ansible.module_utils.json_utils._filter_non_json_lines(data)",
        "snippet": "def _filter_non_json_lines(data):\n    '''\n    Used to filter unrelated output around module JSON output, like messages from\n    tcagetattr, or where dropbear spews MOTD on every single command (which is nuts).\n\n    Filters leading lines before first line-starting occurrence of '{' or '[', and filter all\n    trailing lines after matching close character (working from the bottom of output).\n    '''\n    warnings = []\n\n    # Filter initial junk\n    lines = data.splitlines()\n\n    for start, line in enumerate(lines):\n        line = line.strip()\n        if line.startswith(u'{'):\n            endchar = u'}'\n            break\n        elif line.startswith(u'['):\n            endchar = u']'\n            break\n    else:\n        raise ValueError('No start of json char found')\n\n    # Filter trailing junk\n    lines = lines[start:]\n\n    for reverse_end_offset, line in enumerate(reversed(lines)):\n        if line.strip().endswith(endchar):\n            break\n    else:\n        raise ValueError('No end of json char found')\n\n    if reverse_end_offset > 0:\n        # Trailing junk is uncommon and can point to things the user might\n        # want to change.  So print a warning if we find any\n        trailing_junk = lines[len(lines) - reverse_end_offset:]\n        for line in trailing_junk:\n            if line.strip():\n                warnings.append('Module invocation had junk after the JSON data: %s' % '\\n'.join(trailing_junk))\n                break\n\n    lines = lines[:(len(lines) - reverse_end_offset)]\n\n    return ('\\n'.join(lines), warnings)",
        "begin_line": 32,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.playbook.PlaybookCLI.init_parser#30",
        "src_path": "lib/ansible/cli/playbook.py",
        "class_name": "lib.ansible.cli.playbook.PlaybookCLI",
        "signature": "lib.ansible.cli.playbook.PlaybookCLI.init_parser(self)",
        "snippet": "    def init_parser(self):\n\n        # create parser for CLI options\n        super(PlaybookCLI, self).init_parser(\n            usage=\"%prog [options] playbook.yml [playbook2 ...]\",\n            desc=\"Runs Ansible playbooks, executing the defined tasks on the targeted hosts.\")\n\n        opt_help.add_connect_options(self.parser)\n        opt_help.add_meta_options(self.parser)\n        opt_help.add_runas_options(self.parser)\n        opt_help.add_subset_options(self.parser)\n        opt_help.add_check_options(self.parser)\n        opt_help.add_inventory_options(self.parser)\n        opt_help.add_runtask_options(self.parser)\n        opt_help.add_vault_options(self.parser)\n        opt_help.add_fork_options(self.parser)\n        opt_help.add_module_options(self.parser)\n\n        # ansible playbook specific opts\n        self.parser.add_argument('--list-tasks', dest='listtasks', action='store_true',\n                                 help=\"list all tasks that would be executed\")\n        self.parser.add_argument('--list-tags', dest='listtags', action='store_true',\n                                 help=\"list all available tags\")\n        self.parser.add_argument('--step', dest='step', action='store_true',\n                                 help=\"one-step-at-a-time: confirm each task before running\")\n        self.parser.add_argument('--start-at-task', dest='start_at_task',\n                                 help=\"start the playbook at the task matching this name\")\n        self.parser.add_argument('args', help='Playbook(s)', metavar='playbook', nargs='+')",
        "begin_line": 30,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.playbook.PlaybookCLI.post_process_args#59",
        "src_path": "lib/ansible/cli/playbook.py",
        "class_name": "lib.ansible.cli.playbook.PlaybookCLI",
        "signature": "lib.ansible.cli.playbook.PlaybookCLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        options = super(PlaybookCLI, self).post_process_args(options)\n\n        display.verbosity = options.verbosity\n        self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n\n        return options",
        "begin_line": 59,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.playbook.PlaybookCLI._flush_cache#198",
        "src_path": "lib/ansible/cli/playbook.py",
        "class_name": "lib.ansible.cli.playbook.PlaybookCLI",
        "signature": "lib.ansible.cli.playbook.PlaybookCLI._flush_cache(inventory, variable_manager)",
        "snippet": "    def _flush_cache(inventory, variable_manager):\n        for host in inventory.list_hosts():\n            hostname = host.get_name()\n            variable_manager.clear_facts(hostname)",
        "begin_line": 198,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.sys_info.get_distribution#17",
        "src_path": "lib/ansible/module_utils/common/sys_info.py",
        "class_name": "lib.ansible.module_utils.common.sys_info",
        "signature": "lib.ansible.module_utils.common.sys_info.get_distribution()",
        "snippet": "def get_distribution():\n    '''\n    Return the name of the distribution the module is running on\n\n    :rtype: NativeString or None\n    :returns: Name of the distribution the module is running on\n\n    This function attempts to determine what Linux distribution the code is running on and return\n    a string representing that value.  If the distribution cannot be determined, it returns\n    ``OtherLinux``.  If not run on Linux it returns None.\n    '''\n    distribution = None\n\n    if platform.system() == 'Linux':\n        distribution = distro.id().capitalize()\n\n        if distribution == 'Amzn':\n            distribution = 'Amazon'\n        elif distribution == 'Rhel':\n            distribution = 'Redhat'\n        elif not distribution:\n            distribution = 'OtherLinux'\n\n    return distribution",
        "begin_line": 17,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.sys_info.get_distribution_version#43",
        "src_path": "lib/ansible/module_utils/common/sys_info.py",
        "class_name": "lib.ansible.module_utils.common.sys_info",
        "signature": "lib.ansible.module_utils.common.sys_info.get_distribution_version()",
        "snippet": "def get_distribution_version():\n    '''\n    Get the version of the Linux distribution the code is running on\n\n    :rtype: NativeString or None\n    :returns: A string representation of the version of the distribution. If it cannot determine\n        the version, it returns empty string. If this is not run on a Linux machine it returns None\n    '''\n    version = None\n\n    needs_best_version = frozenset((\n        u'centos',\n        u'debian',\n    ))\n\n    if platform.system() == 'Linux':\n        version = distro.version()\n        distro_id = distro.id()\n\n        if version is not None:\n            if distro_id in needs_best_version:\n                version_best = distro.version(best=True)\n\n                # CentoOS maintainers believe only the major version is appropriate\n                # but Ansible users desire minor version information, e.g., 7.5.\n                # https://github.com/ansible/ansible/issues/50141#issuecomment-449452781\n                if distro_id == u'centos':\n                    version = u'.'.join(version_best.split(u'.')[:2])\n\n                # Debian does not include minor version in /etc/os-release.\n                # Bug report filed upstream requesting this be added to /etc/os-release\n                # https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=931197\n                if distro_id == u'debian':\n                    version = version_best\n\n        else:\n            version = u''\n\n    return version",
        "begin_line": 43,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.sys_info.get_distribution_codename#84",
        "src_path": "lib/ansible/module_utils/common/sys_info.py",
        "class_name": "lib.ansible.module_utils.common.sys_info",
        "signature": "lib.ansible.module_utils.common.sys_info.get_distribution_codename()",
        "snippet": "def get_distribution_codename():\n    '''\n    Return the code name for this Linux Distribution\n\n    :rtype: NativeString or None\n    :returns: A string representation of the distribution's codename or None if not a Linux distro\n    '''\n    codename = None\n    if platform.system() == 'Linux':\n        # Until this gets merged and we update our bundled copy of distro:\n        # https://github.com/nir0s/distro/pull/230\n        # Fixes Fedora 28+ not having a code name and Ubuntu Xenial Xerus needing to be \"xenial\"\n        os_release_info = distro.os_release_info()\n        codename = os_release_info.get('version_codename')\n\n        if codename is None:\n            codename = os_release_info.get('ubuntu_codename')\n\n        if codename is None and distro.id() == 'ubuntu':\n            lsb_release_info = distro.lsb_release_info()\n            codename = lsb_release_info.get('codename')\n\n        if codename is None:\n            codename = distro.codename()\n            if codename == u'':\n                codename = None\n\n    return codename",
        "begin_line": 84,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.sys_info.get_platform_subclass#114",
        "src_path": "lib/ansible/module_utils/common/sys_info.py",
        "class_name": "lib.ansible.module_utils.common.sys_info",
        "signature": "lib.ansible.module_utils.common.sys_info.get_platform_subclass(cls)",
        "snippet": "def get_platform_subclass(cls):\n    '''\n    Finds a subclass implementing desired functionality on the platform the code is running on\n\n    :arg cls: Class to find an appropriate subclass for\n    :returns: A class that implements the functionality on this platform\n\n    Some Ansible modules have different implementations depending on the platform they run on.  This\n    function is used to select between the various implementations and choose one.  You can look at\n    the implementation of the Ansible :ref:`User module<user_module>` module for an example of how to use this.\n\n    This function replaces ``basic.load_platform_subclass()``.  When you port code, you need to\n    change the callers to be explicit about instantiating the class.  For instance, code in the\n    Ansible User module changed from::\n\n    .. code-block:: python\n\n        # Old\n        class User:\n            def __new__(cls, args, kwargs):\n                return load_platform_subclass(User, args, kwargs)\n\n        # New\n        class User:\n            def __new__(cls, *args, **kwargs):\n                new_cls = get_platform_subclass(User)\n                return super(cls, new_cls).__new__(new_cls)\n    '''\n\n    this_platform = platform.system()\n    distribution = get_distribution()\n    subclass = None\n\n    # get the most specific superclass for this platform\n    if distribution is not None:\n        for sc in get_all_subclasses(cls):\n            if sc.distribution is not None and sc.distribution == distribution and sc.platform == this_platform:\n                subclass = sc\n    if subclass is None:\n        for sc in get_all_subclasses(cls):\n            if sc.platform == this_platform and sc.distribution is None:\n                subclass = sc\n    if subclass is None:\n        subclass = cls\n\n    return subclass",
        "begin_line": 114,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__._escape_backslashes#123",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__",
        "signature": "lib.ansible.template.__init__._escape_backslashes(data, jinja_env)",
        "snippet": "def _escape_backslashes(data, jinja_env):\n    \"\"\"Double backslashes within jinja2 expressions\n\n    A user may enter something like this in a playbook::\n\n      debug:\n        msg: \"Test Case 1\\\\3; {{ test1_name | regex_replace('^(.*)_name$', '\\\\1')}}\"\n\n    The string inside of the {{ gets interpreted multiple times First by yaml.\n    Then by python.  And finally by jinja2 as part of it's variable.  Because\n    it is processed by both python and jinja2, the backslash escaped\n    characters get unescaped twice.  This means that we'd normally have to use\n    four backslashes to escape that.  This is painful for playbook authors as\n    they have to remember different rules for inside vs outside of a jinja2\n    expression (The backslashes outside of the \"{{ }}\" only get processed by\n    yaml and python.  So they only need to be escaped once).  The following\n    code fixes this by automatically performing the extra quoting of\n    backslashes inside of a jinja2 expression.\n\n    \"\"\"\n    if '\\\\' in data and '{{' in data:\n        new_data = []\n        d2 = jinja_env.preprocess(data)\n        in_var = False\n\n        for token in jinja_env.lex(d2):\n            if token[1] == 'variable_begin':\n                in_var = True\n                new_data.append(token[2])\n            elif token[1] == 'variable_end':\n                in_var = False\n                new_data.append(token[2])\n            elif in_var and token[1] == 'string':\n                # Double backslashes only if we're inside of a jinja2 variable\n                new_data.append(token[2].replace('\\\\', '\\\\\\\\'))\n            else:\n                new_data.append(token[2])\n\n        data = ''.join(new_data)\n\n    return data",
        "begin_line": 123,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006397952655150352,
            "pseudo_dstar_susp": 0.0006397952655150352,
            "pseudo_tarantula_susp": 0.0006406149903907751,
            "pseudo_op2_susp": 0.0006397952655150352,
            "pseudo_barinel_susp": 0.0006406149903907751
        }
    },
    {
        "name": "lib.ansible.template.__init__.is_template#166",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__",
        "signature": "lib.ansible.template.__init__.is_template(data, jinja_env)",
        "snippet": "def is_template(data, jinja_env):\n    \"\"\"This function attempts to quickly detect whether a value is a jinja2\n    template. To do so, we look for the first 2 matching jinja2 tokens for\n    start and end delimiters.\n    \"\"\"\n    found = None\n    start = True\n    comment = False\n    d2 = jinja_env.preprocess(data)\n\n    # This wraps a lot of code, but this is due to lex returing a generator\n    # so we may get an exception at any part of the loop\n    try:\n        for token in jinja_env.lex(d2):\n            if token[1] in JINJA2_BEGIN_TOKENS:\n                if start and token[1] == 'comment_begin':\n                    # Comments can wrap other token types\n                    comment = True\n                start = False\n                # Example: variable_end -> variable\n                found = token[1].split('_')[0]\n            elif token[1] in JINJA2_END_TOKENS:\n                if token[1].split('_')[0] == found:\n                    return True\n                elif comment:\n                    continue\n                return False\n    except TemplateSyntaxError:\n        return False\n\n    return False",
        "begin_line": 166,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__._count_newlines_from_end#199",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__",
        "signature": "lib.ansible.template.__init__._count_newlines_from_end(in_str)",
        "snippet": "def _count_newlines_from_end(in_str):\n    '''\n    Counts the number of newlines at the end of a string. This is used during\n    the jinja2 templating to ensure the count matches the input, since some newlines\n    may be thrown away during the templating.\n    '''\n\n    try:\n        i = len(in_str)\n        j = i - 1\n        while in_str[j] == '\\n':\n            j -= 1\n        return i - 1 - j\n    except IndexError:\n        # Uncommon cases: zero length string and string containing only newlines\n        return i",
        "begin_line": 199,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007457121551081282,
            "pseudo_dstar_susp": 0.0007451564828614009,
            "pseudo_tarantula_susp": 0.0007739938080495357,
            "pseudo_op2_susp": 0.0007451564828614009,
            "pseudo_barinel_susp": 0.0007739938080495357
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleUndefined.__getattr__#236",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleUndefined",
        "signature": "lib.ansible.template.__init__.AnsibleUndefined.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name == '__UNSAFE__':\n            # AnsibleUndefined should never be assumed to be unsafe\n            # This prevents ``hasattr(val, '__UNSAFE__')`` from evaluating to ``True``\n            raise AttributeError(name)\n        # Return original Undefined object to preserve the first failure context\n        return self",
        "begin_line": 236,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleUndefined.__getitem__#244",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleUndefined",
        "signature": "lib.ansible.template.__init__.AnsibleUndefined.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        # Return original Undefined object to preserve the first failure context\n        return self",
        "begin_line": 244,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleContext.__init__#259",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleContext",
        "signature": "lib.ansible.template.__init__.AnsibleContext.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(AnsibleContext, self).__init__(*args, **kwargs)\n        self.unsafe = False",
        "begin_line": 259,
        "end_line": 261,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005698005698005698,
            "pseudo_dstar_susp": 0.0005698005698005698,
            "pseudo_tarantula_susp": 0.0005701254275940707,
            "pseudo_op2_susp": 0.0005698005698005698,
            "pseudo_barinel_susp": 0.0005701254275940707
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleContext._is_unsafe#263",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleContext",
        "signature": "lib.ansible.template.__init__.AnsibleContext._is_unsafe(self, val)",
        "snippet": "    def _is_unsafe(self, val):\n        '''\n        Our helper function, which will also recursively check dict and\n        list entries due to the fact that they may be repr'd and contain\n        a key or value which contains jinja2 syntax and would otherwise\n        lose the AnsibleUnsafe value.\n        '''\n        if isinstance(val, dict):\n            for key in val.keys():\n                if self._is_unsafe(val[key]):\n                    return True\n        elif isinstance(val, list):\n            for item in val:\n                if self._is_unsafe(item):\n                    return True\n        elif getattr(val, '__UNSAFE__', False) is True:\n            return True\n        return False",
        "begin_line": 263,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006734006734006734,
            "pseudo_dstar_susp": 0.0006734006734006734,
            "pseudo_tarantula_susp": 0.0006743088334457181,
            "pseudo_op2_susp": 0.0006734006734006734,
            "pseudo_barinel_susp": 0.0006743088334457181
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleContext._update_unsafe#282",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleContext",
        "signature": "lib.ansible.template.__init__.AnsibleContext._update_unsafe(self, val)",
        "snippet": "    def _update_unsafe(self, val):\n        if val is not None and not self.unsafe and self._is_unsafe(val):\n            self.unsafe = True",
        "begin_line": 282,
        "end_line": 284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000651890482398957,
            "pseudo_dstar_susp": 0.000651890482398957,
            "pseudo_tarantula_susp": 0.0006527415143603133,
            "pseudo_op2_susp": 0.000651890482398957,
            "pseudo_barinel_susp": 0.0006527415143603133
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleContext.resolve#286",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleContext",
        "signature": "lib.ansible.template.__init__.AnsibleContext.resolve(self, key)",
        "snippet": "    def resolve(self, key):\n        '''\n        The intercepted resolve(), which uses the helper above to set the\n        internal flag whenever an unsafe variable value is returned.\n        '''\n        val = super(AnsibleContext, self).resolve(key)\n        self._update_unsafe(val)\n        return val",
        "begin_line": 286,
        "end_line": 293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleContext.resolve_or_missing#295",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleContext",
        "signature": "lib.ansible.template.__init__.AnsibleContext.resolve_or_missing(self, key)",
        "snippet": "    def resolve_or_missing(self, key):\n        val = super(AnsibleContext, self).resolve_or_missing(key)\n        self._update_unsafe(val)\n        return val",
        "begin_line": 295,
        "end_line": 298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000651890482398957,
            "pseudo_dstar_susp": 0.000651890482398957,
            "pseudo_tarantula_susp": 0.0006527415143603133,
            "pseudo_op2_susp": 0.000651890482398957,
            "pseudo_barinel_susp": 0.0006527415143603133
        }
    },
    {
        "name": "lib.ansible.template.__init__.JinjaPluginIntercept.__init__#336",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.JinjaPluginIntercept",
        "signature": "lib.ansible.template.__init__.JinjaPluginIntercept.__init__(self, delegatee, pluginloader, *args, **kwargs)",
        "snippet": "    def __init__(self, delegatee, pluginloader, *args, **kwargs):\n        super(JinjaPluginIntercept, self).__init__(*args, **kwargs)\n        self._delegatee = delegatee\n        self._pluginloader = pluginloader\n\n        if self._pluginloader.class_name == 'FilterModule':\n            self._method_map_name = 'filters'\n            self._dirname = 'filter'\n        elif self._pluginloader.class_name == 'TestModule':\n            self._method_map_name = 'tests'\n            self._dirname = 'test'\n\n        self._collection_jinja_func_cache = {}",
        "begin_line": 336,
        "end_line": 348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005390835579514825,
            "pseudo_dstar_susp": 0.0005390835579514825,
            "pseudo_tarantula_susp": 0.0005390835579514825,
            "pseudo_op2_susp": 0.0005390835579514825,
            "pseudo_barinel_susp": 0.0005390835579514825
        }
    },
    {
        "name": "lib.ansible.template.__init__.JinjaPluginIntercept.__getitem__#352",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.JinjaPluginIntercept",
        "signature": "lib.ansible.template.__init__.JinjaPluginIntercept.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if not isinstance(key, string_types):\n            raise ValueError('key must be a string')\n\n        key = to_native(key)\n\n        if '.' not in key:  # might be a built-in value, delegate to base dict\n            return self._delegatee.__getitem__(key)\n\n        func = self._collection_jinja_func_cache.get(key)\n\n        if func:\n            return func\n\n        acr = AnsibleCollectionRef.try_parse_fqcr(key, self._dirname)\n\n        if not acr:\n            raise KeyError('invalid plugin name: {0}'.format(key))\n\n        try:\n            pkg = import_module(acr.n_python_package_name)\n        except ImportError:\n            raise KeyError()\n\n        parent_prefix = acr.collection\n\n        if acr.subdirs:\n            parent_prefix = '{0}.{1}'.format(parent_prefix, acr.subdirs)\n\n        for dummy, module_name, ispkg in pkgutil.iter_modules(pkg.__path__, prefix=parent_prefix + '.'):\n            if ispkg:\n                continue\n\n            try:\n                plugin_impl = self._pluginloader.get(module_name)\n            except Exception as e:\n                raise TemplateSyntaxError(to_native(e), 0)\n\n            method_map = getattr(plugin_impl, self._method_map_name)\n\n            for f in iteritems(method_map()):\n                fq_name = '.'.join((parent_prefix, f[0]))\n                # FIXME: detect/warn on intra-collection function name collisions\n                self._collection_jinja_func_cache[fq_name] = f[1]\n\n        function_impl = self._collection_jinja_func_cache[key]\n        return function_impl",
        "begin_line": 352,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.JinjaPluginIntercept.__setitem__#400",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.JinjaPluginIntercept",
        "signature": "lib.ansible.template.__init__.JinjaPluginIntercept.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        return self._delegatee.__setitem__(key, value)",
        "begin_line": 400,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000651890482398957,
            "pseudo_dstar_susp": 0.000651890482398957,
            "pseudo_tarantula_susp": 0.0006527415143603133,
            "pseudo_op2_susp": 0.000651890482398957,
            "pseudo_barinel_susp": 0.0006527415143603133
        }
    },
    {
        "name": "lib.ansible.template.__init__.AnsibleEnvironment.__init__#423",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.AnsibleEnvironment",
        "signature": "lib.ansible.template.__init__.AnsibleEnvironment.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(AnsibleEnvironment, self).__init__(*args, **kwargs)\n\n        self.filters = JinjaPluginIntercept(self.filters, filter_loader)\n        self.tests = JinjaPluginIntercept(self.tests, test_loader)",
        "begin_line": 423,
        "end_line": 427,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005390835579514825,
            "pseudo_dstar_susp": 0.0005390835579514825,
            "pseudo_tarantula_susp": 0.0005390835579514825,
            "pseudo_op2_susp": 0.0005390835579514825,
            "pseudo_barinel_susp": 0.0005390835579514825
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.__init__#435",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.__init__(self, loader, shared_loader_obj=None, variables=None)",
        "snippet": "    def __init__(self, loader, shared_loader_obj=None, variables=None):\n        variables = {} if variables is None else variables\n\n        self._loader = loader\n        self._filters = None\n        self._tests = None\n        self._available_variables = variables\n        self._cached_result = {}\n\n        if loader:\n            self._basedir = loader.get_basedir()\n        else:\n            self._basedir = './'\n\n        if shared_loader_obj:\n            self._filter_loader = getattr(shared_loader_obj, 'filter_loader')\n            self._test_loader = getattr(shared_loader_obj, 'test_loader')\n            self._lookup_loader = getattr(shared_loader_obj, 'lookup_loader')\n        else:\n            self._filter_loader = filter_loader\n            self._test_loader = test_loader\n            self._lookup_loader = lookup_loader\n\n        # flags to determine whether certain failures during templating\n        # should result in fatal errors being raised\n        self._fail_on_lookup_errors = True\n        self._fail_on_filter_errors = True\n        self._fail_on_undefined_errors = C.DEFAULT_UNDEFINED_VAR_BEHAVIOR\n\n        self.environment = AnsibleEnvironment(\n            trim_blocks=True,\n            undefined=AnsibleUndefined,\n            extensions=self._get_extensions(),\n            finalize=self._finalize,\n            loader=FileSystemLoader(self._basedir),\n        )\n\n        # the current rendering context under which the templar class is working\n        self.cur_context = None\n\n        self.SINGLE_VAR = re.compile(r\"^%s\\s*(\\w*)\\s*%s$\" % (self.environment.variable_start_string, self.environment.variable_end_string))\n\n        self._clean_regex = re.compile(r'(?:%s|%s|%s|%s)' % (\n            self.environment.variable_start_string,\n            self.environment.block_start_string,\n            self.environment.block_end_string,\n            self.environment.variable_end_string\n        ))\n        self._no_type_regex = re.compile(r'.*?\\|\\s*(?:%s)(?:\\([^\\|]*\\))?\\s*\\)?\\s*(?:%s)' %\n                                         ('|'.join(C.STRING_TYPE_FILTERS), self.environment.variable_end_string))",
        "begin_line": 435,
        "end_line": 484,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005467468562055768,
            "pseudo_dstar_susp": 0.0005467468562055768,
            "pseudo_tarantula_susp": 0.0005467468562055768,
            "pseudo_op2_susp": 0.0005467468562055768,
            "pseudo_barinel_susp": 0.0005467468562055768
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._get_filters#486",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._get_filters(self)",
        "snippet": "    def _get_filters(self):\n        '''\n        Returns filter plugins, after loading and caching them if need be\n        '''\n\n        if self._filters is not None:\n            return self._filters.copy()\n\n        self._filters = dict()\n\n        for fp in self._filter_loader.all():\n            self._filters.update(fp.filters())\n\n        return self._filters.copy()",
        "begin_line": 486,
        "end_line": 499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006993006993006993,
            "pseudo_dstar_susp": 0.0006993006993006993,
            "pseudo_tarantula_susp": 0.0007047216349541931,
            "pseudo_op2_susp": 0.0006993006993006993,
            "pseudo_barinel_susp": 0.0007047216349541931
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._get_tests#501",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._get_tests(self)",
        "snippet": "    def _get_tests(self):\n        '''\n        Returns tests plugins, after loading and caching them if need be\n        '''\n\n        if self._tests is not None:\n            return self._tests.copy()\n\n        self._tests = dict()\n        for fp in self._test_loader.all():\n            self._tests.update(fp.tests())\n\n        return self._tests.copy()",
        "begin_line": 501,
        "end_line": 513,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006993006993006993,
            "pseudo_dstar_susp": 0.0006993006993006993,
            "pseudo_tarantula_susp": 0.0007047216349541931,
            "pseudo_op2_susp": 0.0006993006993006993,
            "pseudo_barinel_susp": 0.0007047216349541931
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._get_extensions#515",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._get_extensions(self)",
        "snippet": "    def _get_extensions(self):\n        '''\n        Return jinja2 extensions to load.\n\n        If some extensions are set via jinja_extensions in ansible.cfg, we try\n        to load them with the jinja environment.\n        '''\n\n        jinja_exts = []\n        if C.DEFAULT_JINJA2_EXTENSIONS:\n            # make sure the configuration directive doesn't contain spaces\n            # and split extensions in an array\n            jinja_exts = C.DEFAULT_JINJA2_EXTENSIONS.replace(\" \", \"\").split(',')\n\n        return jinja_exts",
        "begin_line": 515,
        "end_line": 529,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005390835579514825,
            "pseudo_dstar_susp": 0.0005390835579514825,
            "pseudo_tarantula_susp": 0.0005390835579514825,
            "pseudo_op2_susp": 0.0005390835579514825,
            "pseudo_barinel_susp": 0.0005390835579514825
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.available_variables#532",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.available_variables(self)",
        "snippet": "    def available_variables(self):\n        return self._available_variables",
        "begin_line": 532,
        "end_line": 533,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005614823133071309,
            "pseudo_dstar_susp": 0.0005614823133071309,
            "pseudo_tarantula_susp": 0.0005614823133071309,
            "pseudo_op2_susp": 0.0005614823133071309,
            "pseudo_barinel_susp": 0.0005614823133071309
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.available_variables#536",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.available_variables(self, variables)",
        "snippet": "    def available_variables(self, variables):\n        '''\n        Sets the list of template variables this Templar instance will use\n        to template things, so we don't have to pass them around between\n        internal methods. We also clear the template cache here, as the variables\n        are being changed.\n        '''\n\n        if not isinstance(variables, Mapping):\n            raise AnsibleAssertionError(\"the type of 'variables' should be a Mapping but was a %s\" % (type(variables)))\n        self._available_variables = variables\n        self._cached_result = {}",
        "begin_line": 536,
        "end_line": 547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.template#586",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.template(self, variable, convert_bare=False, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, convert_data=True, static_vars=None, cache=True, disable_lookups=False)",
        "snippet": "    def template(self, variable, convert_bare=False, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None,\n                 convert_data=True, static_vars=None, cache=True, disable_lookups=False):\n        '''\n        Templates (possibly recursively) any given data as input. If convert_bare is\n        set to True, the given data will be wrapped as a jinja2 variable ('{{foo}}')\n        before being sent through the template engine.\n        '''\n        static_vars = [''] if static_vars is None else static_vars\n\n        # Don't template unsafe variables, just return them.\n        if hasattr(variable, '__UNSAFE__'):\n            return variable\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            if convert_bare:\n                variable = self._convert_bare_variable(variable)\n\n            if isinstance(variable, string_types):\n                result = variable\n\n                if self.is_possibly_template(variable):\n                    # Check to see if the string we are trying to render is just referencing a single\n                    # var.  In this case we don't want to accidentally change the type of the variable\n                    # to a string by using the jinja template renderer. We just want to pass it.\n                    only_one = self.SINGLE_VAR.match(variable)\n                    if only_one:\n                        var_name = only_one.group(1)\n                        if var_name in self._available_variables:\n                            resolved_val = self._available_variables[var_name]\n                            if isinstance(resolved_val, NON_TEMPLATED_TYPES):\n                                return resolved_val\n                            elif resolved_val is None:\n                                return C.DEFAULT_NULL_REPRESENTATION\n\n                    # Using a cache in order to prevent template calls with already templated variables\n                    sha1_hash = None\n                    if cache:\n                        variable_hash = sha1(text_type(variable).encode('utf-8'))\n                        options_hash = sha1(\n                            (\n                                text_type(preserve_trailing_newlines) +\n                                text_type(escape_backslashes) +\n                                text_type(fail_on_undefined) +\n                                text_type(overrides)\n                            ).encode('utf-8')\n                        )\n                        sha1_hash = variable_hash.hexdigest() + options_hash.hexdigest()\n                    if cache and sha1_hash in self._cached_result:\n                        result = self._cached_result[sha1_hash]\n                    else:\n                        result = self.do_template(\n                            variable,\n                            preserve_trailing_newlines=preserve_trailing_newlines,\n                            escape_backslashes=escape_backslashes,\n                            fail_on_undefined=fail_on_undefined,\n                            overrides=overrides,\n                            disable_lookups=disable_lookups,\n                        )\n\n                        if not USE_JINJA2_NATIVE:\n                            unsafe = hasattr(result, '__UNSAFE__')\n                            if convert_data and not self._no_type_regex.match(variable):\n                                # if this looks like a dictionary or list, convert it to such using the safe_eval method\n                                if (result.startswith(\"{\") and not result.startswith(self.environment.variable_start_string)) or \\\n                                        result.startswith(\"[\") or result in (\"True\", \"False\"):\n                                    eval_results = safe_eval(result, include_exceptions=True)\n                                    if eval_results[1] is None:\n                                        result = eval_results[0]\n                                        if unsafe:\n                                            result = wrap_var(result)\n                                    else:\n                                        # FIXME: if the safe_eval raised an error, should we do something with it?\n                                        pass\n\n                        # we only cache in the case where we have a single variable\n                        # name, to make sure we're not putting things which may otherwise\n                        # be dynamic in the cache (filters, lookups, etc.)\n                        if cache and only_one:\n                            self._cached_result[sha1_hash] = result\n\n                return result\n\n            elif is_sequence(variable):\n                return [self.template(\n                    v,\n                    preserve_trailing_newlines=preserve_trailing_newlines,\n                    fail_on_undefined=fail_on_undefined,\n                    overrides=overrides,\n                    disable_lookups=disable_lookups,\n                ) for v in variable]\n            elif isinstance(variable, Mapping):\n                d = {}\n                # we don't use iteritems() here to avoid problems if the underlying dict\n                # changes sizes due to the templating, which can happen with hostvars\n                for k in variable.keys():\n                    if k not in static_vars:\n                        d[k] = self.template(\n                            variable[k],\n                            preserve_trailing_newlines=preserve_trailing_newlines,\n                            fail_on_undefined=fail_on_undefined,\n                            overrides=overrides,\n                            disable_lookups=disable_lookups,\n                        )\n                    else:\n                        d[k] = variable[k]\n                return d\n            else:\n                return variable\n\n        except AnsibleFilterError:\n            if self._fail_on_filter_errors:\n                raise\n            else:\n                return variable",
        "begin_line": 586,
        "end_line": 702,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006891798759476223,
            "pseudo_dstar_susp": 0.0006891798759476223,
            "pseudo_tarantula_susp": 0.0006901311249137336,
            "pseudo_op2_susp": 0.0006891798759476223,
            "pseudo_barinel_susp": 0.0006901311249137336
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.is_template#704",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.is_template(self, data)",
        "snippet": "    def is_template(self, data):\n        '''lets us know if data has a template'''\n        if isinstance(data, string_types):\n            return is_template(data, self.environment)\n        elif isinstance(data, (list, tuple)):\n            for v in data:\n                if self.is_template(v):\n                    return True\n        elif isinstance(data, dict):\n            for k in data:\n                if self.is_template(k) or self.is_template(data[k]):\n                    return True\n        return False",
        "begin_line": 704,
        "end_line": 716,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.is_possibly_template#720",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.is_possibly_template(self, data)",
        "snippet": "    def is_possibly_template(self, data):\n        '''Determines if a string looks like a template, by seeing if it\n        contains a jinja2 start delimiter. Does not guarantee that the string\n        is actually a template.\n\n        This is different than ``is_template`` which is more strict.\n        This method may return ``True`` on a string that is not templatable.\n\n        Useful when guarding passing a string for templating, but when\n        you want to allow the templating engine to make the final\n        assessment which may result in ``TemplateSyntaxError``.\n        '''\n        env = self.environment\n        if isinstance(data, string_types):\n            for marker in (env.block_start_string, env.variable_start_string, env.comment_start_string):\n                if marker in data:\n                    return True\n        return False",
        "begin_line": 720,
        "end_line": 737,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005868544600938967,
            "pseudo_dstar_susp": 0.0005868544600938967,
            "pseudo_tarantula_susp": 0.0005871990604815032,
            "pseudo_op2_susp": 0.0005868544600938967,
            "pseudo_barinel_susp": 0.0005871990604815032
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._convert_bare_variable#739",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._convert_bare_variable(self, variable)",
        "snippet": "    def _convert_bare_variable(self, variable):\n        '''\n        Wraps a bare string, which may have an attribute portion (ie. foo.bar)\n        in jinja2 variable braces so that it is evaluated properly.\n        '''\n\n        if isinstance(variable, string_types):\n            contains_filters = \"|\" in variable\n            first_part = variable.split(\"|\")[0].split(\".\")[0].split(\"[\")[0]\n            if (contains_filters or first_part in self._available_variables) and self.environment.variable_start_string not in variable:\n                return \"%s%s%s\" % (self.environment.variable_start_string, variable, self.environment.variable_end_string)\n\n        # the variable didn't meet the conditions to be converted,\n        # so just return it as-is\n        return variable",
        "begin_line": 739,
        "end_line": 753,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._finalize#755",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._finalize(self, thing)",
        "snippet": "    def _finalize(self, thing):\n        '''\n        A custom finalize method for jinja2, which prevents None from being returned. This\n        avoids a string of ``\"None\"`` as ``None`` has no importance in YAML.\n\n        If using ANSIBLE_JINJA2_NATIVE we bypass this and return the actual value always\n        '''\n        if USE_JINJA2_NATIVE:\n            return thing\n        return thing if thing is not None else ''",
        "begin_line": 755,
        "end_line": 764,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000693000693000693,
            "pseudo_dstar_susp": 0.000693000693000693,
            "pseudo_tarantula_susp": 0.0006949270326615705,
            "pseudo_op2_susp": 0.000693000693000693,
            "pseudo_barinel_susp": 0.0006949270326615705
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar._lookup#786",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar._lookup(self, name, *args, **kwargs)",
        "snippet": "    def _lookup(self, name, *args, **kwargs):\n        instance = self._lookup_loader.get(name, loader=self._loader, templar=self)\n\n        if instance is not None:\n            wantlist = kwargs.pop('wantlist', False)\n            allow_unsafe = kwargs.pop('allow_unsafe', C.DEFAULT_ALLOW_UNSAFE_LOOKUPS)\n            errors = kwargs.pop('errors', 'strict')\n\n            from ansible.utils.listify import listify_lookup_plugin_terms\n            loop_terms = listify_lookup_plugin_terms(terms=args, templar=self, loader=self._loader, fail_on_undefined=True, convert_bare=False)\n            # safely catch run failures per #5059\n            try:\n                ran = instance.run(loop_terms, variables=self._available_variables, **kwargs)\n            except (AnsibleUndefinedVariable, UndefinedError) as e:\n                raise AnsibleUndefinedVariable(e)\n            except Exception as e:\n                if self._fail_on_lookup_errors:\n                    msg = u\"An unhandled exception occurred while running the lookup plugin '%s'. Error was a %s, original message: %s\" % \\\n                          (name, type(e), to_text(e))\n                    if errors == 'warn':\n                        display.warning(msg)\n                    elif errors == 'ignore':\n                        display.display(msg, log_only=True)\n                    else:\n                        raise AnsibleError(to_native(msg))\n                ran = [] if wantlist else None\n\n            if ran and not allow_unsafe:\n                if wantlist:\n                    ran = wrap_var(ran)\n                else:\n                    try:\n                        ran = wrap_var(\",\".join(ran))\n                    except TypeError:\n                        # Lookup Plugins should always return lists.  Throw an error if that's not\n                        # the case:\n                        if not isinstance(ran, Sequence):\n                            raise AnsibleError(\"The lookup plugin '%s' did not return a list.\"\n                                               % name)\n\n                        # The TypeError we can recover from is when the value *inside* of the list\n                        # is not a string\n                        if len(ran) == 1:\n                            ran = wrap_var(ran[0])\n                        else:\n                            ran = wrap_var(ran)\n\n                if self.cur_context:\n                    self.cur_context.unsafe = True\n            return ran\n        else:\n            raise AnsibleError(\"lookup plugin (%s) not found\" % name)",
        "begin_line": 786,
        "end_line": 837,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.__init__.Templar.do_template#839",
        "src_path": "lib/ansible/template/__init__.py",
        "class_name": "lib.ansible.template.__init__.Templar",
        "signature": "lib.ansible.template.__init__.Templar.do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False)",
        "snippet": "    def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n        if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n            return data\n\n        # For preserving the number of input newlines in the output (used\n        # later in this method)\n        data_newlines = _count_newlines_from_end(data)\n\n        if fail_on_undefined is None:\n            fail_on_undefined = self._fail_on_undefined_errors\n\n        try:\n            # allows template header overrides to change jinja2 options.\n            if overrides is None:\n                myenv = self.environment.overlay()\n            else:\n                myenv = self.environment.overlay(overrides)\n\n            # Get jinja env overrides from template\n            if hasattr(data, 'startswith') and data.startswith(JINJA2_OVERRIDE):\n                eol = data.find('\\n')\n                line = data[len(JINJA2_OVERRIDE):eol]\n                data = data[eol + 1:]\n                for pair in line.split(','):\n                    (key, val) = pair.split(':')\n                    key = key.strip()\n                    setattr(myenv, key, ast.literal_eval(val.strip()))\n\n            # Adds Ansible custom filters and tests\n            myenv.filters.update(self._get_filters())\n            myenv.tests.update(self._get_tests())\n\n            if escape_backslashes:\n                # Allow users to specify backslashes in playbooks as \"\\\\\" instead of as \"\\\\\\\\\".\n                data = _escape_backslashes(data, myenv)\n\n            try:\n                t = myenv.from_string(data)\n            except TemplateSyntaxError as e:\n                raise AnsibleError(\"template error while templating string: %s. String: %s\" % (to_native(e), to_native(data)))\n            except Exception as e:\n                if 'recursion' in to_native(e):\n                    raise AnsibleError(\"recursive loop detected in template string: %s\" % to_native(data))\n                else:\n                    return data\n\n            # jinja2 global is inconsistent across versions, this normalizes them\n            t.globals['dict'] = dict\n\n            if disable_lookups:\n                t.globals['query'] = t.globals['q'] = t.globals['lookup'] = self._fail_lookup\n            else:\n                t.globals['lookup'] = self._lookup\n                t.globals['query'] = t.globals['q'] = self._query_lookup\n\n            t.globals['now'] = self._now_datetime\n\n            t.globals['finalize'] = self._finalize\n\n            jvars = AnsibleJ2Vars(self, t.globals)\n\n            self.cur_context = new_context = t.new_context(jvars, shared=True)\n            rf = t.root_render_func(new_context)\n\n            try:\n                res = j2_concat(rf)\n                if getattr(new_context, 'unsafe', False):\n                    res = wrap_var(res)\n            except TypeError as te:\n                if 'AnsibleUndefined' in to_native(te):\n                    errmsg = \"Unable to look up a name or access an attribute in template string (%s).\\n\" % to_native(data)\n                    errmsg += \"Make sure your variable name does not contain invalid characters like '-': %s\" % to_native(te)\n                    raise AnsibleUndefinedVariable(errmsg)\n                else:\n                    display.debug(\"failing because of a type error, template data is: %s\" % to_text(data))\n                    raise AnsibleError(\"Unexpected templating type error occurred on (%s): %s\" % (to_native(data), to_native(te)))\n\n            if USE_JINJA2_NATIVE and not isinstance(res, string_types):\n                return res\n\n            if preserve_trailing_newlines:\n                # The low level calls above do not preserve the newline\n                # characters at the end of the input data, so we use the\n                # calculate the difference in newlines and append them\n                # to the resulting output for parity\n                #\n                # jinja2 added a keep_trailing_newline option in 2.7 when\n                # creating an Environment.  That would let us make this code\n                # better (remove a single newline if\n                # preserve_trailing_newlines is False).  Once we can depend on\n                # that version being present, modify our code to set that when\n                # initializing self.environment and remove a single trailing\n                # newline here if preserve_newlines is False.\n                res_newlines = _count_newlines_from_end(res)\n                if data_newlines > res_newlines:\n                    res += self.environment.newline_sequence * (data_newlines - res_newlines)\n            return res\n        except (UndefinedError, AnsibleUndefinedVariable) as e:\n            if fail_on_undefined:\n                raise AnsibleUndefinedVariable(e)\n            else:\n                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n                return data",
        "begin_line": 839,
        "end_line": 941,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000777000777000777,
            "pseudo_dstar_susp": 0.0007751937984496124,
            "pseudo_tarantula_susp": 0.0008090614886731392,
            "pseudo_op2_susp": 0.0007751937984496124,
            "pseudo_barinel_susp": 0.0008090614886731392
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.env.EnvFactCollector.collect#30",
        "src_path": "lib/ansible/module_utils/facts/system/env.py",
        "class_name": "lib.ansible.module_utils.facts.system.env.EnvFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.env.EnvFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        env_facts = {}\n        env_facts['env'] = {}\n\n        for k, v in iteritems(os.environ):\n            env_facts['env'][k] = v\n\n        return env_facts",
        "begin_line": 30,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.service_mgr.ServiceMgrFactCollector.collect#65",
        "src_path": "lib/ansible/module_utils/facts/system/service_mgr.py",
        "class_name": "lib.ansible.module_utils.facts.system.service_mgr.ServiceMgrFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.service_mgr.ServiceMgrFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n\n        if not module:\n            return facts_dict\n\n        collected_facts = collected_facts or {}\n        service_mgr_name = None\n\n        # TODO: detect more custom init setups like bootscripts, dmd, s6, Epoch, etc\n        # also other OSs other than linux might need to check across several possible candidates\n\n        # Mapping of proc_1 values to more useful names\n        proc_1_map = {\n            'procd': 'openwrt_init',\n            'runit-init': 'runit',\n            'svscan': 'svc',\n            'openrc-init': 'openrc',\n        }\n\n        # try various forms of querying pid 1\n        proc_1 = get_file_content('/proc/1/comm')\n        if proc_1 is None:\n            # FIXME: return code isnt checked\n            # FIXME: if stdout is empty string, odd things\n            # FIXME: other code seems to think we could get proc_1 == None past this point\n            rc, proc_1, err = module.run_command(\"ps -p 1 -o comm|tail -n 1\", use_unsafe_shell=True)\n            # If the output of the command starts with what looks like a PID, then the 'ps' command\n            # probably didn't work the way we wanted, probably because it's busybox\n            if re.match(r' *[0-9]+ ', proc_1):\n                proc_1 = None\n\n        # The ps command above may return \"COMMAND\" if the user cannot read /proc, e.g. with grsecurity\n        if proc_1 == \"COMMAND\\n\":\n            proc_1 = None\n\n        # FIXME: empty string proc_1 staus empty string\n        if proc_1 is not None:\n            proc_1 = os.path.basename(proc_1)\n            proc_1 = to_native(proc_1)\n            proc_1 = proc_1.strip()\n\n        if proc_1 is not None and (proc_1 == 'init' or proc_1.endswith('sh')):\n            # many systems return init, so this cannot be trusted, if it ends in 'sh' it probalby is a shell in a container\n            proc_1 = None\n\n        # if not init/None it should be an identifiable or custom init, so we are done!\n        if proc_1 is not None:\n            # Lookup proc_1 value in map and use proc_1 value itself if no match\n            # FIXME: empty string still falls through\n            service_mgr_name = proc_1_map.get(proc_1, proc_1)\n\n        # FIXME: replace with a system->service_mgr_name map?\n        # start with the easy ones\n        elif collected_facts.get('ansible_distribution', None) == 'MacOSX':\n            # FIXME: find way to query executable, version matching is not ideal\n            if LooseVersion(platform.mac_ver()[0]) >= LooseVersion('10.4'):\n                service_mgr_name = 'launchd'\n            else:\n                service_mgr_name = 'systemstarter'\n        elif 'BSD' in collected_facts.get('ansible_system', '') or collected_facts.get('ansible_system') in ['Bitrig', 'DragonFly']:\n            # FIXME: we might want to break out to individual BSDs or 'rc'\n            service_mgr_name = 'bsdinit'\n        elif collected_facts.get('ansible_system') == 'AIX':\n            service_mgr_name = 'src'\n        elif collected_facts.get('ansible_system') == 'SunOS':\n            service_mgr_name = 'smf'\n        elif collected_facts.get('ansible_distribution') == 'OpenWrt':\n            service_mgr_name = 'openwrt_init'\n        elif collected_facts.get('ansible_system') == 'Linux':\n            # FIXME: mv is_systemd_managed\n            if self.is_systemd_managed(module=module):\n                service_mgr_name = 'systemd'\n            elif module.get_bin_path('initctl') and os.path.exists(\"/etc/init/\"):\n                service_mgr_name = 'upstart'\n            elif os.path.exists('/sbin/openrc'):\n                service_mgr_name = 'openrc'\n            elif self.is_systemd_managed_offline(module=module):\n                service_mgr_name = 'systemd'\n            elif os.path.exists('/etc/init.d/'):\n                service_mgr_name = 'sysvinit'\n\n        if not service_mgr_name:\n            # if we cannot detect, fallback to generic 'service'\n            service_mgr_name = 'service'\n\n        facts_dict['service_mgr'] = service_mgr_name\n        return facts_dict",
        "begin_line": 65,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._parse_parameters#113",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._parse_parameters(term)",
        "snippet": "def _parse_parameters(term):\n    \"\"\"Hacky parsing of params\n\n    See https://github.com/ansible/ansible-modules-core/issues/1968#issuecomment-136842156\n    and the first_found lookup For how we want to fix this later\n    \"\"\"\n    first_split = term.split(' ', 1)\n    if len(first_split) <= 1:\n        # Only a single argument given, therefore it's a path\n        relpath = term\n        params = dict()\n    else:\n        relpath = first_split[0]\n        params = parse_kv(first_split[1])\n        if '_raw_params' in params:\n            # Spaces in the path?\n            relpath = u' '.join((relpath, params['_raw_params']))\n            del params['_raw_params']\n\n            # Check that we parsed the params correctly\n            if not term.startswith(relpath):\n                # Likely, the user had a non parameter following a parameter.\n                # Reject this as a user typo\n                raise AnsibleError('Unrecognized value after key=value parameters given to password lookup')\n        # No _raw_params means we already found the complete path when\n        # we split it initially\n\n    # Check for invalid parameters.  Probably a user typo\n    invalid_params = frozenset(params.keys()).difference(VALID_PARAMS)\n    if invalid_params:\n        raise AnsibleError('Unrecognized parameter(s) given to password lookup: %s' % ', '.join(invalid_params))\n\n    # Set defaults\n    params['length'] = int(params.get('length', DEFAULT_LENGTH))\n    params['encrypt'] = params.get('encrypt', None)\n\n    params['chars'] = params.get('chars', None)\n    if params['chars']:\n        tmp_chars = []\n        if u',,' in params['chars']:\n            tmp_chars.append(u',')\n        tmp_chars.extend(c for c in params['chars'].replace(u',,', u',').split(u',') if c)\n        params['chars'] = tmp_chars\n    else:\n        # Default chars for password\n        params['chars'] = [u'ascii_letters', u'digits', u\".,:-_\"]\n\n    return relpath, params",
        "begin_line": 113,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._read_password_file#163",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._read_password_file(b_path)",
        "snippet": "def _read_password_file(b_path):\n    \"\"\"Read the contents of a password file and return it\n    :arg b_path: A byte string containing the path to the password file\n    :returns: a text string containing the contents of the password file or\n        None if no password file was present.\n    \"\"\"\n    content = None\n\n    if os.path.exists(b_path):\n        with open(b_path, 'rb') as f:\n            b_content = f.read().rstrip()\n        content = to_text(b_content, errors='surrogate_or_strict')\n\n    return content",
        "begin_line": 163,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._gen_candidate_chars#179",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._gen_candidate_chars(characters)",
        "snippet": "def _gen_candidate_chars(characters):\n    '''Generate a string containing all valid chars as defined by ``characters``\n\n    :arg characters: A list of character specs. The character specs are\n        shorthand names for sets of characters like 'digits', 'ascii_letters',\n        or 'punctuation' or a string to be included verbatim.\n\n    The values of each char spec can be:\n\n    * a name of an attribute in the 'strings' module ('digits' for example).\n      The value of the attribute will be added to the candidate chars.\n    * a string of characters. If the string isn't an attribute in 'string'\n      module, the string will be directly added to the candidate chars.\n\n    For example::\n\n        characters=['digits', '?|']``\n\n    will match ``string.digits`` and add all ascii digits.  ``'?|'`` will add\n    the question mark and pipe characters directly. Return will be the string::\n\n        u'0123456789?|'\n    '''\n    chars = []\n    for chars_spec in characters:\n        # getattr from string expands things like \"ascii_letters\" and \"digits\"\n        # into a set of characters.\n        chars.append(to_text(getattr(string, to_native(chars_spec), chars_spec),\n                     errors='strict'))\n    chars = u''.join(chars).replace(u'\"', u'').replace(u\"'\", u'')\n    return chars",
        "begin_line": 179,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._parse_content#212",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._parse_content(content)",
        "snippet": "def _parse_content(content):\n    '''parse our password data format into password and salt\n\n    :arg content: The data read from the file\n    :returns: password and salt\n    '''\n    password = content\n    salt = None\n\n    salt_slug = u' salt='\n    try:\n        sep = content.rindex(salt_slug)\n    except ValueError:\n        # No salt\n        pass\n    else:\n        salt = password[sep + len(salt_slug):]\n        password = content[:sep]\n\n    return password, salt",
        "begin_line": 212,
        "end_line": 231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._format_content#234",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._format_content(password, salt, encrypt=None)",
        "snippet": "def _format_content(password, salt, encrypt=None):\n    \"\"\"Format the password and salt for saving\n    :arg password: the plaintext password to save\n    :arg salt: the salt to use when encrypting a password\n    :arg encrypt: Which method the user requests that this password is encrypted.\n        Note that the password is saved in clear.  Encrypt just tells us if we\n        must save the salt value for idempotence.  Defaults to None.\n    :returns: a text string containing the formatted information\n\n    .. warning:: Passwords are saved in clear.  This is because the playbooks\n        expect to get cleartext passwords from this lookup.\n    \"\"\"\n    if not encrypt and not salt:\n        return password\n\n    # At this point, the calling code should have assured us that there is a salt value.\n    if not salt:\n        raise AnsibleAssertionError('_format_content was called with encryption requested but no salt value')\n\n    return u'%s salt=%s' % (password, salt)",
        "begin_line": 234,
        "end_line": 253,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._write_password_file#256",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._write_password_file(b_path, content)",
        "snippet": "def _write_password_file(b_path, content):\n    b_pathdir = os.path.dirname(b_path)\n    makedirs_safe(b_pathdir, mode=0o700)\n\n    with open(b_path, 'wb') as f:\n        os.chmod(b_path, 0o600)\n        b_content = to_bytes(content, errors='surrogate_or_strict') + b'\\n'\n        f.write(b_content)",
        "begin_line": 256,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._get_lock#266",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._get_lock(b_path)",
        "snippet": "def _get_lock(b_path):\n    \"\"\"Get the lock for writing password file.\"\"\"\n    first_process = False\n    b_pathdir = os.path.dirname(b_path)\n    lockfile_name = to_bytes(\"%s.ansible_lockfile\" % hashlib.sha1(b_path).hexdigest())\n    lockfile = os.path.join(b_pathdir, lockfile_name)\n    if not os.path.exists(lockfile) and b_path != to_bytes('/dev/null'):\n        try:\n            makedirs_safe(b_pathdir, mode=0o700)\n            fd = os.open(lockfile, os.O_CREAT | os.O_EXCL)\n            os.close(fd)\n            first_process = True\n        except OSError as e:\n            if e.strerror != 'File exists':\n                raise\n\n    counter = 0\n    # if the lock is got by other process, wait until it's released\n    while os.path.exists(lockfile) and not first_process:\n        time.sleep(2 ** counter)\n        if counter >= 2:\n            raise AnsibleError(\"Password lookup cannot get the lock in 7 seconds, abort...\"\n                               \"This may caused by un-removed lockfile\"\n                               \"you can manually remove it from controller machine at %s and try again\" % lockfile)\n        counter += 1\n    return first_process, lockfile",
        "begin_line": 266,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password._release_lock#294",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password",
        "signature": "lib.ansible.plugins.lookup.password._release_lock(lockfile)",
        "snippet": "def _release_lock(lockfile):\n    \"\"\"Release the lock so other processes can read the password file.\"\"\"\n    if os.path.exists(lockfile):\n        os.remove(lockfile)",
        "begin_line": 294,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.password.LookupModule.run#301",
        "src_path": "lib/ansible/plugins/lookup/password.py",
        "class_name": "lib.ansible.plugins.lookup.password.LookupModule",
        "signature": "lib.ansible.plugins.lookup.password.LookupModule.run(self, terms, variables, **kwargs)",
        "snippet": "    def run(self, terms, variables, **kwargs):\n        ret = []\n\n        for term in terms:\n            relpath, params = _parse_parameters(term)\n            path = self._loader.path_dwim(relpath)\n            b_path = to_bytes(path, errors='surrogate_or_strict')\n            chars = _gen_candidate_chars(params['chars'])\n\n            changed = None\n            # make sure only one process finishes all the job first\n            first_process, lockfile = _get_lock(b_path)\n\n            content = _read_password_file(b_path)\n\n            if content is None or b_path == to_bytes('/dev/null'):\n                plaintext_password = random_password(params['length'], chars)\n                salt = None\n                changed = True\n            else:\n                plaintext_password, salt = _parse_content(content)\n\n            if params['encrypt'] and not salt:\n                changed = True\n                salt = random_salt()\n\n            if changed and b_path != to_bytes('/dev/null'):\n                content = _format_content(plaintext_password, salt, encrypt=params['encrypt'])\n                _write_password_file(b_path, content)\n\n            if first_process:\n                # let other processes continue\n                _release_lock(lockfile)\n\n            if params['encrypt']:\n                password = do_encrypt(plaintext_password, params['encrypt'], salt=salt)\n                ret.append(password)\n            else:\n                ret.append(plaintext_password)\n\n        return ret",
        "begin_line": 301,
        "end_line": 341,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.__init__#61",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.__init__(self, args, callback=None)",
        "snippet": "    def __init__(self, args, callback=None):\n        \"\"\"\n        Base init method for all command line programs\n        \"\"\"\n\n        if not args:\n            raise ValueError('A non-empty list for args is required')\n\n        self.args = args\n        self.parser = None\n        self.callback = callback\n\n        if C.DEVEL_WARNING and __version__.endswith('dev0'):\n            display.warning(\n                'You are running the development version of Ansible. You should only run Ansible from \"devel\" if '\n                'you are modifying the Ansible engine, or trying out features under development. This is a rapidly '\n                'changing source of code and can become unstable at any point.'\n            )",
        "begin_line": 61,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0034482758620689655,
            "pseudo_dstar_susp": 0.011363636363636364,
            "pseudo_tarantula_susp": 0.0017301038062283738,
            "pseudo_op2_susp": 0.011363636363636364,
            "pseudo_barinel_susp": 0.0017006802721088435
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.run#81",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.run(self)",
        "snippet": "    def run(self):\n        \"\"\"Run the ansible command\n\n        Subclasses must implement this method.  It does the actual work of\n        running an Ansible command.\n        \"\"\"\n        self.parse()\n\n        display.vv(to_text(opt_help.version(self.parser.prog)))\n\n        if C.CONFIG_FILE:\n            display.v(u\"Using %s as config file\" % to_text(C.CONFIG_FILE))\n        else:\n            display.v(u\"No config file found; using defaults\")\n\n        # warn about deprecated config options\n        for deprecated in C.config.DEPRECATED:\n            name = deprecated[0]\n            why = deprecated[1]['why']\n            if 'alternatives' in deprecated[1]:\n                alt = ', use %s instead' % deprecated[1]['alternatives']\n            else:\n                alt = ''\n            ver = deprecated[1]['version']\n            display.deprecated(\"%s option, %s %s\" % (name, why, alt), version=ver)",
        "begin_line": 81,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002531645569620253,
            "pseudo_dstar_susp": 0.006756756756756757,
            "pseudo_tarantula_susp": 0.0016051364365971107,
            "pseudo_op2_susp": 0.006756756756756757,
            "pseudo_barinel_susp": 0.0016051364365971107
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.split_vault_id#108",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.split_vault_id(vault_id)",
        "snippet": "    def split_vault_id(vault_id):\n        # return (before_@, after_@)\n        # if no @, return whole string as after_\n        if '@' not in vault_id:\n            return (None, vault_id)\n\n        parts = vault_id.split('@', 1)\n        ret = tuple(parts)\n        return ret",
        "begin_line": 108,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.build_vault_ids#119",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.build_vault_ids(vault_ids, vault_password_files=None, ask_vault_pass=None, create_new_password=None, auto_prompt=True)",
        "snippet": "    def build_vault_ids(vault_ids, vault_password_files=None,\n                        ask_vault_pass=None, create_new_password=None,\n                        auto_prompt=True):\n        vault_password_files = vault_password_files or []\n        vault_ids = vault_ids or []\n\n        # convert vault_password_files into vault_ids slugs\n        for password_file in vault_password_files:\n            id_slug = u'%s@%s' % (C.DEFAULT_VAULT_IDENTITY, password_file)\n\n            # note this makes --vault-id higher precedence than --vault-password-file\n            # if we want to intertwingle them in order probably need a cli callback to populate vault_ids\n            # used by --vault-id and --vault-password-file\n            vault_ids.append(id_slug)\n\n        # if an action needs an encrypt password (create_new_password=True) and we dont\n        # have other secrets setup, then automatically add a password prompt as well.\n        # prompts cant/shouldnt work without a tty, so dont add prompt secrets\n        if ask_vault_pass or (not vault_ids and auto_prompt):\n\n            id_slug = u'%s@%s' % (C.DEFAULT_VAULT_IDENTITY, u'prompt_ask_vault_pass')\n            vault_ids.append(id_slug)\n\n        return vault_ids",
        "begin_line": 119,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.setup_vault_secrets#146",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.setup_vault_secrets(loader, vault_ids, vault_password_files=None, ask_vault_pass=None, create_new_password=False, auto_prompt=True)",
        "snippet": "    def setup_vault_secrets(loader, vault_ids, vault_password_files=None,\n                            ask_vault_pass=None, create_new_password=False,\n                            auto_prompt=True):\n        # list of tuples\n        vault_secrets = []\n\n        # Depending on the vault_id value (including how --ask-vault-pass / --vault-password-file create a vault_id)\n        # we need to show different prompts. This is for compat with older Towers that expect a\n        # certain vault password prompt format, so 'promp_ask_vault_pass' vault_id gets the old format.\n        prompt_formats = {}\n\n        # If there are configured default vault identities, they are considered 'first'\n        # so we prepend them to vault_ids (from cli) here\n\n        vault_password_files = vault_password_files or []\n        if C.DEFAULT_VAULT_PASSWORD_FILE:\n            vault_password_files.append(C.DEFAULT_VAULT_PASSWORD_FILE)\n\n        if create_new_password:\n            prompt_formats['prompt'] = ['New vault password (%(vault_id)s): ',\n                                        'Confirm new vault password (%(vault_id)s): ']\n            # 2.3 format prompts for --ask-vault-pass\n            prompt_formats['prompt_ask_vault_pass'] = ['New Vault password: ',\n                                                       'Confirm New Vault password: ']\n        else:\n            prompt_formats['prompt'] = ['Vault password (%(vault_id)s): ']\n            # The format when we use just --ask-vault-pass needs to match 'Vault password:\\s*?$'\n            prompt_formats['prompt_ask_vault_pass'] = ['Vault password: ']\n\n        vault_ids = CLI.build_vault_ids(vault_ids,\n                                        vault_password_files,\n                                        ask_vault_pass,\n                                        create_new_password,\n                                        auto_prompt=auto_prompt)\n\n        for vault_id_slug in vault_ids:\n            vault_id_name, vault_id_value = CLI.split_vault_id(vault_id_slug)\n            if vault_id_value in ['prompt', 'prompt_ask_vault_pass']:\n\n                # --vault-id some_name@prompt_ask_vault_pass --vault-id other_name@prompt_ask_vault_pass will be a little\n                # confusing since it will use the old format without the vault id in the prompt\n                built_vault_id = vault_id_name or C.DEFAULT_VAULT_IDENTITY\n\n                # choose the prompt based on --vault-id=prompt or --ask-vault-pass. --ask-vault-pass\n                # always gets the old format for Tower compatibility.\n                # ie, we used --ask-vault-pass, so we need to use the old vault password prompt\n                # format since Tower needs to match on that format.\n                prompted_vault_secret = PromptVaultSecret(prompt_formats=prompt_formats[vault_id_value],\n                                                          vault_id=built_vault_id)\n\n                # a empty or invalid password from the prompt will warn and continue to the next\n                # without erroring globally\n                try:\n                    prompted_vault_secret.load()\n                except AnsibleError as exc:\n                    display.warning('Error in vault password prompt (%s): %s' % (vault_id_name, exc))\n                    raise\n\n                vault_secrets.append((built_vault_id, prompted_vault_secret))\n\n                # update loader with new secrets incrementally, so we can load a vault password\n                # that is encrypted with a vault secret provided earlier\n                loader.set_vault_secrets(vault_secrets)\n                continue\n\n            # assuming anything else is a password file\n            display.vvvvv('Reading vault password file: %s' % vault_id_value)\n            # read vault_pass from a file\n            file_vault_secret = get_file_vault_secret(filename=vault_id_value,\n                                                      vault_id=vault_id_name,\n                                                      loader=loader)\n\n            # an invalid password file will error globally\n            try:\n                file_vault_secret.load()\n            except AnsibleError as exc:\n                display.warning('Error in vault password file loading (%s): %s' % (vault_id_name, to_text(exc)))\n                raise\n\n            if vault_id_name:\n                vault_secrets.append((vault_id_name, file_vault_secret))\n            else:\n                vault_secrets.append((C.DEFAULT_VAULT_IDENTITY, file_vault_secret))\n\n            # update loader with as-yet-known vault secrets\n            loader.set_vault_secrets(vault_secrets)\n\n        return vault_secrets",
        "begin_line": 146,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.validate_conflicts#269",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.validate_conflicts(self, op, runas_opts=False, fork_opts=False)",
        "snippet": "    def validate_conflicts(self, op, runas_opts=False, fork_opts=False):\n        ''' check for conflicting options '''\n\n        if fork_opts:\n            if op.forks < 1:\n                self.parser.error(\"The number of processes (--forks) must be >= 1\")\n\n        return op",
        "begin_line": 269,
        "end_line": 276,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014492753623188406,
            "pseudo_dstar_susp": 0.003401360544217687,
            "pseudo_tarantula_susp": 0.006535947712418301,
            "pseudo_op2_susp": 0.003401360544217687,
            "pseudo_barinel_susp": 0.006535947712418301
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.init_parser#279",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.init_parser(self, usage='', desc=None, epilog=None)",
        "snippet": "    def init_parser(self, usage=\"\", desc=None, epilog=None):\n        \"\"\"\n        Create an options parser for most ansible scripts\n\n        Subclasses need to implement this method.  They will usually call the base class's\n        init_parser to create a basic version and then add their own options on top of that.\n\n        An implementation will look something like this::\n\n            def init_parser(self):\n                super(MyCLI, self).init_parser(usage=\"My Ansible CLI\", inventory_opts=True)\n                ansible.arguments.option_helpers.add_runas_options(self.parser)\n                self.parser.add_option('--my-option', dest='my_option', action='store')\n        \"\"\"\n        self.parser = opt_help.create_base_parser(os.path.basename(self.args[0]), usage=usage, desc=desc, epilog=epilog, )",
        "begin_line": 279,
        "end_line": 293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0037313432835820895,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.0018115942028985507,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.0018115942028985507
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.post_process_args#296",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        \"\"\"Process the command line args\n\n        Subclasses need to implement this method.  This method validates and transforms the command\n        line arguments.  It can be used to check whether conflicting values were given, whether filenames\n        exist, etc.\n\n        An implementation will look something like this::\n\n            def post_process_args(self, options):\n                options = super(MyCLI, self).post_process_args(options)\n                if options.addition and options.subtraction:\n                    raise AnsibleOptionsError('Only one of --addition and --subtraction can be specified')\n                if isinstance(options.listofhosts, string_types):\n                    options.listofhosts = string_types.split(',')\n                return options\n        \"\"\"\n\n        # process tags\n        if hasattr(options, 'tags') and not options.tags:\n            # optparse defaults does not do what's expected\n            options.tags = ['all']\n        if hasattr(options, 'tags') and options.tags:\n            tags = set()\n            for tag_set in options.tags:\n                for tag in tag_set.split(u','):\n                    tags.add(tag.strip())\n            options.tags = list(tags)\n\n        # process skip_tags\n        if hasattr(options, 'skip_tags') and options.skip_tags:\n            skip_tags = set()\n            for tag_set in options.skip_tags:\n                for tag in tag_set.split(u','):\n                    skip_tags.add(tag.strip())\n            options.skip_tags = list(skip_tags)\n\n        # process inventory options except for CLIs that require their own processing\n        if hasattr(options, 'inventory') and not self.SKIP_INVENTORY_DEFAULTS:\n\n            if options.inventory:\n\n                # should always be list\n                if isinstance(options.inventory, string_types):\n                    options.inventory = [options.inventory]\n\n                # Ensure full paths when needed\n                options.inventory = [unfrackpath(opt, follow=False) if ',' not in opt else opt for opt in options.inventory]\n            else:\n                options.inventory = C.DEFAULT_HOST_LIST\n\n        # Dup args set on the root parser and sub parsers results in the root parser ignoring the args. e.g. doing\n        # 'ansible-galaxy -vvv init' has no verbosity set but 'ansible-galaxy init -vvv' sets a level of 3. To preserve\n        # back compat with pre-argparse changes we manually scan and set verbosity based on the argv values.\n        if self.parser.prog in ['ansible-galaxy', 'ansible-vault'] and not options.verbosity:\n            verbosity_arg = next(iter([arg for arg in self.args if arg.startswith('-v')]), None)\n            if verbosity_arg:\n                display.deprecated(\"Setting verbosity before the arg sub command is deprecated, set the verbosity \"\n                                   \"after the sub command\", \"2.13\")\n                options.verbosity = verbosity_arg.count('v')\n\n        return options",
        "begin_line": 296,
        "end_line": 357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014492753623188406,
            "pseudo_dstar_susp": 0.019230769230769232,
            "pseudo_tarantula_susp": 0.006535947712418301,
            "pseudo_op2_susp": 0.019230769230769232,
            "pseudo_barinel_susp": 0.006535947712418301
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.parse#359",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.parse(self)",
        "snippet": "    def parse(self):\n        \"\"\"Parse the command line args\n\n        This method parses the command line arguments.  It uses the parser\n        stored in the self.parser attribute and saves the args and options in\n        context.CLIARGS.\n\n        Subclasses need to implement two helper methods, init_parser() and post_process_args() which\n        are called from this function before and after parsing the arguments.\n        \"\"\"\n        self.init_parser()\n\n        if HAS_ARGCOMPLETE:\n            argcomplete.autocomplete(self.parser)\n\n        options = self.parser.parse_args(self.args[1:])\n        options = self.post_process_args(options)\n        context._init_global_context(options)",
        "begin_line": 359,
        "end_line": 376,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004032258064516129,
            "pseudo_dstar_susp": 0.019230769230769232,
            "pseudo_tarantula_susp": 0.001976284584980237,
            "pseudo_op2_susp": 0.019230769230769232,
            "pseudo_barinel_susp": 0.001968503937007874
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.version_info#379",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.version_info(gitinfo=False)",
        "snippet": "    def version_info(gitinfo=False):\n        ''' return full ansible version info '''\n        if gitinfo:\n            # expensive call, user with care\n            ansible_version_string = opt_help.version()\n        else:\n            ansible_version_string = __version__\n        ansible_version = ansible_version_string.split()[0]\n        ansible_versions = ansible_version.split('.')\n        for counter in range(len(ansible_versions)):\n            if ansible_versions[counter] == \"\":\n                ansible_versions[counter] = 0\n            try:\n                ansible_versions[counter] = int(ansible_versions[counter])\n            except Exception:\n                pass\n        if len(ansible_versions) < 3:\n            for counter in range(len(ansible_versions), 3):\n                ansible_versions.append(0)\n        return {'string': ansible_version_string.strip(),\n                'full': ansible_version,\n                'major': ansible_versions[0],\n                'minor': ansible_versions[1],\n                'revision': ansible_versions[2]}",
        "begin_line": 379,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.__init__.CLI.pager#405",
        "src_path": "lib/ansible/cli/__init__.py",
        "class_name": "lib.ansible.cli.__init__.CLI",
        "signature": "lib.ansible.cli.__init__.CLI.pager(text)",
        "snippet": "    def pager(text):\n        ''' find reasonable way to display text '''\n        # this is a much simpler form of what is in pydoc.py\n        if not sys.stdout.isatty():\n            display.display(text, screen_only=True)\n        elif 'PAGER' in os.environ:\n            if sys.platform == 'win32':\n                display.display(text, screen_only=True)\n            else:\n                CLI.pager_pipe(text, os.environ['PAGER'])\n        else:\n            p = subprocess.Popen('less --version', shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            p.communicate()\n            if p.returncode == 0:\n                CLI.pager_pipe(text, 'less')\n            else:\n                display.display(text, screen_only=True)",
        "begin_line": 405,
        "end_line": 421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.__init__#25",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._cache = {}",
        "begin_line": 25,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.619047619047618e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.get#28",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.get(self, key)",
        "snippet": "    def get(self, key):\n        return self._cache.get(key)",
        "begin_line": 28,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.set#31",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.set(self, key, value)",
        "snippet": "    def set(self, key, value):\n        self._cache[key] = value",
        "begin_line": 31,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.keys#34",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.keys(self)",
        "snippet": "    def keys(self):\n        return self._cache.keys()",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.contains#37",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.contains(self, key)",
        "snippet": "    def contains(self, key):\n        return key in self._cache",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.memory.CacheModule.delete#40",
        "src_path": "lib/ansible/plugins/cache/memory.py",
        "class_name": "lib.ansible.plugins.cache.memory.CacheModule",
        "signature": "lib.ansible.plugins.cache.memory.CacheModule.delete(self, key)",
        "snippet": "    def delete(self, key):\n        del self._cache[key]",
        "begin_line": 40,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base._generic_g#31",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base",
        "signature": "lib.ansible.playbook.base._generic_g(prop_name, self)",
        "snippet": "def _generic_g(prop_name, self):\n    try:\n        value = self._attributes[prop_name]\n    except KeyError:\n        raise AttributeError(\"'%s' object has no attribute '%s'\" % (self.__class__.__name__, prop_name))\n\n    if value is Sentinel:\n        value = self._attr_defaults[prop_name]\n\n    return value",
        "begin_line": 31,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.893699158968702e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base._generic_g_method#43",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base",
        "signature": "lib.ansible.playbook.base._generic_g_method(prop_name, self)",
        "snippet": "def _generic_g_method(prop_name, self):\n    try:\n        if self._squashed:\n            return self._attributes[prop_name]\n        method = \"_get_attr_%s\" % prop_name\n        return getattr(self, method)()\n    except KeyError:\n        raise AttributeError(\"'%s' object has no attribute '%s'\" % (self.__class__.__name__, prop_name))",
        "begin_line": 43,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base._generic_g_parent#53",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base",
        "signature": "lib.ansible.playbook.base._generic_g_parent(prop_name, self)",
        "snippet": "def _generic_g_parent(prop_name, self):\n    try:\n        if self._squashed or self._finalized:\n            value = self._attributes[prop_name]\n        else:\n            try:\n                value = self._get_parent_attribute(prop_name)\n            except AttributeError:\n                value = self._attributes[prop_name]\n    except KeyError:\n        raise AttributeError(\"'%s' object has no attribute '%s'\" % (self.__class__.__name__, prop_name))\n\n    if value is Sentinel:\n        value = self._attr_defaults[prop_name]\n\n    return value",
        "begin_line": 53,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base._generic_s#71",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base",
        "signature": "lib.ansible.playbook.base._generic_s(prop_name, self, value)",
        "snippet": "def _generic_s(prop_name, self, value):\n    self._attributes[prop_name] = value",
        "begin_line": 71,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.86766018817389e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base._generic_d#75",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base",
        "signature": "lib.ansible.playbook.base._generic_d(prop_name, self)",
        "snippet": "def _generic_d(prop_name, self):\n    del self._attributes[prop_name]",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.BaseMeta.__new__#86",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.BaseMeta",
        "signature": "lib.ansible.playbook.base.BaseMeta.__new__(cls, name, parents, dct)",
        "snippet": "    def __new__(cls, name, parents, dct):\n        def _create_attrs(src_dict, dst_dict):\n            '''\n            Helper method which creates the attributes based on those in the\n            source dictionary of attributes. This also populates the other\n            attributes used to keep track of these attributes and via the\n            getter/setter/deleter methods.\n            '''\n            keys = list(src_dict.keys())\n            for attr_name in keys:\n                value = src_dict[attr_name]\n                if isinstance(value, Attribute):\n                    if attr_name.startswith('_'):\n                        attr_name = attr_name[1:]\n\n                    # here we selectively assign the getter based on a few\n                    # things, such as whether we have a _get_attr_<name>\n                    # method, or if the attribute is marked as not inheriting\n                    # its value from a parent object\n                    method = \"_get_attr_%s\" % attr_name\n                    if method in src_dict or method in dst_dict:\n                        getter = partial(_generic_g_method, attr_name)\n                    elif ('_get_parent_attribute' in dst_dict or '_get_parent_attribute' in src_dict) and value.inherit:\n                        getter = partial(_generic_g_parent, attr_name)\n                    else:\n                        getter = partial(_generic_g, attr_name)\n\n                    setter = partial(_generic_s, attr_name)\n                    deleter = partial(_generic_d, attr_name)\n\n                    dst_dict[attr_name] = property(getter, setter, deleter)\n                    dst_dict['_valid_attrs'][attr_name] = value\n                    dst_dict['_attributes'][attr_name] = Sentinel\n                    dst_dict['_attr_defaults'][attr_name] = value.default\n\n                    if value.alias is not None:\n                        dst_dict[value.alias] = property(getter, setter, deleter)\n                        dst_dict['_valid_attrs'][value.alias] = value\n                        dst_dict['_alias_attrs'][value.alias] = attr_name\n\n        def _process_parents(parents, dst_dict):\n            '''\n            Helper method which creates attributes from all parent objects\n            recursively on through grandparent objects\n            '''\n            for parent in parents:\n                if hasattr(parent, '__dict__'):\n                    _create_attrs(parent.__dict__, dst_dict)\n                    new_dst_dict = parent.__dict__.copy()\n                    new_dst_dict.update(dst_dict)\n                    _process_parents(parent.__bases__, new_dst_dict)\n\n        # create some additional class attributes\n        dct['_attributes'] = {}\n        dct['_attr_defaults'] = {}\n        dct['_valid_attrs'] = {}\n        dct['_alias_attrs'] = {}\n\n        # now create the attributes based on the FieldAttributes\n        # available, including from parent (and grandparent) objects\n        _create_attrs(dct, dct)\n        _process_parents(parents, dct)\n\n        return super(BaseMeta, cls).__new__(cls, name, parents, dct)",
        "begin_line": 86,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.BaseMeta._create_attrs#87",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.BaseMeta",
        "signature": "lib.ansible.playbook.base.BaseMeta._create_attrs(src_dict, dst_dict)",
        "snippet": "        def _create_attrs(src_dict, dst_dict):\n            '''\n            Helper method which creates the attributes based on those in the\n            source dictionary of attributes. This also populates the other\n            attributes used to keep track of these attributes and via the\n            getter/setter/deleter methods.\n            '''\n            keys = list(src_dict.keys())\n            for attr_name in keys:\n                value = src_dict[attr_name]\n                if isinstance(value, Attribute):\n                    if attr_name.startswith('_'):\n                        attr_name = attr_name[1:]\n\n                    # here we selectively assign the getter based on a few\n                    # things, such as whether we have a _get_attr_<name>\n                    # method, or if the attribute is marked as not inheriting\n                    # its value from a parent object\n                    method = \"_get_attr_%s\" % attr_name\n                    if method in src_dict or method in dst_dict:\n                        getter = partial(_generic_g_method, attr_name)\n                    elif ('_get_parent_attribute' in dst_dict or '_get_parent_attribute' in src_dict) and value.inherit:\n                        getter = partial(_generic_g_parent, attr_name)\n                    else:\n                        getter = partial(_generic_g, attr_name)\n\n                    setter = partial(_generic_s, attr_name)\n                    deleter = partial(_generic_d, attr_name)\n\n                    dst_dict[attr_name] = property(getter, setter, deleter)\n                    dst_dict['_valid_attrs'][attr_name] = value\n                    dst_dict['_attributes'][attr_name] = Sentinel\n                    dst_dict['_attr_defaults'][attr_name] = value.default\n\n                    if value.alias is not None:\n                        dst_dict[value.alias] = property(getter, setter, deleter)\n                        dst_dict['_valid_attrs'][value.alias] = value\n                        dst_dict['_alias_attrs'][value.alias] = attr_name",
        "begin_line": 87,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.BaseMeta._process_parents#126",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.BaseMeta",
        "signature": "lib.ansible.playbook.base.BaseMeta._process_parents(parents, dst_dict)",
        "snippet": "        def _process_parents(parents, dst_dict):\n            '''\n            Helper method which creates attributes from all parent objects\n            recursively on through grandparent objects\n            '''\n            for parent in parents:\n                if hasattr(parent, '__dict__'):\n                    _create_attrs(parent.__dict__, dst_dict)\n                    new_dst_dict = parent.__dict__.copy()\n                    new_dst_dict.update(dst_dict)\n                    _process_parents(parent.__bases__, new_dst_dict)",
        "begin_line": 126,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.__init__#154",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.__init__(self)",
        "snippet": "    def __init__(self):\n\n        # initialize the data loader and variable manager, which will be provided\n        # later when the object is actually loaded\n        self._loader = None\n        self._variable_manager = None\n\n        # other internal params\n        self._validated = False\n        self._squashed = False\n        self._finalized = False\n\n        # every object gets a random uuid:\n        self._uuid = get_unique_id()\n\n        # we create a copy of the attributes here due to the fact that\n        # it was initialized as a class param in the meta class, so we\n        # need a unique object here (all members contained within are\n        # unique already).\n        self._attributes = self.__class__._attributes.copy()\n        self._attr_defaults = self.__class__._attr_defaults.copy()\n        for key, value in self._attr_defaults.items():\n            if callable(value):\n                self._attr_defaults[key] = value()\n\n        # and init vars, avoid using defaults in field declaration as it lives across plays\n        self.vars = dict()",
        "begin_line": 154,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.86766018817389e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.dump_me#182",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.dump_me(self, depth=0)",
        "snippet": "    def dump_me(self, depth=0):\n        ''' this is never called from production code, it is here to be used when debugging as a 'complex print' '''\n        if depth == 0:\n            display.debug(\"DUMPING OBJECT ------------------------------------------------------\")\n        display.debug(\"%s- %s (%s, id=%s)\" % (\" \" * depth, self.__class__.__name__, self, id(self)))\n        if hasattr(self, '_parent') and self._parent:\n            self._parent.dump_me(depth + 2)\n            dep_chain = self._parent.get_dep_chain()\n            if dep_chain:\n                for dep in dep_chain:\n                    dep.dump_me(depth + 2)\n        if hasattr(self, '_play') and self._play:\n            self._play.dump_me(depth + 2)",
        "begin_line": 182,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.preprocess_data#196",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        ''' infrequently used method to do some pre-processing of legacy terms '''\n        return ds",
        "begin_line": 196,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.896551724137931e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.load_data#200",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.load_data(self, ds, variable_manager=None, loader=None)",
        "snippet": "    def load_data(self, ds, variable_manager=None, loader=None):\n        ''' walk the input datastructure and assign any values '''\n\n        if ds is None:\n            raise AnsibleAssertionError('ds (%s) should not be None but it is.' % ds)\n\n        # cache the datastructure internally\n        setattr(self, '_ds', ds)\n\n        # the variable manager class is used to manage and merge variables\n        # down to a single dictionary for reference in templating, etc.\n        self._variable_manager = variable_manager\n\n        # the data loader class is used to parse data from strings and files\n        if loader is not None:\n            self._loader = loader\n        else:\n            self._loader = DataLoader()\n\n        # call the preprocess_data() function to massage the data into\n        # something we can more easily parse, and then call the validation\n        # function on it to ensure there are no incorrect key values\n        ds = self.preprocess_data(ds)\n        self._validate_attributes(ds)\n\n        # Walk all attributes in the class. We sort them based on their priority\n        # so that certain fields can be loaded before others, if they are dependent.\n        for name, attr in sorted(iteritems(self._valid_attrs), key=operator.itemgetter(1)):\n            # copy the value over unless a _load_field method is defined\n            target_name = name\n            if name in self._alias_attrs:\n                target_name = self._alias_attrs[name]\n            if name in ds:\n                method = getattr(self, '_load_%s' % name, None)\n                if method:\n                    self._attributes[target_name] = method(name, ds[name])\n                else:\n                    self._attributes[target_name] = ds[name]\n\n        # run early, non-critical validation\n        self.validate()\n\n        # return the constructed object\n        return self",
        "begin_line": 200,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.get_ds#245",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.get_ds(self)",
        "snippet": "    def get_ds(self):\n        try:\n            return getattr(self, '_ds')\n        except AttributeError:\n            return None",
        "begin_line": 245,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.get_loader#251",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.get_loader(self)",
        "snippet": "    def get_loader(self):\n        return self._loader",
        "begin_line": 251,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.get_variable_manager#254",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.get_variable_manager(self)",
        "snippet": "    def get_variable_manager(self):\n        return self._variable_manager",
        "begin_line": 254,
        "end_line": 255,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase._validate_attributes#264",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase._validate_attributes(self, ds)",
        "snippet": "    def _validate_attributes(self, ds):\n        '''\n        Ensures that there are no keys in the datastructure which do\n        not map to attributes for this object.\n        '''\n\n        valid_attrs = frozenset(self._valid_attrs.keys())\n        for key in ds:\n            if key not in valid_attrs:\n                raise AnsibleParserError(\"'%s' is not a valid attribute for a %s\" % (key, self.__class__.__name__), obj=ds)",
        "begin_line": 264,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.validate#275",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.validate(self, all_vars=None)",
        "snippet": "    def validate(self, all_vars=None):\n        ''' validation that is done at parse time, not load time '''\n        all_vars = {} if all_vars is None else all_vars\n\n        if not self._validated:\n            # walk all fields in the object\n            for (name, attribute) in iteritems(self._valid_attrs):\n\n                if name in self._alias_attrs:\n                    name = self._alias_attrs[name]\n\n                # run validator only if present\n                method = getattr(self, '_validate_%s' % name, None)\n                if method:\n                    method(attribute, name, getattr(self, name))\n                else:\n                    # and make sure the attribute is of the type it should be\n                    value = self._attributes[name]\n                    if value is not None:\n                        if attribute.isa == 'string' and isinstance(value, (list, dict)):\n                            raise AnsibleParserError(\n                                \"The field '%s' is supposed to be a string type,\"\n                                \" however the incoming data structure is a %s\" % (name, type(value)), obj=self.get_ds()\n                            )\n\n        self._validated = True",
        "begin_line": 275,
        "end_line": 300,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.squash#302",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.squash(self)",
        "snippet": "    def squash(self):\n        '''\n        Evaluates all attributes and sets them to the evaluated version,\n        so that all future accesses of attributes do not need to evaluate\n        parent attributes.\n        '''\n        if not self._squashed:\n            for name in self._valid_attrs.keys():\n                self._attributes[name] = getattr(self, name)\n            self._squashed = True",
        "begin_line": 302,
        "end_line": 311,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.copy#313",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.copy(self)",
        "snippet": "    def copy(self):\n        '''\n        Create a copy of this object and return it.\n        '''\n\n        new_me = self.__class__()\n\n        for name in self._valid_attrs.keys():\n            if name in self._alias_attrs:\n                continue\n            new_me._attributes[name] = shallowcopy(self._attributes[name])\n            new_me._attr_defaults[name] = shallowcopy(self._attr_defaults[name])\n\n        new_me._loader = self._loader\n        new_me._variable_manager = self._variable_manager\n        new_me._validated = self._validated\n        new_me._finalized = self._finalized\n        new_me._uuid = self._uuid\n\n        # if the ds value was set on the object, copy it to the new copy too\n        if hasattr(self, '_ds'):\n            new_me._ds = self._ds\n\n        return new_me",
        "begin_line": 313,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.get_validated_value#338",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.get_validated_value(self, name, attribute, value, templar)",
        "snippet": "    def get_validated_value(self, name, attribute, value, templar):\n        if attribute.isa == 'string':\n            value = to_text(value)\n        elif attribute.isa == 'int':\n            value = int(value)\n        elif attribute.isa == 'float':\n            value = float(value)\n        elif attribute.isa == 'bool':\n            value = boolean(value, strict=True)\n        elif attribute.isa == 'percent':\n            # special value, which may be an integer or float\n            # with an optional '%' at the end\n            if isinstance(value, string_types) and '%' in value:\n                value = value.replace('%', '')\n            value = float(value)\n        elif attribute.isa == 'list':\n            if value is None:\n                value = []\n            elif not isinstance(value, list):\n                value = [value]\n            if attribute.listof is not None:\n                for item in value:\n                    if not isinstance(item, attribute.listof):\n                        raise AnsibleParserError(\"the field '%s' should be a list of %s, \"\n                                                 \"but the item '%s' is a %s\" % (name, attribute.listof, item, type(item)), obj=self.get_ds())\n                    elif attribute.required and attribute.listof == string_types:\n                        if item is None or item.strip() == \"\":\n                            raise AnsibleParserError(\"the field '%s' is required, and cannot have empty values\" % (name,), obj=self.get_ds())\n        elif attribute.isa == 'set':\n            if value is None:\n                value = set()\n            elif not isinstance(value, (list, set)):\n                if isinstance(value, string_types):\n                    value = value.split(',')\n                else:\n                    # Making a list like this handles strings of\n                    # text and bytes properly\n                    value = [value]\n            if not isinstance(value, set):\n                value = set(value)\n        elif attribute.isa == 'dict':\n            if value is None:\n                value = dict()\n            elif not isinstance(value, dict):\n                raise TypeError(\"%s is not a dictionary\" % value)\n        elif attribute.isa == 'class':\n            if not isinstance(value, attribute.class_type):\n                raise TypeError(\"%s is not a valid %s (got a %s instead)\" % (name, attribute.class_type, type(value)))\n            value.post_validate(templar=templar)\n        return value",
        "begin_line": 338,
        "end_line": 387,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.post_validate#389",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.post_validate(self, templar)",
        "snippet": "    def post_validate(self, templar):\n        '''\n        we can't tell that everything is of the right type until we have\n        all the variables.  Run basic types (from isa) as well as\n        any _post_validate_<foo> functions.\n        '''\n\n        # save the omit value for later checking\n        omit_value = templar.available_variables.get('omit')\n\n        for (name, attribute) in iteritems(self._valid_attrs):\n\n            if attribute.static:\n                value = getattr(self, name)\n\n                # we don't template 'vars' but allow template as values for later use\n                if name not in ('vars',) and templar.is_template(value):\n                    display.warning('\"%s\" is not templatable, but we found: %s, '\n                                    'it will not be templated and will be used \"as is\".' % (name, value))\n                continue\n\n            if getattr(self, name) is None:\n                if not attribute.required:\n                    continue\n                else:\n                    raise AnsibleParserError(\"the field '%s' is required but was not set\" % name)\n            elif not attribute.always_post_validate and self.__class__.__name__ not in ('Task', 'Handler', 'PlayContext'):\n                # Intermediate objects like Play() won't have their fields validated by\n                # default, as their values are often inherited by other objects and validated\n                # later, so we don't want them to fail out early\n                continue\n\n            try:\n                # Run the post-validator if present. These methods are responsible for\n                # using the given templar to template the values, if required.\n                method = getattr(self, '_post_validate_%s' % name, None)\n                if method:\n                    value = method(attribute, getattr(self, name), templar)\n                elif attribute.isa == 'class':\n                    value = getattr(self, name)\n                else:\n                    # if the attribute contains a variable, template it now\n                    value = templar.template(getattr(self, name))\n\n                # if this evaluated to the omit value, set the value back to\n                # the default specified in the FieldAttribute and move on\n                if omit_value is not None and value == omit_value:\n                    if callable(attribute.default):\n                        setattr(self, name, attribute.default())\n                    else:\n                        setattr(self, name, attribute.default)\n                    continue\n\n                # and make sure the attribute is of the type it should be\n                if value is not None:\n                    value = self.get_validated_value(name, attribute, value, templar)\n\n                # and assign the massaged value back to the attribute field\n                setattr(self, name, value)\n            except (TypeError, ValueError) as e:\n                value = getattr(self, name)\n                raise AnsibleParserError(\"the field '%s' has an invalid value (%s), and could not be converted to an %s.\"\n                                         \"The error was: %s\" % (name, value, attribute.isa, e), obj=self.get_ds(), orig_exc=e)\n            except (AnsibleUndefinedVariable, UndefinedError) as e:\n                if templar._fail_on_undefined_errors and name != 'name':\n                    if name == 'args':\n                        msg = \"The task includes an option with an undefined variable. The error was: %s\" % (to_native(e))\n                    else:\n                        msg = \"The field '%s' has an invalid value, which includes an undefined variable. The error was: %s\" % (name, to_native(e))\n                    raise AnsibleParserError(msg, obj=self.get_ds(), orig_exc=e)\n\n        self._finalized = True",
        "begin_line": 389,
        "end_line": 460,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase._load_vars#462",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase._load_vars(self, attr, ds)",
        "snippet": "    def _load_vars(self, attr, ds):\n        '''\n        Vars in a play can be specified either as a dictionary directly, or\n        as a list of dictionaries. If the later, this method will turn the\n        list into a single dictionary.\n        '''\n\n        def _validate_variable_keys(ds):\n            for key in ds:\n                if not isidentifier(key):\n                    raise TypeError(\"'%s' is not a valid variable name\" % key)\n\n        try:\n            if isinstance(ds, dict):\n                _validate_variable_keys(ds)\n                return combine_vars(self.vars, ds)\n            elif isinstance(ds, list):\n                all_vars = self.vars\n                for item in ds:\n                    if not isinstance(item, dict):\n                        raise ValueError\n                    _validate_variable_keys(item)\n                    all_vars = combine_vars(all_vars, item)\n                return all_vars\n            elif ds is None:\n                return {}\n            else:\n                raise ValueError\n        except ValueError as e:\n            raise AnsibleParserError(\"Vars in a %s must be specified as a dictionary, or a list of dictionaries\" % self.__class__.__name__,\n                                     obj=ds, orig_exc=e)\n        except TypeError as e:\n            raise AnsibleParserError(\"Invalid variable name in vars specified for %s: %s\" % (self.__class__.__name__, e), obj=ds, orig_exc=e)",
        "begin_line": 462,
        "end_line": 494,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase._validate_variable_keys#469",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase._validate_variable_keys(ds)",
        "snippet": "        def _validate_variable_keys(ds):\n            for key in ds:\n                if not isidentifier(key):\n                    raise TypeError(\"'%s' is not a valid variable name\" % key)",
        "begin_line": 469,
        "end_line": 472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase._extend_value#496",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase._extend_value(self, value, new_value, prepend=False)",
        "snippet": "    def _extend_value(self, value, new_value, prepend=False):\n        '''\n        Will extend the value given with new_value (and will turn both\n        into lists if they are not so already). The values are run through\n        a set to remove duplicate values.\n        '''\n\n        if not isinstance(value, list):\n            value = [value]\n        if not isinstance(new_value, list):\n            new_value = [new_value]\n\n        # Due to where _extend_value may run for some attributes\n        # it is possible to end up with Sentinel in the list of values\n        # ensure we strip them\n        value = [v for v in value if v is not Sentinel]\n        new_value = [v for v in new_value if v is not Sentinel]\n\n        if prepend:\n            combined = new_value + value\n        else:\n            combined = value + new_value\n\n        return [i for i, _ in itertools.groupby(combined) if i is not None]",
        "begin_line": 496,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.499625018749062e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.dump_attrs#521",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.dump_attrs(self)",
        "snippet": "    def dump_attrs(self):\n        '''\n        Dumps all attributes to a dictionary\n        '''\n        attrs = {}\n        for (name, attribute) in iteritems(self._valid_attrs):\n            attr = getattr(self, name)\n            if attribute.isa == 'class' and hasattr(attr, 'serialize'):\n                attrs[name] = attr.serialize()\n            else:\n                attrs[name] = attr\n        return attrs",
        "begin_line": 521,
        "end_line": 532,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.serialize#548",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.serialize(self)",
        "snippet": "    def serialize(self):\n        '''\n        Serializes the object derived from the base object into\n        a dictionary of values. This only serializes the field\n        attributes for the object, so this may need to be overridden\n        for any classes which wish to add additional items not stored\n        as field attributes.\n        '''\n\n        repr = self.dump_attrs()\n\n        # serialize the uuid field\n        repr['uuid'] = self._uuid\n        repr['finalized'] = self._finalized\n        repr['squashed'] = self._squashed\n\n        return repr",
        "begin_line": 548,
        "end_line": 564,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.base.FieldAttributeBase.deserialize#566",
        "src_path": "lib/ansible/playbook/base.py",
        "class_name": "lib.ansible.playbook.base.FieldAttributeBase",
        "signature": "lib.ansible.playbook.base.FieldAttributeBase.deserialize(self, data)",
        "snippet": "    def deserialize(self, data):\n        '''\n        Given a dictionary of values, load up the field attributes for\n        this object. As with serialize(), if there are any non-field\n        attribute data members, this method will need to be overridden\n        and extended.\n        '''\n\n        if not isinstance(data, dict):\n            raise AnsibleAssertionError('data (%s) should be a dict but is a %s' % (data, type(data)))\n\n        for (name, attribute) in iteritems(self._valid_attrs):\n            if name in data:\n                setattr(self, name, data[name])\n            else:\n                if callable(attribute.default):\n                    setattr(self, name, attribute.default())\n                else:\n                    setattr(self, name, attribute.default)\n\n        # restore the UUID field\n        setattr(self, '_uuid', data.get('uuid'))\n        self._finalized = data.get('finalized', False)\n        self._squashed = data.get('squashed', False)",
        "begin_line": 566,
        "end_line": 589,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.raw.ActionModule.run#26",
        "src_path": "lib/ansible/plugins/action/raw.py",
        "class_name": "lib.ansible.plugins.action.raw.ActionModule",
        "signature": "lib.ansible.plugins.action.raw.ActionModule.run(self, tmp=None, task_vars=None)",
        "snippet": "    def run(self, tmp=None, task_vars=None):\n        if task_vars is None:\n            task_vars = dict()\n\n        if self._task.environment and any(self._task.environment):\n            self._display.warning('raw module does not support the environment keyword')\n\n        result = super(ActionModule, self).run(tmp, task_vars)\n        del tmp  # tmp no longer has any effect\n\n        if self._play_context.check_mode:\n            # in --check mode, always skip this module execution\n            result['skipped'] = True\n            return result\n\n        executable = self._task.args.get('executable', False)\n        result.update(self._low_level_execute_command(self._task.args.get('_raw_params'), executable=executable))\n\n        result['changed'] = True\n\n        if 'rc' in result and result['rc'] != 0:\n            result['failed'] = True\n            result['msg'] = 'non-zero return code'\n\n        return result",
        "begin_line": 26,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.dict.LookupModule.run#63",
        "src_path": "lib/ansible/plugins/lookup/dict.py",
        "class_name": "lib.ansible.plugins.lookup.dict.LookupModule",
        "signature": "lib.ansible.plugins.lookup.dict.LookupModule.run(self, terms, variables=None, **kwargs)",
        "snippet": "    def run(self, terms, variables=None, **kwargs):\n\n        # FIXME: can remove once with_ special case is removed\n        if not isinstance(terms, list):\n            terms = [terms]\n\n        results = []\n        for term in terms:\n            # Expect any type of Mapping, notably hostvars\n            if not isinstance(term, Mapping):\n                raise AnsibleError(\"with_dict expects a dict\")\n\n            results.extend(self._flatten_hash_to_list(term))\n        return results",
        "begin_line": 63,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.__init__#37",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.__init__(self, file_name=None, vault_secrets=None)",
        "snippet": "    def __init__(self, file_name=None, vault_secrets=None):\n        self._ansible_file_name = file_name\n        super(AnsibleConstructor, self).__init__()\n        self._vaults = {}\n        self.vault_secrets = vault_secrets or []\n        self._vaults['default'] = VaultLib(secrets=self.vault_secrets)",
        "begin_line": 37,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.998880179171333e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_map#44",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_map(self, node)",
        "snippet": "    def construct_yaml_map(self, node):\n        data = AnsibleMapping()\n        yield data\n        value = self.construct_mapping(node)\n        data.update(value)\n        data.ansible_pos = self._node_position_info(node)",
        "begin_line": 44,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.057163020465773e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_mapping#51",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_mapping(self, node, deep=False)",
        "snippet": "    def construct_mapping(self, node, deep=False):\n        # Most of this is from yaml.constructor.SafeConstructor.  We replicate\n        # it here so that we can warn users when they have duplicate dict keys\n        # (pyyaml silently allows overwriting keys)\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(None, None,\n                                   \"expected a mapping node, but found %s\" % node.id,\n                                   node.start_mark)\n        self.flatten_mapping(node)\n        mapping = AnsibleMapping()\n\n        # Add our extra information to the returned value\n        mapping.ansible_pos = self._node_position_info(node)\n\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            try:\n                hash(key)\n            except TypeError as exc:\n                raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n                                       \"found unacceptable key (%s)\" % exc, key_node.start_mark)\n\n            if key in mapping:\n                msg = (u'While constructing a mapping from {1}, line {2}, column {3}, found a duplicate dict key ({0}).'\n                       u' Using last defined value only.'.format(key, *mapping.ansible_pos))\n                if C.DUPLICATE_YAML_DICT_KEY == 'warn':\n                    display.warning(msg)\n                elif C.DUPLICATE_YAML_DICT_KEY == 'error':\n                    raise ConstructorError(context=None, context_mark=None,\n                                           problem=to_native(msg),\n                                           problem_mark=node.start_mark,\n                                           note=None)\n                else:\n                    # when 'ignore'\n                    display.debug(msg)\n\n            value = self.construct_object(value_node, deep=deep)\n            mapping[key] = value\n\n        return mapping",
        "begin_line": 51,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.057163020465773e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_str#92",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_str(self, node)",
        "snippet": "    def construct_yaml_str(self, node):\n        # Override the default string handling function\n        # to always return unicode objects\n        value = self.construct_scalar(node)\n        ret = AnsibleUnicode(value)\n\n        ret.ansible_pos = self._node_position_info(node)\n\n        return ret",
        "begin_line": 92,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.033338022225348e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_vault_encrypted_unicode#102",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_vault_encrypted_unicode(self, node)",
        "snippet": "    def construct_vault_encrypted_unicode(self, node):\n        value = self.construct_scalar(node)\n        b_ciphertext_data = to_bytes(value)\n        # could pass in a key id here to choose the vault to associate with\n        # TODO/FIXME: plugin vault selector\n        vault = self._vaults['default']\n        if vault.secrets is None:\n            raise ConstructorError(context=None, context_mark=None,\n                                   problem=\"found !vault but no vault password provided\",\n                                   problem_mark=node.start_mark,\n                                   note=None)\n        ret = AnsibleVaultEncryptedUnicode(b_ciphertext_data)\n        ret.vault = vault\n        return ret",
        "begin_line": 102,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_seq#117",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor.construct_yaml_seq(self, node)",
        "snippet": "    def construct_yaml_seq(self, node):\n        data = AnsibleSequence()\n        yield data\n        data.extend(self.construct_sequence(node))\n        data.ansible_pos = self._node_position_info(node)",
        "begin_line": 117,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.133176403452457e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor._node_position_info#126",
        "src_path": "lib/ansible/parsing/yaml/constructor.py",
        "class_name": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor",
        "signature": "lib.ansible.parsing.yaml.constructor.AnsibleConstructor._node_position_info(self, node)",
        "snippet": "    def _node_position_info(self, node):\n        # the line number where the previous token has ended (plus empty lines)\n        # Add one so that the first line is line 1 rather than line 0\n        column = node.start_mark.column + 1\n        line = node.start_mark.line + 1\n\n        # in some cases, we may have pre-read the data and then\n        # passed it to the load() call for YAML, in which case we\n        # want to override the default datasource (which would be\n        # '<string>') to the actual filename we read in\n        datasource = self._ansible_file_name or node.start_mark.name\n\n        return (datasource, line, column)",
        "begin_line": 126,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.033338022225348e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.__init__#340",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self.always_pipeline_modules = True\n        self.has_native_async = True\n\n        self.runspace = None\n        self.host = None\n\n        self._shell_type = 'powershell'\n        super(Connection, self).__init__(*args, **kwargs)\n\n        if not C.DEFAULT_DEBUG:\n            logging.getLogger('pypsrp').setLevel(logging.WARNING)\n            logging.getLogger('requests_credssp').setLevel(logging.INFO)\n            logging.getLogger('urllib3').setLevel(logging.INFO)",
        "begin_line": 340,
        "end_line": 353,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection._connect#355",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection._connect(self)",
        "snippet": "    def _connect(self):\n        if not HAS_PYPSRP:\n            raise AnsibleError(\"pypsrp or dependencies are not installed: %s\"\n                               % to_native(PYPSRP_IMP_ERR))\n        super(Connection, self)._connect()\n        self._build_kwargs()\n        display.vvv(\"ESTABLISH PSRP CONNECTION FOR USER: %s ON PORT %s TO %s\" %\n                    (self._psrp_user, self._psrp_port, self._psrp_host),\n                    host=self._psrp_host)\n\n        if not self.runspace:\n            connection = WSMan(**self._psrp_conn_kwargs)\n\n            # create our psuedo host to capture the exit code and host output\n            host_ui = PSHostUserInterface()\n            self.host = PSHost(None, None, False, \"Ansible PSRP Host\", None,\n                               host_ui, None)\n\n            self.runspace = RunspacePool(\n                connection, host=self.host,\n                configuration_name=self._psrp_configuration_name\n            )\n            display.vvvvv(\n                \"PSRP OPEN RUNSPACE: auth=%s configuration=%s endpoint=%s\" %\n                (self._psrp_auth, self._psrp_configuration_name,\n                 connection.transport.endpoint), host=self._psrp_host\n            )\n            try:\n                self.runspace.open()\n            except AuthenticationError as e:\n                raise AnsibleConnectionFailure(\"failed to authenticate with \"\n                                               \"the server: %s\" % to_native(e))\n            except WinRMError as e:\n                raise AnsibleConnectionFailure(\n                    \"psrp connection failure during runspace open: %s\"\n                    % to_native(e)\n                )\n            except (ConnectionError, ConnectTimeout) as e:\n                raise AnsibleConnectionFailure(\n                    \"Failed to connect to the host via PSRP: %s\"\n                    % to_native(e)\n                )\n\n            self._connected = True\n        return self",
        "begin_line": 355,
        "end_line": 399,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.reset#401",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.reset(self)",
        "snippet": "    def reset(self):\n        display.vvvvv(\"PSRP: Reset Connection\", host=self._psrp_host)\n        self.runspace = None\n        self._connect()",
        "begin_line": 401,
        "end_line": 404,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.exec_command#406",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.exec_command(self, cmd, in_data=None, sudoable=True)",
        "snippet": "    def exec_command(self, cmd, in_data=None, sudoable=True):\n        super(Connection, self).exec_command(cmd, in_data=in_data,\n                                             sudoable=sudoable)\n\n        if cmd.startswith(\" \".join(_common_args) + \" -EncodedCommand\"):\n            # This is a PowerShell script encoded by the shell plugin, we will\n            # decode the script and execute it in the runspace instead of\n            # starting a new interpreter to save on time\n            b_command = base64.b64decode(cmd.split(\" \")[-1])\n            script = to_text(b_command, 'utf-16-le')\n            in_data = to_text(in_data, errors=\"surrogate_or_strict\", nonstring=\"passthru\")\n\n            if in_data and in_data.startswith(u\"#!\"):\n                # ANSIBALLZ wrapper, we need to get the interpreter and execute\n                # that as the script - note this won't work as basic.py relies\n                # on packages not available on Windows, once fixed we can enable\n                # this path\n                interpreter = to_native(in_data.splitlines()[0][2:])\n                # script = \"$input | &'%s' -\" % interpreter\n                # in_data = to_text(in_data)\n                raise AnsibleError(\"cannot run the interpreter '%s' on the psrp \"\n                                   \"connection plugin\" % interpreter)\n\n            # call build_module_command to get the bootstrap wrapper text\n            bootstrap_wrapper = self._shell.build_module_command('', '', '')\n            if bootstrap_wrapper == cmd:\n                # Do not display to the user each invocation of the bootstrap wrapper\n                display.vvv(\"PSRP: EXEC (via pipeline wrapper)\")\n            else:\n                display.vvv(\"PSRP: EXEC %s\" % script, host=self._psrp_host)\n        else:\n            # In other cases we want to execute the cmd as the script. We add on the 'exit $LASTEXITCODE' to ensure the\n            # rc is propagated back to the connection plugin.\n            script = to_text(u\"%s\\nexit $LASTEXITCODE\" % cmd)\n            display.vvv(u\"PSRP: EXEC %s\" % script, host=self._psrp_host)\n\n        rc, stdout, stderr = self._exec_psrp_script(script, in_data)\n        return rc, stdout, stderr",
        "begin_line": 406,
        "end_line": 443,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.put_file#445",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.put_file(self, in_path, out_path)",
        "snippet": "    def put_file(self, in_path, out_path):\n        super(Connection, self).put_file(in_path, out_path)\n        display.vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self._psrp_host)\n\n        out_path = self._shell._unquote(out_path)\n        script = u'''begin {\n    $ErrorActionPreference = \"Stop\"\n\n    $path = '%s'\n    $fd = [System.IO.File]::Create($path)\n    $algo = [System.Security.Cryptography.SHA1CryptoServiceProvider]::Create()\n    $bytes = @()\n} process {\n    $bytes = [System.Convert]::FromBase64String($input)\n    $algo.TransformBlock($bytes, 0, $bytes.Length, $bytes, 0) > $null\n    $fd.Write($bytes, 0, $bytes.Length)\n} end {\n    $fd.Close()\n    $algo.TransformFinalBlock($bytes, 0, 0) > $null\n    $hash = [System.BitConverter]::ToString($algo.Hash)\n    $hash = $hash.Replace(\"-\", \"\").ToLowerInvariant()\n\n    Write-Output -InputObject \"{`\"sha1`\":`\"$hash`\"}\"\n}''' % self._shell._escape(out_path)\n\n        cmd_parts = self._shell._encode_script(script, as_list=True,\n                                               strict_mode=False,\n                                               preserve_rc=False)\n        b_in_path = to_bytes(in_path, errors='surrogate_or_strict')\n        if not os.path.exists(b_in_path):\n            raise AnsibleFileNotFound('file or module does not exist: \"%s\"'\n                                      % to_native(in_path))\n\n        in_size = os.path.getsize(b_in_path)\n        buffer_size = int(self.runspace.connection.max_payload_size / 4 * 3)\n\n        # copying files is faster when using the raw WinRM shell and not PSRP\n        # we will create a WinRS shell just for this process\n        # TODO: speed this up as there is overhead creating a shell for this\n        with WinRS(self.runspace.connection, codepage=65001) as shell:\n            process = Process(shell, cmd_parts[0], cmd_parts[1:])\n            process.begin_invoke()\n\n            offset = 0\n            with open(b_in_path, 'rb') as src_file:\n                for data in iter((lambda: src_file.read(buffer_size)), b\"\"):\n                    offset += len(data)\n                    display.vvvvv(\"PSRP PUT %s to %s (offset=%d, size=%d\" %\n                                  (in_path, out_path, offset, len(data)),\n                                  host=self._psrp_host)\n                    b64_data = base64.b64encode(data) + b\"\\r\\n\"\n                    process.send(b64_data, end=(src_file.tell() == in_size))\n\n                # the file was empty, return empty buffer\n                if offset == 0:\n                    process.send(b\"\", end=True)\n\n            process.end_invoke()\n            process.signal(SignalCode.CTRL_C)\n\n        if process.rc != 0:\n            raise AnsibleError(to_native(process.stderr))\n\n        put_output = json.loads(process.stdout)\n        remote_sha1 = put_output.get(\"sha1\")\n\n        if not remote_sha1:\n            raise AnsibleError(\"Remote sha1 was not returned, stdout: '%s', \"\n                               \"stderr: '%s'\" % (to_native(process.stdout),\n                                                 to_native(process.stderr)))\n\n        local_sha1 = secure_hash(in_path)\n        if not remote_sha1 == local_sha1:\n            raise AnsibleError(\"Remote sha1 hash %s does not match local hash \"\n                               \"%s\" % (to_native(remote_sha1),\n                                       to_native(local_sha1)))",
        "begin_line": 445,
        "end_line": 520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.fetch_file#522",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.fetch_file(self, in_path, out_path)",
        "snippet": "    def fetch_file(self, in_path, out_path):\n        super(Connection, self).fetch_file(in_path, out_path)\n        display.vvv(\"FETCH %s TO %s\" % (in_path, out_path),\n                    host=self._psrp_host)\n\n        in_path = self._shell._unquote(in_path)\n        out_path = out_path.replace('\\\\', '/')\n\n        # because we are dealing with base64 data we need to get the max size\n        # of the bytes that the base64 size would equal\n        max_b64_size = int(self.runspace.connection.max_payload_size -\n                           (self.runspace.connection.max_payload_size / 4 * 3))\n        buffer_size = max_b64_size - (max_b64_size % 1024)\n\n        # setup the file stream with read only mode\n        setup_script = '''$ErrorActionPreference = \"Stop\"\n$path = '%s'\n\nif (Test-Path -Path $path -PathType Leaf) {\n    $fs = New-Object -TypeName System.IO.FileStream -ArgumentList @(\n        $path,\n        [System.IO.FileMode]::Open,\n        [System.IO.FileAccess]::Read,\n        [System.IO.FileShare]::Read\n    )\n    $buffer_size = %d\n} elseif (Test-Path -Path $path -PathType Container) {\n    Write-Output -InputObject \"[DIR]\"\n} else {\n    Write-Error -Message \"$path does not exist\"\n    $host.SetShouldExit(1)\n}''' % (self._shell._escape(in_path), buffer_size)\n\n        # read the file stream at the offset and return the b64 string\n        read_script = '''$ErrorActionPreference = \"Stop\"\n$fs.Seek(%d, [System.IO.SeekOrigin]::Begin) > $null\n$buffer = New-Object -TypeName byte[] -ArgumentList $buffer_size\n$bytes_read = $fs.Read($buffer, 0, $buffer_size)\n\nif ($bytes_read -gt 0) {\n    $bytes = $buffer[0..($bytes_read - 1)]\n    Write-Output -InputObject ([System.Convert]::ToBase64String($bytes))\n}'''\n\n        # need to run the setup script outside of the local scope so the\n        # file stream stays active between fetch operations\n        rc, stdout, stderr = self._exec_psrp_script(setup_script,\n                                                    use_local_scope=False,\n                                                    force_stop=True)\n        if rc != 0:\n            raise AnsibleError(\"failed to setup file stream for fetch '%s': %s\"\n                               % (out_path, to_native(stderr)))\n        elif stdout.strip() == '[DIR]':\n            # to be consistent with other connection plugins, we assume the caller has created the target dir\n            return\n\n        b_out_path = to_bytes(out_path, errors='surrogate_or_strict')\n        # to be consistent with other connection plugins, we assume the caller has created the target dir\n        offset = 0\n        with open(b_out_path, 'wb') as out_file:\n            while True:\n                display.vvvvv(\"PSRP FETCH %s to %s (offset=%d\" %\n                              (in_path, out_path, offset), host=self._psrp_host)\n                rc, stdout, stderr = self._exec_psrp_script(read_script % offset, force_stop=True)\n                if rc != 0:\n                    raise AnsibleError(\"failed to transfer file to '%s': %s\"\n                                       % (out_path, to_native(stderr)))\n\n                data = base64.b64decode(stdout.strip())\n                out_file.write(data)\n                if len(data) < buffer_size:\n                    break\n                offset += len(data)\n\n            rc, stdout, stderr = self._exec_psrp_script(\"$fs.Close()\", force_stop=True)\n            if rc != 0:\n                display.warning(\"failed to close remote file stream of file \"\n                                \"'%s': %s\" % (in_path, to_native(stderr)))",
        "begin_line": 522,
        "end_line": 599,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection.close#601",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection.close(self)",
        "snippet": "    def close(self):\n        if self.runspace and self.runspace.state == RunspacePoolState.OPENED:\n            display.vvvvv(\"PSRP CLOSE RUNSPACE: %s\" % (self.runspace.id),\n                          host=self._psrp_host)\n            self.runspace.close()\n        self.runspace = None\n        self._connected = False",
        "begin_line": 601,
        "end_line": 607,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection._build_kwargs#609",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection._build_kwargs(self)",
        "snippet": "    def _build_kwargs(self):\n        self._psrp_host = self.get_option('remote_addr')\n        self._psrp_user = self.get_option('remote_user')\n        self._psrp_pass = self.get_option('remote_password')\n\n        protocol = self.get_option('protocol')\n        port = self.get_option('port')\n        if protocol is None and port is None:\n            protocol = 'https'\n            port = 5986\n        elif protocol is None:\n            protocol = 'https' if int(port) != 5985 else 'http'\n        elif port is None:\n            port = 5986 if protocol == 'https' else 5985\n\n        self._psrp_protocol = protocol\n        self._psrp_port = int(port)\n\n        self._psrp_path = self.get_option('path')\n        self._psrp_auth = self.get_option('auth')\n        # cert validation can either be a bool or a path to the cert\n        cert_validation = self.get_option('cert_validation')\n        cert_trust_path = self.get_option('ca_cert')\n        if cert_validation == 'ignore':\n            self._psrp_cert_validation = False\n        elif cert_trust_path is not None:\n            self._psrp_cert_validation = cert_trust_path\n        else:\n            self._psrp_cert_validation = True\n\n        self._psrp_connection_timeout = self.get_option('connection_timeout')  # Can be None\n        self._psrp_read_timeout = self.get_option('read_timeout')  # Can be None\n        self._psrp_message_encryption = self.get_option('message_encryption')\n        self._psrp_proxy = self.get_option('proxy')\n        self._psrp_ignore_proxy = boolean(self.get_option('ignore_proxy'))\n        self._psrp_operation_timeout = int(self.get_option('operation_timeout'))\n        self._psrp_max_envelope_size = int(self.get_option('max_envelope_size'))\n        self._psrp_configuration_name = self.get_option('configuration_name')\n        self._psrp_reconnection_retries = int(self.get_option('reconnection_retries'))\n        self._psrp_reconnection_backoff = float(self.get_option('reconnection_backoff'))\n\n        self._psrp_certificate_key_pem = self.get_option('certificate_key_pem')\n        self._psrp_certificate_pem = self.get_option('certificate_pem')\n        self._psrp_credssp_auth_mechanism = self.get_option('credssp_auth_mechanism')\n        self._psrp_credssp_disable_tlsv1_2 = self.get_option('credssp_disable_tlsv1_2')\n        self._psrp_credssp_minimum_version = self.get_option('credssp_minimum_version')\n        self._psrp_negotiate_send_cbt = self.get_option('negotiate_send_cbt')\n        self._psrp_negotiate_delegate = self.get_option('negotiate_delegate')\n        self._psrp_negotiate_hostname_override = self.get_option('negotiate_hostname_override')\n        self._psrp_negotiate_service = self.get_option('negotiate_service')\n\n        supported_args = []\n        for auth_kwarg in AUTH_KWARGS.values():\n            supported_args.extend(auth_kwarg)\n        extra_args = set([v.replace('ansible_psrp_', '') for v in\n                          self.get_option('_extras')])\n        unsupported_args = extra_args.difference(supported_args)\n\n        for arg in unsupported_args:\n            display.warning(\"ansible_psrp_%s is unsupported by the current \"\n                            \"psrp version installed\" % arg)\n\n        self._psrp_conn_kwargs = dict(\n            server=self._psrp_host, port=self._psrp_port,\n            username=self._psrp_user, password=self._psrp_pass,\n            ssl=self._psrp_protocol == 'https', path=self._psrp_path,\n            auth=self._psrp_auth, cert_validation=self._psrp_cert_validation,\n            connection_timeout=self._psrp_connection_timeout,\n            encryption=self._psrp_message_encryption, proxy=self._psrp_proxy,\n            no_proxy=self._psrp_ignore_proxy,\n            max_envelope_size=self._psrp_max_envelope_size,\n            operation_timeout=self._psrp_operation_timeout,\n            certificate_key_pem=self._psrp_certificate_key_pem,\n            certificate_pem=self._psrp_certificate_pem,\n            credssp_auth_mechanism=self._psrp_credssp_auth_mechanism,\n            credssp_disable_tlsv1_2=self._psrp_credssp_disable_tlsv1_2,\n            credssp_minimum_version=self._psrp_credssp_minimum_version,\n            negotiate_send_cbt=self._psrp_negotiate_send_cbt,\n            negotiate_delegate=self._psrp_negotiate_delegate,\n            negotiate_hostname_override=self._psrp_negotiate_hostname_override,\n            negotiate_service=self._psrp_negotiate_service,\n        )\n\n        # Check if PSRP version supports newer read_timeout argument (needs pypsrp 0.3.0+)\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_read_timeout' in pypsrp.FEATURES:\n            self._psrp_conn_kwargs['read_timeout'] = self._psrp_read_timeout\n        elif self._psrp_read_timeout is not None:\n            display.warning(\"ansible_psrp_read_timeout is unsupported by the current psrp version installed, \"\n                            \"using ansible_psrp_connection_timeout value for read_timeout instead.\")\n\n        # Check if PSRP version supports newer reconnection_retries argument (needs pypsrp 0.3.0+)\n        if hasattr(pypsrp, 'FEATURES') and 'wsman_reconnections' in pypsrp.FEATURES:\n            self._psrp_conn_kwargs['reconnection_retries'] = self._psrp_reconnection_retries\n            self._psrp_conn_kwargs['reconnection_backoff'] = self._psrp_reconnection_backoff\n        else:\n            if self._psrp_reconnection_retries is not None:\n                display.warning(\"ansible_psrp_reconnection_retries is unsupported by the current psrp version installed.\")\n            if self._psrp_reconnection_backoff is not None:\n                display.warning(\"ansible_psrp_reconnection_backoff is unsupported by the current psrp version installed.\")\n\n        # add in the extra args that were set\n        for arg in extra_args.intersection(supported_args):\n            option = self.get_option('_extras')['ansible_psrp_%s' % arg]\n            self._psrp_conn_kwargs[arg] = option",
        "begin_line": 609,
        "end_line": 712,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection._exec_psrp_script#714",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection._exec_psrp_script(self, script, input_data=None, use_local_scope=True, force_stop=False)",
        "snippet": "    def _exec_psrp_script(self, script, input_data=None, use_local_scope=True, force_stop=False):\n        ps = PowerShell(self.runspace)\n        ps.add_script(script, use_local_scope=use_local_scope)\n        ps.invoke(input=input_data)\n\n        rc, stdout, stderr = self._parse_pipeline_result(ps)\n\n        if force_stop:\n            # This is usually not needed because we close the Runspace after our exec and we skip the call to close the\n            # pipeline manually to save on some time. Set to True when running multiple exec calls in the same runspace.\n\n            # Current pypsrp versions raise an exception if the current state was not RUNNING. We manually set it so we\n            # can call stop without any issues.\n            ps.state = PSInvocationState.RUNNING\n            ps.stop()\n\n        return rc, stdout, stderr",
        "begin_line": 714,
        "end_line": 730,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.psrp.Connection._parse_pipeline_result#732",
        "src_path": "lib/ansible/plugins/connection/psrp.py",
        "class_name": "lib.ansible.plugins.connection.psrp.Connection",
        "signature": "lib.ansible.plugins.connection.psrp.Connection._parse_pipeline_result(self, pipeline)",
        "snippet": "    def _parse_pipeline_result(self, pipeline):\n        \"\"\"\n        PSRP doesn't have the same concept as other protocols with its output.\n        We need some extra logic to convert the pipeline streams and host\n        output into the format that Ansible understands.\n\n        :param pipeline: The finished PowerShell pipeline that invoked our\n            commands\n        :return: rc, stdout, stderr based on the pipeline output\n        \"\"\"\n        # we try and get the rc from our host implementation, this is set if\n        # exit or $host.SetShouldExit() is called in our pipeline, if not we\n        # set to 0 if the pipeline had not errors and 1 if it did\n        rc = self.host.rc or (1 if pipeline.had_errors else 0)\n\n        # TODO: figure out a better way of merging this with the host output\n        stdout_list = []\n        for output in pipeline.output:\n            # Not all pipeline outputs are a string or contain a __str__ value,\n            # we will create our own output based on the properties of the\n            # complex object if that is the case.\n            if isinstance(output, GenericComplexObject) and output.to_string is None:\n                obj_lines = output.property_sets\n                for key, value in output.adapted_properties.items():\n                    obj_lines.append(u\"%s: %s\" % (key, value))\n                for key, value in output.extended_properties.items():\n                    obj_lines.append(u\"%s: %s\" % (key, value))\n                output_msg = u\"\\n\".join(obj_lines)\n            else:\n                output_msg = to_text(output, nonstring='simplerepr')\n\n            stdout_list.append(output_msg)\n\n        if len(self.host.ui.stdout) > 0:\n            stdout_list += self.host.ui.stdout\n        stdout = u\"\\r\\n\".join(stdout_list)\n\n        stderr_list = []\n        for error in pipeline.streams.error:\n            # the error record is not as fully fleshed out like we usually get\n            # in PS, we will manually create it here\n            command_name = \"%s : \" % error.command_name if error.command_name else ''\n            position = \"%s\\r\\n\" % error.invocation_position_message if error.invocation_position_message else ''\n            error_msg = \"%s%s\\r\\n%s\" \\\n                        \"    + CategoryInfo          : %s\\r\\n\" \\\n                        \"    + FullyQualifiedErrorId : %s\" \\\n                        % (command_name, str(error), position,\n                           error.message, error.fq_error)\n            stacktrace = error.script_stacktrace\n            if self._play_context.verbosity >= 3 and stacktrace is not None:\n                error_msg += \"\\r\\nStackTrace:\\r\\n%s\" % stacktrace\n            stderr_list.append(error_msg)\n\n        if len(self.host.ui.stderr) > 0:\n            stderr_list += self.host.ui.stderr\n        stderr = u\"\\r\\n\".join([to_text(o) for o in stderr_list])\n\n        display.vvvvv(\"PSRP RC: %d\" % rc, host=self._psrp_host)\n        display.vvvvv(\"PSRP STDOUT: %s\" % stdout, host=self._psrp_host)\n        display.vvvvv(\"PSRP STDERR: %s\" % stderr, host=self._psrp_host)\n\n        # reset the host back output back to defaults, needed if running\n        # multiple pipelines on the same RunspacePool\n        self.host.rc = 0\n        self.host.ui.stdout = []\n        self.host.ui.stderr = []\n\n        return rc, to_bytes(stdout, encoding='utf-8'), to_bytes(stderr, encoding='utf-8')",
        "begin_line": 732,
        "end_line": 799,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__init__#30",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__init__(self, isa=None, private=False, default=None, required=False, listof=None, priority=0, class_type=None, always_post_validate=False, inherit=True, alias=None, extend=False, prepend=False, static=False)",
        "snippet": "    def __init__(\n        self,\n        isa=None,\n        private=False,\n        default=None,\n        required=False,\n        listof=None,\n        priority=0,\n        class_type=None,\n        always_post_validate=False,\n        inherit=True,\n        alias=None,\n        extend=False,\n        prepend=False,\n        static=False,\n    ):\n\n        \"\"\"\n        :class:`Attribute` specifies constraints for attributes of objects which\n        derive from playbook data.  The attributes of the object are basically\n        a schema for the yaml playbook.\n\n        :kwarg isa: The type of the attribute.  Allowable values are a string\n            representation of any yaml basic datatype, python class, or percent.\n            (Enforced at post-validation time).\n        :kwarg private: Not used at runtime.  The docs playbook keyword dumper uses it to determine\n            that a keyword should not be documented.  mpdehaan had plans to remove attributes marked\n            private from the ds so they would not have been available at all.\n        :kwarg default: Default value if unspecified in the YAML document.\n        :kwarg required: Whether or not the YAML document must contain this field.\n            If the attribute is None when post-validated, an error will be raised.\n        :kwarg listof: If isa is set to \"list\", this can optionally be set to\n            ensure that all elements in the list are of the given type. Valid\n            values here are the same as those for isa.\n        :kwarg priority: The order in which the fields should be parsed. Generally\n            this does not need to be set, it is for rare situations where another\n            field depends on the fact that another field was parsed first.\n        :kwarg class_type: If isa is set to \"class\", this can be optionally set to\n            a class (not a string name). The YAML data for this field will be\n            passed to the __init__ method of that class during post validation and\n            the field will be an instance of that class.\n        :kwarg always_post_validate: Controls whether a field should be post\n            validated or not (default: False).\n        :kwarg inherit: A boolean value, which controls whether the object\n            containing this field should attempt to inherit the value from its\n            parent object if the local value is None.\n        :kwarg alias: An alias to use for the attribute name, for situations where\n            the attribute name may conflict with a Python reserved word.\n        \"\"\"\n\n        self.isa = isa\n        self.private = private\n        self.default = default\n        self.required = required\n        self.listof = listof\n        self.priority = priority\n        self.class_type = class_type\n        self.always_post_validate = always_post_validate\n        self.inherit = inherit\n        self.alias = alias\n        self.extend = extend\n        self.prepend = prepend\n        self.static = static\n\n        if default is not None and self.isa in _CONTAINERS and not callable(default):\n            raise TypeError('defaults for FieldAttribute may not be mutable, please provide a callable instead')",
        "begin_line": 30,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__eq__#97",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return other.priority == self.priority",
        "begin_line": 97,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__ne__#100",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        return other.priority != self.priority",
        "begin_line": 100,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__lt__#105",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__lt__(self, other)",
        "snippet": "    def __lt__(self, other):\n        return other.priority < self.priority",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.888001102080176e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__gt__#108",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__gt__(self, other)",
        "snippet": "    def __gt__(self, other):\n        return other.priority > self.priority",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__le__#111",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__le__(self, other)",
        "snippet": "    def __le__(self, other):\n        return other.priority <= self.priority",
        "begin_line": 111,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.attribute.Attribute.__ge__#114",
        "src_path": "lib/ansible/playbook/attribute.py",
        "class_name": "lib.ansible.playbook.attribute.Attribute",
        "signature": "lib.ansible.playbook.attribute.Attribute.__ge__(self, other)",
        "snippet": "    def __ge__(self, other):\n        return other.priority >= self.priority",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__._gen_id#18",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__",
        "signature": "lib.ansible.plugins.become.__init__._gen_id(length=32)",
        "snippet": "def _gen_id(length=32):\n    ''' return random string used to identify the current privilege escalation '''\n    return ''.join(choice(ascii_lowercase) for x in range(length))",
        "begin_line": 18,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.__init__#38",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.__init__(self)",
        "snippet": "    def __init__(self):\n        super(BecomeBase, self).__init__()\n        self._id = ''\n        self.success = ''",
        "begin_line": 38,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.get_option#43",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.get_option(self, option, hostvars=None, playcontext=None)",
        "snippet": "    def get_option(self, option, hostvars=None, playcontext=None):\n        \"\"\" Overrides the base get_option to provide a fallback to playcontext vars in case a 3rd party plugin did not\n        implement the base become options required in Ansible. \"\"\"\n        # TODO: add deprecation warning for ValueError in devel that removes the playcontext fallback\n        try:\n            return super(BecomeBase, self).get_option(option, hostvars=hostvars)\n        except KeyError:\n            pc_fallback = ['become_user', 'become_pass', 'become_flags', 'become_exe']\n            if option not in pc_fallback:\n                raise\n\n            return getattr(playcontext, option, None)",
        "begin_line": 43,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.expect_prompt#56",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.expect_prompt(self)",
        "snippet": "    def expect_prompt(self):\n        \"\"\"This function assists connection plugins in determining if they need to wait for\n        a prompt. Both a prompt and a password are required.\n        \"\"\"\n        return self.prompt and self.get_option('become_pass')",
        "begin_line": 56,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase._build_success_command#62",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase._build_success_command(self, cmd, shell, noexe=False)",
        "snippet": "    def _build_success_command(self, cmd, shell, noexe=False):\n        if not all((cmd, shell, self.success)):\n            return cmd\n\n        try:\n            cmd = shlex_quote('%s %s %s %s' % (shell.ECHO, self.success, shell.COMMAND_SEP, cmd))\n        except AttributeError:\n            # TODO: This should probably become some more robust functionlity used to detect incompat\n            raise AnsibleError('The %s shell family is incompatible with the %s become plugin' % (shell.SHELL_FAMILY, self.name))\n        exe = getattr(shell, 'executable', None)\n        if exe and not noexe:\n            cmd = '%s -c %s' % (exe, cmd)\n        return cmd",
        "begin_line": 62,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.build_become_command#77",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.build_become_command(self, cmd, shell)",
        "snippet": "    def build_become_command(self, cmd, shell):\n        self._id = _gen_id()\n        self.success = 'BECOME-SUCCESS-%s' % self._id",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.check_success#81",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.check_success(self, b_output)",
        "snippet": "    def check_success(self, b_output):\n        b_success = to_bytes(self.success)\n        return any(b_success in l.rstrip() for l in b_output.splitlines(True))",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.check_password_prompt#85",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.check_password_prompt(self, b_output)",
        "snippet": "    def check_password_prompt(self, b_output):\n        ''' checks if the expected password prompt exists in b_output '''\n        if self.prompt:\n            b_prompt = to_bytes(self.prompt).strip()\n            return any(l.strip().startswith(b_prompt) for l in b_output.splitlines())\n        return False",
        "begin_line": 85,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase._check_password_error#92",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase._check_password_error(self, b_out, msg)",
        "snippet": "    def _check_password_error(self, b_out, msg):\n        ''' returns True/False if domain specific i18n version of msg is found in b_out '''\n        b_fail = to_bytes(dgettext(self.name, msg))\n        return b_fail and b_fail in b_out",
        "begin_line": 92,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.check_incorrect_password#97",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.check_incorrect_password(self, b_output)",
        "snippet": "    def check_incorrect_password(self, b_output):\n        for errstring in self.fail:\n            if self._check_password_error(b_output, errstring):\n                return True\n        return False",
        "begin_line": 97,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.__init__.BecomeBase.check_missing_password#103",
        "src_path": "lib/ansible/plugins/become/__init__.py",
        "class_name": "lib.ansible.plugins.become.__init__.BecomeBase",
        "signature": "lib.ansible.plugins.become.__init__.BecomeBase.check_missing_password(self, b_output)",
        "snippet": "    def check_missing_password(self, b_output):\n        for errstring in self.missing:\n            if self._check_password_error(b_output, errstring):\n                return True\n        return False",
        "begin_line": 103,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters.to_bytes#33",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters.to_bytes(obj, encoding='utf-8', errors=None, nonstring='simplerepr')",
        "snippet": "def to_bytes(obj, encoding='utf-8', errors=None, nonstring='simplerepr'):\n    \"\"\"Make sure that a string is a byte string\n\n    :arg obj: An object to make sure is a byte string.  In most cases this\n        will be either a text string or a byte string.  However, with\n        ``nonstring='simplerepr'``, this can be used as a traceback-free\n        version of ``str(obj)``.\n    :kwarg encoding: The encoding to use to transform from a text string to\n        a byte string.  Defaults to using 'utf-8'.\n    :kwarg errors: The error handler to use if the text string is not\n        encodable using the specified encoding.  Any valid `codecs error\n        handler <https://docs.python.org/2/library/codecs.html#codec-base-classes>`_\n        may be specified. There are three additional error strategies\n        specifically aimed at helping people to port code.  The first two are:\n\n            :surrogate_or_strict: Will use ``surrogateescape`` if it is a valid\n                handler, otherwise it will use ``strict``\n            :surrogate_or_replace: Will use ``surrogateescape`` if it is a valid\n                handler, otherwise it will use ``replace``.\n\n        Because ``surrogateescape`` was added in Python3 this usually means that\n        Python3 will use ``surrogateescape`` and Python2 will use the fallback\n        error handler. Note that the code checks for ``surrogateescape`` when the\n        module is imported.  If you have a backport of ``surrogateescape`` for\n        Python2, be sure to register the error handler prior to importing this\n        module.\n\n        The last error handler is:\n\n            :surrogate_then_replace: Will use ``surrogateescape`` if it is a valid\n                handler.  If encoding with ``surrogateescape`` would traceback,\n                surrogates are first replaced with a replacement characters\n                and then the string is encoded using ``replace`` (which replaces\n                the rest of the nonencodable bytes).  If ``surrogateescape`` is\n                not present it will simply use ``replace``.  (Added in Ansible 2.3)\n                This strategy is designed to never traceback when it attempts\n                to encode a string.\n\n        The default until Ansible-2.2 was ``surrogate_or_replace``\n        From Ansible-2.3 onwards, the default is ``surrogate_then_replace``.\n\n    :kwarg nonstring: The strategy to use if a nonstring is specified in\n        ``obj``.  Default is 'simplerepr'.  Valid values are:\n\n        :simplerepr: The default.  This takes the ``str`` of the object and\n            then returns the bytes version of that string.\n        :empty: Return an empty byte string\n        :passthru: Return the object passed in\n        :strict: Raise a :exc:`TypeError`\n\n    :returns: Typically this returns a byte string.  If a nonstring object is\n        passed in this may be a different type depending on the strategy\n        specified by nonstring.  This will never return a text string.\n\n    .. note:: If passed a byte string, this function does not check that the\n        string is valid in the specified encoding.  If it's important that the\n        byte string is in the specified encoding do::\n\n            encoded_string = to_bytes(to_text(input_string, 'latin-1'), 'utf-8')\n\n    .. version_changed:: 2.3\n\n        Added the ``surrogate_then_replace`` error handler and made it the default error handler.\n    \"\"\"\n    if isinstance(obj, binary_type):\n        return obj\n\n    # We're given a text string\n    # If it has surrogates, we know because it will decode\n    original_errors = errors\n    if errors in _COMPOSED_ERROR_HANDLERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n            errors = 'strict'\n        else:\n            errors = 'replace'\n\n    if isinstance(obj, text_type):\n        try:\n            # Try this first as it's the fastest\n            return obj.encode(encoding, errors)\n        except UnicodeEncodeError:\n            if original_errors in (None, 'surrogate_then_replace'):\n                # We should only reach this if encoding was non-utf8 original_errors was\n                # surrogate_then_escape and errors was surrogateescape\n\n                # Slow but works\n                return_string = obj.encode('utf-8', 'surrogateescape')\n                return_string = return_string.decode('utf-8', 'replace')\n                return return_string.encode(encoding, 'replace')\n            raise\n\n    # Note: We do these last even though we have to call to_bytes again on the\n    # value because we're optimizing the common case\n    if nonstring == 'simplerepr':\n        try:\n            value = str(obj)\n        except UnicodeError:\n            try:\n                value = repr(obj)\n            except UnicodeError:\n                # Giving up\n                return to_bytes('')\n    elif nonstring == 'passthru':\n        return obj\n    elif nonstring == 'empty':\n        # python2.4 doesn't have b''\n        return to_bytes('')\n    elif nonstring == 'strict':\n        raise TypeError('obj must be a string type')\n    else:\n        raise TypeError('Invalid value %s for to_bytes\\' nonstring parameter' % nonstring)\n\n    return to_bytes(value, encoding, errors)",
        "begin_line": 33,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001072961373390558,
            "pseudo_dstar_susp": 0.005555555555555556,
            "pseudo_tarantula_susp": 0.0007722007722007722,
            "pseudo_op2_susp": 0.005555555555555556,
            "pseudo_barinel_susp": 0.0007722007722007722
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters.to_text#150",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters.to_text(obj, encoding='utf-8', errors=None, nonstring='simplerepr')",
        "snippet": "def to_text(obj, encoding='utf-8', errors=None, nonstring='simplerepr'):\n    \"\"\"Make sure that a string is a text string\n\n    :arg obj: An object to make sure is a text string.  In most cases this\n        will be either a text string or a byte string.  However, with\n        ``nonstring='simplerepr'``, this can be used as a traceback-free\n        version of ``str(obj)``.\n    :kwarg encoding: The encoding to use to transform from a byte string to\n        a text string.  Defaults to using 'utf-8'.\n    :kwarg errors: The error handler to use if the byte string is not\n        decodable using the specified encoding.  Any valid `codecs error\n        handler <https://docs.python.org/2/library/codecs.html#codec-base-classes>`_\n        may be specified.   We support three additional error strategies\n        specifically aimed at helping people to port code:\n\n            :surrogate_or_strict: Will use surrogateescape if it is a valid\n                handler, otherwise it will use strict\n            :surrogate_or_replace: Will use surrogateescape if it is a valid\n                handler, otherwise it will use replace.\n            :surrogate_then_replace: Does the same as surrogate_or_replace but\n                `was added for symmetry with the error handlers in\n                :func:`ansible.module_utils._text.to_bytes` (Added in Ansible 2.3)\n\n        Because surrogateescape was added in Python3 this usually means that\n        Python3 will use `surrogateescape` and Python2 will use the fallback\n        error handler. Note that the code checks for surrogateescape when the\n        module is imported.  If you have a backport of `surrogateescape` for\n        python2, be sure to register the error handler prior to importing this\n        module.\n\n        The default until Ansible-2.2 was `surrogate_or_replace`\n        In Ansible-2.3 this defaults to `surrogate_then_replace` for symmetry\n        with :func:`ansible.module_utils._text.to_bytes` .\n    :kwarg nonstring: The strategy to use if a nonstring is specified in\n        ``obj``.  Default is 'simplerepr'.  Valid values are:\n\n        :simplerepr: The default.  This takes the ``str`` of the object and\n            then returns the text version of that string.\n        :empty: Return an empty text string\n        :passthru: Return the object passed in\n        :strict: Raise a :exc:`TypeError`\n\n    :returns: Typically this returns a text string.  If a nonstring object is\n        passed in this may be a different type depending on the strategy\n        specified by nonstring.  This will never return a byte string.\n        From Ansible-2.3 onwards, the default is `surrogate_then_replace`.\n\n    .. version_changed:: 2.3\n\n        Added the surrogate_then_replace error handler and made it the default error handler.\n    \"\"\"\n    if isinstance(obj, text_type):\n        return obj\n\n    if errors in _COMPOSED_ERROR_HANDLERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n            errors = 'strict'\n        else:\n            errors = 'replace'\n\n    if isinstance(obj, binary_type):\n        # Note: We don't need special handling for surrogate_then_replace\n        # because all bytes will either be made into surrogates or are valid\n        # to decode.\n        return obj.decode(encoding, errors)\n\n    # Note: We do these last even though we have to call to_text again on the\n    # value because we're optimizing the common case\n    if nonstring == 'simplerepr':\n        try:\n            value = str(obj)\n        except UnicodeError:\n            try:\n                value = repr(obj)\n            except UnicodeError:\n                # Giving up\n                return u''\n    elif nonstring == 'passthru':\n        return obj\n    elif nonstring == 'empty':\n        return u''\n    elif nonstring == 'strict':\n        raise TypeError('obj must be a string type')\n    else:\n        raise TypeError('Invalid value %s for to_text\\'s nonstring parameter' % nonstring)\n\n    return to_text(value, encoding, errors)",
        "begin_line": 150,
        "end_line": 238,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002564102564102564,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.0016233766233766235,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.0016233766233766235
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters._json_encode_fallback#262",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters._json_encode_fallback(obj)",
        "snippet": "def _json_encode_fallback(obj):\n    if isinstance(obj, Set):\n        return list(obj)\n    elif isinstance(obj, datetime.datetime):\n        return obj.isoformat()\n    raise TypeError(\"Cannot json serialize %s\" % to_native(obj))",
        "begin_line": 262,
        "end_line": 267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters.jsonify#270",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters.jsonify(data, **kwargs)",
        "snippet": "def jsonify(data, **kwargs):\n    for encoding in (\"utf-8\", \"latin-1\"):\n        try:\n            return json.dumps(data, encoding=encoding, default=_json_encode_fallback, **kwargs)\n        # Old systems using old simplejson module does not support encoding keyword.\n        except TypeError:\n            try:\n                new_data = container_to_text(data, encoding=encoding)\n            except UnicodeDecodeError:\n                continue\n            return json.dumps(new_data, default=_json_encode_fallback, **kwargs)\n        except UnicodeDecodeError:\n            continue\n    raise UnicodeError('Invalid unicode encoding encountered')",
        "begin_line": 270,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters.container_to_bytes#286",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters.container_to_bytes(d, encoding='utf-8', errors='surrogate_or_strict')",
        "snippet": "def container_to_bytes(d, encoding='utf-8', errors='surrogate_or_strict'):\n    ''' Recursively convert dict keys and values to byte str\n\n        Specialized for json return because this only handles, lists, tuples,\n        and dict container types (the containers that the json module returns)\n    '''\n\n    if isinstance(d, text_type):\n        return to_bytes(d, encoding=encoding, errors=errors)\n    elif isinstance(d, dict):\n        return dict(container_to_bytes(o, encoding, errors) for o in iteritems(d))\n    elif isinstance(d, list):\n        return [container_to_bytes(o, encoding, errors) for o in d]\n    elif isinstance(d, tuple):\n        return tuple(container_to_bytes(o, encoding, errors) for o in d)\n    else:\n        return d",
        "begin_line": 286,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.011147724882563e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.converters.container_to_text#305",
        "src_path": "lib/ansible/module_utils/common/text/converters.py",
        "class_name": "lib.ansible.module_utils.common.text.converters",
        "signature": "lib.ansible.module_utils.common.text.converters.container_to_text(d, encoding='utf-8', errors='surrogate_or_strict')",
        "snippet": "def container_to_text(d, encoding='utf-8', errors='surrogate_or_strict'):\n    \"\"\"Recursively convert dict keys and values to byte str\n\n    Specialized for json return because this only handles, lists, tuples,\n    and dict container types (the containers that the json module returns)\n    \"\"\"\n\n    if isinstance(d, binary_type):\n        # Warning, can traceback\n        return to_text(d, encoding=encoding, errors=errors)\n    elif isinstance(d, dict):\n        return dict(container_to_text(o, encoding, errors) for o in iteritems(d))\n    elif isinstance(d, list):\n        return [container_to_text(o, encoding, errors) for o in d]\n    elif isinstance(d, tuple):\n        return tuple(container_to_text(o, encoding, errors) for o in d)\n    else:\n        return d",
        "begin_line": 305,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.003782042302843e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.wrapped#43",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api",
        "signature": "lib.ansible.galaxy.api.wrapped(self, *args, **kwargs)",
        "snippet": "        def wrapped(self, *args, **kwargs):\n            if not self._available_api_versions:\n                display.vvvv(\"Initial connection to galaxy_server: %s\" % self.api_server)\n\n                # Determine the type of Galaxy server we are talking to. First try it unauthenticated then with Bearer\n                # auth for Automation Hub.\n                n_url = self.api_server\n                error_context_msg = 'Error when finding available api versions from %s (%s)' % (self.name, n_url)\n\n                if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':\n                    n_url = 'https://galaxy.ansible.com/api/'\n\n                try:\n                    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)\n                except (AnsibleError, GalaxyError, ValueError, KeyError) as err:\n                    # Either the URL doesnt exist, or other error. Or the URL exists, but isn't a galaxy API\n                    # root (not JSON, no 'available_versions') so try appending '/api/'\n                    if n_url.endswith('/api') or n_url.endswith('/api/'):\n                        raise\n\n                    # Let exceptions here bubble up but raise the original if this returns a 404 (/api/ wasn't found).\n                    n_url = _urljoin(n_url, '/api/')\n                    try:\n                        data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg)\n                    except GalaxyError as new_err:\n                        if new_err.http_code == 404:\n                            raise err\n                        raise\n\n                if 'available_versions' not in data:\n                    raise AnsibleError(\"Tried to find galaxy API root at %s but no 'available_versions' are available \"\n                                       \"on %s\" % (n_url, self.api_server))\n\n                # Update api_server to point to the \"real\" API root, which in this case could have been the configured\n                # url + '/api/' appended.\n                self.api_server = n_url\n\n                # Default to only supporting v1, if only v1 is returned we also assume that v2 is available even though\n                # it isn't returned in the available_versions dict.\n                available_versions = data.get('available_versions', {u'v1': u'v1/'})\n                if list(available_versions.keys()) == [u'v1']:\n                    available_versions[u'v2'] = u'v2/'\n\n                self._available_api_versions = available_versions\n                display.vvvv(\"Found API version '%s' with Galaxy server %s (%s)\"\n                             % (', '.join(available_versions.keys()), self.name, self.api_server))\n\n            # Verify that the API versions the function works with are available on the server specified.\n            available_versions = set(self._available_api_versions.keys())\n            common_versions = set(versions).intersection(available_versions)\n            if not common_versions:\n                raise AnsibleError(\"Galaxy action %s requires API versions '%s' but only '%s' are available on %s %s\"\n                                   % (method.__name__, \", \".join(versions), \", \".join(available_versions),\n                                      self.name, self.api_server))\n\n            return method(self, *args, **kwargs)",
        "begin_line": 43,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006060606060606061,
            "pseudo_dstar_susp": 0.008064516129032258,
            "pseudo_tarantula_susp": 0.0035842293906810036,
            "pseudo_op2_susp": 0.008064516129032258,
            "pseudo_barinel_susp": 0.0035842293906810036
        }
    },
    {
        "name": "lib.ansible.galaxy.api._urljoin#103",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api",
        "signature": "lib.ansible.galaxy.api._urljoin(*args)",
        "snippet": "def _urljoin(*args):\n    return '/'.join(to_native(a, errors='surrogate_or_strict').strip('/') for a in args + ('',) if a)",
        "begin_line": 103,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.058823529411764705,
            "pseudo_dstar_susp": 0.16666666666666666,
            "pseudo_tarantula_susp": 0.004132231404958678,
            "pseudo_op2_susp": 0.16666666666666666,
            "pseudo_barinel_susp": 0.004132231404958678
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyError.__init__#110",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyError",
        "signature": "lib.ansible.galaxy.api.GalaxyError.__init__(self, http_error, message)",
        "snippet": "    def __init__(self, http_error, message):\n        super(GalaxyError, self).__init__(message)\n        self.http_code = http_error.code\n        self.url = http_error.geturl()\n\n        try:\n            http_msg = to_text(http_error.read())\n            err_info = json.loads(http_msg)\n        except (AttributeError, ValueError):\n            err_info = {}\n\n        url_split = self.url.split('/')\n        if 'v2' in url_split:\n            galaxy_msg = err_info.get('message', http_error.reason)\n            code = err_info.get('code', 'Unknown')\n            full_error_msg = u\"%s (HTTP Code: %d, Message: %s Code: %s)\" % (message, self.http_code, galaxy_msg, code)\n        elif 'v3' in url_split:\n            errors = err_info.get('errors', [])\n            if not errors:\n                errors = [{}]  # Defaults are set below, we just need to make sure 1 error is present.\n\n            message_lines = []\n            for error in errors:\n                error_msg = error.get('detail') or error.get('title') or http_error.reason\n                error_code = error.get('code') or 'Unknown'\n                message_line = u\"(HTTP Code: %d, Message: %s Code: %s)\" % (self.http_code, error_msg, error_code)\n                message_lines.append(message_line)\n\n            full_error_msg = \"%s %s\" % (message, ', '.join(message_lines))\n        else:\n            # v1 and unknown API endpoints\n            galaxy_msg = err_info.get('default', http_error.reason)\n            full_error_msg = u\"%s (HTTP Code: %d, Message: %s)\" % (message, self.http_code, galaxy_msg)\n\n        self.message = to_native(full_error_msg)",
        "begin_line": 110,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.CollectionVersionMetadata.__init__#149",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.CollectionVersionMetadata",
        "signature": "lib.ansible.galaxy.api.CollectionVersionMetadata.__init__(self, namespace, name, version, download_url, artifact_sha256, dependencies)",
        "snippet": "    def __init__(self, namespace, name, version, download_url, artifact_sha256, dependencies):\n        \"\"\"\n        Contains common information about a collection on a Galaxy server to smooth through API differences for\n        Collection and define a standard meta info for a collection.\n\n        :param namespace: The namespace name.\n        :param name: The collection name.\n        :param version: The version that the metadata refers to.\n        :param download_url: The URL to download the collection.\n        :param artifact_sha256: The SHA256 of the collection artifact for later verification.\n        :param dependencies: A dict of dependencies of the collection.\n        \"\"\"\n        self.namespace = namespace\n        self.name = name\n        self.version = version\n        self.download_url = download_url\n        self.artifact_sha256 = artifact_sha256\n        self.dependencies = dependencies",
        "begin_line": 149,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000749063670411985,
            "pseudo_dstar_susp": 0.0007485029940119761,
            "pseudo_tarantula_susp": 0.0007776049766718507,
            "pseudo_op2_susp": 0.0007485029940119761,
            "pseudo_barinel_susp": 0.0007776049766718507
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.__init__#172",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.__init__(self, galaxy, name, url, username=None, password=None, token=None)",
        "snippet": "    def __init__(self, galaxy, name, url, username=None, password=None, token=None):\n        self.galaxy = galaxy\n        self.name = name\n        self.username = username\n        self.password = password\n        self.token = token\n        self.api_server = url\n        self.validate_certs = not context.CLIARGS['ignore_certs']\n        self._available_api_versions = {}\n\n        display.debug('Validate TLS certificates for %s: %s' % (self.api_server, self.validate_certs))",
        "begin_line": 172,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004901960784313725,
            "pseudo_dstar_susp": 0.07142857142857142,
            "pseudo_tarantula_susp": 0.0017574692442882249,
            "pseudo_op2_susp": 0.07142857142857142,
            "pseudo_barinel_susp": 0.0017574692442882249
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.available_api_versions#186",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.available_api_versions(self)",
        "snippet": "    def available_api_versions(self):\n        # Calling g_connect will populate self._available_api_versions\n        return self._available_api_versions",
        "begin_line": 186,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006289308176100629,
            "pseudo_dstar_susp": 0.00847457627118644,
            "pseudo_tarantula_susp": 0.00398406374501992,
            "pseudo_op2_susp": 0.00847457627118644,
            "pseudo_barinel_susp": 0.00398406374501992
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI._call_galaxy#190",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI._call_galaxy(self, url, args=None, headers=None, method=None, auth_required=False, error_context_msg=None)",
        "snippet": "    def _call_galaxy(self, url, args=None, headers=None, method=None, auth_required=False, error_context_msg=None):\n        headers = headers or {}\n        self._add_auth_token(headers, url, required=auth_required)\n\n        try:\n            display.vvvv(\"Calling Galaxy at %s\" % url)\n            resp = open_url(to_native(url), data=args, validate_certs=self.validate_certs, headers=headers,\n                            method=method, timeout=20, http_agent=user_agent(), follow_redirects='safe')\n        except HTTPError as e:\n            raise GalaxyError(e, error_context_msg)\n        except Exception as e:\n            raise AnsibleError(\"Unknown error when attempting to call Galaxy at '%s': %s\" % (url, to_native(e)))\n\n        resp_data = to_text(resp.read(), errors='surrogate_or_strict')\n        try:\n            data = json.loads(resp_data)\n        except ValueError:\n            raise AnsibleError(\"Failed to parse Galaxy response from '%s' as JSON:\\n%s\"\n                               % (resp.url, to_native(resp_data)))\n\n        return data",
        "begin_line": 190,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015625,
            "pseudo_dstar_susp": 0.009345794392523364,
            "pseudo_tarantula_susp": 0.004219409282700422,
            "pseudo_op2_susp": 0.009345794392523364,
            "pseudo_barinel_susp": 0.004219409282700422
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI._add_auth_token#212",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI._add_auth_token(self, headers, url, token_type=None, required=False)",
        "snippet": "    def _add_auth_token(self, headers, url, token_type=None, required=False):\n        # Don't add the auth token if one is already present\n        if 'Authorization' in headers:\n            return\n\n        if not self.token and required:\n            raise AnsibleError(\"No access token or username set. A token can be set with --api-key, with \"\n                               \"'ansible-galaxy login', or set in ansible.cfg.\")\n\n        if self.token:\n            headers.update(self.token.headers())",
        "begin_line": 212,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005747126436781609,
            "pseudo_dstar_susp": 0.007518796992481203,
            "pseudo_tarantula_susp": 0.003215434083601286,
            "pseudo_op2_susp": 0.007518796992481203,
            "pseudo_barinel_susp": 0.003215434083601286
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.authenticate#225",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.authenticate(self, github_token)",
        "snippet": "    def authenticate(self, github_token):\n        \"\"\"\n        Retrieve an authentication token\n        \"\"\"\n        url = _urljoin(self.api_server, self.available_api_versions['v1'], \"tokens\") + '/'\n        args = urlencode({\"github_token\": github_token})\n        resp = open_url(url, data=args, validate_certs=self.validate_certs, method=\"POST\", http_agent=user_agent())\n        data = json.loads(to_text(resp.read(), errors='surrogate_or_strict'))\n        return data",
        "begin_line": 225,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.fetch_role_related#295",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.fetch_role_related(self, related, role_id)",
        "snippet": "    def fetch_role_related(self, related, role_id):\n        \"\"\"\n        Fetch the list of related items for the given role.\n        The url comes from the 'related' field of the role.\n        \"\"\"\n\n        results = []\n        try:\n            url = _urljoin(self.api_server, self.available_api_versions['v1'], \"roles\", role_id, related,\n                           \"?page_size=50\")\n            data = self._call_galaxy(url)\n            results = data['results']\n            done = (data.get('next_link', None) is None)\n\n            # https://github.com/ansible/ansible/issues/64355\n            # api_server contains part of the API path but next_link includes the /api part so strip it out.\n            url_info = urlparse(self.api_server)\n            base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n\n            while not done:\n                url = _urljoin(base_url, data['next_link'])\n                data = self._call_galaxy(url)\n                results += data['results']\n                done = (data.get('next_link', None) is None)\n        except Exception as e:\n            display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                            % (role_id, related, to_text(e)))\n        return results",
        "begin_line": 295,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.publish_collection#412",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.publish_collection(self, collection_path)",
        "snippet": "    def publish_collection(self, collection_path):\n        \"\"\"\n        Publishes a collection to a Galaxy server and returns the import task URI.\n\n        :param collection_path: The path to the collection tarball to publish.\n        :return: The import task URI that contains the import results.\n        \"\"\"\n        display.display(\"Publishing collection artifact '%s' to %s %s\" % (collection_path, self.name, self.api_server))\n\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n        if not os.path.exists(b_collection_path):\n            raise AnsibleError(\"The collection path specified '%s' does not exist.\" % to_native(collection_path))\n        elif not tarfile.is_tarfile(b_collection_path):\n            raise AnsibleError(\"The collection path specified '%s' is not a tarball, use 'ansible-galaxy collection \"\n                               \"build' to create a proper release artifact.\" % to_native(collection_path))\n\n        with open(b_collection_path, 'rb') as collection_tar:\n            data = collection_tar.read()\n\n        boundary = '--------------------------%s' % uuid.uuid4().hex\n        b_file_name = os.path.basename(b_collection_path)\n        part_boundary = b\"--\" + to_bytes(boundary, errors='surrogate_or_strict')\n\n        form = [\n            part_boundary,\n            b\"Content-Disposition: form-data; name=\\\"sha256\\\"\",\n            to_bytes(secure_hash_s(data, hash_func=hashlib.sha256), errors='surrogate_or_strict'),\n            part_boundary,\n            b\"Content-Disposition: file; name=\\\"file\\\"; filename=\\\"%s\\\"\" % b_file_name,\n            b\"Content-Type: application/octet-stream\",\n            b\"\",\n            data,\n            b\"%s--\" % part_boundary,\n        ]\n        data = b\"\\r\\n\".join(form)\n\n        headers = {\n            'Content-type': 'multipart/form-data; boundary=%s' % boundary,\n            'Content-length': len(data),\n        }\n\n        if 'v3' in self.available_api_versions:\n            n_url = _urljoin(self.api_server, self.available_api_versions['v3'], 'artifacts', 'collections') + '/'\n        else:\n            n_url = _urljoin(self.api_server, self.available_api_versions['v2'], 'collections') + '/'\n\n        resp = self._call_galaxy(n_url, args=data, headers=headers, method='POST', auth_required=True,\n                                 error_context_msg='Error when publishing collection to %s (%s)'\n                                                   % (self.name, self.api_server))\n        return resp['task']",
        "begin_line": 412,
        "end_line": 461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.wait_import_task#464",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.wait_import_task(self, task_id, timeout=0)",
        "snippet": "    def wait_import_task(self, task_id, timeout=0):\n        \"\"\"\n        Waits until the import process on the Galaxy server has completed or the timeout is reached.\n\n        :param task_id: The id of the import task to wait for. This can be parsed out of the return\n            value for GalaxyAPI.publish_collection.\n        :param timeout: The timeout in seconds, 0 is no timeout.\n        \"\"\"\n        state = 'waiting'\n        data = None\n\n        # Construct the appropriate URL per version\n        if 'v3' in self.available_api_versions:\n            full_url = _urljoin(self.api_server, self.available_api_versions['v3'],\n                                'imports/collections', task_id, '/')\n        else:\n            full_url = _urljoin(self.api_server, self.available_api_versions['v2'],\n                                'collection-imports', task_id, '/')\n\n        display.display(\"Waiting until Galaxy import task %s has completed\" % full_url)\n        start = time.time()\n        wait = 2\n\n        while timeout == 0 or (time.time() - start) < timeout:\n            data = self._call_galaxy(full_url, method='GET', auth_required=True,\n                                     error_context_msg='Error when getting import task results at %s' % full_url)\n\n            state = data.get('state', 'waiting')\n\n            if data.get('finished_at', None):\n                break\n\n            display.vvv('Galaxy import process has a status of %s, wait %d seconds before trying again'\n                        % (state, wait))\n            time.sleep(wait)\n\n            # poor man's exponential backoff algo so we don't flood the Galaxy API, cap at 30 seconds.\n            wait = min(30, wait * 1.5)\n        if state == 'waiting':\n            raise AnsibleError(\"Timeout while waiting for the Galaxy import process to finish, check progress at '%s'\"\n                               % to_native(full_url))\n\n        for message in data.get('messages', []):\n            level = message['level']\n            if level == 'error':\n                display.error(\"Galaxy import error message: %s\" % message['message'])\n            elif level == 'warning':\n                display.warning(\"Galaxy import warning message: %s\" % message['message'])\n            else:\n                display.vvv(\"Galaxy import message: %s - %s\" % (level, message['message']))\n\n        if state == 'failed':\n            code = to_native(data['error'].get('code', 'UNKNOWN'))\n            description = to_native(\n                data['error'].get('description', \"Unknown error, see %s for more details\" % full_url))\n            raise AnsibleError(\"Galaxy import process failed: %s (Code: %s)\" % (description, code))",
        "begin_line": 464,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.14285714285714285,
            "pseudo_dstar_susp": 0.010101010101010102,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.010101010101010102,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.get_collection_version_metadata#522",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.get_collection_version_metadata(self, namespace, name, version)",
        "snippet": "    def get_collection_version_metadata(self, namespace, name, version):\n        \"\"\"\n        Gets the collection information from the Galaxy server about a specific Collection version.\n\n        :param namespace: The collection namespace.\n        :param name: The collection name.\n        :param version: Optional version of the collection to get the information for.\n        :return: CollectionVersionMetadata about the collection at the version requested.\n        \"\"\"\n        api_path = self.available_api_versions.get('v3', self.available_api_versions.get('v2'))\n        url_paths = [self.api_server, api_path, 'collections', namespace, name, 'versions', version, '/']\n\n        n_collection_url = _urljoin(*url_paths)\n        error_context_msg = 'Error when getting collection version metadata for %s.%s:%s from %s (%s)' \\\n                            % (namespace, name, version, self.name, self.api_server)\n        data = self._call_galaxy(n_collection_url, error_context_msg=error_context_msg)\n\n        return CollectionVersionMetadata(data['namespace']['name'], data['collection']['name'], data['version'],\n                                         data['download_url'], data['artifact']['sha256'],\n                                         data['metadata']['dependencies'])",
        "begin_line": 522,
        "end_line": 541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.api.GalaxyAPI.get_collection_versions#544",
        "src_path": "lib/ansible/galaxy/api.py",
        "class_name": "lib.ansible.galaxy.api.GalaxyAPI",
        "signature": "lib.ansible.galaxy.api.GalaxyAPI.get_collection_versions(self, namespace, name)",
        "snippet": "    def get_collection_versions(self, namespace, name):\n        \"\"\"\n        Gets a list of available versions for a collection on a Galaxy server.\n\n        :param namespace: The collection namespace.\n        :param name: The collection name.\n        :return: A list of versions that are available.\n        \"\"\"\n        relative_link = False\n        if 'v3' in self.available_api_versions:\n            api_path = self.available_api_versions['v3']\n            results_key = 'data'\n            pagination_path = ['links', 'next']\n            relative_link = True  # AH pagination results are relative an not an absolute URI.\n        else:\n            api_path = self.available_api_versions['v2']\n            results_key = 'results'\n            pagination_path = ['next']\n\n        n_url = _urljoin(self.api_server, api_path, 'collections', namespace, name, 'versions', '/')\n\n        error_context_msg = 'Error when getting available collection versions for %s.%s from %s (%s)' \\\n                            % (namespace, name, self.name, self.api_server)\n        data = self._call_galaxy(n_url, error_context_msg=error_context_msg)\n\n        versions = []\n        while True:\n            versions += [v['version'] for v in data[results_key]]\n\n            next_link = data\n            for path in pagination_path:\n                next_link = next_link.get(path, {})\n\n            if not next_link:\n                break\n            elif relative_link:\n                # TODO: This assumes the pagination result is relative to the root server. Will need to be verified\n                # with someone who knows the AH API.\n                next_link = n_url.replace(urlparse(n_url).path, next_link)\n\n            data = self._call_galaxy(to_native(next_link, errors='surrogate_or_strict'),\n                                     error_context_msg=error_context_msg)\n\n        return versions",
        "begin_line": 544,
        "end_line": 587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.path.unfrackpath#31",
        "src_path": "lib/ansible/utils/path.py",
        "class_name": "lib.ansible.utils.path",
        "signature": "lib.ansible.utils.path.unfrackpath(path, follow=True, basedir=None)",
        "snippet": "def unfrackpath(path, follow=True, basedir=None):\n    '''\n    Returns a path that is free of symlinks (if follow=True), environment variables, relative path traversals and symbols (~)\n\n    :arg path: A byte or text string representing a path to be canonicalized\n    :arg follow: A boolean to indicate of symlinks should be resolved or not\n    :raises UnicodeDecodeError: If the canonicalized version of the path\n        contains non-utf8 byte sequences.\n    :rtype: A text string (unicode on pyyhon2, str on python3).\n    :returns: An absolute path with symlinks, environment variables, and tilde\n        expanded.  Note that this does not check whether a path exists.\n\n    example::\n        '$HOME/../../var/mail' becomes '/var/spool/mail'\n    '''\n\n    b_basedir = to_bytes(basedir, errors='surrogate_or_strict', nonstring='passthru')\n\n    if b_basedir is None:\n        b_basedir = to_bytes(os.getcwd(), errors='surrogate_or_strict')\n    elif os.path.isfile(b_basedir):\n        b_basedir = os.path.dirname(b_basedir)\n\n    b_final_path = os.path.expanduser(os.path.expandvars(to_bytes(path, errors='surrogate_or_strict')))\n\n    if not os.path.isabs(b_final_path):\n        b_final_path = os.path.join(b_basedir, b_final_path)\n\n    if follow:\n        b_final_path = os.path.realpath(b_final_path)\n\n    return to_text(os.path.normpath(b_final_path), errors='surrogate_or_strict')",
        "begin_line": 31,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010162601626016261,
            "pseudo_dstar_susp": 0.0013386880856760374,
            "pseudo_tarantula_susp": 0.0007722007722007722,
            "pseudo_op2_susp": 0.0013386880856760374,
            "pseudo_barinel_susp": 0.0007722007722007722
        }
    },
    {
        "name": "lib.ansible.utils.path.makedirs_safe#65",
        "src_path": "lib/ansible/utils/path.py",
        "class_name": "lib.ansible.utils.path",
        "signature": "lib.ansible.utils.path.makedirs_safe(path, mode=None)",
        "snippet": "def makedirs_safe(path, mode=None):\n    '''\n    A *potentially insecure* way to ensure the existence of a directory chain. The \"safe\" in this function's name\n    refers only to its ability to ignore `EEXIST` in the case of multiple callers operating on the same part of\n    the directory chain. This function is not safe to use under world-writable locations when the first level of the\n    path to be created contains a predictable component. Always create a randomly-named element first if there is any\n    chance the parent directory might be world-writable (eg, /tmp) to prevent symlink hijacking and potential\n    disclosure or modification of sensitive file contents.\n\n    :arg path: A byte or text string representing a directory chain to be created\n    :kwarg mode: If given, the mode to set the directory to\n    :raises AnsibleError: If the directory cannot be created and does not already exist.\n    :raises UnicodeDecodeError: if the path is not decodable in the utf-8 encoding.\n    '''\n\n    rpath = unfrackpath(path)\n    b_rpath = to_bytes(rpath)\n    if not os.path.exists(b_rpath):\n        try:\n            if mode:\n                os.makedirs(b_rpath, mode)\n            else:\n                os.makedirs(b_rpath)\n        except OSError as e:\n            if e.errno != EEXIST:\n                raise AnsibleError(\"Unable to create local directories(%s): %s\" % (to_native(rpath), to_native(e)))",
        "begin_line": 65,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.path.basedir#93",
        "src_path": "lib/ansible/utils/path.py",
        "class_name": "lib.ansible.utils.path",
        "signature": "lib.ansible.utils.path.basedir(source)",
        "snippet": "def basedir(source):\n    \"\"\" returns directory for inventory or playbook \"\"\"\n    source = to_bytes(source, errors='surrogate_or_strict')\n    dname = None\n    if os.path.isdir(source):\n        dname = source\n    elif source in [None, '', '.']:\n        dname = os.getcwd()\n    elif os.path.isfile(source):\n        dname = os.path.dirname(source)\n\n    if dname:\n        # don't follow symlinks for basedir, enables source re-use\n        dname = os.path.abspath(dname)\n\n    return to_text(dname, errors='surrogate_or_strict')",
        "begin_line": 93,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.path.cleanup_tmp_file#111",
        "src_path": "lib/ansible/utils/path.py",
        "class_name": "lib.ansible.utils.path",
        "signature": "lib.ansible.utils.path.cleanup_tmp_file(path, warn=False)",
        "snippet": "def cleanup_tmp_file(path, warn=False):\n    \"\"\"\n    Removes temporary file or directory. Optionally display a warning if unable\n    to remove the file or directory.\n\n    :arg path: Path to file or directory to be removed\n    :kwarg warn: Whether or not to display a warning when the file or directory\n        cannot be removed\n    \"\"\"\n    try:\n        if os.path.exists(path):\n            try:\n                if os.path.isdir(path):\n                    shutil.rmtree(path)\n                elif os.path.isfile(path):\n                    os.unlink(path)\n            except Exception as e:\n                if warn:\n                    # Importing here to avoid circular import\n                    from ansible.utils.display import Display\n                    display = Display()\n                    display.display(u'Unable to remove temporary file {0}'.format(to_text(e)))\n    except Exception:\n        pass",
        "begin_line": 111,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.requirement.RoleRequirement.repo_url_to_role_name#61",
        "src_path": "lib/ansible/playbook/role/requirement.py",
        "class_name": "lib.ansible.playbook.role.requirement.RoleRequirement",
        "signature": "lib.ansible.playbook.role.requirement.RoleRequirement.repo_url_to_role_name(repo_url)",
        "snippet": "    def repo_url_to_role_name(repo_url):\n        # gets the role name out of a repo like\n        # http://git.example.com/repos/repo.git\" => \"repo\"\n\n        if '://' not in repo_url and '@' not in repo_url:\n            return repo_url\n        trailing_path = repo_url.split('/')[-1]\n        if trailing_path.endswith('.git'):\n            trailing_path = trailing_path[:-4]\n        if trailing_path.endswith('.tar.gz'):\n            trailing_path = trailing_path[:-7]\n        if ',' in trailing_path:\n            trailing_path = trailing_path.split(',')[0]\n        return trailing_path",
        "begin_line": 61,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.requirement.RoleRequirement.role_yaml_parse#77",
        "src_path": "lib/ansible/playbook/role/requirement.py",
        "class_name": "lib.ansible.playbook.role.requirement.RoleRequirement",
        "signature": "lib.ansible.playbook.role.requirement.RoleRequirement.role_yaml_parse(role)",
        "snippet": "    def role_yaml_parse(role):\n\n        if isinstance(role, string_types):\n            name = None\n            scm = None\n            src = None\n            version = None\n            if ',' in role:\n                if role.count(',') == 1:\n                    (src, version) = role.strip().split(',', 1)\n                elif role.count(',') == 2:\n                    (src, version, name) = role.strip().split(',', 2)\n                else:\n                    raise AnsibleError(\"Invalid role line (%s). Proper format is 'role_name[,version[,name]]'\" % role)\n            else:\n                src = role\n\n            if name is None:\n                name = RoleRequirement.repo_url_to_role_name(src)\n            if '+' in src:\n                (scm, src) = src.split('+', 1)\n\n            return dict(name=name, src=src, scm=scm, version=version)\n\n        if 'role' in role:\n            name = role['role']\n            if ',' in name:\n                raise AnsibleError(\"Invalid old style role requirement: %s\" % name)\n            else:\n                del role['role']\n                role['name'] = name\n        else:\n            role = role.copy()\n\n            if 'src'in role:\n                # New style: { src: 'galaxy.role,version,name', other_vars: \"here\" }\n                if 'github.com' in role[\"src\"] and 'http' in role[\"src\"] and '+' not in role[\"src\"] and not role[\"src\"].endswith('.tar.gz'):\n                    role[\"src\"] = \"git+\" + role[\"src\"]\n\n                if '+' in role[\"src\"]:\n                    (scm, src) = role[\"src\"].split('+')\n                    role[\"scm\"] = scm\n                    role[\"src\"] = src\n\n                if 'name' not in role:\n                    role[\"name\"] = RoleRequirement.repo_url_to_role_name(role[\"src\"])\n\n            if 'version' not in role:\n                role['version'] = ''\n\n            if 'scm' not in role:\n                role['scm'] = None\n\n        for key in list(role.keys()):\n            if key not in VALID_SPEC_KEYS:\n                role.pop(key)\n\n        return role",
        "begin_line": 77,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.is_encrypted#111",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.is_encrypted(data)",
        "snippet": "def is_encrypted(data):\n    \"\"\" Test if this is vault encrypted data blob\n\n    :arg data: a byte or text string to test whether it is recognized as vault\n        encrypted data\n    :returns: True if it is recognized.  Otherwise, False.\n    \"\"\"\n    try:\n        # Make sure we have a byte string and that it only contains ascii\n        # bytes.\n        b_data = to_bytes(to_text(data, encoding='ascii', errors='strict', nonstring='strict'), encoding='ascii', errors='strict')\n    except (UnicodeError, TypeError):\n        # The vault format is pure ascii so if we failed to encode to bytes\n        # via ascii we know that this is not vault data.\n        # Similarly, if it's not a string, it's not vault data\n        return False\n\n    if b_data.startswith(b_HEADER):\n        return True\n    return False",
        "begin_line": 111,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006811989100817438,
            "pseudo_dstar_susp": 0.0006811989100817438,
            "pseudo_tarantula_susp": 0.0006821282401091405,
            "pseudo_op2_susp": 0.0006811989100817438,
            "pseudo_barinel_susp": 0.0006821282401091405
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.is_encrypted_file#133",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.is_encrypted_file(file_obj, start_pos=0, count=-1)",
        "snippet": "def is_encrypted_file(file_obj, start_pos=0, count=-1):\n    \"\"\"Test if the contents of a file obj are a vault encrypted data blob.\n\n    :arg file_obj: A file object that will be read from.\n    :kwarg start_pos: A byte offset in the file to start reading the header\n        from.  Defaults to 0, the beginning of the file.\n    :kwarg count: Read up to this number of bytes from the file to determine\n        if it looks like encrypted vault data.  The default is -1, read to the\n        end of file.\n    :returns: True if the file looks like a vault file. Otherwise, False.\n    \"\"\"\n    # read the header and reset the file stream to where it started\n    current_position = file_obj.tell()\n    try:\n        file_obj.seek(start_pos)\n        return is_encrypted(file_obj.read(count))\n\n    finally:\n        file_obj.seek(current_position)",
        "begin_line": 133,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__._parse_vaulttext_envelope#154",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__._parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None)",
        "snippet": "def _parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None):\n\n    b_tmpdata = b_vaulttext_envelope.splitlines()\n    b_tmpheader = b_tmpdata[0].strip().split(b';')\n\n    b_version = b_tmpheader[1].strip()\n    cipher_name = to_text(b_tmpheader[2].strip())\n    vault_id = default_vault_id\n\n    # Only attempt to find vault_id if the vault file is version 1.2 or newer\n    # if self.b_version == b'1.2':\n    if len(b_tmpheader) >= 4:\n        vault_id = to_text(b_tmpheader[3].strip())\n\n    b_ciphertext = b''.join(b_tmpdata[1:])\n\n    return b_ciphertext, b_version, cipher_name, vault_id",
        "begin_line": 154,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.parse_vaulttext_envelope#173",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None, filename=None)",
        "snippet": "def parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None, filename=None):\n    \"\"\"Parse the vaulttext envelope\n\n    When data is saved, it has a header prepended and is formatted into 80\n    character lines.  This method extracts the information from the header\n    and then removes the header and the inserted newlines.  The string returned\n    is suitable for processing by the Cipher classes.\n\n    :arg b_vaulttext: byte str containing the data from a save file\n    :kwarg default_vault_id: The vault_id name to use if the vaulttext does not provide one.\n    :kwarg filename: The filename that the data came from.  This is only\n        used to make better error messages in case the data cannot be\n        decrypted. This is optional.\n    :returns: A tuple of byte str of the vaulttext suitable to pass to parse_vaultext,\n        a byte str of the vault format version,\n        the name of the cipher used, and the vault_id.\n    :raises: AnsibleVaultFormatError: if the vaulttext_envelope format is invalid\n    \"\"\"\n    # used by decrypt\n    default_vault_id = default_vault_id or C.DEFAULT_VAULT_IDENTITY\n\n    try:\n        return _parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id)\n    except Exception as exc:\n        msg = \"Vault envelope format error\"\n        if filename:\n            msg += ' in %s' % (filename)\n        msg += ': %s' % exc\n        raise AnsibleVaultFormatError(msg)",
        "begin_line": 173,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.2811999417504e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.format_vaulttext_envelope#204",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.format_vaulttext_envelope(b_ciphertext, cipher_name, version=None, vault_id=None)",
        "snippet": "def format_vaulttext_envelope(b_ciphertext, cipher_name, version=None, vault_id=None):\n    \"\"\" Add header and format to 80 columns\n\n        :arg b_ciphertext: the encrypted and hexlified data as a byte string\n        :arg cipher_name: unicode cipher name (for ex, u'AES256')\n        :arg version: unicode vault version (for ex, '1.2'). Optional ('1.1' is default)\n        :arg vault_id: unicode vault identifier. If provided, the version will be bumped to 1.2.\n        :returns: a byte str that should be dumped into a file.  It's\n            formatted to 80 char columns and has the header prepended\n    \"\"\"\n\n    if not cipher_name:\n        raise AnsibleError(\"the cipher must be set before adding a header\")\n\n    version = version or '1.1'\n\n    # If we specify a vault_id, use format version 1.2. For no vault_id, stick to 1.1\n    if vault_id and vault_id != u'default':\n        version = '1.2'\n\n    b_version = to_bytes(version, 'utf-8', errors='strict')\n    b_vault_id = to_bytes(vault_id, 'utf-8', errors='strict')\n    b_cipher_name = to_bytes(cipher_name, 'utf-8', errors='strict')\n\n    header_parts = [b_HEADER,\n                    b_version,\n                    b_cipher_name]\n\n    if b_version == b'1.2' and b_vault_id:\n        header_parts.append(b_vault_id)\n\n    header = b';'.join(header_parts)\n\n    b_vaulttext = [header]\n    b_vaulttext += [b_ciphertext[i:i + 80] for i in range(0, len(b_ciphertext), 80)]\n    b_vaulttext += [b'']\n    b_vaulttext = b'\\n'.join(b_vaulttext)\n\n    return b_vaulttext",
        "begin_line": 204,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__._unhexlify#245",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__._unhexlify(b_data)",
        "snippet": "def _unhexlify(b_data):\n    try:\n        return unhexlify(b_data)\n    except (BinasciiError, TypeError) as exc:\n        raise AnsibleVaultFormatError('Vault format unhexlify error: %s' % exc)",
        "begin_line": 245,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__._parse_vaulttext#252",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__._parse_vaulttext(b_vaulttext)",
        "snippet": "def _parse_vaulttext(b_vaulttext):\n    b_vaulttext = _unhexlify(b_vaulttext)\n    b_salt, b_crypted_hmac, b_ciphertext = b_vaulttext.split(b\"\\n\", 2)\n    b_salt = _unhexlify(b_salt)\n    b_ciphertext = _unhexlify(b_ciphertext)\n\n    return b_ciphertext, b_salt, b_crypted_hmac",
        "begin_line": 252,
        "end_line": 258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.521058965102287e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.parse_vaulttext#261",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.parse_vaulttext(b_vaulttext)",
        "snippet": "def parse_vaulttext(b_vaulttext):\n    \"\"\"Parse the vaulttext\n\n    :arg b_vaulttext: byte str containing the vaulttext (ciphertext, salt, crypted_hmac)\n    :returns: A tuple of byte str of the ciphertext suitable for passing to a\n        Cipher class's decrypt() function, a byte str of the salt,\n        and a byte str of the crypted_hmac\n    :raises: AnsibleVaultFormatError: if the vaulttext format is invalid\n    \"\"\"\n    # SPLIT SALT, DIGEST, AND DATA\n    try:\n        return _parse_vaulttext(b_vaulttext)\n    except AnsibleVaultFormatError:\n        raise\n    except Exception as exc:\n        msg = \"Vault vaulttext format error: %s\" % exc\n        raise AnsibleVaultFormatError(msg)",
        "begin_line": 261,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.verify_secret_is_not_empty#280",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.verify_secret_is_not_empty(secret, msg=None)",
        "snippet": "def verify_secret_is_not_empty(secret, msg=None):\n    '''Check the secret against minimal requirements.\n\n    Raises: AnsibleVaultPasswordError if the password does not meet requirements.\n\n    Currently, only requirement is that the password is not None or an empty string.\n    '''\n    msg = msg or 'Invalid vault password was provided'\n    if not secret:\n        raise AnsibleVaultPasswordError(msg)",
        "begin_line": 280,
        "end_line": 289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultSecret.__init__#295",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.VaultSecret.__init__(self, _bytes=None)",
        "snippet": "    def __init__(self, _bytes=None):\n        # FIXME: ? that seems wrong... Unset etc?\n        self._bytes = _bytes",
        "begin_line": 295,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.942034015966679e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultSecret.bytes#300",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.VaultSecret.bytes(self)",
        "snippet": "    def bytes(self):\n        '''The secret as a bytestring.\n\n        Sub classes that store text types will need to override to encode the text to bytes.\n        '''\n        return self._bytes",
        "begin_line": 300,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultSecret.load#307",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.VaultSecret.load(self)",
        "snippet": "    def load(self):\n        return self._bytes",
        "begin_line": 307,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.__init__#314",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.__init__(self, _bytes=None, vault_id=None, prompt_formats=None)",
        "snippet": "    def __init__(self, _bytes=None, vault_id=None, prompt_formats=None):\n        super(PromptVaultSecret, self).__init__(_bytes=_bytes)\n        self.vault_id = vault_id\n\n        if prompt_formats is None:\n            self.prompt_formats = self.default_prompt_formats\n        else:\n            self.prompt_formats = prompt_formats",
        "begin_line": 314,
        "end_line": 321,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.load#327",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.load(self)",
        "snippet": "    def load(self):\n        self._bytes = self.ask_vault_passwords()",
        "begin_line": 327,
        "end_line": 328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.ask_vault_passwords#330",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.ask_vault_passwords(self)",
        "snippet": "    def ask_vault_passwords(self):\n        b_vault_passwords = []\n\n        for prompt_format in self.prompt_formats:\n            prompt = prompt_format % {'vault_id': self.vault_id}\n            try:\n                vault_pass = display.prompt(prompt, private=True)\n            except EOFError:\n                raise AnsibleVaultError('EOFError (ctrl-d) on prompt for (%s)' % self.vault_id)\n\n            verify_secret_is_not_empty(vault_pass)\n\n            b_vault_pass = to_bytes(vault_pass, errors='strict', nonstring='simplerepr').strip()\n            b_vault_passwords.append(b_vault_pass)\n\n        # Make sure the passwords match by comparing them all to the first password\n        for b_vault_password in b_vault_passwords:\n            self.confirm(b_vault_passwords[0], b_vault_password)\n\n        if b_vault_passwords:\n            return b_vault_passwords[0]\n\n        return None",
        "begin_line": 330,
        "end_line": 352,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.confirm#354",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.PromptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.PromptVaultSecret.confirm(self, b_vault_pass_1, b_vault_pass_2)",
        "snippet": "    def confirm(self, b_vault_pass_1, b_vault_pass_2):\n        # enforce no newline chars at the end of passwords\n\n        if b_vault_pass_1 != b_vault_pass_2:\n            # FIXME: more specific exception\n            raise AnsibleError(\"Passwords do not match\")",
        "begin_line": 354,
        "end_line": 359,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.script_is_client#362",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.script_is_client(filename)",
        "snippet": "def script_is_client(filename):\n    '''Determine if a vault secret script is a client script that can be given --vault-id args'''\n\n    # if password script is 'something-client' or 'something-client.[sh|py|rb|etc]'\n    # script_name can still have '.' or could be entire filename if there is no ext\n    script_name, dummy = os.path.splitext(filename)\n\n    # TODO: for now, this is entirely based on filename\n    if script_name.endswith('-client'):\n        return True\n\n    return False",
        "begin_line": 362,
        "end_line": 373,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.get_file_vault_secret#376",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.get_file_vault_secret(filename=None, vault_id=None, encoding=None, loader=None)",
        "snippet": "def get_file_vault_secret(filename=None, vault_id=None, encoding=None, loader=None):\n    this_path = os.path.realpath(os.path.expanduser(filename))\n\n    if not os.path.exists(this_path):\n        raise AnsibleError(\"The vault password file %s was not found\" % this_path)\n\n    if loader.is_executable(this_path):\n        if script_is_client(filename):\n            display.vvvv(u'The vault password file %s is a client script.' % to_text(filename))\n            # TODO: pass vault_id_name to script via cli\n            return ClientScriptVaultSecret(filename=this_path, vault_id=vault_id,\n                                           encoding=encoding, loader=loader)\n        # just a plain vault password script. No args, returns a byte array\n        return ScriptVaultSecret(filename=this_path, encoding=encoding, loader=loader)\n\n    return FileVaultSecret(filename=this_path, encoding=encoding, loader=loader)",
        "begin_line": 376,
        "end_line": 391,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.FileVaultSecret.__init__#396",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.FileVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.FileVaultSecret.__init__(self, filename=None, encoding=None, loader=None)",
        "snippet": "    def __init__(self, filename=None, encoding=None, loader=None):\n        super(FileVaultSecret, self).__init__()\n        self.filename = filename\n        self.loader = loader\n\n        self.encoding = encoding or 'utf8'\n\n        # We could load from file here, but that is eventually a pain to test\n        self._bytes = None\n        self._text = None",
        "begin_line": 396,
        "end_line": 405,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.784524365561264e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.FileVaultSecret.bytes#408",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.FileVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.FileVaultSecret.bytes(self)",
        "snippet": "    def bytes(self):\n        if self._bytes:\n            return self._bytes\n        if self._text:\n            return self._text.encode(self.encoding)\n        return None",
        "begin_line": 408,
        "end_line": 413,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.FileVaultSecret.load#415",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.FileVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.FileVaultSecret.load(self)",
        "snippet": "    def load(self):\n        self._bytes = self._read_file(self.filename)",
        "begin_line": 415,
        "end_line": 416,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.258320257659592e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.FileVaultSecret._read_file#418",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.FileVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.FileVaultSecret._read_file(self, filename)",
        "snippet": "    def _read_file(self, filename):\n        \"\"\"\n        Read a vault password from a file or if executable, execute the script and\n        retrieve password from STDOUT\n        \"\"\"\n\n        # TODO: replace with use of self.loader\n        try:\n            f = open(filename, \"rb\")\n            vault_pass = f.read().strip()\n            f.close()\n        except (OSError, IOError) as e:\n            raise AnsibleError(\"Could not read vault password file %s: %s\" % (filename, e))\n\n        b_vault_data, dummy = self.loader._decrypt_if_vault_data(vault_pass, filename)\n\n        vault_pass = b_vault_data.strip(b'\\r\\n')\n\n        verify_secret_is_not_empty(vault_pass,\n                                   msg='Invalid vault password was provided from file (%s)' % filename)\n\n        return vault_pass",
        "begin_line": 418,
        "end_line": 439,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.FileVaultSecret.__repr__#441",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.FileVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.FileVaultSecret.__repr__(self)",
        "snippet": "    def __repr__(self):\n        if self.filename:\n            return \"%s(filename='%s')\" % (self.__class__.__name__, self.filename)\n        return \"%s()\" % (self.__class__.__name__)",
        "begin_line": 441,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._read_file#448",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._read_file(self, filename)",
        "snippet": "    def _read_file(self, filename):\n        if not self.loader.is_executable(filename):\n            raise AnsibleVaultError(\"The vault password script %s was not executable\" % filename)\n\n        command = self._build_command()\n\n        stdout, stderr, p = self._run(command)\n\n        self._check_results(stdout, stderr, p)\n\n        vault_pass = stdout.strip(b'\\r\\n')\n\n        empty_password_msg = 'Invalid vault password was provided from script (%s)' % filename\n        verify_secret_is_not_empty(vault_pass,\n                                   msg=empty_password_msg)\n\n        return vault_pass",
        "begin_line": 448,
        "end_line": 464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._run#466",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._run(self, command)",
        "snippet": "    def _run(self, command):\n        try:\n            # STDERR not captured to make it easier for users to prompt for input in their scripts\n            p = subprocess.Popen(command, stdout=subprocess.PIPE)\n        except OSError as e:\n            msg_format = \"Problem running vault password script %s (%s).\" \\\n                \" If this is not a script, remove the executable bit from the file.\"\n            msg = msg_format % (self.filename, e)\n\n            raise AnsibleError(msg)\n\n        stdout, stderr = p.communicate()\n        return stdout, stderr, p",
        "begin_line": 466,
        "end_line": 478,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._check_results#480",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._check_results(self, stdout, stderr, popen)",
        "snippet": "    def _check_results(self, stdout, stderr, popen):\n        if popen.returncode != 0:\n            raise AnsibleError(\"Vault password script %s returned non-zero (%s): %s\" %\n                               (self.filename, popen.returncode, stderr))",
        "begin_line": 480,
        "end_line": 483,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._build_command#485",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret",
        "signature": "lib.ansible.parsing.vault.__init__.ScriptVaultSecret._build_command(self)",
        "snippet": "    def _build_command(self):\n        return [self.filename]",
        "begin_line": 485,
        "end_line": 486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.match_secrets#537",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.match_secrets(secrets, target_vault_ids)",
        "snippet": "def match_secrets(secrets, target_vault_ids):\n    '''Find all VaultSecret objects that are mapped to any of the target_vault_ids in secrets'''\n    if not secrets:\n        return []\n\n    matches = [(vault_id, secret) for vault_id, secret in secrets if vault_id in target_vault_ids]\n    return matches",
        "begin_line": 537,
        "end_line": 543,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.match_best_secret#546",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.match_best_secret(secrets, target_vault_ids)",
        "snippet": "def match_best_secret(secrets, target_vault_ids):\n    '''Find the best secret from secrets that matches target_vault_ids\n\n    Since secrets should be ordered so the early secrets are 'better' than later ones, this\n    just finds all the matches, then returns the first secret'''\n    matches = match_secrets(secrets, target_vault_ids)\n    if matches:\n        return matches[0]\n    # raise exception?\n    return None",
        "begin_line": 546,
        "end_line": 555,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.152052639107424e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.match_encrypt_secret#578",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__",
        "signature": "lib.ansible.parsing.vault.__init__.match_encrypt_secret(secrets, encrypt_vault_id=None)",
        "snippet": "def match_encrypt_secret(secrets, encrypt_vault_id=None):\n    '''Find the best/first/only secret in secrets to use for encrypting'''\n\n    display.vvvv(u'encrypt_vault_id=%s' % to_text(encrypt_vault_id))\n    # See if the --encrypt-vault-id matches a vault-id\n    if encrypt_vault_id:\n        return match_encrypt_vault_id_secret(secrets,\n                                             encrypt_vault_id=encrypt_vault_id)\n\n    # Find the best/first secret from secrets since we didnt specify otherwise\n    # ie, consider all of the available secrets as matches\n    _vault_id_matchers = [_vault_id for _vault_id, dummy in secrets]\n    best_secret = match_best_secret(secrets, _vault_id_matchers)\n\n    # can be empty list sans any tuple\n    return best_secret",
        "begin_line": 578,
        "end_line": 593,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.152052639107424e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultLib.__init__#597",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultLib",
        "signature": "lib.ansible.parsing.vault.__init__.VaultLib.__init__(self, secrets=None)",
        "snippet": "    def __init__(self, secrets=None):\n        self.secrets = secrets or []\n        self.cipher_name = None\n        self.b_version = b'1.2'",
        "begin_line": 597,
        "end_line": 600,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005293806246691371,
            "pseudo_dstar_susp": 0.0005293806246691371,
            "pseudo_tarantula_susp": 0.0005293806246691371,
            "pseudo_op2_susp": 0.0005293806246691371,
            "pseudo_barinel_susp": 0.0005293806246691371
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultLib.encrypt#602",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultLib",
        "signature": "lib.ansible.parsing.vault.__init__.VaultLib.encrypt(self, plaintext, secret=None, vault_id=None)",
        "snippet": "    def encrypt(self, plaintext, secret=None, vault_id=None):\n        \"\"\"Vault encrypt a piece of data.\n\n        :arg plaintext: a text or byte string to encrypt.\n        :returns: a utf-8 encoded byte str of encrypted data.  The string\n            contains a header identifying this as vault encrypted data and\n            formatted to newline terminated lines of 80 characters.  This is\n            suitable for dumping as is to a vault file.\n\n        If the string passed in is a text string, it will be encoded to UTF-8\n        before encryption.\n        \"\"\"\n\n        if secret is None:\n            if self.secrets:\n                dummy, secret = match_encrypt_secret(self.secrets)\n            else:\n                raise AnsibleVaultError(\"A vault password must be specified to encrypt data\")\n\n        b_plaintext = to_bytes(plaintext, errors='surrogate_or_strict')\n\n        if is_encrypted(b_plaintext):\n            raise AnsibleError(\"input is already encrypted\")\n\n        if not self.cipher_name or self.cipher_name not in CIPHER_WRITE_WHITELIST:\n            self.cipher_name = u\"AES256\"\n\n        try:\n            this_cipher = CIPHER_MAPPING[self.cipher_name]()\n        except KeyError:\n            raise AnsibleError(u\"{0} cipher could not be found\".format(self.cipher_name))\n\n        # encrypt data\n        if vault_id:\n            display.vvvvv(u'Encrypting with vault_id \"%s\" and vault secret %s' % (to_text(vault_id), to_text(secret)))\n        else:\n            display.vvvvv(u'Encrypting without a vault_id using vault secret %s' % to_text(secret))\n\n        b_ciphertext = this_cipher.encrypt(b_plaintext, secret)\n\n        # format the data for output to the file\n        b_vaulttext = format_vaulttext_envelope(b_ciphertext,\n                                                self.cipher_name,\n                                                vault_id=vault_id)\n        return b_vaulttext",
        "begin_line": 602,
        "end_line": 646,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultLib.decrypt#648",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultLib",
        "signature": "lib.ansible.parsing.vault.__init__.VaultLib.decrypt(self, vaulttext, filename=None)",
        "snippet": "    def decrypt(self, vaulttext, filename=None):\n        '''Decrypt a piece of vault encrypted data.\n\n        :arg vaulttext: a string to decrypt.  Since vault encrypted data is an\n            ascii text format this can be either a byte str or unicode string.\n        :kwarg filename: a filename that the data came from.  This is only\n            used to make better error messages in case the data cannot be\n            decrypted.\n        :returns: a byte string containing the decrypted data and the vault-id that was used\n\n        '''\n        plaintext, vault_id, vault_secret = self.decrypt_and_get_vault_id(vaulttext, filename=filename)\n        return plaintext",
        "begin_line": 648,
        "end_line": 660,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.619047619047618e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultLib.decrypt_and_get_vault_id#662",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultLib",
        "signature": "lib.ansible.parsing.vault.__init__.VaultLib.decrypt_and_get_vault_id(self, vaulttext, filename=None)",
        "snippet": "    def decrypt_and_get_vault_id(self, vaulttext, filename=None):\n        \"\"\"Decrypt a piece of vault encrypted data.\n\n        :arg vaulttext: a string to decrypt.  Since vault encrypted data is an\n            ascii text format this can be either a byte str or unicode string.\n        :kwarg filename: a filename that the data came from.  This is only\n            used to make better error messages in case the data cannot be\n            decrypted.\n        :returns: a byte string containing the decrypted data and the vault-id vault-secret that was used\n\n        \"\"\"\n        b_vaulttext = to_bytes(vaulttext, errors='strict', encoding='utf-8')\n\n        if self.secrets is None:\n            raise AnsibleVaultError(\"A vault password must be specified to decrypt data\")\n\n        if not is_encrypted(b_vaulttext):\n            msg = \"input is not vault encrypted data\"\n            if filename:\n                msg += \"%s is not a vault encrypted file\" % to_native(filename)\n            raise AnsibleError(msg)\n\n        b_vaulttext, dummy, cipher_name, vault_id = parse_vaulttext_envelope(b_vaulttext,\n                                                                             filename=filename)\n\n        # create the cipher object, note that the cipher used for decrypt can\n        # be different than the cipher used for encrypt\n        if cipher_name in CIPHER_WHITELIST:\n            this_cipher = CIPHER_MAPPING[cipher_name]()\n        else:\n            raise AnsibleError(\"{0} cipher could not be found\".format(cipher_name))\n\n        b_plaintext = None\n\n        if not self.secrets:\n            raise AnsibleVaultError('Attempting to decrypt but no vault secrets found')\n\n        # WARNING: Currently, the vault id is not required to match the vault id in the vault blob to\n        #          decrypt a vault properly. The vault id in the vault blob is not part of the encrypted\n        #          or signed vault payload. There is no cryptographic checking/verification/validation of the\n        #          vault blobs vault id. It can be tampered with and changed. The vault id is just a nick\n        #          name to use to pick the best secret and provide some ux/ui info.\n\n        # iterate over all the applicable secrets (all of them by default) until one works...\n        # if we specify a vault_id, only the corresponding vault secret is checked and\n        # we check it first.\n\n        vault_id_matchers = []\n        vault_id_used = None\n        vault_secret_used = None\n\n        if vault_id:\n            display.vvvvv(u'Found a vault_id (%s) in the vaulttext' % to_text(vault_id))\n            vault_id_matchers.append(vault_id)\n            _matches = match_secrets(self.secrets, vault_id_matchers)\n            if _matches:\n                display.vvvvv(u'We have a secret associated with vault id (%s), will try to use to decrypt %s' % (to_text(vault_id), to_text(filename)))\n            else:\n                display.vvvvv(u'Found a vault_id (%s) in the vault text, but we do not have a associated secret (--vault-id)' % to_text(vault_id))\n\n        # Not adding the other secrets to vault_secret_ids enforces a match between the vault_id from the vault_text and\n        # the known vault secrets.\n        if not C.DEFAULT_VAULT_ID_MATCH:\n            # Add all of the known vault_ids as candidates for decrypting a vault.\n            vault_id_matchers.extend([_vault_id for _vault_id, _dummy in self.secrets if _vault_id != vault_id])\n\n        matched_secrets = match_secrets(self.secrets, vault_id_matchers)\n\n        # for vault_secret_id in vault_secret_ids:\n        for vault_secret_id, vault_secret in matched_secrets:\n            display.vvvvv(u'Trying to use vault secret=(%s) id=%s to decrypt %s' % (to_text(vault_secret), to_text(vault_secret_id), to_text(filename)))\n\n            try:\n                # secret = self.secrets[vault_secret_id]\n                display.vvvv(u'Trying secret %s for vault_id=%s' % (to_text(vault_secret), to_text(vault_secret_id)))\n                b_plaintext = this_cipher.decrypt(b_vaulttext, vault_secret)\n                if b_plaintext is not None:\n                    vault_id_used = vault_secret_id\n                    vault_secret_used = vault_secret\n                    file_slug = ''\n                    if filename:\n                        file_slug = ' of \"%s\"' % filename\n                    display.vvvvv(\n                        u'Decrypt%s successful with secret=%s and vault_id=%s' % (to_text(file_slug), to_text(vault_secret), to_text(vault_secret_id))\n                    )\n                    break\n            except AnsibleVaultFormatError as exc:\n                msg = u\"There was a vault format error\"\n                if filename:\n                    msg += u' in %s' % (to_text(filename))\n                msg += u': %s' % exc\n                display.warning(msg)\n                raise\n            except AnsibleError as e:\n                display.vvvv(u'Tried to use the vault secret (%s) to decrypt (%s) but it failed. Error: %s' %\n                             (to_text(vault_secret_id), to_text(filename), e))\n                continue\n        else:\n            msg = \"Decryption failed (no vault secrets were found that could decrypt)\"\n            if filename:\n                msg += \" on %s\" % to_native(filename)\n            raise AnsibleVaultError(msg)\n\n        if b_plaintext is None:\n            msg = \"Decryption failed\"\n            if filename:\n                msg += \" on %s\" % to_native(filename)\n            raise AnsibleError(msg)\n\n        return b_plaintext, vault_id_used, vault_secret_used",
        "begin_line": 662,
        "end_line": 771,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.__init__#776",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.__init__(self, vault=None)",
        "snippet": "    def __init__(self, vault=None):\n        # TODO: it may be more useful to just make VaultSecrets and index of VaultLib objects...\n        self.vault = vault or VaultLib()",
        "begin_line": 776,
        "end_line": 778,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.593591009188245e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor._shred_file_custom#781",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor._shred_file_custom(self, tmp_path)",
        "snippet": "    def _shred_file_custom(self, tmp_path):\n        \"\"\"\"Destroy a file, when shred (core-utils) is not available\n\n        Unix `shred' destroys files \"so that they can be recovered only with great difficulty with\n        specialised hardware, if at all\". It is based on the method from the paper\n        \"Secure Deletion of Data from Magnetic and Solid-State Memory\",\n        Proceedings of the Sixth USENIX Security Symposium (San Jose, California, July 22-25, 1996).\n\n        We do not go to that length to re-implement shred in Python; instead, overwriting with a block\n        of random data should suffice.\n\n        See https://github.com/ansible/ansible/pull/13700 .\n        \"\"\"\n\n        file_len = os.path.getsize(tmp_path)\n\n        if file_len > 0:  # avoid work when file was empty\n            max_chunk_len = min(1024 * 1024 * 2, file_len)\n\n            passes = 3\n            with open(tmp_path, \"wb\") as fh:\n                for _ in range(passes):\n                    fh.seek(0, 0)\n                    # get a random chunk of data, each pass with other length\n                    chunk_len = random.randint(max_chunk_len // 2, max_chunk_len)\n                    data = os.urandom(chunk_len)\n\n                    for _ in range(0, file_len // chunk_len):\n                        fh.write(data)\n                    fh.write(data[:file_len % chunk_len])\n\n                    # FIXME remove this assert once we have unittests to check its accuracy\n                    if fh.tell() != file_len:\n                        raise AnsibleAssertionError()\n\n                    os.fsync(fh)",
        "begin_line": 781,
        "end_line": 816,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor._shred_file#818",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor._shred_file(self, tmp_path)",
        "snippet": "    def _shred_file(self, tmp_path):\n        \"\"\"Securely destroy a decrypted file\n\n        Note standard limitations of GNU shred apply (For flash, overwriting would have no effect\n        due to wear leveling; for other storage systems, the async kernel->filesystem->disk calls never\n        guarantee data hits the disk; etc). Furthermore, if your tmp dirs is on tmpfs (ramdisks),\n        it is a non-issue.\n\n        Nevertheless, some form of overwriting the data (instead of just removing the fs index entry) is\n        a good idea. If shred is not available (e.g. on windows, or no core-utils installed), fall back on\n        a custom shredding method.\n        \"\"\"\n\n        if not os.path.isfile(tmp_path):\n            # file is already gone\n            return\n\n        try:\n            r = subprocess.call(['shred', tmp_path])\n        except (OSError, ValueError):\n            # shred is not available on this system, or some other error occurred.\n            # ValueError caught because macOS El Capitan is raising an\n            # exception big enough to hit a limit in python2-2.7.11 and below.\n            # Symptom is ValueError: insecure pickle when shred is not\n            # installed there.\n            r = 1\n\n        if r != 0:\n            # we could not successfully execute unix shred; therefore, do custom shred.\n            self._shred_file_custom(tmp_path)\n\n        os.remove(tmp_path)",
        "begin_line": 818,
        "end_line": 849,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor._edit_file_helper#851",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor._edit_file_helper(self, filename, secret, existing_data=None, force_save=False, vault_id=None)",
        "snippet": "    def _edit_file_helper(self, filename, secret, existing_data=None, force_save=False, vault_id=None):\n\n        # Create a tempfile\n        root, ext = os.path.splitext(os.path.realpath(filename))\n        fd, tmp_path = tempfile.mkstemp(suffix=ext, dir=C.DEFAULT_LOCAL_TMP)\n\n        cmd = self._editor_shell_command(tmp_path)\n        try:\n            if existing_data:\n                self.write_data(existing_data, fd, shred=False)\n        except Exception:\n            # if an error happens, destroy the decrypted file\n            self._shred_file(tmp_path)\n            raise\n        finally:\n            os.close(fd)\n\n        try:\n            # drop the user into an editor on the tmp file\n            subprocess.call(cmd)\n        except Exception as e:\n            # if an error happens, destroy the decrypted file\n            self._shred_file(tmp_path)\n            raise AnsibleError('Unable to execute the command \"%s\": %s' % (' '.join(cmd), to_native(e)))\n\n        b_tmpdata = self.read_data(tmp_path)\n\n        # Do nothing if the content has not changed\n        if force_save or existing_data != b_tmpdata:\n\n            # encrypt new data and write out to tmp\n            # An existing vaultfile will always be UTF-8,\n            # so decode to unicode here\n            b_ciphertext = self.vault.encrypt(b_tmpdata, secret, vault_id=vault_id)\n            self.write_data(b_ciphertext, tmp_path)\n\n            # shuffle tmp file into place\n            self.shuffle_files(tmp_path, filename)\n            display.vvvvv(u'Saved edited file \"%s\" encrypted using %s and  vault id \"%s\"' % (to_text(filename), to_text(secret), to_text(vault_id)))\n\n        # always shred temp, jic\n        self._shred_file(tmp_path)",
        "begin_line": 851,
        "end_line": 892,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor._real_path#894",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor._real_path(self, filename)",
        "snippet": "    def _real_path(self, filename):\n        # '-' is special to VaultEditor, dont expand it.\n        if filename == '-':\n            return filename\n\n        real_path = os.path.realpath(filename)\n        return real_path",
        "begin_line": 894,
        "end_line": 900,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.encrypt_file#908",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.encrypt_file(self, filename, secret, vault_id=None, output_file=None)",
        "snippet": "    def encrypt_file(self, filename, secret, vault_id=None, output_file=None):\n\n        # A file to be encrypted into a vaultfile could be any encoding\n        # so treat the contents as a byte string.\n\n        # follow the symlink\n        filename = self._real_path(filename)\n\n        b_plaintext = self.read_data(filename)\n        b_ciphertext = self.vault.encrypt(b_plaintext, secret, vault_id=vault_id)\n        self.write_data(b_ciphertext, output_file or filename)",
        "begin_line": 908,
        "end_line": 918,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.decrypt_file#920",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.decrypt_file(self, filename, output_file=None)",
        "snippet": "    def decrypt_file(self, filename, output_file=None):\n\n        # follow the symlink\n        filename = self._real_path(filename)\n\n        ciphertext = self.read_data(filename)\n\n        try:\n            plaintext = self.vault.decrypt(ciphertext, filename=filename)\n        except AnsibleError as e:\n            raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n        self.write_data(plaintext, output_file or filename, shred=False)",
        "begin_line": 920,
        "end_line": 931,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.create_file#933",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.create_file(self, filename, secret, vault_id=None)",
        "snippet": "    def create_file(self, filename, secret, vault_id=None):\n        \"\"\" create a new encrypted file \"\"\"\n\n        dirname = os.path.dirname(filename)\n        if dirname and not os.path.exists(dirname):\n            display.warning(u\"%s does not exist, creating...\" % to_text(dirname))\n            makedirs_safe(dirname)\n\n        # FIXME: If we can raise an error here, we can probably just make it\n        # behave like edit instead.\n        if os.path.isfile(filename):\n            raise AnsibleError(\"%s exists, please use 'edit' instead\" % filename)\n\n        self._edit_file_helper(filename, secret, vault_id=vault_id)",
        "begin_line": 933,
        "end_line": 946,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.edit_file#948",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.edit_file(self, filename)",
        "snippet": "    def edit_file(self, filename):\n        vault_id_used = None\n        vault_secret_used = None\n        # follow the symlink\n        filename = self._real_path(filename)\n\n        b_vaulttext = self.read_data(filename)\n\n        # vault or yaml files are always utf8\n        vaulttext = to_text(b_vaulttext)\n\n        try:\n            # vaulttext gets converted back to bytes, but alas\n            # TODO: return the vault_id that worked?\n            plaintext, vault_id_used, vault_secret_used = self.vault.decrypt_and_get_vault_id(vaulttext)\n        except AnsibleError as e:\n            raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n\n        # Figure out the vault id from the file, to select the right secret to re-encrypt it\n        # (duplicates parts of decrypt, but alas...)\n        dummy, dummy, cipher_name, vault_id = parse_vaulttext_envelope(b_vaulttext, filename=filename)\n\n        # vault id here may not be the vault id actually used for decrypting\n        # as when the edited file has no vault-id but is decrypted by non-default id in secrets\n        # (vault_id=default, while a different vault-id decrypted)\n\n        # we want to get rid of files encrypted with the AES cipher\n        force_save = (cipher_name not in CIPHER_WRITE_WHITELIST)\n\n        # Keep the same vault-id (and version) as in the header\n        self._edit_file_helper(filename, vault_secret_used, existing_data=plaintext, force_save=force_save, vault_id=vault_id)",
        "begin_line": 948,
        "end_line": 978,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.plaintext#980",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.plaintext(self, filename)",
        "snippet": "    def plaintext(self, filename):\n\n        b_vaulttext = self.read_data(filename)\n        vaulttext = to_text(b_vaulttext)\n\n        try:\n            plaintext = self.vault.decrypt(vaulttext, filename=filename)\n            return plaintext\n        except AnsibleError as e:\n            raise AnsibleVaultError(\"%s for %s\" % (to_native(e), to_native(filename)))",
        "begin_line": 980,
        "end_line": 989,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.rekey_file#992",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.rekey_file(self, filename, new_vault_secret, new_vault_id=None)",
        "snippet": "    def rekey_file(self, filename, new_vault_secret, new_vault_id=None):\n\n        # follow the symlink\n        filename = self._real_path(filename)\n\n        prev = os.stat(filename)\n        b_vaulttext = self.read_data(filename)\n        vaulttext = to_text(b_vaulttext)\n\n        display.vvvvv(u'Rekeying file \"%s\" to with new vault-id \"%s\" and vault secret %s' %\n                      (to_text(filename), to_text(new_vault_id), to_text(new_vault_secret)))\n        try:\n            plaintext, vault_id_used, _dummy = self.vault.decrypt_and_get_vault_id(vaulttext)\n        except AnsibleError as e:\n            raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n\n        # This is more or less an assert, see #18247\n        if new_vault_secret is None:\n            raise AnsibleError('The value for the new_password to rekey %s with is not valid' % filename)\n\n        # FIXME: VaultContext...?  could rekey to a different vault_id in the same VaultSecrets\n\n        # Need a new VaultLib because the new vault data can be a different\n        # vault lib format or cipher (for ex, when we migrate 1.0 style vault data to\n        # 1.1 style data we change the version and the cipher). This is where a VaultContext might help\n\n        # the new vault will only be used for encrypting, so it doesn't need the vault secrets\n        # (we will pass one in directly to encrypt)\n        new_vault = VaultLib(secrets={})\n        b_new_vaulttext = new_vault.encrypt(plaintext, new_vault_secret, vault_id=new_vault_id)\n\n        self.write_data(b_new_vaulttext, filename)\n\n        # preserve permissions\n        os.chmod(filename, prev.st_mode)\n        os.chown(filename, prev.st_uid, prev.st_gid)\n\n        display.vvvvv(u'Rekeyed file \"%s\" (decrypted with vault id \"%s\") was encrypted with new vault-id \"%s\" and vault secret %s' %\n                      (to_text(filename), to_text(vault_id_used), to_text(new_vault_id), to_text(new_vault_secret)))",
        "begin_line": 992,
        "end_line": 1030,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.read_data#1032",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.read_data(self, filename)",
        "snippet": "    def read_data(self, filename):\n\n        try:\n            if filename == '-':\n                data = sys.stdin.read()\n            else:\n                with open(filename, \"rb\") as fh:\n                    data = fh.read()\n        except Exception as e:\n            msg = to_native(e)\n            if not msg:\n                msg = repr(e)\n            raise AnsibleError('Unable to read source file (%s): %s' % (to_native(filename), msg))\n\n        return data",
        "begin_line": 1032,
        "end_line": 1046,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.write_data#1048",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.write_data(self, data, thefile, shred=True, mode=384)",
        "snippet": "    def write_data(self, data, thefile, shred=True, mode=0o600):\n        # TODO: add docstrings for arg types since this code is picky about that\n        \"\"\"Write the data bytes to given path\n\n        This is used to write a byte string to a file or stdout. It is used for\n        writing the results of vault encryption or decryption. It is used for\n        saving the ciphertext after encryption and it is also used for saving the\n        plaintext after decrypting a vault. The type of the 'data' arg should be bytes,\n        since in the plaintext case, the original contents can be of any text encoding\n        or arbitrary binary data.\n\n        When used to write the result of vault encryption, the val of the 'data' arg\n        should be a utf-8 encoded byte string and not a text typ and not a text type..\n\n        When used to write the result of vault decryption, the val of the 'data' arg\n        should be a byte string and not a text type.\n\n        :arg data: the byte string (bytes) data\n        :arg thefile: file descriptor or filename to save 'data' to.\n        :arg shred: if shred==True, make sure that the original data is first shredded so that is cannot be recovered.\n        :returns: None\n        \"\"\"\n        # FIXME: do we need this now? data_bytes should always be a utf-8 byte string\n        b_file_data = to_bytes(data, errors='strict')\n\n        # check if we have a file descriptor instead of a path\n        is_fd = False\n        try:\n            is_fd = (isinstance(thefile, int) and fcntl.fcntl(thefile, fcntl.F_GETFD) != -1)\n        except Exception:\n            pass\n\n        if is_fd:\n            # if passed descriptor, use that to ensure secure access, otherwise it is a string.\n            # assumes the fd is securely opened by caller (mkstemp)\n            os.ftruncate(thefile, 0)\n            os.write(thefile, b_file_data)\n        elif thefile == '-':\n            # get a ref to either sys.stdout.buffer for py3 or plain old sys.stdout for py2\n            # We need sys.stdout.buffer on py3 so we can write bytes to it since the plaintext\n            # of the vaulted object could be anything/binary/etc\n            output = getattr(sys.stdout, 'buffer', sys.stdout)\n            output.write(b_file_data)\n        else:\n            # file names are insecure and prone to race conditions, so remove and create securely\n            if os.path.isfile(thefile):\n                if shred:\n                    self._shred_file(thefile)\n                else:\n                    os.remove(thefile)\n\n            # when setting new umask, we get previous as return\n            current_umask = os.umask(0o077)\n            try:\n                try:\n                    # create file with secure permissions\n                    fd = os.open(thefile, os.O_CREAT | os.O_EXCL | os.O_RDWR | os.O_TRUNC, mode)\n                except OSError as ose:\n                    # Want to catch FileExistsError, which doesn't exist in Python 2, so catch OSError\n                    # and compare the error number to get equivalent behavior in Python 2/3\n                    if ose.errno == errno.EEXIST:\n                        raise AnsibleError('Vault file got recreated while we were operating on it: %s' % to_native(ose))\n\n                    raise AnsibleError('Problem creating temporary vault file: %s' % to_native(ose))\n\n                try:\n                    # now write to the file and ensure ours is only data in it\n                    os.ftruncate(fd, 0)\n                    os.write(fd, b_file_data)\n                except OSError as e:\n                    raise AnsibleError('Unable to write to temporary vault file: %s' % to_native(e))\n                finally:\n                    # Make sure the file descriptor is always closed and reset umask\n                    os.close(fd)\n            finally:\n                os.umask(current_umask)",
        "begin_line": 1048,
        "end_line": 1123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor.shuffle_files#1125",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor.shuffle_files(self, src, dest)",
        "snippet": "    def shuffle_files(self, src, dest):\n        prev = None\n        # overwrite dest with src\n        if os.path.isfile(dest):\n            prev = os.stat(dest)\n            # old file 'dest' was encrypted, no need to _shred_file\n            os.remove(dest)\n        shutil.move(src, dest)\n\n        # reset permissions if needed\n        if prev is not None:\n            # TODO: selinux, ACLs, xattr?\n            os.chmod(dest, prev.st_mode)\n            os.chown(dest, prev.st_uid, prev.st_gid)",
        "begin_line": 1125,
        "end_line": 1138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultEditor._editor_shell_command#1140",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultEditor",
        "signature": "lib.ansible.parsing.vault.__init__.VaultEditor._editor_shell_command(self, filename)",
        "snippet": "    def _editor_shell_command(self, filename):\n        env_editor = os.environ.get('EDITOR', 'vi')\n        editor = shlex.split(env_editor)\n        editor.append(filename)\n\n        return editor",
        "begin_line": 1140,
        "end_line": 1145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256.__init__#1163",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256.__init__(self)",
        "snippet": "    def __init__(self):\n        if not HAS_CRYPTOGRAPHY and not HAS_PYCRYPTO:\n            raise AnsibleError(NEED_CRYPTO_LIBRARY)",
        "begin_line": 1163,
        "end_line": 1165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009727626459143969,
            "pseudo_dstar_susp": 0.0012422360248447205,
            "pseudo_tarantula_susp": 0.000791765637371338,
            "pseudo_op2_susp": 0.0012422360248447205,
            "pseudo_barinel_susp": 0.000791765637371338
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._create_key_cryptography#1168",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._create_key_cryptography(b_password, b_salt, key_length, iv_length)",
        "snippet": "    def _create_key_cryptography(b_password, b_salt, key_length, iv_length):\n        kdf = PBKDF2HMAC(\n            algorithm=hashes.SHA256(),\n            length=2 * key_length + iv_length,\n            salt=b_salt,\n            iterations=10000,\n            backend=CRYPTOGRAPHY_BACKEND)\n        b_derivedkey = kdf.derive(b_password)\n\n        return b_derivedkey",
        "begin_line": 1168,
        "end_line": 1177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007251631617113851,
            "pseudo_dstar_susp": 0.0007251631617113851,
            "pseudo_tarantula_susp": 0.0007331378299120235,
            "pseudo_op2_susp": 0.0007251631617113851,
            "pseudo_barinel_susp": 0.0007331378299120235
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._create_key_pycrypto#1185",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._create_key_pycrypto(cls, b_password, b_salt, key_length, iv_length)",
        "snippet": "    def _create_key_pycrypto(cls, b_password, b_salt, key_length, iv_length):\n\n        # make two keys and one iv\n\n        b_derivedkey = PBKDF2_pycrypto(b_password, b_salt, dkLen=(2 * key_length) + iv_length,\n                                       count=10000, prf=cls._pbkdf2_prf)\n        return b_derivedkey",
        "begin_line": 1185,
        "end_line": 1191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005235602094240838,
            "pseudo_dstar_susp": 0.0013123359580052493,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0013123359580052493,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._gen_key_initctr#1194",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._gen_key_initctr(cls, b_password, b_salt)",
        "snippet": "    def _gen_key_initctr(cls, b_password, b_salt):\n        # 16 for AES 128, 32 for AES256\n        key_length = 32\n\n        if HAS_CRYPTOGRAPHY:\n            # AES is a 128-bit block cipher, so IVs and counter nonces are 16 bytes\n            iv_length = algorithms.AES.block_size // 8\n\n            b_derivedkey = cls._create_key_cryptography(b_password, b_salt, key_length, iv_length)\n            b_iv = b_derivedkey[(key_length * 2):(key_length * 2) + iv_length]\n        elif HAS_PYCRYPTO:\n            # match the size used for counter.new to avoid extra work\n            iv_length = 16\n\n            b_derivedkey = cls._create_key_pycrypto(b_password, b_salt, key_length, iv_length)\n            b_iv = hexlify(b_derivedkey[(key_length * 2):(key_length * 2) + iv_length])\n        else:\n            raise AnsibleError(NEED_CRYPTO_LIBRARY + '(Detected in initctr)')\n\n        b_key1 = b_derivedkey[:key_length]\n        b_key2 = b_derivedkey[key_length:(key_length * 2)]\n\n        return b_key1, b_key2, b_iv",
        "begin_line": 1194,
        "end_line": 1216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.168972686214065e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._encrypt_cryptography#1219",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._encrypt_cryptography(b_plaintext, b_key1, b_key2, b_iv)",
        "snippet": "    def _encrypt_cryptography(b_plaintext, b_key1, b_key2, b_iv):\n        cipher = C_Cipher(algorithms.AES(b_key1), modes.CTR(b_iv), CRYPTOGRAPHY_BACKEND)\n        encryptor = cipher.encryptor()\n        padder = padding.PKCS7(algorithms.AES.block_size).padder()\n        b_ciphertext = encryptor.update(padder.update(b_plaintext) + padder.finalize())\n        b_ciphertext += encryptor.finalize()\n\n        # COMBINE SALT, DIGEST AND DATA\n        hmac = HMAC(b_key2, hashes.SHA256(), CRYPTOGRAPHY_BACKEND)\n        hmac.update(b_ciphertext)\n        b_hmac = hmac.finalize()\n\n        return to_bytes(hexlify(b_hmac), errors='surrogate_or_strict'), hexlify(b_ciphertext)",
        "begin_line": 1219,
        "end_line": 1231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.499625018749062e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256.encrypt#1262",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256.encrypt(cls, b_plaintext, secret)",
        "snippet": "    def encrypt(cls, b_plaintext, secret):\n        if secret is None:\n            raise AnsibleVaultError('The secret passed to encrypt() was None')\n        b_salt = os.urandom(32)\n        b_password = secret.bytes\n        b_key1, b_key2, b_iv = cls._gen_key_initctr(b_password, b_salt)\n\n        if HAS_CRYPTOGRAPHY:\n            b_hmac, b_ciphertext = cls._encrypt_cryptography(b_plaintext, b_key1, b_key2, b_iv)\n        elif HAS_PYCRYPTO:\n            b_hmac, b_ciphertext = cls._encrypt_pycrypto(b_plaintext, b_key1, b_key2, b_iv)\n        else:\n            raise AnsibleError(NEED_CRYPTO_LIBRARY + '(Detected in encrypt)')\n\n        b_vaulttext = b'\\n'.join([hexlify(b_salt), b_hmac, b_ciphertext])\n        # Unnecessary but getting rid of it is a backwards incompatible vault\n        # format change\n        b_vaulttext = hexlify(b_vaulttext)\n        return b_vaulttext",
        "begin_line": 1262,
        "end_line": 1280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.499625018749062e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._decrypt_cryptography#1283",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._decrypt_cryptography(cls, b_ciphertext, b_crypted_hmac, b_key1, b_key2, b_iv)",
        "snippet": "    def _decrypt_cryptography(cls, b_ciphertext, b_crypted_hmac, b_key1, b_key2, b_iv):\n        # b_key1, b_key2, b_iv = self._gen_key_initctr(b_password, b_salt)\n        # EXIT EARLY IF DIGEST DOESN'T MATCH\n        hmac = HMAC(b_key2, hashes.SHA256(), CRYPTOGRAPHY_BACKEND)\n        hmac.update(b_ciphertext)\n        try:\n            hmac.verify(_unhexlify(b_crypted_hmac))\n        except InvalidSignature as e:\n            raise AnsibleVaultError('HMAC verification failed: %s' % e)\n\n        cipher = C_Cipher(algorithms.AES(b_key1), modes.CTR(b_iv), CRYPTOGRAPHY_BACKEND)\n        decryptor = cipher.decryptor()\n        unpadder = padding.PKCS7(128).unpadder()\n        b_plaintext = unpadder.update(\n            decryptor.update(b_ciphertext) + decryptor.finalize()\n        ) + unpadder.finalize()\n\n        return b_plaintext",
        "begin_line": 1283,
        "end_line": 1300,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256._is_equal#1303",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256._is_equal(b_a, b_b)",
        "snippet": "    def _is_equal(b_a, b_b):\n        \"\"\"\n        Comparing 2 byte arrrays in constant time\n        to avoid timing attacks.\n\n        It would be nice if there was a library for this but\n        hey.\n        \"\"\"\n        if not (isinstance(b_a, binary_type) and isinstance(b_b, binary_type)):\n            raise TypeError('_is_equal can only be used to compare two byte strings')\n\n        # http://codahale.com/a-lesson-in-timing-attacks/\n        if len(b_a) != len(b_b):\n            return False\n\n        result = 0\n        for b_x, b_y in zip(b_a, b_b):\n            if PY3:\n                result |= b_x ^ b_y\n            else:\n                result |= ord(b_x) ^ ord(b_y)\n        return result == 0",
        "begin_line": 1303,
        "end_line": 1324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.vault.__init__.VaultAES256.decrypt#1350",
        "src_path": "lib/ansible/parsing/vault/__init__.py",
        "class_name": "lib.ansible.parsing.vault.__init__.VaultAES256",
        "signature": "lib.ansible.parsing.vault.__init__.VaultAES256.decrypt(cls, b_vaulttext, secret)",
        "snippet": "    def decrypt(cls, b_vaulttext, secret):\n\n        b_ciphertext, b_salt, b_crypted_hmac = parse_vaulttext(b_vaulttext)\n\n        # TODO: would be nice if a VaultSecret could be passed directly to _decrypt_*\n        #       (move _gen_key_initctr() to a AES256 VaultSecret or VaultContext impl?)\n        # though, likely needs to be python cryptography specific impl that basically\n        # creates a Cipher() with b_key1, a Mode.CTR() with b_iv, and a HMAC() with sign key b_key2\n        b_password = secret.bytes\n\n        b_key1, b_key2, b_iv = cls._gen_key_initctr(b_password, b_salt)\n\n        if HAS_CRYPTOGRAPHY:\n            b_plaintext = cls._decrypt_cryptography(b_ciphertext, b_crypted_hmac, b_key1, b_key2, b_iv)\n        elif HAS_PYCRYPTO:\n            b_plaintext = cls._decrypt_pycrypto(b_ciphertext, b_crypted_hmac, b_key1, b_key2, b_iv)\n        else:\n            raise AnsibleError(NEED_CRYPTO_LIBRARY + '(Detected in decrypt)')\n\n        return b_plaintext",
        "begin_line": 1350,
        "end_line": 1369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.593591009188245e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.hash_params#47",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__",
        "signature": "lib.ansible.playbook.role.__init__.hash_params(params)",
        "snippet": "def hash_params(params):\n    \"\"\"\n    Construct a data structure of parameters that is hashable.\n\n    This requires changing any mutable data structures into immutable ones.\n    We chose a frozenset because role parameters have to be unique.\n\n    .. warning::  this does not handle unhashable scalars.  Two things\n        mitigate that limitation:\n\n        1) There shouldn't be any unhashable scalars specified in the yaml\n        2) Our only choice would be to return an error anyway.\n    \"\"\"\n    # Any container is unhashable if it contains unhashable items (for\n    # instance, tuple() is a Hashable subclass but if it contains a dict, it\n    # cannot be hashed)\n    if isinstance(params, Container) and not isinstance(params, (text_type, binary_type)):\n        if isinstance(params, Mapping):\n            try:\n                # Optimistically hope the contents are all hashable\n                new_params = frozenset(params.items())\n            except TypeError:\n                new_params = set()\n                for k, v in params.items():\n                    # Hash each entry individually\n                    new_params.add((k, hash_params(v)))\n                new_params = frozenset(new_params)\n\n        elif isinstance(params, (Set, Sequence)):\n            try:\n                # Optimistically hope the contents are all hashable\n                new_params = frozenset(params)\n            except TypeError:\n                new_params = set()\n                for v in params:\n                    # Hash each entry individually\n                    new_params.add(hash_params(v))\n                new_params = frozenset(new_params)\n        else:\n            # This is just a guess.\n            new_params = frozenset(params)\n        return new_params\n\n    # Note: We do not handle unhashable scalars but our only choice would be\n    # to raise an error there anyway.\n    return frozenset((params,))",
        "begin_line": 47,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.__init__#100",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.__init__(self, play=None, from_files=None, from_include=False)",
        "snippet": "    def __init__(self, play=None, from_files=None, from_include=False):\n        self._role_name = None\n        self._role_path = None\n        self._role_collection = None\n        self._role_params = dict()\n        self._loader = None\n\n        self._metadata = None\n        self._play = play\n        self._parents = []\n        self._dependencies = []\n        self._task_blocks = []\n        self._handler_blocks = []\n        self._compiled_handler_blocks = None\n        self._default_vars = dict()\n        self._role_vars = dict()\n        self._had_task_run = dict()\n        self._completed = dict()\n\n        if from_files is None:\n            from_files = {}\n        self._from_files = from_files\n\n        # Indicates whether this role was included via include/import_role\n        self.from_include = from_include\n\n        super(Role, self).__init__()",
        "begin_line": 100,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.__repr__#128",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return self.get_name()",
        "begin_line": 128,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_name#131",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_name(self, include_role_fqcn=True)",
        "snippet": "    def get_name(self, include_role_fqcn=True):\n        if include_role_fqcn:\n            return '.'.join(x for x in (self._role_collection, self._role_name) if x)\n        return self._role_name",
        "begin_line": 131,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.load#137",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.load(role_include, play, parent_role=None, from_files=None, from_include=False)",
        "snippet": "    def load(role_include, play, parent_role=None, from_files=None, from_include=False):\n\n        if from_files is None:\n            from_files = {}\n        try:\n            # The ROLE_CACHE is a dictionary of role names, with each entry\n            # containing another dictionary corresponding to a set of parameters\n            # specified for a role as the key and the Role() object itself.\n            # We use frozenset to make the dictionary hashable.\n\n            params = role_include.get_role_params()\n            if role_include.when is not None:\n                params['when'] = role_include.when\n            if role_include.tags is not None:\n                params['tags'] = role_include.tags\n            if from_files is not None:\n                params['from_files'] = from_files\n            if role_include.vars:\n                params['vars'] = role_include.vars\n\n            params['from_include'] = from_include\n\n            hashed_params = hash_params(params)\n            if role_include.get_name() in play.ROLE_CACHE:\n                for (entry, role_obj) in iteritems(play.ROLE_CACHE[role_include.get_name()]):\n                    if hashed_params == entry:\n                        if parent_role:\n                            role_obj.add_parent(parent_role)\n                        return role_obj\n\n            # TODO: need to fix cycle detection in role load (maybe use an empty dict\n            #  for the in-flight in role cache as a sentinel that we're already trying to load\n            #  that role?)\n            # see https://github.com/ansible/ansible/issues/61527\n            r = Role(play=play, from_files=from_files, from_include=from_include)\n            r._load_role_data(role_include, parent_role=parent_role)\n\n            if role_include.get_name() not in play.ROLE_CACHE:\n                play.ROLE_CACHE[role_include.get_name()] = dict()\n\n            # FIXME: how to handle cache keys for collection-based roles, since they're technically adjustable per task?\n            play.ROLE_CACHE[role_include.get_name()][hashed_params] = r\n            return r\n\n        except RuntimeError:\n            raise AnsibleError(\"A recursion loop was detected with the roles specified. Make sure child roles do not have dependencies on parent roles\",\n                               obj=role_include._ds)",
        "begin_line": 137,
        "end_line": 183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role._load_role_data#185",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role._load_role_data(self, role_include, parent_role=None)",
        "snippet": "    def _load_role_data(self, role_include, parent_role=None):\n        self._role_name = role_include.role\n        self._role_path = role_include.get_role_path()\n        self._role_collection = role_include._role_collection\n        self._role_params = role_include.get_role_params()\n        self._variable_manager = role_include.get_variable_manager()\n        self._loader = role_include.get_loader()\n\n        if parent_role:\n            self.add_parent(parent_role)\n\n        # copy over all field attributes from the RoleInclude\n        # update self._attributes directly, to avoid squashing\n        for (attr_name, _) in iteritems(self._valid_attrs):\n            if attr_name in ('when', 'tags'):\n                self._attributes[attr_name] = self._extend_value(\n                    self._attributes[attr_name],\n                    role_include._attributes[attr_name],\n                )\n            else:\n                self._attributes[attr_name] = role_include._attributes[attr_name]\n\n        # vars and default vars are regular dictionaries\n        self._role_vars = self._load_role_yaml('vars', main=self._from_files.get('vars'), allow_dir=True)\n        if self._role_vars is None:\n            self._role_vars = dict()\n        elif not isinstance(self._role_vars, dict):\n            raise AnsibleParserError(\"The vars/main.yml file for role '%s' must contain a dictionary of variables\" % self._role_name)\n\n        self._default_vars = self._load_role_yaml('defaults', main=self._from_files.get('defaults'), allow_dir=True)\n        if self._default_vars is None:\n            self._default_vars = dict()\n        elif not isinstance(self._default_vars, dict):\n            raise AnsibleParserError(\"The defaults/main.yml file for role '%s' must contain a dictionary of variables\" % self._role_name)\n\n        # load the role's other files, if they exist\n        metadata = self._load_role_yaml('meta')\n        if metadata:\n            self._metadata = RoleMetadata.load(metadata, owner=self, variable_manager=self._variable_manager, loader=self._loader)\n            self._dependencies = self._load_dependencies()\n        else:\n            self._metadata = RoleMetadata()\n\n        # reset collections list; roles do not inherit collections from parents, just use the defaults\n        # FUTURE: use a private config default for this so we can allow it to be overridden later\n        self.collections = []\n\n        # configure plugin/collection loading; either prepend the current role's collection or configure legacy plugin loading\n        # FIXME: need exception for explicit ansible.legacy?\n        if self._role_collection:  # this is a collection-hosted role\n            self.collections.insert(0, self._role_collection)\n        else:  # this is a legacy role, but set the default collection if there is one\n            default_collection = AnsibleCollectionLoader().default_collection\n            if default_collection:\n                self.collections.insert(0, default_collection)\n            # legacy role, ensure all plugin dirs under the role are added to plugin search path\n            add_all_plugin_dirs(self._role_path)\n\n        # collections can be specified in metadata for legacy or collection-hosted roles\n        if self._metadata.collections:\n            self.collections.extend((c for c in self._metadata.collections if c not in self.collections))\n\n        # if any collections were specified, ensure that core or legacy synthetic collections are always included\n        if self.collections:\n            # default append collection is core for collection-hosted roles, legacy for others\n            default_append_collection = 'ansible.builtin' if self._role_collection else 'ansible.legacy'\n            if 'ansible.builtin' not in self.collections and 'ansible.legacy' not in self.collections:\n                self.collections.append(default_append_collection)\n\n        task_data = self._load_role_yaml('tasks', main=self._from_files.get('tasks'))\n        if task_data:\n            try:\n                self._task_blocks = load_list_of_blocks(task_data, play=self._play, role=self, loader=self._loader, variable_manager=self._variable_manager)\n            except AssertionError as e:\n                raise AnsibleParserError(\"The tasks/main.yml file for role '%s' must contain a list of tasks\" % self._role_name,\n                                         obj=task_data, orig_exc=e)\n\n        handler_data = self._load_role_yaml('handlers', main=self._from_files.get('handlers'))\n        if handler_data:\n            try:\n                self._handler_blocks = load_list_of_blocks(handler_data, play=self._play, role=self, use_handlers=True, loader=self._loader,\n                                                           variable_manager=self._variable_manager)\n            except AssertionError as e:\n                raise AnsibleParserError(\"The handlers/main.yml file for role '%s' must contain a list of tasks\" % self._role_name,\n                                         obj=handler_data, orig_exc=e)",
        "begin_line": 185,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role._load_role_yaml#271",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role._load_role_yaml(self, subdir, main=None, allow_dir=False)",
        "snippet": "    def _load_role_yaml(self, subdir, main=None, allow_dir=False):\n        file_path = os.path.join(self._role_path, subdir)\n        if self._loader.path_exists(file_path) and self._loader.is_directory(file_path):\n            # Valid extensions and ordering for roles is hard-coded to maintain\n            # role portability\n            extensions = ['.yml', '.yaml', '.json']\n            # If no <main> is specified by the user, look for files with\n            # extensions before bare name. Otherwise, look for bare name first.\n            if main is None:\n                _main = 'main'\n                extensions.append('')\n            else:\n                _main = main\n                extensions.insert(0, '')\n            found_files = self._loader.find_vars_files(file_path, _main, extensions, allow_dir)\n            if found_files:\n                data = {}\n                for found in found_files:\n                    new_data = self._loader.load_from_file(found)\n                    if new_data and allow_dir:\n                        data = combine_vars(data, new_data)\n                    else:\n                        data = new_data\n                return data\n            elif main is not None:\n                raise AnsibleParserError(\"Could not find specified file in role: %s/%s\" % (subdir, main))\n        return None",
        "begin_line": 271,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role._load_dependencies#299",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role._load_dependencies(self)",
        "snippet": "    def _load_dependencies(self):\n        '''\n        Recursively loads role dependencies from the metadata list of\n        dependencies, if it exists\n        '''\n\n        deps = []\n        if self._metadata:\n            for role_include in self._metadata.dependencies:\n                r = Role.load(role_include, play=self._play, parent_role=self)\n                deps.append(r)\n\n        return deps",
        "begin_line": 299,
        "end_line": 311,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.add_parent#315",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.add_parent(self, parent_role)",
        "snippet": "    def add_parent(self, parent_role):\n        ''' adds a role to the list of this roles parents '''\n        if not isinstance(parent_role, Role):\n            raise AnsibleAssertionError()\n\n        if parent_role not in self._parents:\n            self._parents.append(parent_role)",
        "begin_line": 315,
        "end_line": 321,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_parents#323",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_parents(self)",
        "snippet": "    def get_parents(self):\n        return self._parents",
        "begin_line": 323,
        "end_line": 324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_default_vars#326",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_default_vars(self, dep_chain=None)",
        "snippet": "    def get_default_vars(self, dep_chain=None):\n        dep_chain = [] if dep_chain is None else dep_chain\n\n        default_vars = dict()\n        for dep in self.get_all_dependencies():\n            default_vars = combine_vars(default_vars, dep.get_default_vars())\n        if dep_chain:\n            for parent in dep_chain:\n                default_vars = combine_vars(default_vars, parent._default_vars)\n        default_vars = combine_vars(default_vars, self._default_vars)\n        return default_vars",
        "begin_line": 326,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_inherited_vars#338",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_inherited_vars(self, dep_chain=None)",
        "snippet": "    def get_inherited_vars(self, dep_chain=None):\n        dep_chain = [] if dep_chain is None else dep_chain\n\n        inherited_vars = dict()\n\n        if dep_chain:\n            for parent in dep_chain:\n                inherited_vars = combine_vars(inherited_vars, parent._role_vars)\n        return inherited_vars",
        "begin_line": 338,
        "end_line": 346,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_role_params#348",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_role_params(self, dep_chain=None)",
        "snippet": "    def get_role_params(self, dep_chain=None):\n        dep_chain = [] if dep_chain is None else dep_chain\n\n        params = {}\n        if dep_chain:\n            for parent in dep_chain:\n                params = combine_vars(params, parent._role_params)\n        params = combine_vars(params, self._role_params)\n        return params",
        "begin_line": 348,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_vars#358",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_vars(self, dep_chain=None, include_params=True)",
        "snippet": "    def get_vars(self, dep_chain=None, include_params=True):\n        dep_chain = [] if dep_chain is None else dep_chain\n\n        all_vars = self.get_inherited_vars(dep_chain)\n\n        for dep in self.get_all_dependencies():\n            all_vars = combine_vars(all_vars, dep.get_vars(include_params=include_params))\n\n        all_vars = combine_vars(all_vars, self.vars)\n        all_vars = combine_vars(all_vars, self._role_vars)\n        if include_params:\n            all_vars = combine_vars(all_vars, self.get_role_params(dep_chain=dep_chain))\n\n        return all_vars",
        "begin_line": 358,
        "end_line": 371,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_direct_dependencies#373",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_direct_dependencies(self)",
        "snippet": "    def get_direct_dependencies(self):\n        return self._dependencies[:]",
        "begin_line": 373,
        "end_line": 374,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_all_dependencies#376",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_all_dependencies(self)",
        "snippet": "    def get_all_dependencies(self):\n        '''\n        Returns a list of all deps, built recursively from all child dependencies,\n        in the proper order in which they should be executed or evaluated.\n        '''\n\n        child_deps = []\n\n        for dep in self.get_direct_dependencies():\n            for child_dep in dep.get_all_dependencies():\n                child_deps.append(child_dep)\n            child_deps.append(dep)\n\n        return child_deps",
        "begin_line": 376,
        "end_line": 389,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.get_handler_blocks#394",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.get_handler_blocks(self, play, dep_chain=None)",
        "snippet": "    def get_handler_blocks(self, play, dep_chain=None):\n        # Do not recreate this list each time ``get_handler_blocks`` is called.\n        # Cache the results so that we don't potentially overwrite with copied duplicates\n        #\n        # ``get_handler_blocks`` may be called when handling ``import_role`` during parsing\n        # as well as with ``Play.compile_roles_handlers`` from ``TaskExecutor``\n        if self._compiled_handler_blocks:\n            return self._compiled_handler_blocks\n\n        self._compiled_handler_blocks = block_list = []\n\n        # update the dependency chain here\n        if dep_chain is None:\n            dep_chain = []\n        new_dep_chain = dep_chain + [self]\n\n        for dep in self.get_direct_dependencies():\n            dep_blocks = dep.get_handler_blocks(play=play, dep_chain=new_dep_chain)\n            block_list.extend(dep_blocks)\n\n        for task_block in self._handler_blocks:\n            new_task_block = task_block.copy()\n            new_task_block._dep_chain = new_dep_chain\n            new_task_block._play = play\n            block_list.append(new_task_block)\n\n        return block_list",
        "begin_line": 394,
        "end_line": 420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.__init__.Role.compile#430",
        "src_path": "lib/ansible/playbook/role/__init__.py",
        "class_name": "lib.ansible.playbook.role.__init__.Role",
        "signature": "lib.ansible.playbook.role.__init__.Role.compile(self, play, dep_chain=None)",
        "snippet": "    def compile(self, play, dep_chain=None):\n        '''\n        Returns the task list for this role, which is created by first\n        recursively compiling the tasks for all direct dependencies, and\n        then adding on the tasks for this role.\n\n        The role compile() also remembers and saves the dependency chain\n        with each task, so tasks know by which route they were found, and\n        can correctly take their parent's tags/conditionals into account.\n        '''\n\n        block_list = []\n\n        # update the dependency chain here\n        if dep_chain is None:\n            dep_chain = []\n        new_dep_chain = dep_chain + [self]\n\n        deps = self.get_direct_dependencies()\n        for dep in deps:\n            dep_blocks = dep.compile(play=play, dep_chain=new_dep_chain)\n            block_list.extend(dep_blocks)\n\n        for idx, task_block in enumerate(self._task_blocks):\n            new_task_block = task_block.copy()\n            new_task_block._dep_chain = new_dep_chain\n            new_task_block._play = play\n            if idx == len(self._task_blocks) - 1:\n                new_task_block._eor = True\n            block_list.append(new_task_block)\n\n        return block_list",
        "begin_line": 430,
        "end_line": 461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.template.AnsibleJ2Template.new_context#35",
        "src_path": "lib/ansible/template/template.py",
        "class_name": "lib.ansible.template.template.AnsibleJ2Template",
        "signature": "lib.ansible.template.template.AnsibleJ2Template.new_context(self, vars=None, shared=False, locals=None)",
        "snippet": "    def new_context(self, vars=None, shared=False, locals=None):\n        if vars is not None:\n            if isinstance(vars, dict):\n                vars = vars.copy()\n                if locals is not None:\n                    vars.update(locals)\n            else:\n                vars = vars.add_locals(locals)\n        return self.environment.context_class(self.environment, vars, self.name, self.blocks)",
        "begin_line": 35,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006675567423230974,
            "pseudo_dstar_susp": 0.0006675567423230974,
            "pseudo_tarantula_susp": 0.0006684491978609625,
            "pseudo_op2_susp": 0.0006675567423230974,
            "pseudo_barinel_susp": 0.0006684491978609625
        }
    },
    {
        "name": "lib.ansible.playbook.__init__.Playbook.__init__#40",
        "src_path": "lib/ansible/playbook/__init__.py",
        "class_name": "lib.ansible.playbook.__init__.Playbook",
        "signature": "lib.ansible.playbook.__init__.Playbook.__init__(self, loader)",
        "snippet": "    def __init__(self, loader):\n        # Entries in the datastructure of a playbook may\n        # be either a play or an include statement\n        self._entries = []\n        self._basedir = to_text(os.getcwd(), errors='surrogate_or_strict')\n        self._loader = loader\n        self._file_name = None",
        "begin_line": 40,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.__init__.Playbook.load#49",
        "src_path": "lib/ansible/playbook/__init__.py",
        "class_name": "lib.ansible.playbook.__init__.Playbook",
        "signature": "lib.ansible.playbook.__init__.Playbook.load(file_name, variable_manager=None, loader=None)",
        "snippet": "    def load(file_name, variable_manager=None, loader=None):\n        pb = Playbook(loader=loader)\n        pb._load_playbook_data(file_name=file_name, variable_manager=variable_manager)\n        return pb",
        "begin_line": 49,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.__init__.Playbook._load_playbook_data#54",
        "src_path": "lib/ansible/playbook/__init__.py",
        "class_name": "lib.ansible.playbook.__init__.Playbook",
        "signature": "lib.ansible.playbook.__init__.Playbook._load_playbook_data(self, file_name, variable_manager, vars=None)",
        "snippet": "    def _load_playbook_data(self, file_name, variable_manager, vars=None):\n\n        if os.path.isabs(file_name):\n            self._basedir = os.path.dirname(file_name)\n        else:\n            self._basedir = os.path.normpath(os.path.join(self._basedir, os.path.dirname(file_name)))\n\n        # set the loaders basedir\n        cur_basedir = self._loader.get_basedir()\n        self._loader.set_basedir(self._basedir)\n\n        add_all_plugin_dirs(self._basedir)\n\n        self._file_name = file_name\n\n        try:\n            ds = self._loader.load_from_file(os.path.basename(file_name))\n        except UnicodeDecodeError as e:\n            raise AnsibleParserError(\"Could not read playbook (%s) due to encoding issues: %s\" % (file_name, to_native(e)))\n\n        # check for errors and restore the basedir in case this error is caught and handled\n        if ds is None:\n            self._loader.set_basedir(cur_basedir)\n            raise AnsibleParserError(\"Empty playbook, nothing to do\", obj=ds)\n        elif not isinstance(ds, list):\n            self._loader.set_basedir(cur_basedir)\n            raise AnsibleParserError(\"A playbook must be a list of plays, got a %s instead\" % type(ds), obj=ds)\n        elif not ds:\n            display.deprecated(\"Empty plays will currently be skipped, in the future they will cause a syntax error\", version='2.12')\n\n        # Parse the playbook entries. For plays, we simply parse them\n        # using the Play() object, and includes are parsed using the\n        # PlaybookInclude() object\n        for entry in ds:\n            if not isinstance(entry, dict):\n                # restore the basedir in case this error is caught and handled\n                self._loader.set_basedir(cur_basedir)\n                raise AnsibleParserError(\"playbook entries must be either a valid play or an include statement\", obj=entry)\n\n            if any(action in entry for action in ('import_playbook', 'include')):\n                if 'include' in entry:\n                    display.deprecated(\"'include' for playbook includes. You should use 'import_playbook' instead\", version=\"2.12\")\n                pb = PlaybookInclude.load(entry, basedir=self._basedir, variable_manager=variable_manager, loader=self._loader)\n                if pb is not None:\n                    self._entries.extend(pb._entries)\n                else:\n                    which = entry.get('import_playbook', entry.get('include', entry))\n                    display.display(\"skipping playbook '%s' due to conditional test failure\" % which, color=C.COLOR_SKIP)\n            else:\n                entry_obj = Play.load(entry, variable_manager=variable_manager, loader=self._loader, vars=vars)\n                self._entries.append(entry_obj)\n\n        # we're done, so restore the old basedir in the loader\n        self._loader.set_basedir(cur_basedir)",
        "begin_line": 54,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.__init__.Playbook.get_plays#112",
        "src_path": "lib/ansible/playbook/__init__.py",
        "class_name": "lib.ansible.playbook.__init__.Playbook",
        "signature": "lib.ansible.playbook.__init__.Playbook.get_plays(self)",
        "snippet": "    def get_plays(self):\n        return self._entries[:]",
        "begin_line": 112,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.constants._deprecated#32",
        "src_path": "lib/ansible/constants.py",
        "class_name": "lib.ansible.constants",
        "signature": "lib.ansible.constants._deprecated(msg, version='2.8')",
        "snippet": "def _deprecated(msg, version='2.8'):\n    ''' display is not guaranteed here, nor it being the full class, but try anyways, fallback to sys.stderr.write '''\n    try:\n        from ansible.utils.display import Display\n        Display().deprecated(msg, version=version)\n    except Exception:\n        import sys\n        sys.stderr.write(' [DEPRECATED] %s, to be removed in %s\\n' % (msg, version))",
        "begin_line": 32,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.constants.mk_boolean#42",
        "src_path": "lib/ansible/constants.py",
        "class_name": "lib.ansible.constants",
        "signature": "lib.ansible.constants.mk_boolean(value)",
        "snippet": "def mk_boolean(value):\n    ''' moved to module_utils'''\n    _deprecated('ansible.constants.mk_boolean() is deprecated.  Use ansible.module_utils.parsing.convert_bool.boolean() instead')\n    return boolean(value, strict=False)",
        "begin_line": 42,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.ParseResultDottedDict.__init__#547",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.ParseResultDottedDict",
        "signature": "lib.ansible.module_utils.urls.ParseResultDottedDict.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ParseResultDottedDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self",
        "begin_line": 547,
        "end_line": 549,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.ParseResultDottedDict.as_list#551",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.ParseResultDottedDict",
        "signature": "lib.ansible.module_utils.urls.ParseResultDottedDict.as_list(self)",
        "snippet": "    def as_list(self):\n        '''\n        Generate a list from this dict, that looks like the ParseResult named tuple\n        '''\n        return [self.get(k, None) for k in ('scheme', 'netloc', 'path', 'params', 'query', 'fragment')]",
        "begin_line": 551,
        "end_line": 555,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.generic_urlparse#558",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls",
        "signature": "lib.ansible.module_utils.urls.generic_urlparse(parts)",
        "snippet": "def generic_urlparse(parts):\n    '''\n    Returns a dictionary of url parts as parsed by urlparse,\n    but accounts for the fact that older versions of that\n    library do not support named attributes (ie. .netloc)\n    '''\n    generic_parts = ParseResultDottedDict()\n    if hasattr(parts, 'netloc'):\n        # urlparse is newer, just read the fields straight\n        # from the parts object\n        generic_parts['scheme'] = parts.scheme\n        generic_parts['netloc'] = parts.netloc\n        generic_parts['path'] = parts.path\n        generic_parts['params'] = parts.params\n        generic_parts['query'] = parts.query\n        generic_parts['fragment'] = parts.fragment\n        generic_parts['username'] = parts.username\n        generic_parts['password'] = parts.password\n        hostname = parts.hostname\n        if hostname and hostname[0] == '[' and '[' in parts.netloc and ']' in parts.netloc:\n            # Py2.6 doesn't parse IPv6 addresses correctly\n            hostname = parts.netloc.split(']')[0][1:].lower()\n        generic_parts['hostname'] = hostname\n\n        try:\n            port = parts.port\n        except ValueError:\n            # Py2.6 doesn't parse IPv6 addresses correctly\n            netloc = parts.netloc.split('@')[-1].split(']')[-1]\n            if ':' in netloc:\n                port = netloc.split(':')[1]\n                if port:\n                    port = int(port)\n            else:\n                port = None\n        generic_parts['port'] = port\n    else:\n        # we have to use indexes, and then parse out\n        # the other parts not supported by indexing\n        generic_parts['scheme'] = parts[0]\n        generic_parts['netloc'] = parts[1]\n        generic_parts['path'] = parts[2]\n        generic_parts['params'] = parts[3]\n        generic_parts['query'] = parts[4]\n        generic_parts['fragment'] = parts[5]\n        # get the username, password, etc.\n        try:\n            netloc_re = re.compile(r'^((?:\\w)+(?::(?:\\w)+)?@)?([A-Za-z0-9.-]+)(:\\d+)?$')\n            match = netloc_re.match(parts[1])\n            auth = match.group(1)\n            hostname = match.group(2)\n            port = match.group(3)\n            if port:\n                # the capture group for the port will include the ':',\n                # so remove it and convert the port to an integer\n                port = int(port[1:])\n            if auth:\n                # the capture group above includes the @, so remove it\n                # and then split it up based on the first ':' found\n                auth = auth[:-1]\n                username, password = auth.split(':', 1)\n            else:\n                username = password = None\n            generic_parts['username'] = username\n            generic_parts['password'] = password\n            generic_parts['hostname'] = hostname\n            generic_parts['port'] = port\n        except Exception:\n            generic_parts['username'] = None\n            generic_parts['password'] = None\n            generic_parts['hostname'] = parts[1]\n            generic_parts['port'] = None\n    return generic_parts",
        "begin_line": 558,
        "end_line": 630,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.RequestWithMethod.__init__#639",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.RequestWithMethod",
        "signature": "lib.ansible.module_utils.urls.RequestWithMethod.__init__(self, url, method, data=None, headers=None, origin_req_host=None, unverifiable=True)",
        "snippet": "    def __init__(self, url, method, data=None, headers=None, origin_req_host=None, unverifiable=True):\n        if headers is None:\n            headers = {}\n        self._method = method.upper()\n        urllib_request.Request.__init__(self, url, data, headers, origin_req_host, unverifiable)",
        "begin_line": 639,
        "end_line": 643,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.RequestWithMethod.get_method#645",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.RequestWithMethod",
        "signature": "lib.ansible.module_utils.urls.RequestWithMethod.get_method(self)",
        "snippet": "    def get_method(self):\n        if self._method:\n            return self._method\n        else:\n            return urllib_request.Request.get_method(self)",
        "begin_line": 645,
        "end_line": 649,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.RedirectHandlerFactory#652",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls",
        "signature": "lib.ansible.module_utils.urls.RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None)",
        "snippet": "def RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None):\n    \"\"\"This is a class factory that closes over the value of\n    ``follow_redirects`` so that the RedirectHandler class has access to\n    that value without having to use globals, and potentially cause problems\n    where ``open_url`` or ``fetch_url`` are used multiple times in a module.\n    \"\"\"\n\n    class RedirectHandler(urllib_request.HTTPRedirectHandler):\n        \"\"\"This is an implementation of a RedirectHandler to match the\n        functionality provided by httplib2. It will utilize the value of\n        ``follow_redirects`` that is passed into ``RedirectHandlerFactory``\n        to determine how redirects should be handled in urllib2.\n        \"\"\"\n\n        def redirect_request(self, req, fp, code, msg, hdrs, newurl):\n            if not HAS_SSLCONTEXT:\n                handler = maybe_add_ssl_handler(newurl, validate_certs, ca_path=ca_path)\n                if handler:\n                    urllib_request._opener.add_handler(handler)\n\n            # Preserve urllib2 compatibility\n            if follow_redirects == 'urllib2':\n                return urllib_request.HTTPRedirectHandler.redirect_request(self, req, fp, code, msg, hdrs, newurl)\n\n            # Handle disabled redirects\n            elif follow_redirects in ['no', 'none', False]:\n                raise urllib_error.HTTPError(newurl, code, msg, hdrs, fp)\n\n            method = req.get_method()\n\n            # Handle non-redirect HTTP status or invalid follow_redirects\n            if follow_redirects in ['all', 'yes', True]:\n                if code < 300 or code >= 400:\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            elif follow_redirects == 'safe':\n                if code < 300 or code >= 400 or method not in ('GET', 'HEAD'):\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            else:\n                raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n\n            try:\n                # Python 2-3.3\n                data = req.get_data()\n                origin_req_host = req.get_origin_req_host()\n            except AttributeError:\n                # Python 3.4+\n                data = req.data\n                origin_req_host = req.origin_req_host\n\n            # Be conciliant with URIs containing a space\n            newurl = newurl.replace(' ', '%20')\n\n            # Suport redirect with payload and original headers\n            if code in (307, 308):\n                # Preserve payload and headers\n                headers = req.headers\n            else:\n                # Do not preserve payload and filter headers\n                data = None\n                headers = dict((k, v) for k, v in req.headers.items()\n                               if k.lower() not in (\"content-length\", \"content-type\", \"transfer-encoding\"))\n\n                # http://tools.ietf.org/html/rfc7231#section-6.4.4\n                if code == 303 and method != 'HEAD':\n                    method = 'GET'\n\n                # Do what the browsers do, despite standards...\n                # First, turn 302s into GETs.\n                if code == 302 and method != 'HEAD':\n                    method = 'GET'\n\n                # Second, if a POST is responded to with a 301, turn it into a GET.\n                if code == 301 and method == 'POST':\n                    method = 'GET'\n\n            return RequestWithMethod(newurl,\n                                     method=method,\n                                     headers=headers,\n                                     data=data,\n                                     origin_req_host=origin_req_host,\n                                     unverifiable=True,\n                                     )\n\n    return RedirectHandler",
        "begin_line": 652,
        "end_line": 735,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.RedirectHandler.RedirectHandlerFactory#652",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.RedirectHandler",
        "signature": "lib.ansible.module_utils.urls.RedirectHandler.RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None)",
        "snippet": "def RedirectHandlerFactory(follow_redirects=None, validate_certs=True, ca_path=None):\n    \"\"\"This is a class factory that closes over the value of\n    ``follow_redirects`` so that the RedirectHandler class has access to\n    that value without having to use globals, and potentially cause problems\n    where ``open_url`` or ``fetch_url`` are used multiple times in a module.\n    \"\"\"\n\n    class RedirectHandler(urllib_request.HTTPRedirectHandler):\n        \"\"\"This is an implementation of a RedirectHandler to match the\n        functionality provided by httplib2. It will utilize the value of\n        ``follow_redirects`` that is passed into ``RedirectHandlerFactory``\n        to determine how redirects should be handled in urllib2.\n        \"\"\"\n\n        def redirect_request(self, req, fp, code, msg, hdrs, newurl):\n            if not HAS_SSLCONTEXT:\n                handler = maybe_add_ssl_handler(newurl, validate_certs, ca_path=ca_path)\n                if handler:\n                    urllib_request._opener.add_handler(handler)\n\n            # Preserve urllib2 compatibility\n            if follow_redirects == 'urllib2':\n                return urllib_request.HTTPRedirectHandler.redirect_request(self, req, fp, code, msg, hdrs, newurl)\n\n            # Handle disabled redirects\n            elif follow_redirects in ['no', 'none', False]:\n                raise urllib_error.HTTPError(newurl, code, msg, hdrs, fp)\n\n            method = req.get_method()\n\n            # Handle non-redirect HTTP status or invalid follow_redirects\n            if follow_redirects in ['all', 'yes', True]:\n                if code < 300 or code >= 400:\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            elif follow_redirects == 'safe':\n                if code < 300 or code >= 400 or method not in ('GET', 'HEAD'):\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            else:\n                raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n\n            try:\n                # Python 2-3.3\n                data = req.get_data()\n                origin_req_host = req.get_origin_req_host()\n            except AttributeError:\n                # Python 3.4+\n                data = req.data\n                origin_req_host = req.origin_req_host\n\n            # Be conciliant with URIs containing a space\n            newurl = newurl.replace(' ', '%20')\n\n            # Suport redirect with payload and original headers\n            if code in (307, 308):\n                # Preserve payload and headers\n                headers = req.headers\n            else:\n                # Do not preserve payload and filter headers\n                data = None\n                headers = dict((k, v) for k, v in req.headers.items()\n                               if k.lower() not in (\"content-length\", \"content-type\", \"transfer-encoding\"))\n\n                # http://tools.ietf.org/html/rfc7231#section-6.4.4\n                if code == 303 and method != 'HEAD':\n                    method = 'GET'\n\n                # Do what the browsers do, despite standards...\n                # First, turn 302s into GETs.\n                if code == 302 and method != 'HEAD':\n                    method = 'GET'\n\n                # Second, if a POST is responded to with a 301, turn it into a GET.\n                if code == 301 and method == 'POST':\n                    method = 'GET'\n\n            return RequestWithMethod(newurl,\n                                     method=method,\n                                     headers=headers,\n                                     data=data,\n                                     origin_req_host=origin_req_host,\n                                     unverifiable=True,\n                                     )\n\n    return RedirectHandler",
        "begin_line": 652,
        "end_line": 735,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.RedirectHandler.redirect_request#666",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls.RedirectHandler",
        "signature": "lib.ansible.module_utils.urls.RedirectHandler.redirect_request(self, req, fp, code, msg, hdrs, newurl)",
        "snippet": "        def redirect_request(self, req, fp, code, msg, hdrs, newurl):\n            if not HAS_SSLCONTEXT:\n                handler = maybe_add_ssl_handler(newurl, validate_certs, ca_path=ca_path)\n                if handler:\n                    urllib_request._opener.add_handler(handler)\n\n            # Preserve urllib2 compatibility\n            if follow_redirects == 'urllib2':\n                return urllib_request.HTTPRedirectHandler.redirect_request(self, req, fp, code, msg, hdrs, newurl)\n\n            # Handle disabled redirects\n            elif follow_redirects in ['no', 'none', False]:\n                raise urllib_error.HTTPError(newurl, code, msg, hdrs, fp)\n\n            method = req.get_method()\n\n            # Handle non-redirect HTTP status or invalid follow_redirects\n            if follow_redirects in ['all', 'yes', True]:\n                if code < 300 or code >= 400:\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            elif follow_redirects == 'safe':\n                if code < 300 or code >= 400 or method not in ('GET', 'HEAD'):\n                    raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n            else:\n                raise urllib_error.HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n\n            try:\n                # Python 2-3.3\n                data = req.get_data()\n                origin_req_host = req.get_origin_req_host()\n            except AttributeError:\n                # Python 3.4+\n                data = req.data\n                origin_req_host = req.origin_req_host\n\n            # Be conciliant with URIs containing a space\n            newurl = newurl.replace(' ', '%20')\n\n            # Suport redirect with payload and original headers\n            if code in (307, 308):\n                # Preserve payload and headers\n                headers = req.headers\n            else:\n                # Do not preserve payload and filter headers\n                data = None\n                headers = dict((k, v) for k, v in req.headers.items()\n                               if k.lower() not in (\"content-length\", \"content-type\", \"transfer-encoding\"))\n\n                # http://tools.ietf.org/html/rfc7231#section-6.4.4\n                if code == 303 and method != 'HEAD':\n                    method = 'GET'\n\n                # Do what the browsers do, despite standards...\n                # First, turn 302s into GETs.\n                if code == 302 and method != 'HEAD':\n                    method = 'GET'\n\n                # Second, if a POST is responded to with a 301, turn it into a GET.\n                if code == 301 and method == 'POST':\n                    method = 'GET'\n\n            return RequestWithMethod(newurl,\n                                     method=method,\n                                     headers=headers,\n                                     data=data,\n                                     origin_req_host=origin_req_host,\n                                     unverifiable=True,\n                                     )",
        "begin_line": 666,
        "end_line": 733,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.urls.basic_auth_header#1391",
        "src_path": "lib/ansible/module_utils/urls.py",
        "class_name": "lib.ansible.module_utils.urls",
        "signature": "lib.ansible.module_utils.urls.basic_auth_header(username, password)",
        "snippet": "def basic_auth_header(username, password):\n    \"\"\"Takes a username and password and returns a byte string suitable for\n    using as value of an Authorization header to do basic auth.\n    \"\"\"\n    return b\"Basic %s\" % base64.b64encode(to_bytes(\"%s:%s\" % (username, password), errors='surrogate_or_strict'))",
        "begin_line": 1391,
        "end_line": 1395,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations.camel_dict_to_snake_dict#14",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations.camel_dict_to_snake_dict(camel_dict, reversible=False, ignore_list=())",
        "snippet": "def camel_dict_to_snake_dict(camel_dict, reversible=False, ignore_list=()):\n    \"\"\"\n    reversible allows two way conversion of a camelized dict\n    such that snake_dict_to_camel_dict(camel_dict_to_snake_dict(x)) == x\n\n    This is achieved through mapping e.g. HTTPEndpoint to h_t_t_p_endpoint\n    where the default would be simply http_endpoint, which gets turned into\n    HttpEndpoint if recamelized.\n\n    ignore_list is used to avoid converting a sub-tree of a dict. This is\n    particularly important for tags, where keys are case-sensitive. We convert\n    the 'Tags' key but nothing below.\n    \"\"\"\n\n    def value_is_list(camel_list):\n\n        checked_list = []\n        for item in camel_list:\n            if isinstance(item, dict):\n                checked_list.append(camel_dict_to_snake_dict(item, reversible))\n            elif isinstance(item, list):\n                checked_list.append(value_is_list(item))\n            else:\n                checked_list.append(item)\n\n        return checked_list\n\n    snake_dict = {}\n    for k, v in camel_dict.items():\n        if isinstance(v, dict) and k not in ignore_list:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = camel_dict_to_snake_dict(v, reversible)\n        elif isinstance(v, list) and k not in ignore_list:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = value_is_list(v)\n        else:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = v\n\n    return snake_dict",
        "begin_line": 14,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations.value_is_list#28",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations.value_is_list(camel_list)",
        "snippet": "    def value_is_list(camel_list):\n\n        checked_list = []\n        for item in camel_list:\n            if isinstance(item, dict):\n                checked_list.append(camel_dict_to_snake_dict(item, reversible))\n            elif isinstance(item, list):\n                checked_list.append(value_is_list(item))\n            else:\n                checked_list.append(item)\n\n        return checked_list",
        "begin_line": 28,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations._snake_to_camel#77",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations._snake_to_camel(snake, capitalize_first=False)",
        "snippet": "def _snake_to_camel(snake, capitalize_first=False):\n    if capitalize_first:\n        return ''.join(x.capitalize() or '_' for x in snake.split('_'))\n    else:\n        return snake.split('_')[0] + ''.join(x.capitalize() or '_' for x in snake.split('_')[1:])",
        "begin_line": 77,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations._camel_to_snake#84",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations._camel_to_snake(name, reversible=False)",
        "snippet": "def _camel_to_snake(name, reversible=False):\n\n    def prepend_underscore_and_lower(m):\n        return '_' + m.group(0).lower()\n\n    if reversible:\n        upper_pattern = r'[A-Z]'\n    else:\n        # Cope with pluralized abbreviations such as TargetGroupARNs\n        # that would otherwise be rendered target_group_ar_ns\n        upper_pattern = r'[A-Z]{3,}s$'\n\n    s1 = re.sub(upper_pattern, prepend_underscore_and_lower, name)\n    # Handle when there was nothing before the plural_pattern\n    if s1.startswith(\"_\") and not name.startswith(\"_\"):\n        s1 = s1[1:]\n    if reversible:\n        return s1\n\n    # Remainder of solution seems to be https://stackoverflow.com/a/1176023\n    first_cap_pattern = r'(.)([A-Z][a-z]+)'\n    all_cap_pattern = r'([a-z0-9])([A-Z]+)'\n    s2 = re.sub(first_cap_pattern, r'\\1_\\2', s1)\n    return re.sub(all_cap_pattern, r'\\1_\\2', s2).lower()",
        "begin_line": 84,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations.prepend_underscore_and_lower#86",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations.prepend_underscore_and_lower(m)",
        "snippet": "    def prepend_underscore_and_lower(m):\n        return '_' + m.group(0).lower()",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.dict_transformations.dict_merge#110",
        "src_path": "lib/ansible/module_utils/common/dict_transformations.py",
        "class_name": "lib.ansible.module_utils.common.dict_transformations",
        "signature": "lib.ansible.module_utils.common.dict_transformations.dict_merge(a, b)",
        "snippet": "def dict_merge(a, b):\n    '''recursively merges dicts. not just simple a['key'] = b['key'], if\n    both a and b have a key whose value is a dict then dict_merge is called\n    on both values and the result stored in the returned dictionary.'''\n    if not isinstance(b, dict):\n        return b\n    result = deepcopy(a)\n    for k, v in b.items():\n        if k in result and isinstance(result[k], dict):\n            result[k] = dict_merge(result[k], v)\n        else:\n            result[k] = deepcopy(v)\n    return result",
        "begin_line": 110,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.context._init_global_context#32",
        "src_path": "lib/ansible/context.py",
        "class_name": "lib.ansible.context",
        "signature": "lib.ansible.context._init_global_context(cli_args)",
        "snippet": "def _init_global_context(cli_args):\n    \"\"\"Initialize the global context objects\"\"\"\n    global CLIARGS\n    CLIARGS = GlobalCLIArgs.from_options(cli_args)",
        "begin_line": 32,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003952569169960474,
            "pseudo_dstar_susp": 0.017543859649122806,
            "pseudo_tarantula_susp": 0.001941747572815534,
            "pseudo_op2_susp": 0.017543859649122806,
            "pseudo_barinel_susp": 0.001941747572815534
        }
    },
    {
        "name": "lib.ansible.context.inner#47",
        "src_path": "lib/ansible/context.py",
        "class_name": "lib.ansible.context",
        "signature": "lib.ansible.context.inner()",
        "snippet": "    def inner():\n        value = CLIARGS.get(key, default=default)\n        if not shallowcopy:\n            return value\n        elif is_sequence(value):\n            return value[:]\n        elif isinstance(value, (Mapping, Set)):\n            return value.copy()\n        return value",
        "begin_line": 47,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.86766018817389e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.helpers.load_list_of_blocks#34",
        "src_path": "lib/ansible/playbook/helpers.py",
        "class_name": "lib.ansible.playbook.helpers",
        "signature": "lib.ansible.playbook.helpers.load_list_of_blocks(ds, play, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None)",
        "snippet": "def load_list_of_blocks(ds, play, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\n    '''\n    Given a list of mixed task/block data (parsed from YAML),\n    return a list of Block() objects, where implicit blocks\n    are created for each bare Task.\n    '''\n\n    # we import here to prevent a circular dependency with imports\n    from ansible.playbook.block import Block\n\n    if not isinstance(ds, (list, type(None))):\n        raise AnsibleAssertionError('%s should be a list or None but is %s' % (ds, type(ds)))\n\n    block_list = []\n    if ds:\n        count = iter(range(len(ds)))\n        for i in count:\n            block_ds = ds[i]\n            # Implicit blocks are created by bare tasks listed in a play without\n            # an explicit block statement. If we have two implicit blocks in a row,\n            # squash them down to a single block to save processing time later.\n            implicit_blocks = []\n            while block_ds is not None and not Block.is_block(block_ds):\n                implicit_blocks.append(block_ds)\n                i += 1\n                # Advance the iterator, so we don't repeat\n                next(count, None)\n                try:\n                    block_ds = ds[i]\n                except IndexError:\n                    block_ds = None\n\n            # Loop both implicit blocks and block_ds as block_ds is the next in the list\n            for b in (implicit_blocks, block_ds):\n                if b:\n                    block_list.append(\n                        Block.load(\n                            b,\n                            play=play,\n                            parent_block=parent_block,\n                            role=role,\n                            task_include=task_include,\n                            use_handlers=use_handlers,\n                            variable_manager=variable_manager,\n                            loader=loader,\n                        )\n                    )\n\n    return block_list",
        "begin_line": 34,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.helpers.load_list_of_tasks#85",
        "src_path": "lib/ansible/playbook/helpers.py",
        "class_name": "lib.ansible.playbook.helpers",
        "signature": "lib.ansible.playbook.helpers.load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None)",
        "snippet": "def load_list_of_tasks(ds, play, block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\n    '''\n    Given a list of task datastructures (parsed from YAML),\n    return a list of Task() or TaskInclude() objects.\n    '''\n\n    # we import here to prevent a circular dependency with imports\n    from ansible.playbook.block import Block\n    from ansible.playbook.handler import Handler\n    from ansible.playbook.task import Task\n    from ansible.playbook.task_include import TaskInclude\n    from ansible.playbook.role_include import IncludeRole\n    from ansible.playbook.handler_task_include import HandlerTaskInclude\n    from ansible.template import Templar\n\n    if not isinstance(ds, list):\n        raise AnsibleAssertionError('The ds (%s) should be a list but was a %s' % (ds, type(ds)))\n\n    task_list = []\n    for task_ds in ds:\n        if not isinstance(task_ds, dict):\n            raise AnsibleAssertionError('The ds (%s) should be a dict but was a %s' % (ds, type(ds)))\n\n        if 'block' in task_ds:\n            t = Block.load(\n                task_ds,\n                play=play,\n                parent_block=block,\n                role=role,\n                task_include=task_include,\n                use_handlers=use_handlers,\n                variable_manager=variable_manager,\n                loader=loader,\n            )\n            task_list.append(t)\n        else:\n            args_parser = ModuleArgsParser(task_ds)\n            try:\n                (action, args, delegate_to) = args_parser.parse(skip_action_validation=True)\n            except AnsibleParserError as e:\n                # if the raises exception was created with obj=ds args, then it includes the detail\n                # so we dont need to add it so we can just re raise.\n                if e._obj:\n                    raise\n                # But if it wasn't, we can add the yaml object now to get more detail\n                raise AnsibleParserError(to_native(e), obj=task_ds, orig_exc=e)\n\n            if action in ('include', 'import_tasks', 'include_tasks'):\n\n                if use_handlers:\n                    include_class = HandlerTaskInclude\n                else:\n                    include_class = TaskInclude\n\n                t = include_class.load(\n                    task_ds,\n                    block=block,\n                    role=role,\n                    task_include=None,\n                    variable_manager=variable_manager,\n                    loader=loader\n                )\n\n                all_vars = variable_manager.get_vars(play=play, task=t)\n                templar = Templar(loader=loader, variables=all_vars)\n\n                # check to see if this include is dynamic or static:\n                # 1. the user has set the 'static' option to false or true\n                # 2. one of the appropriate config options was set\n                if action == 'include_tasks':\n                    is_static = False\n                elif action == 'import_tasks':\n                    is_static = True\n                elif t.static is not None:\n                    display.deprecated(\"The use of 'static' has been deprecated. \"\n                                       \"Use 'import_tasks' for static inclusion, or 'include_tasks' for dynamic inclusion\", version='2.12')\n                    is_static = t.static\n                else:\n                    is_static = C.DEFAULT_TASK_INCLUDES_STATIC or \\\n                        (use_handlers and C.DEFAULT_HANDLER_INCLUDES_STATIC) or \\\n                        (not templar.is_template(t.args['_raw_params']) and t.all_parents_static() and not t.loop)\n\n                if is_static:\n                    if t.loop is not None:\n                        if action == 'import_tasks':\n                            raise AnsibleParserError(\"You cannot use loops on 'import_tasks' statements. You should use 'include_tasks' instead.\", obj=task_ds)\n                        else:\n                            raise AnsibleParserError(\"You cannot use 'static' on an include with a loop\", obj=task_ds)\n\n                    # we set a flag to indicate this include was static\n                    t.statically_loaded = True\n\n                    # handle relative includes by walking up the list of parent include\n                    # tasks and checking the relative result to see if it exists\n                    parent_include = block\n                    cumulative_path = None\n\n                    found = False\n                    subdir = 'tasks'\n                    if use_handlers:\n                        subdir = 'handlers'\n                    while parent_include is not None:\n                        if not isinstance(parent_include, TaskInclude):\n                            parent_include = parent_include._parent\n                            continue\n                        try:\n                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n                        except AnsibleUndefinedVariable as e:\n                            if not parent_include.statically_loaded:\n                                raise AnsibleParserError(\n                                    \"Error when evaluating variable in dynamic parent include path: %s. \"\n                                    \"When using static imports, the parent dynamic include cannot utilize host facts \"\n                                    \"or variables from inventory\" % parent_include.args.get('_raw_params'),\n                                    obj=task_ds,\n                                    suppress_extended_error=True,\n                                    orig_exc=e\n                                )\n                            raise\n                        if cumulative_path is None:\n                            cumulative_path = parent_include_dir\n                        elif not os.path.isabs(cumulative_path):\n                            cumulative_path = os.path.join(parent_include_dir, cumulative_path)\n                        include_target = templar.template(t.args['_raw_params'])\n                        if t._role:\n                            new_basedir = os.path.join(t._role._role_path, subdir, cumulative_path)\n                            include_file = loader.path_dwim_relative(new_basedir, subdir, include_target)\n                        else:\n                            include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\n\n                        if os.path.exists(include_file):\n                            found = True\n                            break\n                        else:\n                            parent_include = parent_include._parent\n\n                    if not found:\n                        try:\n                            include_target = templar.template(t.args['_raw_params'])\n                        except AnsibleUndefinedVariable as e:\n                            raise AnsibleParserError(\n                                \"Error when evaluating variable in import path: %s.\\n\\n\"\n                                \"When using static imports, ensure that any variables used in their names are defined in vars/vars_files\\n\"\n                                \"or extra-vars passed in from the command line. Static imports cannot use variables from facts or inventory\\n\"\n                                \"sources like group or host vars.\" % t.args['_raw_params'],\n                                obj=task_ds,\n                                suppress_extended_error=True,\n                                orig_exc=e)\n                        if t._role:\n                            include_file = loader.path_dwim_relative(t._role._role_path, subdir, include_target)\n                        else:\n                            include_file = loader.path_dwim(include_target)\n\n                    try:\n                        data = loader.load_from_file(include_file)\n                        if data is None:\n                            display.warning('file %s is empty and had no tasks to include' % include_file)\n                            continue\n                        elif not isinstance(data, list):\n                            raise AnsibleParserError(\"included task files must contain a list of tasks\", obj=data)\n\n                        # since we can't send callbacks here, we display a message directly in\n                        # the same fashion used by the on_include callback. We also do it here,\n                        # because the recursive nature of helper methods means we may be loading\n                        # nested includes, and we want the include order printed correctly\n                        display.vv(\"statically imported: %s\" % include_file)\n                    except AnsibleFileNotFound:\n                        if action != 'include' or t.static or \\\n                           C.DEFAULT_TASK_INCLUDES_STATIC or \\\n                           C.DEFAULT_HANDLER_INCLUDES_STATIC and use_handlers:\n                            raise\n                        display.deprecated(\n                            \"Included file '%s' not found, however since this include is not \"\n                            \"explicitly marked as 'static: yes', we will try and include it dynamically \"\n                            \"later. In the future, this will be an error unless 'static: no' is used \"\n                            \"on the include task. If you do not want missing includes to be considered \"\n                            \"dynamic, use 'static: yes' on the include or set the global ansible.cfg \"\n                            \"options to make all includes static for tasks and/or handlers\" % include_file, version=\"2.12\"\n                        )\n                        task_list.append(t)\n                        continue\n\n                    ti_copy = t.copy(exclude_parent=True)\n                    ti_copy._parent = block\n                    included_blocks = load_list_of_blocks(\n                        data,\n                        play=play,\n                        parent_block=None,\n                        task_include=ti_copy,\n                        role=role,\n                        use_handlers=use_handlers,\n                        loader=loader,\n                        variable_manager=variable_manager,\n                    )\n\n                    # FIXME: remove once 'include' is removed\n                    # pop tags out of the include args, if they were specified there, and assign\n                    # them to the include. If the include already had tags specified, we raise an\n                    # error so that users know not to specify them both ways\n                    tags = ti_copy.vars.pop('tags', [])\n                    if isinstance(tags, string_types):\n                        tags = tags.split(',')\n\n                    if len(tags) > 0:\n                        if action in ('include_tasks', 'import_tasks'):\n                            raise AnsibleParserError('You cannot specify \"tags\" inline to the task, it is a task keyword')\n                        if len(ti_copy.tags) > 0:\n                            raise AnsibleParserError(\n                                \"Include tasks should not specify tags in more than one way (both via args and directly on the task). \"\n                                \"Mixing styles in which tags are specified is prohibited for whole import hierarchy, not only for single import statement\",\n                                obj=task_ds,\n                                suppress_extended_error=True,\n                            )\n                        display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n                                           version=\"2.12\")\n                    else:\n                        tags = ti_copy.tags[:]\n\n                    # now we extend the tags on each of the included blocks\n                    for b in included_blocks:\n                        b.tags = list(set(b.tags).union(tags))\n                    # END FIXME\n\n                    # FIXME: handlers shouldn't need this special handling, but do\n                    #        right now because they don't iterate blocks correctly\n                    if use_handlers:\n                        for b in included_blocks:\n                            task_list.extend(b.block)\n                    else:\n                        task_list.extend(included_blocks)\n                else:\n                    t.is_static = False\n                    task_list.append(t)\n\n            elif action in ('include_role', 'import_role'):\n                ir = IncludeRole.load(\n                    task_ds,\n                    block=block,\n                    role=role,\n                    task_include=None,\n                    variable_manager=variable_manager,\n                    loader=loader,\n                )\n\n                #   1. the user has set the 'static' option to false or true\n                #   2. one of the appropriate config options was set\n                is_static = False\n                if action == 'import_role':\n                    is_static = True\n\n                elif ir.static is not None:\n                    display.deprecated(\"The use of 'static' for 'include_role' has been deprecated. \"\n                                       \"Use 'import_role' for static inclusion, or 'include_role' for dynamic inclusion\", version='2.12')\n                    is_static = ir.static\n\n                if is_static:\n                    if ir.loop is not None:\n                        if action == 'import_role':\n                            raise AnsibleParserError(\"You cannot use loops on 'import_role' statements. You should use 'include_role' instead.\", obj=task_ds)\n                        else:\n                            raise AnsibleParserError(\"You cannot use 'static' on an include_role with a loop\", obj=task_ds)\n\n                    # we set a flag to indicate this include was static\n                    ir.statically_loaded = True\n\n                    # template the role name now, if needed\n                    all_vars = variable_manager.get_vars(play=play, task=ir)\n                    templar = Templar(loader=loader, variables=all_vars)\n                    ir._role_name = templar.template(ir._role_name)\n\n                    # uses compiled list from object\n                    blocks, _ = ir.get_block_list(variable_manager=variable_manager, loader=loader)\n                    task_list.extend(blocks)\n                else:\n                    # passes task object itself for latter generation of list\n                    task_list.append(ir)\n            else:\n                if use_handlers:\n                    t = Handler.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\n                else:\n                    t = Task.load(task_ds, block=block, role=role, task_include=task_include, variable_manager=variable_manager, loader=loader)\n\n                task_list.append(t)\n\n    return task_list",
        "begin_line": 85,
        "end_line": 368,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.helpers.load_list_of_roles#371",
        "src_path": "lib/ansible/playbook/helpers.py",
        "class_name": "lib.ansible.playbook.helpers",
        "signature": "lib.ansible.playbook.helpers.load_list_of_roles(ds, play, current_role_path=None, variable_manager=None, loader=None, collection_search_list=None)",
        "snippet": "def load_list_of_roles(ds, play, current_role_path=None, variable_manager=None, loader=None, collection_search_list=None):\n    \"\"\"\n    Loads and returns a list of RoleInclude objects from the ds list of role definitions\n    :param ds: list of roles to load\n    :param play: calling Play object\n    :param current_role_path: path of the owning role, if any\n    :param variable_manager: varmgr to use for templating\n    :param loader: loader to use for DS parsing/services\n    :param collection_search_list: list of collections to search for unqualified role names\n    :return:\n    \"\"\"\n    # we import here to prevent a circular dependency with imports\n    from ansible.playbook.role.include import RoleInclude\n\n    if not isinstance(ds, list):\n        raise AnsibleAssertionError('ds (%s) should be a list but was a %s' % (ds, type(ds)))\n\n    roles = []\n    for role_def in ds:\n        i = RoleInclude.load(role_def, play=play, current_role_path=current_role_path, variable_manager=variable_manager,\n                             loader=loader, collection_list=collection_search_list)\n        roles.append(i)\n\n    return roles",
        "begin_line": 371,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.context_objects._make_immutable#20",
        "src_path": "lib/ansible/utils/context_objects.py",
        "class_name": "lib.ansible.utils.context_objects",
        "signature": "lib.ansible.utils.context_objects._make_immutable(obj)",
        "snippet": "def _make_immutable(obj):\n    \"\"\"Recursively convert a container and objects inside of it into immutable data types\"\"\"\n    if isinstance(obj, (text_type, binary_type)):\n        # Strings first because they are also sequences\n        return obj\n    elif isinstance(obj, Mapping):\n        temp_dict = {}\n        for key, value in obj.items():\n            if isinstance(value, Container):\n                temp_dict[key] = _make_immutable(value)\n            else:\n                temp_dict[key] = value\n        return ImmutableDict(temp_dict)\n    elif isinstance(obj, Set):\n        temp_set = set()\n        for value in obj:\n            if isinstance(value, Container):\n                temp_set.add(_make_immutable(value))\n            else:\n                temp_set.add(value)\n        return frozenset(temp_set)\n    elif isinstance(obj, Sequence):\n        temp_sequence = []\n        for value in obj:\n            if isinstance(value, Container):\n                temp_sequence.append(_make_immutable(value))\n            else:\n                temp_sequence.append(value)\n        return tuple(temp_sequence)\n\n    return obj",
        "begin_line": 20,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012106537530266344,
            "pseudo_dstar_susp": 0.0015290519877675841,
            "pseudo_tarantula_susp": 0.0010131712259371835,
            "pseudo_op2_susp": 0.0015290519877675841,
            "pseudo_barinel_susp": 0.0010131712259371835
        }
    },
    {
        "name": "lib.ansible.utils.context_objects.CLIArgs.__init__#74",
        "src_path": "lib/ansible/utils/context_objects.py",
        "class_name": "lib.ansible.utils.context_objects.CLIArgs",
        "signature": "lib.ansible.utils.context_objects.CLIArgs.__init__(self, mapping)",
        "snippet": "    def __init__(self, mapping):\n        toplevel = {}\n        for key, value in mapping.items():\n            toplevel[key] = _make_immutable(value)\n        super(CLIArgs, self).__init__(toplevel)",
        "begin_line": 74,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011961722488038277,
            "pseudo_dstar_susp": 0.0015060240963855422,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0015060240963855422,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "lib.ansible.utils.context_objects.CLIArgs.from_options#81",
        "src_path": "lib/ansible/utils/context_objects.py",
        "class_name": "lib.ansible.utils.context_objects.CLIArgs",
        "signature": "lib.ansible.utils.context_objects.CLIArgs.from_options(cls, options)",
        "snippet": "    def from_options(cls, options):\n        return cls(vars(options))",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00392156862745098,
            "pseudo_dstar_susp": 0.01694915254237288,
            "pseudo_tarantula_susp": 0.0019342359767891683,
            "pseudo_op2_susp": 0.01694915254237288,
            "pseudo_barinel_susp": 0.0019342359767891683
        }
    },
    {
        "name": "lib.ansible.cli.console.ConsoleCLI.__init__#54",
        "src_path": "lib/ansible/cli/console.py",
        "class_name": "lib.ansible.cli.console.ConsoleCLI",
        "signature": "lib.ansible.cli.console.ConsoleCLI.__init__(self, args)",
        "snippet": "    def __init__(self, args):\n\n        super(ConsoleCLI, self).__init__(args)\n\n        self.intro = 'Welcome to the ansible console.\\nType help or ? to list commands.\\n'\n\n        self.groups = []\n        self.hosts = []\n        self.pattern = None\n        self.variable_manager = None\n        self.loader = None\n        self.passwords = dict()\n\n        self.modules = None\n        self.cwd = '*'\n\n        # Defaults for these are set from the CLI in run()\n        self.remote_user = None\n        self.become = None\n        self.become_user = None\n        self.become_method = None\n        self.check_mode = None\n        self.diff = None\n        self.forks = None\n\n        cmd.Cmd.__init__(self)",
        "begin_line": 54,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.console.ConsoleCLI.init_parser#81",
        "src_path": "lib/ansible/cli/console.py",
        "class_name": "lib.ansible.cli.console.ConsoleCLI",
        "signature": "lib.ansible.cli.console.ConsoleCLI.init_parser(self)",
        "snippet": "    def init_parser(self):\n        super(ConsoleCLI, self).init_parser(\n            desc=\"REPL console for executing Ansible tasks.\",\n            epilog=\"This is not a live session/connection, each task executes in the background and returns it's results.\"\n        )\n        opt_help.add_runas_options(self.parser)\n        opt_help.add_inventory_options(self.parser)\n        opt_help.add_connect_options(self.parser)\n        opt_help.add_check_options(self.parser)\n        opt_help.add_vault_options(self.parser)\n        opt_help.add_fork_options(self.parser)\n        opt_help.add_module_options(self.parser)\n        opt_help.add_basedir_options(self.parser)\n\n        # options unique to shell\n        self.parser.add_argument('pattern', help='host pattern', metavar='pattern', default='all', nargs='?')\n        self.parser.add_argument('--step', dest='step', action='store_true',\n                                 help=\"one-step-at-a-time: confirm each task before running\")",
        "begin_line": 81,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.console.ConsoleCLI.post_process_args#100",
        "src_path": "lib/ansible/cli/console.py",
        "class_name": "lib.ansible.cli.console.ConsoleCLI",
        "signature": "lib.ansible.cli.console.ConsoleCLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        options = super(ConsoleCLI, self).post_process_args(options)\n        display.verbosity = options.verbosity\n        self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n        return options",
        "begin_line": 100,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.console.ConsoleCLI.helpdefault#364",
        "src_path": "lib/ansible/cli/console.py",
        "class_name": "lib.ansible.cli.console.ConsoleCLI",
        "signature": "lib.ansible.cli.console.ConsoleCLI.helpdefault(self, module_name)",
        "snippet": "    def helpdefault(self, module_name):\n        if module_name in self.modules:\n            in_path = module_loader.find_plugin(module_name)\n            if in_path:\n                oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader)\n                if oc:\n                    display.display(oc['short_description'])\n                    display.display('Parameters:')\n                    for opt in oc['options'].keys():\n                        display.display('  ' + stringc(opt, self.NORMAL_PROMPT) + ' ' + oc['options'][opt]['description'][0])\n                else:\n                    display.error('No documentation found for %s.' % module_name)\n            else:\n                display.error('%s is not a valid command, use ? to list all valid commands.' % module_name)",
        "begin_line": 364,
        "end_line": 377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.console.ConsoleCLI.module_args#398",
        "src_path": "lib/ansible/cli/console.py",
        "class_name": "lib.ansible.cli.console.ConsoleCLI",
        "signature": "lib.ansible.cli.console.ConsoleCLI.module_args(self, module_name)",
        "snippet": "    def module_args(self, module_name):\n        in_path = module_loader.find_plugin(module_name)\n        oc, a, _, _ = plugin_docs.get_docstring(in_path, fragment_loader)\n        return list(oc['options'].keys())",
        "begin_line": 398,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.singleton.Singleton.__call__#21",
        "src_path": "lib/ansible/utils/singleton.py",
        "class_name": "lib.ansible.utils.singleton.Singleton",
        "signature": "lib.ansible.utils.singleton.Singleton.__call__(cls, *args, **kw)",
        "snippet": "    def __call__(cls, *args, **kw):\n        if cls.__instance is not None:\n            return cls.__instance\n\n        with cls.__rlock:\n            if cls.__instance is None:\n                cls.__instance = super(Singleton, cls).__call__(*args, **kw)\n\n        return cls.__instance",
        "begin_line": 21,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026595744680851063,
            "pseudo_dstar_susp": 0.038461538461538464,
            "pseudo_tarantula_susp": 0.0013568521031207597,
            "pseudo_op2_susp": 0.038461538461538464,
            "pseudo_barinel_susp": 0.0013568521031207597
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition.__init__#47",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition.__init__(self, play=None, role_basedir=None, variable_manager=None, loader=None, collection_list=None)",
        "snippet": "    def __init__(self, play=None, role_basedir=None, variable_manager=None, loader=None, collection_list=None):\n\n        super(RoleDefinition, self).__init__()\n\n        self._play = play\n        self._variable_manager = variable_manager\n        self._loader = loader\n\n        self._role_path = None\n        self._role_collection = None\n        self._role_basedir = role_basedir\n        self._role_params = dict()\n        self._collection_list = collection_list",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition.preprocess_data#68",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        # role names that are simply numbers can be parsed by PyYAML\n        # as integers even when quoted, so turn it into a string type\n        if isinstance(ds, int):\n            ds = \"%s\" % ds\n\n        if not isinstance(ds, dict) and not isinstance(ds, string_types) and not isinstance(ds, AnsibleBaseYAMLObject):\n            raise AnsibleAssertionError()\n\n        if isinstance(ds, dict):\n            ds = super(RoleDefinition, self).preprocess_data(ds)\n\n        # save the original ds for use later\n        self._ds = ds\n\n        # we create a new data structure here, using the same\n        # object used internally by the YAML parsing code so we\n        # can preserve file:line:column information if it exists\n        new_ds = AnsibleMapping()\n        if isinstance(ds, AnsibleBaseYAMLObject):\n            new_ds.ansible_pos = ds.ansible_pos\n\n        # first we pull the role name out of the data structure,\n        # and then use that to determine the role path (which may\n        # result in a new role name, if it was a file path)\n        role_name = self._load_role_name(ds)\n        (role_name, role_path) = self._load_role_path(role_name)\n\n        # next, we split the role params out from the valid role\n        # attributes and update the new datastructure with that\n        # result and the role name\n        if isinstance(ds, dict):\n            (new_role_def, role_params) = self._split_role_params(ds)\n            new_ds.update(new_role_def)\n            self._role_params = role_params\n\n        # set the role name in the new ds\n        new_ds['role'] = role_name\n\n        # we store the role path internally\n        self._role_path = role_path\n\n        # and return the cleaned-up data structure\n        return new_ds",
        "begin_line": 68,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition._load_role_name#113",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition._load_role_name(self, ds)",
        "snippet": "    def _load_role_name(self, ds):\n        '''\n        Returns the role name (either the role: or name: field) from\n        the role definition, or (when the role definition is a simple\n        string), just that string\n        '''\n\n        if isinstance(ds, string_types):\n            return ds\n\n        role_name = ds.get('role', ds.get('name'))\n        if not role_name or not isinstance(role_name, string_types):\n            raise AnsibleError('role definitions must contain a role name', obj=ds)\n\n        # if we have the required datastructures, and if the role_name\n        # contains a variable, try and template it now\n        if self._variable_manager:\n            all_vars = self._variable_manager.get_vars(play=self._play)\n            templar = Templar(loader=self._loader, variables=all_vars)\n            role_name = templar.template(role_name)\n\n        return role_name",
        "begin_line": 113,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition._load_role_path#136",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition._load_role_path(self, role_name)",
        "snippet": "    def _load_role_path(self, role_name):\n        '''\n        the 'role', as specified in the ds (or as a bare string), can either\n        be a simple name or a full path. If it is a full path, we use the\n        basename as the role name, otherwise we take the name as-given and\n        append it to the default role path\n        '''\n\n        # create a templar class to template the dependency names, in\n        # case they contain variables\n        if self._variable_manager is not None:\n            all_vars = self._variable_manager.get_vars(play=self._play)\n        else:\n            all_vars = dict()\n\n        templar = Templar(loader=self._loader, variables=all_vars)\n        role_name = templar.template(role_name)\n\n        role_tuple = None\n\n        # try to load as a collection-based role first\n        if self._collection_list or AnsibleCollectionRef.is_valid_fqcr(role_name):\n            role_tuple = get_collection_role_path(role_name, self._collection_list)\n\n        if role_tuple:\n            # we found it, stash collection data and return the name/path tuple\n            self._role_collection = role_tuple[2]\n            return role_tuple[0:2]\n\n        # We didn't find a collection role, look in defined role paths\n        # FUTURE: refactor this to be callable from internal so we can properly order\n        # ansible.legacy searches with the collections keyword\n\n        # we always start the search for roles in the base directory of the playbook\n        role_search_paths = [\n            os.path.join(self._loader.get_basedir(), u'roles'),\n        ]\n\n        # also search in the configured roles path\n        if C.DEFAULT_ROLES_PATH:\n            role_search_paths.extend(C.DEFAULT_ROLES_PATH)\n\n        # next, append the roles basedir, if it was set, so we can\n        # search relative to that directory for dependent roles\n        if self._role_basedir:\n            role_search_paths.append(self._role_basedir)\n\n        # finally as a last resort we look in the current basedir as set\n        # in the loader (which should be the playbook dir itself) but without\n        # the roles/ dir appended\n        role_search_paths.append(self._loader.get_basedir())\n\n        # now iterate through the possible paths and return the first one we find\n        for path in role_search_paths:\n            path = templar.template(path)\n            role_path = unfrackpath(os.path.join(path, role_name))\n            if self._loader.path_exists(role_path):\n                return (role_name, role_path)\n\n        # if not found elsewhere try to extract path from name\n        role_path = unfrackpath(role_name)\n        if self._loader.path_exists(role_path):\n            role_name = os.path.basename(role_name)\n            return (role_name, role_path)\n\n        searches = (self._collection_list or []) + role_search_paths\n        raise AnsibleError(\"the role '%s' was not found in %s\" % (role_name, \":\".join(searches)), obj=self._ds)",
        "begin_line": 136,
        "end_line": 202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition._split_role_params#204",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition._split_role_params(self, ds)",
        "snippet": "    def _split_role_params(self, ds):\n        '''\n        Splits any random role params off from the role spec and store\n        them in a dictionary of params for parsing later\n        '''\n\n        role_def = dict()\n        role_params = dict()\n        base_attribute_names = frozenset(self._valid_attrs.keys())\n        for (key, value) in iteritems(ds):\n            # use the list of FieldAttribute values to determine what is and is not\n            # an extra parameter for this role (or sub-class of this role)\n            # FIXME: hard-coded list of exception key names here corresponds to the\n            #        connection fields in the Base class. There may need to be some\n            #        other mechanism where we exclude certain kinds of field attributes,\n            #        or make this list more automatic in some way so we don't have to\n            #        remember to update it manually.\n            if key not in base_attribute_names:\n                # this key does not match a field attribute, so it must be a role param\n                role_params[key] = value\n            else:\n                # this is a field attribute, so copy it over directly\n                role_def[key] = value\n\n        return (role_def, role_params)",
        "begin_line": 204,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition.get_role_params#230",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition.get_role_params(self)",
        "snippet": "    def get_role_params(self):\n        return self._role_params.copy()",
        "begin_line": 230,
        "end_line": 231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition.get_role_path#233",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition.get_role_path(self)",
        "snippet": "    def get_role_path(self):\n        return self._role_path",
        "begin_line": 233,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.definition.RoleDefinition.get_name#236",
        "src_path": "lib/ansible/playbook/role/definition.py",
        "class_name": "lib.ansible.playbook.role.definition.RoleDefinition",
        "signature": "lib.ansible.playbook.role.definition.RoleDefinition.get_name(self, include_role_fqcn=True)",
        "snippet": "    def get_name(self, include_role_fqcn=True):\n        if include_role_fqcn:\n            return '.'.join(x for x in (self._role_collection, self.role) if x)\n        return self.role",
        "begin_line": 236,
        "end_line": 239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.__init__#31",
        "src_path": "lib/ansible/module_utils/facts/other/ohai.py",
        "class_name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.__init__(self, collectors=None, namespace=None)",
        "snippet": "    def __init__(self, collectors=None, namespace=None):\n        namespace = PrefixFactNamespace(namespace_name='ohai',\n                                        prefix='ohai_')\n        super(OhaiFactCollector, self).__init__(collectors=collectors,\n                                                namespace=namespace)",
        "begin_line": 31,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.find_ohai#37",
        "src_path": "lib/ansible/module_utils/facts/other/ohai.py",
        "class_name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.find_ohai(self, module)",
        "snippet": "    def find_ohai(self, module):\n        ohai_path = module.get_bin_path('ohai')\n        return ohai_path",
        "begin_line": 37,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.run_ohai#41",
        "src_path": "lib/ansible/module_utils/facts/other/ohai.py",
        "class_name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.run_ohai(self, module, ohai_path)",
        "snippet": "    def run_ohai(self, module, ohai_path,):\n        rc, out, err = module.run_command(ohai_path)\n        return rc, out, err",
        "begin_line": 41,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.get_ohai_output#45",
        "src_path": "lib/ansible/module_utils/facts/other/ohai.py",
        "class_name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.get_ohai_output(self, module)",
        "snippet": "    def get_ohai_output(self, module):\n        ohai_path = self.find_ohai(module)\n        if not ohai_path:\n            return None\n\n        rc, out, err = self.run_ohai(module, ohai_path)\n        if rc != 0:\n            return None\n\n        return out",
        "begin_line": 45,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.collect#56",
        "src_path": "lib/ansible/module_utils/facts/other/ohai.py",
        "class_name": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.ohai.OhaiFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        ohai_facts = {}\n        if not module:\n            return ohai_facts\n\n        ohai_output = self.get_ohai_output(module)\n\n        if ohai_output is None:\n            return ohai_facts\n\n        try:\n            ohai_facts = json.loads(ohai_output)\n        except Exception:\n            # FIXME: useful error, logging, something...\n            pass\n\n        return ohai_facts",
        "begin_line": 56,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.user.UserFactCollector.collect#33",
        "src_path": "lib/ansible/module_utils/facts/system/user.py",
        "class_name": "lib.ansible.module_utils.facts.system.user.UserFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.user.UserFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        user_facts = {}\n\n        user_facts['user_id'] = getpass.getuser()\n\n        pwent = pwd.getpwnam(getpass.getuser())\n\n        user_facts['user_uid'] = pwent.pw_uid\n        user_facts['user_gid'] = pwent.pw_gid\n        user_facts['user_gecos'] = pwent.pw_gecos\n        user_facts['user_dir'] = pwent.pw_dir\n        user_facts['user_shell'] = pwent.pw_shell\n        user_facts['real_user_id'] = os.getuid()\n        user_facts['effective_user_id'] = os.geteuid()\n        user_facts['real_group_id'] = os.getgid()\n        user_facts['effective_group_id'] = os.getgid()\n\n        return user_facts",
        "begin_line": 33,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.list.LookupModule.run#39",
        "src_path": "lib/ansible/plugins/lookup/list.py",
        "class_name": "lib.ansible.plugins.lookup.list.LookupModule",
        "signature": "lib.ansible.plugins.lookup.list.LookupModule.run(self, terms, **kwargs)",
        "snippet": "    def run(self, terms, **kwargs):\n        if not isinstance(terms, Sequence):\n            raise AnsibleError(\"with_list expects a list\")\n        return terms",
        "begin_line": 39,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.timeout.wrapper#40",
        "src_path": "lib/ansible/module_utils/facts/timeout.py",
        "class_name": "lib.ansible.module_utils.facts.timeout",
        "signature": "lib.ansible.module_utils.facts.timeout.wrapper(*args, **kwargs)",
        "snippet": "        def wrapper(*args, **kwargs):\n            timeout_value = seconds\n            if timeout_value is None:\n                timeout_value = globals().get('GATHER_TIMEOUT') or DEFAULT_GATHER_TIMEOUT\n\n            pool = mp.ThreadPool(processes=1)\n            res = pool.apply_async(func, args, kwargs)\n            pool.close()\n            try:\n                return res.get(timeout_value)\n            except multiprocessing.TimeoutError:\n                # This is an ansible.module_utils.common.facts.timeout.TimeoutError\n                raise TimeoutError('Timer expired after %s seconds' % timeout_value)",
        "begin_line": 40,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011560693641618498,
            "pseudo_dstar_susp": 0.0010741138560687433,
            "pseudo_tarantula_susp": 0.0029154518950437317,
            "pseudo_op2_susp": 0.0010741138560687433,
            "pseudo_barinel_susp": 0.0029154518950437317
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.__init__#26",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder",
        "signature": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.__init__(self)",
        "snippet": "    def __init__(self):\n        # This is also used by validate-modules to get a module's required utils in base and a collection.\n        self.ps_modules = dict()\n        self.exec_scripts = dict()\n\n        # by defining an explicit dict of cs utils and where they are used, we\n        # can potentially save time by not adding the type multiple times if it\n        # isn't needed\n        self.cs_utils_wrapper = dict()\n        self.cs_utils_module = dict()\n\n        self.ps_version = None\n        self.os_version = None\n        self.become = False\n\n        self._re_cs_module = [\n            # Reference C# module_util in another C# util, this must always be the fully qualified name.\n            # 'using ansible_collections.{namespace}.{collection}.plugins.module_utils.{name}'\n            re.compile(to_bytes(r'(?i)^using\\s((Ansible\\..+)|'\n                                r'(ansible_collections\\.\\w+\\.\\w+\\.plugins\\.module_utils\\.[\\w\\.]+));\\s*$')),\n        ]\n\n        self._re_cs_in_ps_module = [\n            # Reference C# module_util in a PowerShell module\n            # '#AnsibleRequires -CSharpUtil Ansible.{name}'\n            # '#AnsibleRequires -CSharpUtil ansible_collections.{namespace}.{collection}.plugins.module_utils.{name}'\n            # '#AnsibleRequires -CSharpUtil ..module_utils.{name}'\n            re.compile(to_bytes(r'(?i)^#\\s*ansiblerequires\\s+-csharputil\\s+((Ansible\\..+)|'\n                                r'(ansible_collections\\.\\w+\\.\\w+\\.plugins\\.module_utils\\.[\\w\\.]+)|'\n                                r'(\\.[\\w\\.]+))')),\n        ]\n\n        self._re_ps_module = [\n            # Original way of referencing a builtin module_util\n            # '#Requires -Module Ansible.ModuleUtils.{name}\n            re.compile(to_bytes(r'(?i)^#\\s*requires\\s+\\-module(?:s?)\\s*(Ansible\\.ModuleUtils\\..+)')),\n            # New way of referencing a builtin and collection module_util\n            # '#AnsibleRequires -PowerShell Ansible.ModuleUtils.{name}'\n            # '#AnsibleRequires -PowerShell ansible_collections.{namespace}.{collection}.plugins.module_utils.{name}'\n            # '#AnsibleRequires -PowerShell ..module_utils.{name}'\n            re.compile(to_bytes(r'(?i)^#\\s*ansiblerequires\\s+-powershell\\s+((Ansible\\.ModuleUtils\\..+)|'\n                                r'(ansible_collections\\.\\w+\\.\\w+\\.plugins\\.module_utils\\.[\\w\\.]+)|'\n                                r'(\\.[\\w\\.]+))')),\n        ]\n\n        self._re_wrapper = re.compile(to_bytes(r'(?i)^#\\s*ansiblerequires\\s+-wrapper\\s+(\\w*)'))\n        self._re_ps_version = re.compile(to_bytes(r'(?i)^#requires\\s+\\-version\\s+([0-9]+(\\.[0-9]+){0,3})$'))\n        self._re_os_version = re.compile(to_bytes(r'(?i)^#ansiblerequires\\s+\\-osversion\\s+([0-9]+(\\.[0-9]+){0,3})$'))\n        self._re_become = re.compile(to_bytes(r'(?i)^#ansiblerequires\\s+\\-become$'))",
        "begin_line": 26,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.scan_module#76",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder",
        "signature": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.scan_module(self, module_data, fqn=None, wrapper=False, powershell=True)",
        "snippet": "    def scan_module(self, module_data, fqn=None, wrapper=False, powershell=True):\n        lines = module_data.split(b'\\n')\n        module_utils = set()\n        if wrapper:\n            cs_utils = self.cs_utils_wrapper\n        else:\n            cs_utils = self.cs_utils_module\n\n        if powershell:\n            checks = [\n                # PS module contains '#Requires -Module Ansible.ModuleUtils.*'\n                # PS module contains '#AnsibleRequires -Powershell Ansible.*' (or collections module_utils ref)\n                (self._re_ps_module, self.ps_modules, \".psm1\"),\n                # PS module contains '#AnsibleRequires -CSharpUtil Ansible.*' (or collections module_utils ref)\n                (self._re_cs_in_ps_module, cs_utils, \".cs\"),\n            ]\n        else:\n            checks = [\n                # CS module contains 'using Ansible.*;' or 'using ansible_collections.ns.coll.plugins.module_utils.*;'\n                (self._re_cs_module, cs_utils, \".cs\"),\n            ]\n\n        for line in lines:\n            for check in checks:\n                for pattern in check[0]:\n                    match = pattern.match(line)\n                    if match:\n                        # tolerate windows line endings by stripping any remaining\n                        # newline chars\n                        module_util_name = to_text(match.group(1).rstrip())\n\n                        if module_util_name not in check[1].keys():\n                            module_utils.add((module_util_name, check[2], fqn))\n\n                        break\n\n            if powershell:\n                ps_version_match = self._re_ps_version.match(line)\n                if ps_version_match:\n                    self._parse_version_match(ps_version_match, \"ps_version\")\n\n                os_version_match = self._re_os_version.match(line)\n                if os_version_match:\n                    self._parse_version_match(os_version_match, \"os_version\")\n\n                # once become is set, no need to keep on checking recursively\n                if not self.become:\n                    become_match = self._re_become.match(line)\n                    if become_match:\n                        self.become = True\n\n            if wrapper:\n                wrapper_match = self._re_wrapper.match(line)\n                if wrapper_match:\n                    self.scan_exec_script(wrapper_match.group(1).rstrip())\n\n        # recursively drill into each Requires to see if there are any more\n        # requirements\n        for m in set(module_utils):\n            self._add_module(m, wrapper=wrapper)",
        "begin_line": 76,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.scan_exec_script#137",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder",
        "signature": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder.scan_exec_script(self, name)",
        "snippet": "    def scan_exec_script(self, name):\n        # scans lib/ansible/executor/powershell for scripts used in the module\n        # exec side. It also scans these scripts for any dependencies\n        name = to_text(name)\n        if name in self.exec_scripts.keys():\n            return\n\n        data = pkgutil.get_data(\"ansible.executor.powershell\", name + \".ps1\")\n        if data is None:\n            raise AnsibleError(\"Could not find executor powershell script \"\n                               \"for '%s'\" % name)\n\n        b_data = to_bytes(data)\n\n        # remove comments to reduce the payload size in the exec wrappers\n        if C.DEFAULT_DEBUG:\n            exec_script = b_data\n        else:\n            exec_script = _strip_comments(b_data)\n        self.exec_scripts[name] = to_bytes(exec_script)\n        self.scan_module(b_data, wrapper=True, powershell=True)",
        "begin_line": 137,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder._add_module#159",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder",
        "signature": "lib.ansible.executor.powershell.module_manifest.PSModuleDepFinder._add_module(self, name, wrapper=False)",
        "snippet": "    def _add_module(self, name, wrapper=False):\n        m, ext, fqn = name\n        m = to_text(m)\n\n        util_fqn = None\n\n        if m.startswith(\"Ansible.\"):\n            # Builtin util, use plugin loader to get the data\n            mu_path = ps_module_utils_loader.find_plugin(m, ext)\n\n            if not mu_path:\n                raise AnsibleError('Could not find imported module support code '\n                                   'for \\'%s\\'' % m)\n\n            module_util_data = to_bytes(_slurp(mu_path))\n        else:\n            # Collection util, load the package data based on the util import.\n\n            submodules = m.split(\".\")\n            if m.startswith('.'):\n                fqn_submodules = fqn.split('.')\n                for submodule in submodules:\n                    if submodule:\n                        break\n                    del fqn_submodules[-1]\n\n                submodules = fqn_submodules + [s for s in submodules if s]\n\n            n_package_name = to_native('.'.join(submodules[:-1]), errors='surrogate_or_strict')\n            n_resource_name = to_native(submodules[-1] + ext, errors='surrogate_or_strict')\n\n            try:\n                module_util = import_module(n_package_name)\n                module_util_data = to_bytes(pkgutil.get_data(n_package_name, n_resource_name),\n                                            errors='surrogate_or_strict')\n                util_fqn = to_text(\"%s.%s \" % (n_package_name, submodules[-1]), errors='surrogate_or_strict')\n\n                # Get the path of the util which is required for coverage collection.\n                resource_paths = list(module_util.__path__)\n                if len(resource_paths) != 1:\n                    # This should never happen with a collection but we are just being defensive about it.\n                    raise AnsibleError(\"Internal error: Referenced module_util package '%s' contains 0 or multiple \"\n                                       \"import locations when we only expect 1.\" % n_package_name)\n                mu_path = os.path.join(resource_paths[0], n_resource_name)\n            except OSError as err:\n                if err.errno == errno.ENOENT:\n                    raise AnsibleError('Could not find collection imported module support code for \\'%s\\''\n                                       % to_native(m))\n                else:\n                    raise\n\n        util_info = {\n            'data': module_util_data,\n            'path': to_text(mu_path),\n        }\n        if ext == \".psm1\":\n            self.ps_modules[m] = util_info\n        else:\n            if wrapper:\n                self.cs_utils_wrapper[m] = util_info\n            else:\n                self.cs_utils_module[m] = util_info\n        self.scan_module(module_util_data, fqn=util_fqn, wrapper=wrapper, powershell=(ext == \".psm1\"))",
        "begin_line": 159,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest._slurp#240",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest",
        "signature": "lib.ansible.executor.powershell.module_manifest._slurp(path)",
        "snippet": "def _slurp(path):\n    if not os.path.exists(path):\n        raise AnsibleError(\"imported module support code does not exist at %s\"\n                           % os.path.abspath(path))\n    fd = open(path, 'rb')\n    data = fd.read()\n    fd.close()\n    return data",
        "begin_line": 240,
        "end_line": 247,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest._strip_comments#250",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest",
        "signature": "lib.ansible.executor.powershell.module_manifest._strip_comments(source)",
        "snippet": "def _strip_comments(source):\n    # Strip comments and blank lines from the wrapper\n    buf = []\n    start_block = False\n    for line in source.splitlines():\n        l = line.strip()\n\n        if start_block and l.endswith(b'#>'):\n            start_block = False\n            continue\n        elif start_block:\n            continue\n        elif l.startswith(b'<#'):\n            start_block = True\n            continue\n        elif not l or l.startswith(b'#'):\n            continue\n\n        buf.append(line)\n    return b'\\n'.join(buf)",
        "begin_line": 250,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.powershell.module_manifest._create_powershell_wrapper#272",
        "src_path": "lib/ansible/executor/powershell/module_manifest.py",
        "class_name": "lib.ansible.executor.powershell.module_manifest",
        "signature": "lib.ansible.executor.powershell.module_manifest._create_powershell_wrapper(b_module_data, module_path, module_args, environment, async_timeout, become, become_method, become_user, become_password, become_flags, substyle, task_vars, module_fqn)",
        "snippet": "def _create_powershell_wrapper(b_module_data, module_path, module_args,\n                               environment, async_timeout, become,\n                               become_method, become_user, become_password,\n                               become_flags, substyle, task_vars, module_fqn):\n    # creates the manifest/wrapper used in PowerShell/C# modules to enable\n    # things like become and async - this is also called in action/script.py\n\n    # FUTURE: add process_wrapper.ps1 to run module_wrapper in a new process\n    # if running under a persistent connection and substyle is C# so we\n    # don't have type conflicts\n    finder = PSModuleDepFinder()\n    if substyle != 'script':\n        # don't scan the module for util dependencies and other Ansible related\n        # flags if the substyle is 'script' which is set by action/script\n        finder.scan_module(b_module_data, fqn=module_fqn, powershell=(substyle == \"powershell\"))\n\n    module_wrapper = \"module_%s_wrapper\" % substyle\n    exec_manifest = dict(\n        module_entry=to_text(base64.b64encode(b_module_data)),\n        powershell_modules=dict(),\n        csharp_utils=dict(),\n        csharp_utils_module=list(),  # csharp_utils only required by a module\n        module_args=module_args,\n        actions=[module_wrapper],\n        environment=environment,\n        encoded_output=False,\n    )\n    finder.scan_exec_script(module_wrapper)\n\n    if async_timeout > 0:\n        finder.scan_exec_script('exec_wrapper')\n        finder.scan_exec_script('async_watchdog')\n        finder.scan_exec_script('async_wrapper')\n\n        exec_manifest[\"actions\"].insert(0, 'async_watchdog')\n        exec_manifest[\"actions\"].insert(0, 'async_wrapper')\n        exec_manifest[\"async_jid\"] = str(random.randint(0, 999999999999))\n        exec_manifest[\"async_timeout_sec\"] = async_timeout\n        exec_manifest[\"async_startup_timeout\"] = C.config.get_config_value(\"WIN_ASYNC_STARTUP_TIMEOUT\", variables=task_vars)\n\n    if become and become_method.split('.')[-1] == 'runas':  # runas and namespace.collection.runas\n        finder.scan_exec_script('exec_wrapper')\n        finder.scan_exec_script('become_wrapper')\n\n        exec_manifest[\"actions\"].insert(0, 'become_wrapper')\n        exec_manifest[\"become_user\"] = become_user\n        exec_manifest[\"become_password\"] = become_password\n        exec_manifest['become_flags'] = become_flags\n\n    exec_manifest['min_ps_version'] = finder.ps_version\n    exec_manifest['min_os_version'] = finder.os_version\n    if finder.become and 'become_wrapper' not in exec_manifest['actions']:\n        finder.scan_exec_script('exec_wrapper')\n        finder.scan_exec_script('become_wrapper')\n\n        exec_manifest['actions'].insert(0, 'become_wrapper')\n        exec_manifest['become_user'] = 'SYSTEM'\n        exec_manifest['become_password'] = None\n        exec_manifest['become_flags'] = None\n\n    coverage_manifest = dict(\n        module_path=module_path,\n        module_util_paths=dict(),\n        output=None,\n    )\n    coverage_output = C.config.get_config_value('COVERAGE_REMOTE_OUTPUT', variables=task_vars)\n    if coverage_output and substyle == 'powershell':\n        finder.scan_exec_script('coverage_wrapper')\n        coverage_manifest['output'] = coverage_output\n\n        coverage_whitelist = C.config.get_config_value('COVERAGE_REMOTE_WHITELIST', variables=task_vars)\n        coverage_manifest['whitelist'] = coverage_whitelist\n\n    # make sure Ansible.ModuleUtils.AddType is added if any C# utils are used\n    if len(finder.cs_utils_wrapper) > 0 or len(finder.cs_utils_module) > 0:\n        finder._add_module((b\"Ansible.ModuleUtils.AddType\", \".psm1\", None),\n                           wrapper=False)\n\n    # exec_wrapper is only required to be part of the payload if using\n    # become or async, to save on payload space we check if exec_wrapper has\n    # already been added, and remove it manually if it hasn't later\n    exec_required = \"exec_wrapper\" in finder.exec_scripts.keys()\n    finder.scan_exec_script(\"exec_wrapper\")\n    # must contain an empty newline so it runs the begin/process/end block\n    finder.exec_scripts[\"exec_wrapper\"] += b\"\\n\\n\"\n\n    exec_wrapper = finder.exec_scripts[\"exec_wrapper\"]\n    if not exec_required:\n        finder.exec_scripts.pop(\"exec_wrapper\")\n\n    for name, data in finder.exec_scripts.items():\n        b64_data = to_text(base64.b64encode(data))\n        exec_manifest[name] = b64_data\n\n    for name, data in finder.ps_modules.items():\n        b64_data = to_text(base64.b64encode(data['data']))\n        exec_manifest['powershell_modules'][name] = b64_data\n        coverage_manifest['module_util_paths'][name] = data['path']\n\n    cs_utils = {}\n    for cs_util in [finder.cs_utils_wrapper, finder.cs_utils_module]:\n        for name, data in cs_util.items():\n            cs_utils[name] = data['data']\n\n    for name, data in cs_utils.items():\n        b64_data = to_text(base64.b64encode(data))\n        exec_manifest['csharp_utils'][name] = b64_data\n    exec_manifest['csharp_utils_module'] = list(finder.cs_utils_module.keys())\n\n    # To save on the data we are sending across we only add the coverage info if coverage is being run\n    if 'coverage_wrapper' in exec_manifest:\n        exec_manifest['coverage'] = coverage_manifest\n\n    b_json = to_bytes(json.dumps(exec_manifest))\n    # delimit the payload JSON from the wrapper to keep sensitive contents out of scriptblocks (which can be logged)\n    b_data = exec_wrapper + b'\\0\\0\\0\\0' + b_json\n    return b_data",
        "begin_line": 272,
        "end_line": 388,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__getstate__#34",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__getstate__(self)",
        "snippet": "    def __getstate__(self):\n        return self.serialize()",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__setstate__#37",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__setstate__(self, data)",
        "snippet": "    def __setstate__(self, data):\n        return self.deserialize(data)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__eq__#40",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        if not isinstance(other, Host):\n            return False\n        return self._uuid == other._uuid",
        "begin_line": 40,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__ne__#45",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        return not self.__eq__(other)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__hash__#48",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.name)",
        "begin_line": 48,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__repr__#54",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return self.get_name()",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.serialize#57",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.serialize(self)",
        "snippet": "    def serialize(self):\n        groups = []\n        for group in self.groups:\n            groups.append(group.serialize())\n\n        return dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            address=self.address,\n            uuid=self._uuid,\n            groups=groups,\n            implicit=self.implicit,\n        )",
        "begin_line": 57,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.deserialize#71",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.deserialize(self, data)",
        "snippet": "    def deserialize(self, data):\n        self.__init__(gen_uuid=False)\n\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.address = data.get('address', '')\n        self._uuid = data.get('uuid', None)\n        self.implicit = data.get('implicit', False)\n\n        groups = data.get('groups', [])\n        for group_data in groups:\n            g = Group()\n            g.deserialize(group_data)\n            self.groups.append(g)",
        "begin_line": 71,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.__init__#86",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.__init__(self, name=None, port=None, gen_uuid=True)",
        "snippet": "    def __init__(self, name=None, port=None, gen_uuid=True):\n\n        self.vars = {}\n        self.groups = []\n        self._uuid = None\n\n        self.name = name\n        self.address = name\n\n        if port:\n            self.set_variable('ansible_port', int(port))\n\n        if gen_uuid:\n            self._uuid = get_unique_id()\n        self.implicit = False",
        "begin_line": 86,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.get_name#102",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.get_name(self)",
        "snippet": "    def get_name(self):\n        return self.name",
        "begin_line": 102,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.populate_ancestors#105",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.populate_ancestors(self, additions=None)",
        "snippet": "    def populate_ancestors(self, additions=None):\n        # populate ancestors\n        if additions is None:\n            for group in self.groups:\n                self.add_group(group)\n        else:\n            for group in additions:\n                if group not in self.groups:\n                    self.groups.append(group)",
        "begin_line": 105,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.add_group#115",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.add_group(self, group)",
        "snippet": "    def add_group(self, group):\n\n        # populate ancestors first\n        for oldg in group.get_ancestors():\n            if oldg not in self.groups:\n                self.groups.append(oldg)\n\n        # actually add group\n        if group not in self.groups:\n            self.groups.append(group)",
        "begin_line": 115,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.remove_group#126",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.remove_group(self, group)",
        "snippet": "    def remove_group(self, group):\n\n        if group in self.groups:\n            self.groups.remove(group)\n\n            # remove exclusive ancestors, xcept all!\n            for oldg in group.get_ancestors():\n                if oldg.name != 'all':\n                    for childg in self.groups:\n                        if oldg in childg.get_ancestors():\n                            break\n                    else:\n                        self.remove_group(oldg)",
        "begin_line": 126,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.set_variable#140",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.set_variable(self, key, value)",
        "snippet": "    def set_variable(self, key, value):\n        if key in self.vars and isinstance(self.vars[key], MutableMapping) and isinstance(value, Mapping):\n            self.vars[key] = combine_vars(self.vars[key], value)\n        else:\n            self.vars[key] = value",
        "begin_line": 140,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.get_groups#146",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.get_groups(self)",
        "snippet": "    def get_groups(self):\n        return self.groups",
        "begin_line": 146,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.get_magic_vars#149",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.get_magic_vars(self)",
        "snippet": "    def get_magic_vars(self):\n        results = {}\n        results['inventory_hostname'] = self.name\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n        return results",
        "begin_line": 149,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.host.Host.get_vars#157",
        "src_path": "lib/ansible/inventory/host.py",
        "class_name": "lib.ansible.inventory.host.Host",
        "signature": "lib.ansible.inventory.host.Host.get_vars(self)",
        "snippet": "    def get_vars(self):\n        return combine_vars(self.vars, self.get_magic_vars())",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.SharedPluginLoaderObj#69",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__",
        "signature": "lib.ansible.plugins.strategy.__init__.SharedPluginLoaderObj()",
        "snippet": "def SharedPluginLoaderObj():\n    '''This only exists for backwards compat, do not use.\n    '''\n    display.deprecated('SharedPluginLoaderObj is deprecated, please directly use ansible.plugins.loader',\n                       version='2.11')\n    return plugin_loader",
        "begin_line": 69,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.637668983426258e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.results_thread_main#80",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__",
        "signature": "lib.ansible.plugins.strategy.__init__.results_thread_main(strategy)",
        "snippet": "def results_thread_main(strategy):\n    while True:\n        try:\n            result = strategy._final_q.get()\n            if isinstance(result, StrategySentinel):\n                break\n            else:\n                strategy._results_lock.acquire()\n                strategy._results.append(result)\n                strategy._results_lock.release()\n        except (IOError, EOFError):\n            break\n        except Queue.Empty:\n            pass",
        "begin_line": 80,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.debug_closure#96",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__",
        "signature": "lib.ansible.plugins.strategy.__init__.debug_closure(func)",
        "snippet": "def debug_closure(func):\n    \"\"\"Closure to wrap ``StrategyBase._process_pending_results`` and invoke the task debugger\"\"\"\n    @functools.wraps(func)\n    def inner(self, iterator, one_pass=False, max_passes=None):\n        status_to_stats_map = (\n            ('is_failed', 'failures'),\n            ('is_unreachable', 'dark'),\n            ('is_changed', 'changed'),\n            ('is_skipped', 'skipped'),\n        )\n\n        # We don't know the host yet, copy the previous states, for lookup after we process new results\n        prev_host_states = iterator._host_states.copy()\n\n        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes)\n        _processed_results = []\n\n        for result in results:\n            task = result._task\n            host = result._host\n            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n            task_vars = _queued_task_args['task_vars']\n            play_context = _queued_task_args['play_context']\n            # Try to grab the previous host state, if it doesn't exist use get_host_state to generate an empty state\n            try:\n                prev_host_state = prev_host_states[host.name]\n            except KeyError:\n                prev_host_state = iterator.get_host_state(host)\n\n            while result.needs_debugger(globally_enabled=self.debugger_active):\n                next_action = NextAction()\n                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n                dbg.cmdloop()\n\n                if next_action.result == NextAction.REDO:\n                    # rollback host state\n                    self._tqm.clear_failed_hosts()\n                    iterator._host_states[host.name] = prev_host_state\n                    for method, what in status_to_stats_map:\n                        if getattr(result, method)():\n                            self._tqm._stats.decrement(what, host.name)\n                    self._tqm._stats.decrement('ok', host.name)\n\n                    # redo\n                    self._queue_task(host, task, task_vars, play_context)\n\n                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n                    break\n                elif next_action.result == NextAction.CONTINUE:\n                    _processed_results.append(result)\n                    break\n                elif next_action.result == NextAction.EXIT:\n                    # Matches KeyboardInterrupt from bin/ansible\n                    sys.exit(99)\n            else:\n                _processed_results.append(result)\n\n        return _processed_results\n    return inner",
        "begin_line": 96,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.inner#99",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__",
        "signature": "lib.ansible.plugins.strategy.__init__.inner(self, iterator, one_pass=False, max_passes=None)",
        "snippet": "    def inner(self, iterator, one_pass=False, max_passes=None):\n        status_to_stats_map = (\n            ('is_failed', 'failures'),\n            ('is_unreachable', 'dark'),\n            ('is_changed', 'changed'),\n            ('is_skipped', 'skipped'),\n        )\n\n        # We don't know the host yet, copy the previous states, for lookup after we process new results\n        prev_host_states = iterator._host_states.copy()\n\n        results = func(self, iterator, one_pass=one_pass, max_passes=max_passes)\n        _processed_results = []\n\n        for result in results:\n            task = result._task\n            host = result._host\n            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n            task_vars = _queued_task_args['task_vars']\n            play_context = _queued_task_args['play_context']\n            # Try to grab the previous host state, if it doesn't exist use get_host_state to generate an empty state\n            try:\n                prev_host_state = prev_host_states[host.name]\n            except KeyError:\n                prev_host_state = iterator.get_host_state(host)\n\n            while result.needs_debugger(globally_enabled=self.debugger_active):\n                next_action = NextAction()\n                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n                dbg.cmdloop()\n\n                if next_action.result == NextAction.REDO:\n                    # rollback host state\n                    self._tqm.clear_failed_hosts()\n                    iterator._host_states[host.name] = prev_host_state\n                    for method, what in status_to_stats_map:\n                        if getattr(result, method)():\n                            self._tqm._stats.decrement(what, host.name)\n                    self._tqm._stats.decrement('ok', host.name)\n\n                    # redo\n                    self._queue_task(host, task, task_vars, play_context)\n\n                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n                    break\n                elif next_action.result == NextAction.CONTINUE:\n                    _processed_results.append(result)\n                    break\n                elif next_action.result == NextAction.EXIT:\n                    # Matches KeyboardInterrupt from bin/ansible\n                    sys.exit(99)\n            else:\n                _processed_results.append(result)\n\n        return _processed_results",
        "begin_line": 99,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.__init__#169",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.__init__(self, tqm)",
        "snippet": "    def __init__(self, tqm):\n        self._tqm = tqm\n        self._inventory = tqm.get_inventory()\n        self._workers = tqm._workers\n        self._variable_manager = tqm.get_variable_manager()\n        self._loader = tqm.get_loader()\n        self._final_q = tqm._final_q\n        self._step = context.CLIARGS.get('step', False)\n        self._diff = context.CLIARGS.get('diff', False)\n        self.flush_cache = context.CLIARGS.get('flush_cache', False)\n\n        # the task cache is a dictionary of tuples of (host.name, task._uuid)\n        # used to find the original task object of in-flight tasks and to store\n        # the task args/vars and play context info used to queue the task.\n        self._queued_task_cache = {}\n\n        # Backwards compat: self._display isn't really needed, just import the global display and use that.\n        self._display = display\n\n        # internal counters\n        self._pending_results = 0\n        self._cur_worker = 0\n\n        # this dictionary is used to keep track of hosts that have\n        # outstanding tasks still in queue\n        self._blocked_hosts = dict()\n\n        # this dictionary is used to keep track of hosts that have\n        # flushed handlers\n        self._flushed_hosts = dict()\n\n        self._results = deque()\n        self._results_lock = threading.Condition(threading.Lock())\n\n        # create the result processing thread for reading results in the background\n        self._results_thread = threading.Thread(target=results_thread_main, args=(self,))\n        self._results_thread.daemon = True\n        self._results_thread.start()\n\n        # holds the list of active (persistent) connections to be shutdown at\n        # play completion\n        self._active_connections = dict()\n\n        # Caches for get_host calls, to avoid calling excessively\n        # These values should be set at the top of the ``run`` method of each\n        # strategy plugin. Use ``_set_hosts_cache`` to set these values\n        self._hosts_cache = []\n        self._hosts_cache_all = []\n\n        self.debugger_active = C.ENABLE_TASK_DEBUGGER",
        "begin_line": 169,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._set_hosts_cache#220",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._set_hosts_cache(self, play, refresh=True)",
        "snippet": "    def _set_hosts_cache(self, play, refresh=True):\n        \"\"\"Responsible for setting _hosts_cache and _hosts_cache_all\n\n        See comment in ``__init__`` for the purpose of these caches\n        \"\"\"\n        if not refresh and all((self._hosts_cache, self._hosts_cache_all)):\n            return\n\n        if Templar(None).is_template(play.hosts):\n            _pattern = 'all'\n        else:\n            _pattern = play.hosts or 'all'\n        self._hosts_cache_all = [h.name for h in self._inventory.get_hosts(pattern=_pattern, ignore_restrictions=True)]\n        self._hosts_cache = [h.name for h in self._inventory.get_hosts(play.hosts, order=play.order)]",
        "begin_line": 220,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.cleanup#235",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.cleanup(self)",
        "snippet": "    def cleanup(self):\n        # close active persistent connections\n        for sock in itervalues(self._active_connections):\n            try:\n                conn = Connection(sock)\n                conn.reset()\n            except ConnectionError as e:\n                # most likely socket is already closed\n                display.debug(\"got an error while closing persistent connection: %s\" % e)\n        self._final_q.put(_sentinel)\n        self._results_thread.join()",
        "begin_line": 235,
        "end_line": 245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.run#247",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.run(self, iterator, play_context, result=0)",
        "snippet": "    def run(self, iterator, play_context, result=0):\n        # execute one more pass through the iterator without peeking, to\n        # make sure that all of the hosts are advanced to their final task.\n        # This should be safe, as everything should be ITERATING_COMPLETE by\n        # this point, though the strategy may not advance the hosts itself.\n\n        for host in self._hosts_cache:\n            if host not in self._tqm._unreachable_hosts:\n                try:\n                    iterator.get_next_task_for_host(self._inventory.hosts[host])\n                except KeyError:\n                    iterator.get_next_task_for_host(self._inventory.get_host(host))\n\n        # save the failed/unreachable hosts, as the run_handlers()\n        # method will clear that information during its execution\n        failed_hosts = iterator.get_failed_hosts()\n        unreachable_hosts = self._tqm._unreachable_hosts.keys()\n\n        display.debug(\"running handlers\")\n        handler_result = self.run_handlers(iterator, play_context)\n        if isinstance(handler_result, bool) and not handler_result:\n            result |= self._tqm.RUN_ERROR\n        elif not handler_result:\n            result |= handler_result\n\n        # now update with the hosts (if any) that failed or were\n        # unreachable during the handler execution phase\n        failed_hosts = set(failed_hosts).union(iterator.get_failed_hosts())\n        unreachable_hosts = set(unreachable_hosts).union(self._tqm._unreachable_hosts.keys())\n\n        # return the appropriate code, depending on the status hosts after the run\n        if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n            return result\n        elif len(unreachable_hosts) > 0:\n            return self._tqm.RUN_UNREACHABLE_HOSTS\n        elif len(failed_hosts) > 0:\n            return self._tqm.RUN_FAILED_HOSTS\n        else:\n            return self._tqm.RUN_OK",
        "begin_line": 247,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_hosts_remaining#287",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_hosts_remaining(self, play)",
        "snippet": "    def get_hosts_remaining(self, play):\n        self._set_hosts_cache(play, refresh=False)\n        ignore = set(self._tqm._failed_hosts).union(self._tqm._unreachable_hosts)\n        return [host for host in self._hosts_cache if host not in ignore]",
        "begin_line": 287,
        "end_line": 290,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_failed_hosts#292",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_failed_hosts(self, play)",
        "snippet": "    def get_failed_hosts(self, play):\n        self._set_hosts_cache(play, refresh=False)\n        return [host for host in self._hosts_cache if host in self._tqm._failed_hosts]",
        "begin_line": 292,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._queue_task#304",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._queue_task(self, host, task, task_vars, play_context)",
        "snippet": "    def _queue_task(self, host, task, task_vars, play_context):\n        ''' handles queueing the task up to be sent to a worker '''\n\n        display.debug(\"entering _queue_task() for %s/%s\" % (host.name, task.action))\n\n        # Add a write lock for tasks.\n        # Maybe this should be added somewhere further up the call stack but\n        # this is the earliest in the code where we have task (1) extracted\n        # into its own variable and (2) there's only a single code path\n        # leading to the module being run.  This is called by three\n        # functions: __init__.py::_do_handler_run(), linear.py::run(), and\n        # free.py::run() so we'd have to add to all three to do it there.\n        # The next common higher level is __init__.py::run() and that has\n        # tasks inside of play_iterator so we'd have to extract them to do it\n        # there.\n\n        if task.action not in action_write_locks.action_write_locks:\n            display.debug('Creating lock for %s' % task.action)\n            action_write_locks.action_write_locks[task.action] = Lock()\n\n        # create a templar and template things we need later for the queuing process\n        templar = Templar(loader=self._loader, variables=task_vars)\n\n        try:\n            throttle = int(templar.template(task.throttle))\n        except Exception as e:\n            raise AnsibleError(\"Failed to convert the throttle value to an integer.\", obj=task._ds, orig_exc=e)\n\n        # and then queue the new task\n        try:\n            # Determine the \"rewind point\" of the worker list. This means we start\n            # iterating over the list of workers until the end of the list is found.\n            # Normally, that is simply the length of the workers list (as determined\n            # by the forks or serial setting), however a task/block/play may \"throttle\"\n            # that limit down.\n            rewind_point = len(self._workers)\n            if throttle > 0 and self.ALLOW_BASE_THROTTLING:\n                if task.run_once:\n                    display.debug(\"Ignoring 'throttle' as 'run_once' is also set for '%s'\" % task.get_name())\n                else:\n                    if throttle <= rewind_point:\n                        display.debug(\"task: %s, throttle: %d\" % (task.get_name(), throttle))\n                        rewind_point = throttle\n\n            queued = False\n            starting_worker = self._cur_worker\n            while True:\n                if self._cur_worker >= rewind_point:\n                    self._cur_worker = 0\n\n                worker_prc = self._workers[self._cur_worker]\n                if worker_prc is None or not worker_prc.is_alive():\n                    self._queued_task_cache[(host.name, task._uuid)] = {\n                        'host': host,\n                        'task': task,\n                        'task_vars': task_vars,\n                        'play_context': play_context\n                    }\n\n                    worker_prc = WorkerProcess(self._final_q, task_vars, host, task, play_context, self._loader, self._variable_manager, plugin_loader)\n                    self._workers[self._cur_worker] = worker_prc\n                    self._tqm.send_callback('v2_runner_on_start', host, task)\n                    worker_prc.start()\n                    display.debug(\"worker is %d (out of %d available)\" % (self._cur_worker + 1, len(self._workers)))\n                    queued = True\n\n                self._cur_worker += 1\n\n                if self._cur_worker >= rewind_point:\n                    self._cur_worker = 0\n\n                if queued:\n                    break\n                elif self._cur_worker == starting_worker:\n                    time.sleep(0.0001)\n\n            self._pending_results += 1\n        except (EOFError, IOError, AssertionError) as e:\n            # most likely an abort\n            display.debug(\"got an error while queuing: %s\" % e)\n            return\n        display.debug(\"exiting _queue_task() for %s/%s\" % (host.name, task.action))",
        "begin_line": 304,
        "end_line": 385,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_task_hosts#387",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_task_hosts(self, iterator, task_host, task)",
        "snippet": "    def get_task_hosts(self, iterator, task_host, task):\n        if task.run_once:\n            host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n        else:\n            host_list = [task_host.name]\n        return host_list",
        "begin_line": 387,
        "end_line": 392,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._process_pending_results#427",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._process_pending_results(self, iterator, one_pass=False, max_passes=None)",
        "snippet": "    def _process_pending_results(self, iterator, one_pass=False, max_passes=None):\n        '''\n        Reads results off the final queue and takes appropriate action\n        based on the result (executing callbacks, updating state, etc.).\n        '''\n\n        ret_results = []\n        handler_templar = Templar(self._loader)\n\n        def get_original_host(host_name):\n            # FIXME: this should not need x2 _inventory\n            host_name = to_text(host_name)\n            if host_name in self._inventory.hosts:\n                return self._inventory.hosts[host_name]\n            else:\n                return self._inventory.get_host(host_name)\n\n        def search_handler_blocks_by_name(handler_name, handler_blocks):\n            # iterate in reversed order since last handler loaded with the same name wins\n            for handler_block in reversed(handler_blocks):\n                for handler_task in handler_block.block:\n                    if handler_task.name:\n                        if not handler_task.cached_name:\n                            if handler_templar.is_template(handler_task.name):\n                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n                                                                                                      task=handler_task,\n                                                                                                      _hosts=self._hosts_cache,\n                                                                                                      _hosts_all=self._hosts_cache_all)\n                                handler_task.name = handler_templar.template(handler_task.name)\n                            handler_task.cached_name = True\n\n                        try:\n                            # first we check with the full result of get_name(), which may\n                            # include the role name (if the handler is from a role). If that\n                            # is not found, we resort to the simple name field, which doesn't\n                            # have anything extra added to it.\n                            candidates = (\n                                handler_task.name,\n                                handler_task.get_name(include_role_fqcn=False),\n                                handler_task.get_name(include_role_fqcn=True),\n                            )\n\n                            if handler_name in candidates:\n                                return handler_task\n                        except (UndefinedError, AnsibleUndefinedVariable):\n                            # We skip this handler due to the fact that it may be using\n                            # a variable in the name that was conditionally included via\n                            # set_fact or some other method, and we don't want to error\n                            # out unnecessarily\n                            continue\n            return None\n\n        cur_pass = 0\n        while True:\n            try:\n                self._results_lock.acquire()\n                task_result = self._results.popleft()\n            except IndexError:\n                break\n            finally:\n                self._results_lock.release()\n\n            # get the original host and task. We then assign them to the TaskResult for use in callbacks/etc.\n            original_host = get_original_host(task_result._host)\n            queue_cache_entry = (original_host.name, task_result._task)\n            found_task = self._queued_task_cache.get(queue_cache_entry)['task']\n            original_task = found_task.copy(exclude_parent=True, exclude_tasks=True)\n            original_task._parent = found_task._parent\n            original_task.from_attrs(task_result._task_fields)\n\n            task_result._host = original_host\n            task_result._task = original_task\n\n            # send callbacks for 'non final' results\n            if '_ansible_retry' in task_result._result:\n                self._tqm.send_callback('v2_runner_retry', task_result)\n                continue\n            elif '_ansible_item_result' in task_result._result:\n                if task_result.is_failed() or task_result.is_unreachable():\n                    self._tqm.send_callback('v2_runner_item_on_failed', task_result)\n                elif task_result.is_skipped():\n                    self._tqm.send_callback('v2_runner_item_on_skipped', task_result)\n                else:\n                    if 'diff' in task_result._result:\n                        if self._diff or getattr(original_task, 'diff', False):\n                            self._tqm.send_callback('v2_on_file_diff', task_result)\n                    self._tqm.send_callback('v2_runner_item_on_ok', task_result)\n                continue\n\n            if original_task.register:\n                host_list = self.get_task_hosts(iterator, original_host, original_task)\n\n                clean_copy = strip_internal_keys(module_response_deepcopy(task_result._result))\n                if 'invocation' in clean_copy:\n                    del clean_copy['invocation']\n\n                for target_host in host_list:\n                    self._variable_manager.set_nonpersistent_facts(target_host, {original_task.register: clean_copy})\n\n            # all host status messages contain 2 entries: (msg, task_result)\n            role_ran = False\n            if task_result.is_failed():\n                role_ran = True\n                ignore_errors = original_task.ignore_errors\n                if not ignore_errors:\n                    display.debug(\"marking %s as failed\" % original_host.name)\n                    if original_task.run_once:\n                        # if we're using run_once, we have to fail every host here\n                        for h in self._inventory.get_hosts(iterator._play.hosts):\n                            if h.name not in self._tqm._unreachable_hosts:\n                                state, _ = iterator.get_next_task_for_host(h, peek=True)\n                                iterator.mark_host_failed(h)\n                                state, new_task = iterator.get_next_task_for_host(h, peek=True)\n                    else:\n                        iterator.mark_host_failed(original_host)\n\n                    # grab the current state and if we're iterating on the rescue portion\n                    # of a block then we save the failed task in a special var for use\n                    # within the rescue/always\n                    state, _ = iterator.get_next_task_for_host(original_host, peek=True)\n\n                    if iterator.is_failed(original_host) and state and state.run_state == iterator.ITERATING_COMPLETE:\n                        self._tqm._failed_hosts[original_host.name] = True\n\n                    if state and iterator.get_active_state(state).run_state == iterator.ITERATING_RESCUE:\n                        self._tqm._stats.increment('rescued', original_host.name)\n                        self._variable_manager.set_nonpersistent_facts(\n                            original_host.name,\n                            dict(\n                                ansible_failed_task=original_task.serialize(),\n                                ansible_failed_result=task_result._result,\n                            ),\n                        )\n                    else:\n                        self._tqm._stats.increment('failures', original_host.name)\n                else:\n                    self._tqm._stats.increment('ok', original_host.name)\n                    self._tqm._stats.increment('ignored', original_host.name)\n                    if 'changed' in task_result._result and task_result._result['changed']:\n                        self._tqm._stats.increment('changed', original_host.name)\n                self._tqm.send_callback('v2_runner_on_failed', task_result, ignore_errors=ignore_errors)\n            elif task_result.is_unreachable():\n                ignore_unreachable = original_task.ignore_unreachable\n                if not ignore_unreachable:\n                    self._tqm._unreachable_hosts[original_host.name] = True\n                    iterator._play._removed_hosts.append(original_host.name)\n                else:\n                    self._tqm._stats.increment('skipped', original_host.name)\n                    task_result._result['skip_reason'] = 'Host %s is unreachable' % original_host.name\n                self._tqm._stats.increment('dark', original_host.name)\n                self._tqm.send_callback('v2_runner_on_unreachable', task_result)\n            elif task_result.is_skipped():\n                self._tqm._stats.increment('skipped', original_host.name)\n                self._tqm.send_callback('v2_runner_on_skipped', task_result)\n            else:\n                role_ran = True\n\n                if original_task.loop:\n                    # this task had a loop, and has more than one result, so\n                    # loop over all of them instead of a single result\n                    result_items = task_result._result.get('results', [])\n                else:\n                    result_items = [task_result._result]\n\n                for result_item in result_items:\n                    if '_ansible_notify' in result_item:\n                        if task_result.is_changed():\n                            # The shared dictionary for notified handlers is a proxy, which\n                            # does not detect when sub-objects within the proxy are modified.\n                            # So, per the docs, we reassign the list so the proxy picks up and\n                            # notifies all other threads\n                            for handler_name in result_item['_ansible_notify']:\n                                found = False\n                                # Find the handler using the above helper.  First we look up the\n                                # dependency chain of the current task (if it's from a role), otherwise\n                                # we just look through the list of handlers in the current play/all\n                                # roles and use the first one that matches the notify name\n                                target_handler = search_handler_blocks_by_name(handler_name, iterator._play.handlers)\n                                if target_handler is not None:\n                                    found = True\n                                    if target_handler.notify_host(original_host):\n                                        self._tqm.send_callback('v2_playbook_on_notify', target_handler, original_host)\n\n                                for listening_handler_block in iterator._play.handlers:\n                                    for listening_handler in listening_handler_block.block:\n                                        listeners = getattr(listening_handler, 'listen', []) or []\n                                        if not listeners:\n                                            continue\n\n                                        listeners = listening_handler.get_validated_value(\n                                            'listen', listening_handler._valid_attrs['listen'], listeners, handler_templar\n                                        )\n                                        if handler_name not in listeners:\n                                            continue\n                                        else:\n                                            found = True\n\n                                        if listening_handler.notify_host(original_host):\n                                            self._tqm.send_callback('v2_playbook_on_notify', listening_handler, original_host)\n\n                                # and if none were found, then we raise an error\n                                if not found:\n                                    msg = (\"The requested handler '%s' was not found in either the main handlers list nor in the listening \"\n                                           \"handlers list\" % handler_name)\n                                    if C.ERROR_ON_MISSING_HANDLER:\n                                        raise AnsibleError(msg)\n                                    else:\n                                        display.warning(msg)\n\n                    if 'add_host' in result_item:\n                        # this task added a new host (add_host module)\n                        new_host_info = result_item.get('add_host', dict())\n                        self._add_host(new_host_info, iterator)\n\n                    elif 'add_group' in result_item:\n                        # this task added a new group (group_by module)\n                        self._add_group(original_host, result_item)\n\n                    if 'ansible_facts' in result_item:\n                        # if delegated fact and we are delegating facts, we need to change target host for them\n                        if original_task.delegate_to is not None and original_task.delegate_facts:\n                            host_list = self.get_delegated_hosts(result_item, original_task)\n                        else:\n                            # Set facts that should always be on the delegated hosts\n                            self._set_always_delegated_facts(result_item, original_task)\n\n                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n\n                        if original_task.action == 'include_vars':\n                            for (var_name, var_value) in iteritems(result_item['ansible_facts']):\n                                # find the host we're actually referring too here, which may\n                                # be a host that is not really in inventory at all\n                                for target_host in host_list:\n                                    self._variable_manager.set_host_variable(target_host, var_name, var_value)\n                        else:\n                            cacheable = result_item.pop('_ansible_facts_cacheable', False)\n                            for target_host in host_list:\n                                # so set_fact is a misnomer but 'cacheable = true' was meant to create an 'actual fact'\n                                # to avoid issues with precedence and confusion with set_fact normal operation,\n                                # we set BOTH fact and nonpersistent_facts (aka hostvar)\n                                # when fact is retrieved from cache in subsequent operations it will have the lower precedence,\n                                # but for playbook setting it the 'higher' precedence is kept\n                                if original_task.action != 'set_fact' or cacheable:\n                                    self._variable_manager.set_host_facts(target_host, result_item['ansible_facts'].copy())\n                                if original_task.action == 'set_fact':\n                                    self._variable_manager.set_nonpersistent_facts(target_host, result_item['ansible_facts'].copy())\n\n                    if 'ansible_stats' in result_item and 'data' in result_item['ansible_stats'] and result_item['ansible_stats']['data']:\n\n                        if 'per_host' not in result_item['ansible_stats'] or result_item['ansible_stats']['per_host']:\n                            host_list = self.get_task_hosts(iterator, original_host, original_task)\n                        else:\n                            host_list = [None]\n\n                        data = result_item['ansible_stats']['data']\n                        aggregate = 'aggregate' in result_item['ansible_stats'] and result_item['ansible_stats']['aggregate']\n                        for myhost in host_list:\n                            for k in data.keys():\n                                if aggregate:\n                                    self._tqm._stats.update_custom_stats(k, data[k], myhost)\n                                else:\n                                    self._tqm._stats.set_custom_stats(k, data[k], myhost)\n\n                if 'diff' in task_result._result:\n                    if self._diff or getattr(original_task, 'diff', False):\n                        self._tqm.send_callback('v2_on_file_diff', task_result)\n\n                if not isinstance(original_task, TaskInclude):\n                    self._tqm._stats.increment('ok', original_host.name)\n                    if 'changed' in task_result._result and task_result._result['changed']:\n                        self._tqm._stats.increment('changed', original_host.name)\n\n                # finally, send the ok for this task\n                self._tqm.send_callback('v2_runner_on_ok', task_result)\n\n            self._pending_results -= 1\n            if original_host.name in self._blocked_hosts:\n                del self._blocked_hosts[original_host.name]\n\n            # If this is a role task, mark the parent role as being run (if\n            # the task was ok or failed, but not skipped or unreachable)\n            if original_task._role is not None and role_ran:  # TODO:  and original_task.action != 'include_role':?\n                # lookup the role in the ROLE_CACHE to make sure we're dealing\n                # with the correct object and mark it as executed\n                for (entry, role_obj) in iteritems(iterator._play.ROLE_CACHE[original_task._role.get_name()]):\n                    if role_obj._uuid == original_task._role._uuid:\n                        role_obj._had_task_run[original_host.name] = True\n\n            ret_results.append(task_result)\n\n            if one_pass or max_passes is not None and (cur_pass + 1) >= max_passes:\n                break\n\n            cur_pass += 1\n\n        return ret_results",
        "begin_line": 427,
        "end_line": 722,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_original_host#436",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_original_host(host_name)",
        "snippet": "        def get_original_host(host_name):\n            # FIXME: this should not need x2 _inventory\n            host_name = to_text(host_name)\n            if host_name in self._inventory.hosts:\n                return self._inventory.hosts[host_name]\n            else:\n                return self._inventory.get_host(host_name)",
        "begin_line": 436,
        "end_line": 442,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.search_handler_blocks_by_name#444",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.search_handler_blocks_by_name(handler_name, handler_blocks)",
        "snippet": "        def search_handler_blocks_by_name(handler_name, handler_blocks):\n            # iterate in reversed order since last handler loaded with the same name wins\n            for handler_block in reversed(handler_blocks):\n                for handler_task in handler_block.block:\n                    if handler_task.name:\n                        if not handler_task.cached_name:\n                            if handler_templar.is_template(handler_task.name):\n                                handler_templar.available_variables = self._variable_manager.get_vars(play=iterator._play,\n                                                                                                      task=handler_task,\n                                                                                                      _hosts=self._hosts_cache,\n                                                                                                      _hosts_all=self._hosts_cache_all)\n                                handler_task.name = handler_templar.template(handler_task.name)\n                            handler_task.cached_name = True\n\n                        try:\n                            # first we check with the full result of get_name(), which may\n                            # include the role name (if the handler is from a role). If that\n                            # is not found, we resort to the simple name field, which doesn't\n                            # have anything extra added to it.\n                            candidates = (\n                                handler_task.name,\n                                handler_task.get_name(include_role_fqcn=False),\n                                handler_task.get_name(include_role_fqcn=True),\n                            )\n\n                            if handler_name in candidates:\n                                return handler_task\n                        except (UndefinedError, AnsibleUndefinedVariable):\n                            # We skip this handler due to the fact that it may be using\n                            # a variable in the name that was conditionally included via\n                            # set_fact or some other method, and we don't want to error\n                            # out unnecessarily\n                            continue\n            return None",
        "begin_line": 444,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._wait_on_pending_results#753",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._wait_on_pending_results(self, iterator)",
        "snippet": "    def _wait_on_pending_results(self, iterator):\n        '''\n        Wait for the shared counter to drop to zero, using a short sleep\n        between checks to ensure we don't spin lock\n        '''\n\n        ret_results = []\n\n        display.debug(\"waiting for pending results...\")\n        while self._pending_results > 0 and not self._tqm._terminated:\n\n            if self._tqm.has_dead_workers():\n                raise AnsibleError(\"A worker was found in a dead state\")\n\n            results = self._process_pending_results(iterator)\n            ret_results.extend(results)\n            if self._pending_results > 0:\n                time.sleep(C.DEFAULT_INTERNAL_POLL_INTERVAL)\n\n        display.debug(\"no more pending results, returning what we have\")\n\n        return ret_results",
        "begin_line": 753,
        "end_line": 774,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._add_host#776",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._add_host(self, host_info, iterator)",
        "snippet": "    def _add_host(self, host_info, iterator):\n        '''\n        Helper function to add a new host to inventory based on a task result.\n        '''\n\n        if host_info:\n            host_name = host_info.get('host_name')\n\n            # Check if host in inventory, add if not\n            if host_name not in self._inventory.hosts:\n                self._inventory.add_host(host_name, 'all')\n                self._hosts_cache_all.append(host_name)\n            new_host = self._inventory.hosts.get(host_name)\n\n            # Set/update the vars for this host\n            new_host.vars = combine_vars(new_host.get_vars(), host_info.get('host_vars', dict()))\n\n            new_groups = host_info.get('groups', [])\n            for group_name in new_groups:\n                if group_name not in self._inventory.groups:\n                    group_name = self._inventory.add_group(group_name)\n                new_group = self._inventory.groups[group_name]\n                new_group.add_host(self._inventory.hosts[host_name])\n\n            # reconcile inventory, ensures inventory rules are followed\n            self._inventory.reconcile_inventory()",
        "begin_line": 776,
        "end_line": 801,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._add_group#803",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._add_group(self, host, result_item)",
        "snippet": "    def _add_group(self, host, result_item):\n        '''\n        Helper function to add a group (if it does not exist), and to assign the\n        specified host to that group.\n        '''\n\n        changed = False\n\n        # the host here is from the executor side, which means it was a\n        # serialized/cloned copy and we'll need to look up the proper\n        # host object from the master inventory\n        real_host = self._inventory.hosts.get(host.name)\n        if real_host is None:\n            if host.name == self._inventory.localhost.name:\n                real_host = self._inventory.localhost\n            else:\n                raise AnsibleError('%s cannot be matched in inventory' % host.name)\n        group_name = result_item.get('add_group')\n        parent_group_names = result_item.get('parent_groups', [])\n\n        if group_name not in self._inventory.groups:\n            group_name = self._inventory.add_group(group_name)\n\n        for name in parent_group_names:\n            if name not in self._inventory.groups:\n                # create the new group and add it to inventory\n                self._inventory.add_group(name)\n                changed = True\n\n        group = self._inventory.groups[group_name]\n        for parent_group_name in parent_group_names:\n            parent_group = self._inventory.groups[parent_group_name]\n            parent_group.add_child_group(group)\n\n        if real_host.name not in group.get_hosts():\n            group.add_host(real_host)\n            changed = True\n\n        if group_name not in host.get_groups():\n            real_host.add_group(group)\n            changed = True\n\n        if changed:\n            self._inventory.reconcile_inventory()\n\n        return changed",
        "begin_line": 803,
        "end_line": 848,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._copy_included_file#850",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._copy_included_file(self, included_file)",
        "snippet": "    def _copy_included_file(self, included_file):\n        '''\n        A proven safe and performant way to create a copy of an included file\n        '''\n        ti_copy = included_file._task.copy(exclude_parent=True)\n        ti_copy._parent = included_file._task._parent\n\n        temp_vars = ti_copy.vars.copy()\n        temp_vars.update(included_file._vars)\n\n        ti_copy.vars = temp_vars\n\n        return ti_copy",
        "begin_line": 850,
        "end_line": 862,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase._load_included_file#864",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase._load_included_file(self, included_file, iterator, is_handler=False)",
        "snippet": "    def _load_included_file(self, included_file, iterator, is_handler=False):\n        '''\n        Loads an included YAML file of tasks, applying the optional set of variables.\n        '''\n\n        display.debug(\"loading included file: %s\" % included_file._filename)\n        try:\n            data = self._loader.load_from_file(included_file._filename)\n            if data is None:\n                return []\n            elif not isinstance(data, list):\n                raise AnsibleError(\"included task files must contain a list of tasks\")\n\n            ti_copy = self._copy_included_file(included_file)\n            # pop tags out of the include args, if they were specified there, and assign\n            # them to the include. If the include already had tags specified, we raise an\n            # error so that users know not to specify them both ways\n            tags = included_file._task.vars.pop('tags', [])\n            if isinstance(tags, string_types):\n                tags = tags.split(',')\n            if len(tags) > 0:\n                if len(included_file._task.tags) > 0:\n                    raise AnsibleParserError(\"Include tasks should not specify tags in more than one way (both via args and directly on the task). \"\n                                             \"Mixing tag specify styles is prohibited for whole import hierarchy, not only for single import statement\",\n                                             obj=included_file._task._ds)\n                display.deprecated(\"You should not specify tags in the include parameters. All tags should be specified using the task-level option\",\n                                   version='2.12')\n                included_file._task.tags = tags\n\n            block_list = load_list_of_blocks(\n                data,\n                play=iterator._play,\n                parent_block=ti_copy.build_parent_block(),\n                role=included_file._task._role,\n                use_handlers=is_handler,\n                loader=self._loader,\n                variable_manager=self._variable_manager,\n            )\n\n            # since we skip incrementing the stats when the task result is\n            # first processed, we do so now for each host in the list\n            for host in included_file._hosts:\n                self._tqm._stats.increment('ok', host.name)\n\n        except AnsibleError as e:\n            if isinstance(e, AnsibleFileNotFound):\n                reason = \"Could not find or access '%s' on the Ansible Controller.\" % to_text(e.file_name)\n            else:\n                reason = to_text(e)\n\n            # mark all of the hosts including this file as failed, send callbacks,\n            # and increment the stats for this host\n            for host in included_file._hosts:\n                tr = TaskResult(host=host, task=included_file._task, return_data=dict(failed=True, reason=reason))\n                iterator.mark_host_failed(host)\n                self._tqm._failed_hosts[host.name] = True\n                self._tqm._stats.increment('failures', host.name)\n                self._tqm.send_callback('v2_runner_on_failed', tr)\n            return []\n\n        # finally, send the callback and return the list of blocks loaded\n        self._tqm.send_callback('v2_playbook_on_include', included_file)\n        display.debug(\"done processing included file\")\n        return block_list",
        "begin_line": 864,
        "end_line": 927,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.run_handlers#929",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.run_handlers(self, iterator, play_context)",
        "snippet": "    def run_handlers(self, iterator, play_context):\n        '''\n        Runs handlers on those hosts which have been notified.\n        '''\n\n        result = self._tqm.RUN_OK\n\n        for handler_block in iterator._play.handlers:\n            # FIXME: handlers need to support the rescue/always portions of blocks too,\n            #        but this may take some work in the iterator and gets tricky when\n            #        we consider the ability of meta tasks to flush handlers\n            for handler in handler_block.block:\n                if handler.notified_hosts:\n                    result = self._do_handler_run(handler, handler.get_name(), iterator=iterator, play_context=play_context)\n                    if not result:\n                        break\n        return result",
        "begin_line": 929,
        "end_line": 945,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_hosts_left#1196",
        "src_path": "lib/ansible/plugins/strategy/__init__.py",
        "class_name": "lib.ansible.plugins.strategy.__init__.StrategyBase",
        "signature": "lib.ansible.plugins.strategy.__init__.StrategyBase.get_hosts_left(self, iterator)",
        "snippet": "    def get_hosts_left(self, iterator):\n        ''' returns list of available hosts for this iterator by filtering out unreachables '''\n\n        hosts_left = []\n        for host in self._hosts_cache:\n            if host not in self._tqm._unreachable_hosts:\n                try:\n                    hosts_left.append(self._inventory.hosts[host])\n                except KeyError:\n                    hosts_left.append(self._inventory.get_host(host))\n        return hosts_left",
        "begin_line": 1196,
        "end_line": 1206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.ini._parse_params#71",
        "src_path": "lib/ansible/plugins/lookup/ini.py",
        "class_name": "lib.ansible.plugins.lookup.ini",
        "signature": "lib.ansible.plugins.lookup.ini._parse_params(term)",
        "snippet": "def _parse_params(term):\n    '''Safely split parameter term to preserve spaces'''\n\n    keys = ['key', 'type', 'section', 'file', 're', 'default', 'encoding']\n    params = {}\n    for k in keys:\n        params[k] = ''\n\n    thiskey = 'key'\n    for idp, phrase in enumerate(term.split()):\n        for k in keys:\n            if ('%s=' % k) in phrase:\n                thiskey = k\n        if idp == 0 or not params[thiskey]:\n            params[thiskey] = phrase\n        else:\n            params[thiskey] += ' ' + phrase\n\n    rparams = [params[x] for x in keys if params[x]]\n    return rparams",
        "begin_line": 71,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.clean.module_response_deepcopy#22",
        "src_path": "lib/ansible/vars/clean.py",
        "class_name": "lib.ansible.vars.clean",
        "signature": "lib.ansible.vars.clean.module_response_deepcopy(v)",
        "snippet": "def module_response_deepcopy(v):\n    \"\"\"Function to create a deep copy of module response data\n\n    Designed to be used within the Ansible \"engine\" to improve performance\n    issues where ``copy.deepcopy`` was used previously, largely with CPU\n    and memory contention.\n\n    This only supports the following data types, and was designed to only\n    handle specific workloads:\n\n    * ``dict``\n    * ``list``\n\n    The data we pass here will come from a serialization such\n    as JSON, so we shouldn't have need for other data types such as\n    ``set`` or ``tuple``.\n\n    Take note that this function should not be used extensively as a\n    replacement for ``deepcopy`` due to the naive way in which this\n    handles other data types.\n\n    Do not expect uses outside of those listed below to maintain\n    backwards compatibility, in case we need to extend this function\n    to handle our specific needs:\n\n    * ``ansible.executor.task_result.TaskResult.clean_copy``\n    * ``ansible.vars.clean.clean_facts``\n    * ``ansible.vars.namespace_facts``\n    \"\"\"\n    if isinstance(v, dict):\n        ret = v.copy()\n        items = six.iteritems(ret)\n    elif isinstance(v, list):\n        ret = v[:]\n        items = enumerate(ret)\n    else:\n        return v\n\n    for key, value in items:\n        if isinstance(value, (dict, list)):\n            ret[key] = module_response_deepcopy(value)\n        else:\n            ret[key] = value\n\n    return ret",
        "begin_line": 22,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.clean.strip_internal_keys#69",
        "src_path": "lib/ansible/vars/clean.py",
        "class_name": "lib.ansible.vars.clean",
        "signature": "lib.ansible.vars.clean.strip_internal_keys(dirty, exceptions=None)",
        "snippet": "def strip_internal_keys(dirty, exceptions=None):\n    # All keys starting with _ansible_ are internal, so change the 'dirty' mapping and remove them.\n\n    if exceptions is None:\n        exceptions = tuple()\n\n    if isinstance(dirty, MutableSequence):\n\n        for element in dirty:\n            if isinstance(element, (MutableMapping, MutableSequence)):\n                strip_internal_keys(element, exceptions=exceptions)\n\n    elif isinstance(dirty, MutableMapping):\n\n        # listify to avoid updating dict while iterating over it\n        for k in list(dirty.keys()):\n            if isinstance(k, six.string_types):\n                if k.startswith('_ansible_') and k not in exceptions:\n                    del dirty[k]\n                    continue\n\n            if isinstance(dirty[k], (MutableMapping, MutableSequence)):\n                strip_internal_keys(dirty[k], exceptions=exceptions)\n    else:\n        raise AnsibleError(\"Cannot strip invalid keys from %s\" % type(dirty))\n\n    return dirty",
        "begin_line": 69,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.clean.remove_internal_keys#98",
        "src_path": "lib/ansible/vars/clean.py",
        "class_name": "lib.ansible.vars.clean",
        "signature": "lib.ansible.vars.clean.remove_internal_keys(data)",
        "snippet": "def remove_internal_keys(data):\n    '''\n    More nuanced version of strip_internal_keys\n    '''\n    for key in list(data.keys()):\n        if (key.startswith('_ansible_') and key != '_ansible_parsed') or key in C.INTERNAL_RESULT_KEYS:\n            display.warning(\"Removed unexpected internal key in module return: %s = %s\" % (key, data[key]))\n            del data[key]\n\n    # remove bad/empty internal keys\n    for key in ['warnings', 'deprecations']:\n        if key in data and not data[key]:\n            del data[key]\n\n    # cleanse fact values that are allowed from actions but not modules\n    for key in list(data.get('ansible_facts', {}).keys()):\n        if key.startswith('discovered_interpreter_') or key.startswith('ansible_discovered_interpreter_'):\n            del data['ansible_facts'][key]",
        "begin_line": 98,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.clean.clean_facts#118",
        "src_path": "lib/ansible/vars/clean.py",
        "class_name": "lib.ansible.vars.clean",
        "signature": "lib.ansible.vars.clean.clean_facts(facts)",
        "snippet": "def clean_facts(facts):\n    ''' remove facts that can override internal keys or otherwise deemed unsafe '''\n    data = module_response_deepcopy(facts)\n\n    remove_keys = set()\n    fact_keys = set(data.keys())\n    # first we add all of our magic variable names to the set of\n    # keys we want to remove from facts\n    # NOTE: these will eventually disappear in favor of others below\n    for magic_var in C.MAGIC_VARIABLE_MAPPING:\n        remove_keys.update(fact_keys.intersection(C.MAGIC_VARIABLE_MAPPING[magic_var]))\n\n    # remove common connection vars\n    remove_keys.update(fact_keys.intersection(C.COMMON_CONNECTION_VARS))\n\n    # next we remove any connection plugin specific vars\n    for conn_path in connection_loader.all(path_only=True):\n        conn_name = os.path.splitext(os.path.basename(conn_path))[0]\n        re_key = re.compile('^ansible_%s_' % conn_name)\n        for fact_key in fact_keys:\n            # most lightweight VM or container tech creates devices with this pattern, this avoids filtering them out\n            if (re_key.match(fact_key) and not fact_key.endswith(('_bridge', '_gwbridge'))) or fact_key.startswith('ansible_become_'):\n                remove_keys.add(fact_key)\n\n    # remove some KNOWN keys\n    for hard in C.RESTRICTED_RESULT_KEYS + C.INTERNAL_RESULT_KEYS:\n        if hard in fact_keys:\n            remove_keys.add(hard)\n\n    # finally, we search for interpreter keys to remove\n    re_interp = re.compile('^ansible_.*_interpreter$')\n    for fact_key in fact_keys:\n        if re_interp.match(fact_key):\n            remove_keys.add(fact_key)\n    # then we remove them (except for ssh host keys)\n    for r_key in remove_keys:\n        if not r_key.startswith('ansible_ssh_host_key_'):\n            try:\n                r_val = to_text(data[r_key])\n                if len(r_val) > 24:\n                    r_val = '%s ... %s' % (r_val[:13], r_val[-6:])\n            except Exception:\n                r_val = ' <failed to convert value to a string> '\n            display.warning(\"Removed restricted key from module data: %s = %s\" % (r_key, r_val))\n            del data[r_key]\n\n    return strip_internal_keys(data)",
        "begin_line": 118,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.clean.namespace_facts#167",
        "src_path": "lib/ansible/vars/clean.py",
        "class_name": "lib.ansible.vars.clean",
        "signature": "lib.ansible.vars.clean.namespace_facts(facts)",
        "snippet": "def namespace_facts(facts):\n    ''' return all facts inside 'ansible_facts' w/o an ansible_ prefix '''\n    deprefixed = {}\n    for k in facts:\n        if k.startswith('ansible_') and k not in ('ansible_local',):\n            deprefixed[k[8:]] = module_response_deepcopy(facts[k])\n        else:\n            deprefixed[k] = module_response_deepcopy(facts[k])\n\n    return {'ansible_facts': deprefixed}",
        "begin_line": 167,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.json._preprocess_unsafe_encode#18",
        "src_path": "lib/ansible/module_utils/common/json.py",
        "class_name": "lib.ansible.module_utils.common.json",
        "signature": "lib.ansible.module_utils.common.json._preprocess_unsafe_encode(value)",
        "snippet": "def _preprocess_unsafe_encode(value):\n    \"\"\"Recursively preprocess a data structure converting instances of ``AnsibleUnsafe``\n    into their JSON dict representations\n\n    Used in ``AnsibleJSONEncoder.iterencode``\n    \"\"\"\n    if getattr(value, '__UNSAFE__', False) and not getattr(value, '__ENCRYPTED__', False):\n        value = {'__ansible_unsafe': to_text(value, errors='surrogate_or_strict', nonstring='strict')}\n    elif is_sequence(value):\n        value = [_preprocess_unsafe_encode(v) for v in value]\n    elif isinstance(value, Mapping):\n        value = dict((k, _preprocess_unsafe_encode(v)) for k, v in value.items())\n\n    return value",
        "begin_line": 18,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.__init__#39",
        "src_path": "lib/ansible/module_utils/common/json.py",
        "class_name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder",
        "signature": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.__init__(self, preprocess_unsafe=False, **kwargs)",
        "snippet": "    def __init__(self, preprocess_unsafe=False, **kwargs):\n        self._preprocess_unsafe = preprocess_unsafe\n        super(AnsibleJSONEncoder, self).__init__(**kwargs)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.593591009188245e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.default#44",
        "src_path": "lib/ansible/module_utils/common/json.py",
        "class_name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder",
        "signature": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.default(self, o)",
        "snippet": "    def default(self, o):\n        if getattr(o, '__ENCRYPTED__', False):\n            # vault object\n            value = {'__ansible_vault': to_text(o._ciphertext, errors='surrogate_or_strict', nonstring='strict')}\n        elif getattr(o, '__UNSAFE__', False):\n            # unsafe object, this will never be triggered, see ``AnsibleJSONEncoder.iterencode``\n            value = {'__ansible_unsafe': to_text(o, errors='surrogate_or_strict', nonstring='strict')}\n        elif isinstance(o, Mapping):\n            # hostvars and other objects\n            value = dict(o)\n        elif isinstance(o, (datetime.date, datetime.datetime)):\n            # date object\n            value = o.isoformat()\n        else:\n            # use default encoder\n            value = super(AnsibleJSONEncoder, self).default(o)\n        return value",
        "begin_line": 44,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.iterencode#62",
        "src_path": "lib/ansible/module_utils/common/json.py",
        "class_name": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder",
        "signature": "lib.ansible.module_utils.common.json.AnsibleJSONEncoder.iterencode(self, o, **kwargs)",
        "snippet": "    def iterencode(self, o, **kwargs):\n        \"\"\"Custom iterencode, primarily design to handle encoding ``AnsibleUnsafe``\n        as the ``AnsibleUnsafe`` subclasses inherit from string types and\n        ``json.JSONEncoder`` does not support custom encoders for string types\n        \"\"\"\n        if self._preprocess_unsafe:\n            o = _preprocess_unsafe_encode(o)\n\n        return super(AnsibleJSONEncoder, self).iterencode(o, **kwargs)",
        "begin_line": 62,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.platform.PlatformFactCollector.collect#42",
        "src_path": "lib/ansible/module_utils/facts/system/platform.py",
        "class_name": "lib.ansible.module_utils.facts.system.platform.PlatformFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.platform.PlatformFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        platform_facts = {}\n        # platform.system() can be Linux, Darwin, Java, or Windows\n        platform_facts['system'] = platform.system()\n        platform_facts['kernel'] = platform.release()\n        platform_facts['kernel_version'] = platform.version()\n        platform_facts['machine'] = platform.machine()\n\n        platform_facts['python_version'] = platform.python_version()\n\n        platform_facts['fqdn'] = socket.getfqdn()\n        platform_facts['hostname'] = platform.node().split('.')[0]\n        platform_facts['nodename'] = platform.node()\n\n        platform_facts['domain'] = '.'.join(platform_facts['fqdn'].split('.')[1:])\n\n        arch_bits = platform.architecture()[0]\n\n        platform_facts['userspace_bits'] = arch_bits.replace('bit', '')\n        if platform_facts['machine'] == 'x86_64':\n            platform_facts['architecture'] = platform_facts['machine']\n            if platform_facts['userspace_bits'] == '64':\n                platform_facts['userspace_architecture'] = 'x86_64'\n            elif platform_facts['userspace_bits'] == '32':\n                platform_facts['userspace_architecture'] = 'i386'\n        elif solaris_i86_re.search(platform_facts['machine']):\n            platform_facts['architecture'] = 'i386'\n            if platform_facts['userspace_bits'] == '64':\n                platform_facts['userspace_architecture'] = 'x86_64'\n            elif platform_facts['userspace_bits'] == '32':\n                platform_facts['userspace_architecture'] = 'i386'\n        else:\n            platform_facts['architecture'] = platform_facts['machine']\n\n        if platform_facts['system'] == 'AIX':\n            # Attempt to use getconf to figure out architecture\n            # fall back to bootinfo if needed\n            getconf_bin = module.get_bin_path('getconf')\n            if getconf_bin:\n                rc, out, err = module.run_command([getconf_bin, 'MACHINE_ARCHITECTURE'])\n                data = out.splitlines()\n                platform_facts['architecture'] = data[0]\n            else:\n                bootinfo_bin = module.get_bin_path('bootinfo')\n                rc, out, err = module.run_command([bootinfo_bin, '-p'])\n                data = out.splitlines()\n                platform_facts['architecture'] = data[0]\n        elif platform_facts['system'] == 'OpenBSD':\n            platform_facts['architecture'] = platform.uname()[5]\n\n        machine_id = get_file_content(\"/var/lib/dbus/machine-id\") or get_file_content(\"/etc/machine-id\")\n        if machine_id:\n            machine_id = machine_id.splitlines()[0]\n            platform_facts[\"machine_id\"] = machine_id\n\n        return platform_facts",
        "begin_line": 42,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.caps.SystemCapabilitiesFactCollector.collect#30",
        "src_path": "lib/ansible/module_utils/facts/system/caps.py",
        "class_name": "lib.ansible.module_utils.facts.system.caps.SystemCapabilitiesFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.caps.SystemCapabilitiesFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        if not module:\n            return facts_dict\n\n        capsh_path = module.get_bin_path('capsh')\n        # NOTE: early exit 'if not crash_path' and unindent rest of method -akl\n        if capsh_path:\n            # NOTE: -> get_caps_data()/parse_caps_data() for easier mocking -akl\n            rc, out, err = module.run_command([capsh_path, \"--print\"], errors='surrogate_then_replace')\n            enforced_caps = []\n            enforced = 'NA'\n            for line in out.splitlines():\n                if len(line) < 1:\n                    continue\n                if line.startswith('Current:'):\n                    if line.split(':')[1].strip() == '=ep':\n                        enforced = 'False'\n                    else:\n                        enforced = 'True'\n                        enforced_caps = [i.strip() for i in line.split('=')[1].split(',')]\n\n            facts_dict['system_capabilities_enforced'] = enforced\n            facts_dict['system_capabilities'] = enforced_caps\n\n        return facts_dict",
        "begin_line": 30,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.formatters.lenient_lowercase#25",
        "src_path": "lib/ansible/module_utils/common/text/formatters.py",
        "class_name": "lib.ansible.module_utils.common.text.formatters",
        "signature": "lib.ansible.module_utils.common.text.formatters.lenient_lowercase(lst)",
        "snippet": "def lenient_lowercase(lst):\n    \"\"\"Lowercase elements of a list.\n\n    If an element is not a string, pass it through untouched.\n    \"\"\"\n    lowered = []\n    for value in lst:\n        try:\n            lowered.append(value.lower())\n        except AttributeError:\n            lowered.append(value)\n    return lowered",
        "begin_line": 25,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.formatters.human_to_bytes#39",
        "src_path": "lib/ansible/module_utils/common/text/formatters.py",
        "class_name": "lib.ansible.module_utils.common.text.formatters",
        "signature": "lib.ansible.module_utils.common.text.formatters.human_to_bytes(number, default_unit=None, isbits=False)",
        "snippet": "def human_to_bytes(number, default_unit=None, isbits=False):\n    \"\"\"Convert number in string format into bytes (ex: '2K' => 2048) or using unit argument.\n\n    example: human_to_bytes('10M') <=> human_to_bytes(10, 'M').\n\n    When isbits is False (default), converts bytes from a human-readable format to integer.\n        example: human_to_bytes('1MB') returns 1048576 (int).\n        The function expects 'B' (uppercase) as a byte identifier passed\n        as a part of 'name' param string or 'unit', e.g. 'MB'/'KB'/etc.\n        (except when the identifier is single 'b', it is perceived as a byte identifier too).\n        if 'Mb'/'Kb'/... is passed, the ValueError will be rased.\n\n    When isbits is True, converts bits from a human-readable format to integer.\n        example: human_to_bytes('1Mb', isbits=True) returns 1048576 (int) -\n        string bits representation was passed and return as a number or bits.\n        The function expects 'b' (lowercase) as a bit identifier, e.g. 'Mb'/'Kb'/etc.\n        if 'MB'/'KB'/... is passed, the ValueError will be rased.\n    \"\"\"\n    m = re.search(r'^\\s*(\\d*\\.?\\d*)\\s*([A-Za-z]+)?', str(number), flags=re.IGNORECASE)\n    if m is None:\n        raise ValueError(\"human_to_bytes() can't interpret following string: %s\" % str(number))\n    try:\n        num = float(m.group(1))\n    except Exception:\n        raise ValueError(\"human_to_bytes() can't interpret following number: %s (original input string: %s)\" % (m.group(1), number))\n\n    unit = m.group(2)\n    if unit is None:\n        unit = default_unit\n\n    if unit is None:\n        ''' No unit given, returning raw number '''\n        return int(round(num))\n    range_key = unit[0].upper()\n    try:\n        limit = SIZE_RANGES[range_key]\n    except Exception:\n        raise ValueError(\"human_to_bytes() failed to convert %s (unit = %s). The suffix must be one of %s\" % (number, unit, \", \".join(SIZE_RANGES.keys())))\n\n    # default value\n    unit_class = 'B'\n    unit_class_name = 'byte'\n    # handling bits case\n    if isbits:\n        unit_class = 'b'\n        unit_class_name = 'bit'\n    # check unit value if more than one character (KB, MB)\n    if len(unit) > 1:\n        expect_message = 'expect %s%s or %s' % (range_key, unit_class, range_key)\n        if range_key == 'B':\n            expect_message = 'expect %s or %s' % (unit_class, unit_class_name)\n\n        if unit_class_name in unit.lower():\n            pass\n        elif unit[1] != unit_class:\n            raise ValueError(\"human_to_bytes() failed to convert %s. Value is not a valid string (%s)\" % (number, expect_message))\n\n    return int(round(num * limit))",
        "begin_line": 39,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.text.formatters.bytes_to_human#99",
        "src_path": "lib/ansible/module_utils/common/text/formatters.py",
        "class_name": "lib.ansible.module_utils.common.text.formatters",
        "signature": "lib.ansible.module_utils.common.text.formatters.bytes_to_human(size, isbits=False, unit=None)",
        "snippet": "def bytes_to_human(size, isbits=False, unit=None):\n    base = 'Bytes'\n    if isbits:\n        base = 'bits'\n    suffix = ''\n\n    for suffix, limit in sorted(iteritems(SIZE_RANGES), key=lambda item: -item[1]):\n        if (unit is None and size >= limit) or unit is not None and unit.upper() == suffix[0]:\n            break\n\n    if limit != 1:\n        suffix += base[0]\n    else:\n        suffix = base\n\n    return '%.2f %s' % (size / limit, suffix)",
        "begin_line": 99,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.112273870365864e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.__init__#30",
        "src_path": "lib/ansible/module_utils/facts/other/facter.py",
        "class_name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.__init__(self, collectors=None, namespace=None)",
        "snippet": "    def __init__(self, collectors=None, namespace=None):\n        namespace = PrefixFactNamespace(namespace_name='facter',\n                                        prefix='facter_')\n        super(FacterFactCollector, self).__init__(collectors=collectors,\n                                                  namespace=namespace)",
        "begin_line": 30,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.find_facter#36",
        "src_path": "lib/ansible/module_utils/facts/other/facter.py",
        "class_name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.find_facter(self, module)",
        "snippet": "    def find_facter(self, module):\n        facter_path = module.get_bin_path('facter', opt_dirs=['/opt/puppetlabs/bin'])\n        cfacter_path = module.get_bin_path('cfacter', opt_dirs=['/opt/puppetlabs/bin'])\n\n        # Prefer to use cfacter if available\n        if cfacter_path is not None:\n            facter_path = cfacter_path\n\n        return facter_path",
        "begin_line": 36,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.run_facter#46",
        "src_path": "lib/ansible/module_utils/facts/other/facter.py",
        "class_name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.run_facter(self, module, facter_path)",
        "snippet": "    def run_facter(self, module, facter_path):\n        # if facter is installed, and we can use --json because\n        # ruby-json is ALSO installed, include facter data in the JSON\n        rc, out, err = module.run_command(facter_path + \" --puppet --json\")\n        return rc, out, err",
        "begin_line": 46,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.get_facter_output#52",
        "src_path": "lib/ansible/module_utils/facts/other/facter.py",
        "class_name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.get_facter_output(self, module)",
        "snippet": "    def get_facter_output(self, module):\n        facter_path = self.find_facter(module)\n        if not facter_path:\n            return None\n\n        rc, out, err = self.run_facter(module, facter_path)\n\n        if rc != 0:\n            return None\n\n        return out",
        "begin_line": 52,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.collect#64",
        "src_path": "lib/ansible/module_utils/facts/other/facter.py",
        "class_name": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector",
        "signature": "lib.ansible.module_utils.facts.other.facter.FacterFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        # Note that this mirrors previous facter behavior, where there isnt\n        # a 'ansible_facter' key in the main fact dict, but instead, 'facter_whatever'\n        # items are added to the main dict.\n        facter_dict = {}\n\n        if not module:\n            return facter_dict\n\n        facter_output = self.get_facter_output(module)\n\n        # TODO: if we fail, should we add a empty facter key or nothing?\n        if facter_output is None:\n            return facter_dict\n\n        try:\n            facter_dict = json.loads(facter_output)\n        except Exception:\n            # FIXME: maybe raise a FactCollectorError with some info attrs?\n            pass\n\n        return facter_dict",
        "begin_line": 64,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.dumper.represent_vault_encrypted_unicode#43",
        "src_path": "lib/ansible/parsing/yaml/dumper.py",
        "class_name": "lib.ansible.parsing.yaml.dumper",
        "signature": "lib.ansible.parsing.yaml.dumper.represent_vault_encrypted_unicode(self, data)",
        "snippet": "def represent_vault_encrypted_unicode(self, data):\n    return self.represent_scalar(u'!vault', data._ciphertext.decode(), style='|')",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.packaging.os.yum.YumModule.parse_check_update#1184",
        "src_path": "lib/ansible/modules/packaging/os/yum.py",
        "class_name": "lib.ansible.modules.packaging.os.yum.YumModule",
        "signature": "lib.ansible.modules.packaging.os.yum.YumModule.parse_check_update(check_update_output)",
        "snippet": "    def parse_check_update(check_update_output):\n        updates = {}\n        obsoletes = {}\n\n        # remove incorrect new lines in longer columns in output from yum check-update\n        # yum line wrapping can move the repo to the next line\n        #\n        # Meant to filter out sets of lines like:\n        #  some_looooooooooooooooooooooooooooooooooooong_package_name   1:1.2.3-1.el7\n        #                                                                    some-repo-label\n        #\n        # But it also needs to avoid catching lines like:\n        # Loading mirror speeds from cached hostfile\n        #\n        # ceph.x86_64                               1:11.2.0-0.el7                    ceph\n\n        # preprocess string and filter out empty lines so the regex below works\n        out = re.sub(r'\\n[^\\w]\\W+(.*)', r' \\1', check_update_output)\n\n        available_updates = out.split('\\n')\n\n        # build update dictionary\n        for line in available_updates:\n            line = line.split()\n            # ignore irrelevant lines\n            # '*' in line matches lines like mirror lists:\n            #      * base: mirror.corbina.net\n            # len(line) != 3 or 6 could be junk or a continuation\n            # len(line) = 6 is package obsoletes\n            #\n            # FIXME: what is  the '.' not in line  conditional for?\n\n            if '*' in line or len(line) not in [3, 6] or '.' not in line[0]:\n                continue\n\n            pkg, version, repo = line[0], line[1], line[2]\n            name, dist = pkg.rsplit('.', 1)\n            updates.update({name: {'version': version, 'dist': dist, 'repo': repo}})\n\n            if len(line) == 6:\n                obsolete_pkg, obsolete_version, obsolete_repo = line[3], line[4], line[5]\n                obsolete_name, obsolete_dist = obsolete_pkg.rsplit('.', 1)\n                obsoletes.update({obsolete_name: {'version': obsolete_version, 'dist': obsolete_dist, 'repo': obsolete_repo}})\n\n        return updates, obsoletes",
        "begin_line": 1184,
        "end_line": 1228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.safe_eval.safe_eval#30",
        "src_path": "lib/ansible/template/safe_eval.py",
        "class_name": "lib.ansible.template.safe_eval",
        "signature": "lib.ansible.template.safe_eval.safe_eval(expr, locals=None, include_exceptions=False)",
        "snippet": "def safe_eval(expr, locals=None, include_exceptions=False):\n    '''\n    This is intended for allowing things like:\n    with_items: a_list_variable\n\n    Where Jinja2 would return a string but we do not want to allow it to\n    call functions (outside of Jinja2, where the env is constrained).\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n    locals = {} if locals is None else locals\n\n    # define certain JSON types\n    # eg. JSON booleans are unknown to python eval()\n    OUR_GLOBALS = {\n        '__builtins__': {},  # avoid global builtins as per eval docs\n        'false': False,\n        'null': None,\n        'true': True,\n        # also add back some builtins we do need\n        'True': True,\n        'False': False,\n        'None': None\n    }\n\n    # this is the whitelist of AST nodes we are going to\n    # allow in the evaluation. Any node type other than\n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Add,\n            ast.BinOp,\n            # ast.Call,\n            ast.Compare,\n            ast.Dict,\n            ast.Div,\n            ast.Expression,\n            ast.List,\n            ast.Load,\n            ast.Mult,\n            ast.Num,\n            ast.Name,\n            ast.Str,\n            ast.Sub,\n            ast.USub,\n            ast.Tuple,\n            ast.UnaryOp,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if sys.version_info[:2] >= (2, 7):\n        SAFE_NODES.update(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    # And in Python 3.4 too\n    if sys.version_info[:2] >= (3, 4):\n        SAFE_NODES.update(\n            set(\n                (ast.NameConstant,)\n            )\n        )\n\n    # And in Python 3.6 too, although not encountered until Python 3.8, see https://bugs.python.org/issue32892\n    if sys.version_info[:2] >= (3, 6):\n        SAFE_NODES.update(\n            set(\n                (ast.Constant,)\n            )\n        )\n\n    filter_list = []\n    for filter_ in filter_loader.all():\n        filter_list.extend(filter_.filters().keys())\n\n    test_list = []\n    for test in test_loader.all():\n        test_list.extend(test.tests().keys())\n\n    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list + test_list\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                # Disallow calls to builtin functions that we have not vetted\n                # as safe.  Other functions are excluded by setting locals in\n                # the call to eval() later on\n                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)\n\n    if not isinstance(expr, string_types):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    cnv = CleansingNodeVisitor()\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        # Note: passing our own globals and locals here constrains what\n        # callables (and other identifiers) are recognized.  this is in\n        # addition to the filtering of builtins done in CleansingNodeVisitor\n        result = eval(compiled, OUR_GLOBALS, dict(locals))\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError as e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is to support late evaluation\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception as e:\n        if include_exceptions:\n            return (expr, e)\n        return expr",
        "begin_line": 30,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.safe_eval.CleansingNodeVisitor.safe_eval#30",
        "src_path": "lib/ansible/template/safe_eval.py",
        "class_name": "lib.ansible.template.safe_eval.CleansingNodeVisitor",
        "signature": "lib.ansible.template.safe_eval.CleansingNodeVisitor.safe_eval(expr, locals=None, include_exceptions=False)",
        "snippet": "def safe_eval(expr, locals=None, include_exceptions=False):\n    '''\n    This is intended for allowing things like:\n    with_items: a_list_variable\n\n    Where Jinja2 would return a string but we do not want to allow it to\n    call functions (outside of Jinja2, where the env is constrained).\n\n    Based on:\n    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe\n    '''\n    locals = {} if locals is None else locals\n\n    # define certain JSON types\n    # eg. JSON booleans are unknown to python eval()\n    OUR_GLOBALS = {\n        '__builtins__': {},  # avoid global builtins as per eval docs\n        'false': False,\n        'null': None,\n        'true': True,\n        # also add back some builtins we do need\n        'True': True,\n        'False': False,\n        'None': None\n    }\n\n    # this is the whitelist of AST nodes we are going to\n    # allow in the evaluation. Any node type other than\n    # those listed here will raise an exception in our custom\n    # visitor class defined below.\n    SAFE_NODES = set(\n        (\n            ast.Add,\n            ast.BinOp,\n            # ast.Call,\n            ast.Compare,\n            ast.Dict,\n            ast.Div,\n            ast.Expression,\n            ast.List,\n            ast.Load,\n            ast.Mult,\n            ast.Num,\n            ast.Name,\n            ast.Str,\n            ast.Sub,\n            ast.USub,\n            ast.Tuple,\n            ast.UnaryOp,\n        )\n    )\n\n    # AST node types were expanded after 2.6\n    if sys.version_info[:2] >= (2, 7):\n        SAFE_NODES.update(\n            set(\n                (ast.Set,)\n            )\n        )\n\n    # And in Python 3.4 too\n    if sys.version_info[:2] >= (3, 4):\n        SAFE_NODES.update(\n            set(\n                (ast.NameConstant,)\n            )\n        )\n\n    # And in Python 3.6 too, although not encountered until Python 3.8, see https://bugs.python.org/issue32892\n    if sys.version_info[:2] >= (3, 6):\n        SAFE_NODES.update(\n            set(\n                (ast.Constant,)\n            )\n        )\n\n    filter_list = []\n    for filter_ in filter_loader.all():\n        filter_list.extend(filter_.filters().keys())\n\n    test_list = []\n    for test in test_loader.all():\n        test_list.extend(test.tests().keys())\n\n    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list + test_list\n\n    class CleansingNodeVisitor(ast.NodeVisitor):\n        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                # Disallow calls to builtin functions that we have not vetted\n                # as safe.  Other functions are excluded by setting locals in\n                # the call to eval() later on\n                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)\n\n    if not isinstance(expr, string_types):\n        # already templated to a datastructure, perhaps?\n        if include_exceptions:\n            return (expr, None)\n        return expr\n\n    cnv = CleansingNodeVisitor()\n    try:\n        parsed_tree = ast.parse(expr, mode='eval')\n        cnv.visit(parsed_tree)\n        compiled = compile(parsed_tree, expr, 'eval')\n        # Note: passing our own globals and locals here constrains what\n        # callables (and other identifiers) are recognized.  this is in\n        # addition to the filtering of builtins done in CleansingNodeVisitor\n        result = eval(compiled, OUR_GLOBALS, dict(locals))\n\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except SyntaxError as e:\n        # special handling for syntax errors, we just return\n        # the expression string back as-is to support late evaluation\n        if include_exceptions:\n            return (expr, None)\n        return expr\n    except Exception as e:\n        if include_exceptions:\n            return (expr, e)\n        return expr",
        "begin_line": 30,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.safe_eval.CleansingNodeVisitor.generic_visit#117",
        "src_path": "lib/ansible/template/safe_eval.py",
        "class_name": "lib.ansible.template.safe_eval.CleansingNodeVisitor",
        "signature": "lib.ansible.template.safe_eval.CleansingNodeVisitor.generic_visit(self, node, inside_call=False)",
        "snippet": "        def generic_visit(self, node, inside_call=False):\n            if type(node) not in SAFE_NODES:\n                raise Exception(\"invalid expression (%s)\" % expr)\n            elif isinstance(node, ast.Call):\n                inside_call = True\n            elif isinstance(node, ast.Name) and inside_call:\n                # Disallow calls to builtin functions that we have not vetted\n                # as safe.  Other functions are excluded by setting locals in\n                # the call to eval() later on\n                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:\n                    raise Exception(\"invalid function: %s\" % node.id)\n            # iterate over all child nodes\n            for child_node in ast.iter_child_nodes(node):\n                self.generic_visit(child_node, inside_call)",
        "begin_line": 117,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.listify.listify_lookup_plugin_terms#30",
        "src_path": "lib/ansible/utils/listify.py",
        "class_name": "lib.ansible.utils.listify",
        "signature": "lib.ansible.utils.listify.listify_lookup_plugin_terms(terms, templar, loader, fail_on_undefined=True, convert_bare=False)",
        "snippet": "def listify_lookup_plugin_terms(terms, templar, loader, fail_on_undefined=True, convert_bare=False):\n\n    if isinstance(terms, string_types):\n        terms = templar.template(terms.strip(), convert_bare=convert_bare, fail_on_undefined=fail_on_undefined)\n    else:\n        terms = templar.template(terms, fail_on_undefined=fail_on_undefined)\n\n    if isinstance(terms, string_types) or not isinstance(terms, Iterable):\n        terms = [terms]\n\n    return terms",
        "begin_line": 30,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeBytes.decode#70",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeBytes",
        "signature": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeBytes.decode(self, *args, **kwargs)",
        "snippet": "    def decode(self, *args, **kwargs):\n        \"\"\"Wrapper method to ensure type conversions maintain unsafe context\"\"\"\n        return AnsibleUnsafeText(super(AnsibleUnsafeBytes, self).decode(*args, **kwargs))",
        "begin_line": 70,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeText.encode#76",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeText",
        "signature": "lib.ansible.utils.unsafe_proxy.AnsibleUnsafeText.encode(self, *args, **kwargs)",
        "snippet": "    def encode(self, *args, **kwargs):\n        \"\"\"Wrapper method to ensure type conversions maintain unsafe context\"\"\"\n        return AnsibleUnsafeBytes(super(AnsibleUnsafeText, self).encode(*args, **kwargs))",
        "begin_line": 76,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy._wrap_dict#100",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy",
        "signature": "lib.ansible.utils.unsafe_proxy._wrap_dict(v)",
        "snippet": "def _wrap_dict(v):\n    return dict((wrap_var(k), wrap_var(item)) for k, item in v.items())",
        "begin_line": 100,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy._wrap_sequence#104",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy",
        "signature": "lib.ansible.utils.unsafe_proxy._wrap_sequence(v)",
        "snippet": "def _wrap_sequence(v):\n    \"\"\"Wraps a sequence with unsafe, not meant for strings, primarily\n    ``tuple`` and ``list``\n    \"\"\"\n    v_type = type(v)\n    return v_type(wrap_var(item) for item in v)",
        "begin_line": 104,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy._wrap_set#112",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy",
        "signature": "lib.ansible.utils.unsafe_proxy._wrap_set(v)",
        "snippet": "def _wrap_set(v):\n    return set(wrap_var(item) for item in v)",
        "begin_line": 112,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy.wrap_var#116",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy",
        "signature": "lib.ansible.utils.unsafe_proxy.wrap_var(v)",
        "snippet": "def wrap_var(v):\n    if v is None or isinstance(v, AnsibleUnsafe):\n        return v\n\n    if isinstance(v, Mapping):\n        v = _wrap_dict(v)\n    elif isinstance(v, Set):\n        v = _wrap_set(v)\n    elif is_sequence(v):\n        v = _wrap_sequence(v)\n    elif isinstance(v, binary_type):\n        v = AnsibleUnsafeBytes(v)\n    elif isinstance(v, text_type):\n        v = AnsibleUnsafeText(v)\n\n    return v",
        "begin_line": 116,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.unsafe_proxy.to_unsafe_text#138",
        "src_path": "lib/ansible/utils/unsafe_proxy.py",
        "class_name": "lib.ansible.utils.unsafe_proxy",
        "signature": "lib.ansible.utils.unsafe_proxy.to_unsafe_text(*args, **kwargs)",
        "snippet": "def to_unsafe_text(*args, **kwargs):\n    return wrap_var(to_text(*args, **kwargs))",
        "begin_line": 138,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector.__init__#48",
        "src_path": "lib/ansible/module_utils/facts/ansible_collector.py",
        "class_name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector",
        "signature": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector.__init__(self, collectors=None, namespace=None, filter_spec=None)",
        "snippet": "    def __init__(self, collectors=None, namespace=None, filter_spec=None):\n\n        super(AnsibleFactCollector, self).__init__(collectors=collectors,\n                                                   namespace=namespace)\n\n        self.filter_spec = filter_spec",
        "begin_line": 48,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector._filter#55",
        "src_path": "lib/ansible/module_utils/facts/ansible_collector.py",
        "class_name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector",
        "signature": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector._filter(self, facts_dict, filter_spec)",
        "snippet": "    def _filter(self, facts_dict, filter_spec):\n        # assume a filter_spec='' is equilv to filter_spec='*'\n        if not filter_spec or filter_spec == '*':\n            return facts_dict\n\n        return [(x, y) for x, y in facts_dict.items() if fnmatch.fnmatch(x, filter_spec)]",
        "begin_line": 55,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector.collect#62",
        "src_path": "lib/ansible/module_utils/facts/ansible_collector.py",
        "class_name": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector",
        "signature": "lib.ansible.module_utils.facts.ansible_collector.AnsibleFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        collected_facts = collected_facts or {}\n\n        facts_dict = {}\n\n        for collector_obj in self.collectors:\n            info_dict = {}\n\n            try:\n\n                # Note: this collects with namespaces, so collected_facts also includes namespaces\n                info_dict = collector_obj.collect_with_namespace(module=module,\n                                                                 collected_facts=collected_facts)\n            except Exception as e:\n                sys.stderr.write(repr(e))\n                sys.stderr.write('\\n')\n\n            # shallow copy of the new facts to pass to each collector in collected_facts so facts\n            # can reference other facts they depend on.\n            collected_facts.update(info_dict.copy())\n\n            # NOTE: If we want complicated fact dict merging, this is where it would hook in\n            facts_dict.update(self._filter(info_dict, self.filter_spec))\n\n        return facts_dict",
        "begin_line": 62,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector.__init__#95",
        "src_path": "lib/ansible/module_utils/facts/ansible_collector.py",
        "class_name": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector",
        "signature": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector.__init__(self, collectors=None, namespace=None, gather_subset=None, module_setup=None)",
        "snippet": "    def __init__(self, collectors=None, namespace=None, gather_subset=None, module_setup=None):\n        super(CollectorMetaDataCollector, self).__init__(collectors, namespace)\n        self.gather_subset = gather_subset\n        self.module_setup = module_setup",
        "begin_line": 95,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.637668983426258e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector.collect#100",
        "src_path": "lib/ansible/module_utils/facts/ansible_collector.py",
        "class_name": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector",
        "signature": "lib.ansible.module_utils.facts.ansible_collector.CollectorMetaDataCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        meta_facts = {'gather_subset': self.gather_subset}\n        if self.module_setup:\n            meta_facts['module_setup'] = self.module_setup\n        return meta_facts",
        "begin_line": 100,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.637668983426258e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.to_safe_group_name#43",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__",
        "signature": "lib.ansible.plugins.inventory.__init__.to_safe_group_name(name)",
        "snippet": "def to_safe_group_name(name):\n    # placeholder for backwards compat\n    return original_safe(name, force=True, silent=True)",
        "begin_line": 43,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.detect_range#48",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__",
        "signature": "lib.ansible.plugins.inventory.__init__.detect_range(line=None)",
        "snippet": "def detect_range(line=None):\n    '''\n    A helper function that checks a given host line to see if it contains\n    a range pattern described in the docstring above.\n\n    Returns True if the given line contains a pattern, else False.\n    '''\n    return '[' in line",
        "begin_line": 48,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.__init__#155",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(BaseInventoryPlugin, self).__init__()\n\n        self._options = {}\n        self.inventory = None\n        self.display = display",
        "begin_line": 155,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.parse#163",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.parse(self, inventory, loader, path, cache=True)",
        "snippet": "    def parse(self, inventory, loader, path, cache=True):\n        ''' Populates inventory from the given data. Raises an error on any parse failure\n            :arg inventory: a copy of the previously accumulated inventory data,\n                 to be updated with any new data this plugin provides.\n                 The inventory can be empty if no other source/plugin ran successfully.\n            :arg loader: a reference to the DataLoader, which can read in YAML and JSON files,\n                 it also has Vault support to automatically decrypt files.\n            :arg path: the string that represents the 'inventory source',\n                 normally a path to a configuration file for this inventory,\n                 but it can also be a raw string for this plugin to consume\n            :arg cache: a boolean that indicates if the plugin should use the cache or not\n                 you can ignore if this plugin does not implement caching.\n        '''\n\n        self.loader = loader\n        self.inventory = inventory\n        self.templar = Templar(loader=loader)",
        "begin_line": 163,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.verify_file#181",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin.verify_file(self, path)",
        "snippet": "    def verify_file(self, path):\n        ''' Verify if file is usable by this plugin, base does minimal accessibility check\n            :arg path: a string that was passed as an inventory source,\n                 it normally is a path to a config file, but this is not a requirement,\n                 it can also be parsed itself as the inventory data to process.\n                 So only call this base class if you expect it to be a file.\n        '''\n\n        valid = False\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if (os.path.exists(b_path) and os.access(b_path, os.R_OK)):\n            valid = True\n        else:\n            self.display.vvv('Skipping due to inventory source not existing or not being readable by the current user')\n        return valid",
        "begin_line": 181,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin._populate_host_vars#197",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin._populate_host_vars(self, hosts, variables, group=None, port=None)",
        "snippet": "    def _populate_host_vars(self, hosts, variables, group=None, port=None):\n        if not isinstance(variables, Mapping):\n            raise AnsibleParserError(\"Invalid data from file, expected dictionary and got:\\n\\n%s\" % to_native(variables))\n\n        for host in hosts:\n            self.inventory.add_host(host, group=group, port=port)\n            for k in variables:\n                self.inventory.set_variable(host, k, variables[k])",
        "begin_line": 197,
        "end_line": 204,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin._expand_hostpattern#247",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseInventoryPlugin._expand_hostpattern(self, hostpattern)",
        "snippet": "    def _expand_hostpattern(self, hostpattern):\n        '''\n        Takes a single host pattern and returns a list of hostnames and an\n        optional port number that applies to all of them.\n        '''\n        # Can the given hostpattern be parsed as a host with an optional port\n        # specification?\n\n        try:\n            (pattern, port) = parse_address(hostpattern, allow_ranges=True)\n        except Exception:\n            # not a recognizable host pattern\n            pattern = hostpattern\n            port = None\n\n        # Once we have separated the pattern, we expand it into list of one or\n        # more hostnames, depending on whether it contains any [x:y] ranges.\n\n        if detect_range(pattern):\n            hostnames = expand_hostname_range(pattern)\n        else:\n            hostnames = [pattern]\n\n        return (hostnames, port)",
        "begin_line": 247,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.BaseFileInventoryPlugin.__init__#278",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.BaseFileInventoryPlugin",
        "signature": "lib.ansible.plugins.inventory.__init__.BaseFileInventoryPlugin.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(BaseFileInventoryPlugin, self).__init__()",
        "begin_line": 278,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.Constructable._compose#349",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.Constructable",
        "signature": "lib.ansible.plugins.inventory.__init__.Constructable._compose(self, template, variables)",
        "snippet": "    def _compose(self, template, variables):\n        ''' helper method for plugins to compose variables for Ansible based on jinja2 expression and inventory vars'''\n        t = self.templar\n        t.available_variables = variables\n        return t.template('%s%s%s' % (t.environment.variable_start_string, template, t.environment.variable_end_string), disable_lookups=True)",
        "begin_line": 349,
        "end_line": 353,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.__init__.Constructable._add_host_to_keyed_groups#389",
        "src_path": "lib/ansible/plugins/inventory/__init__.py",
        "class_name": "lib.ansible.plugins.inventory.__init__.Constructable",
        "signature": "lib.ansible.plugins.inventory.__init__.Constructable._add_host_to_keyed_groups(self, keys, variables, host, strict=False)",
        "snippet": "    def _add_host_to_keyed_groups(self, keys, variables, host, strict=False):\n        ''' helper to create groups for plugins based on variable values and add the corresponding hosts to it'''\n        if keys and isinstance(keys, list):\n            for keyed in keys:\n                if keyed and isinstance(keyed, dict):\n\n                    variables = combine_vars(variables, self.inventory.get_host(host).get_vars())\n                    try:\n                        key = self._compose(keyed.get('key'), variables)\n                    except Exception as e:\n                        if strict:\n                            raise AnsibleParserError(\"Could not generate group for host %s from %s entry: %s\" % (host, keyed.get('key'), to_native(e)))\n                        continue\n\n                    if key:\n                        prefix = keyed.get('prefix', '')\n                        sep = keyed.get('separator', '_')\n                        raw_parent_name = keyed.get('parent_group', None)\n                        if raw_parent_name:\n                            try:\n                                raw_parent_name = self.templar.template(raw_parent_name)\n                            except AnsibleError as e:\n                                if strict:\n                                    raise AnsibleParserError(\"Could not generate parent group %s for group %s: %s\" % (raw_parent_name, key, to_native(e)))\n                                continue\n\n                        new_raw_group_names = []\n                        if isinstance(key, string_types):\n                            new_raw_group_names.append(key)\n                        elif isinstance(key, list):\n                            for name in key:\n                                new_raw_group_names.append(name)\n                        elif isinstance(key, Mapping):\n                            for (gname, gval) in key.items():\n                                name = '%s%s%s' % (gname, sep, gval)\n                                new_raw_group_names.append(name)\n                        else:\n                            raise AnsibleParserError(\"Invalid group name format, expected a string or a list of them or dictionary, got: %s\" % type(key))\n\n                        for bare_name in new_raw_group_names:\n                            gname = self._sanitize_group_name('%s%s%s' % (prefix, sep, bare_name))\n                            result_gname = self.inventory.add_group(gname)\n                            self.inventory.add_host(host, result_gname)\n\n                            if raw_parent_name:\n                                parent_name = self._sanitize_group_name(raw_parent_name)\n                                self.inventory.add_group(parent_name)\n                                self.inventory.add_child(parent_name, result_gname)\n\n                    else:\n                        # exclude case of empty list and dictionary, because these are valid constructions\n                        # simply no groups need to be constructed, but are still falsy\n                        if strict and key not in ([], {}):\n                            raise AnsibleParserError(\"No key or key resulted empty for %s in host %s, invalid entry\" % (keyed.get('key'), host))\n                else:\n                    raise AnsibleParserError(\"Invalid keyed group entry, it must be a dictionary: %s \" % keyed)",
        "begin_line": 389,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.process.worker.WorkerProcess.__init__#57",
        "src_path": "lib/ansible/executor/process/worker.py",
        "class_name": "lib.ansible.executor.process.worker.WorkerProcess",
        "signature": "lib.ansible.executor.process.worker.WorkerProcess.__init__(self, final_q, task_vars, host, task, play_context, loader, variable_manager, shared_loader_obj)",
        "snippet": "    def __init__(self, final_q, task_vars, host, task, play_context, loader, variable_manager, shared_loader_obj):\n\n        super(WorkerProcess, self).__init__()\n        # takes a task queue manager as the sole param:\n        self._final_q = final_q\n        self._task_vars = task_vars\n        self._host = host\n        self._task = task\n        self._play_context = play_context\n        self._loader = loader\n        self._variable_manager = variable_manager\n        self._shared_loader_obj = shared_loader_obj\n\n        # NOTE: this works due to fork, if switching to threads this should change to per thread storage of temp files\n        # clear var to ensure we only delete files for this child\n        self._loader._tempfiles = set()",
        "begin_line": 57,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.process.worker.WorkerProcess._save_stdin#74",
        "src_path": "lib/ansible/executor/process/worker.py",
        "class_name": "lib.ansible.executor.process.worker.WorkerProcess",
        "signature": "lib.ansible.executor.process.worker.WorkerProcess._save_stdin(self)",
        "snippet": "    def _save_stdin(self):\n        self._new_stdin = os.devnull\n        try:\n            if sys.stdin.isatty() and sys.stdin.fileno() is not None:\n                try:\n                    self._new_stdin = os.fdopen(os.dup(sys.stdin.fileno()))\n                except OSError:\n                    # couldn't dupe stdin, most likely because it's\n                    # not a valid file descriptor, so we just rely on\n                    # using the one that was passed in\n                    pass\n        except (AttributeError, ValueError):\n            # couldn't get stdin's fileno, so we just carry on\n            pass",
        "begin_line": 74,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.process.worker.WorkerProcess.start#89",
        "src_path": "lib/ansible/executor/process/worker.py",
        "class_name": "lib.ansible.executor.process.worker.WorkerProcess",
        "signature": "lib.ansible.executor.process.worker.WorkerProcess.start(self)",
        "snippet": "    def start(self):\n        '''\n        multiprocessing.Process replaces the worker's stdin with a new file\n        opened on os.devnull, but we wish to preserve it if it is connected to\n        a terminal. Therefore dup a copy prior to calling the real start(),\n        ensuring the descriptor is preserved somewhere in the new child, and\n        make sure it is closed in the parent when start() completes.\n        '''\n\n        self._save_stdin()\n        try:\n            return super(WorkerProcess, self).start()\n        finally:\n            if self._new_stdin != os.devnull:\n                self._new_stdin.close()",
        "begin_line": 89,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector._lsb_release_bin#32",
        "src_path": "lib/ansible/module_utils/facts/system/lsb.py",
        "class_name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector._lsb_release_bin(self, lsb_path, module)",
        "snippet": "    def _lsb_release_bin(self, lsb_path, module):\n        lsb_facts = {}\n\n        if not lsb_path:\n            return lsb_facts\n\n        rc, out, err = module.run_command([lsb_path, \"-a\"], errors='surrogate_then_replace')\n        if rc != 0:\n            return lsb_facts\n\n        for line in out.splitlines():\n            if len(line) < 1 or ':' not in line:\n                continue\n            value = line.split(':', 1)[1].strip()\n\n            if 'LSB Version:' in line:\n                lsb_facts['release'] = value\n            elif 'Distributor ID:' in line:\n                lsb_facts['id'] = value\n            elif 'Description:' in line:\n                lsb_facts['description'] = value\n            elif 'Release:' in line:\n                lsb_facts['release'] = value\n            elif 'Codename:' in line:\n                lsb_facts['codename'] = value\n\n        return lsb_facts",
        "begin_line": 32,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector._lsb_release_file#60",
        "src_path": "lib/ansible/module_utils/facts/system/lsb.py",
        "class_name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector._lsb_release_file(self, etc_lsb_release_location)",
        "snippet": "    def _lsb_release_file(self, etc_lsb_release_location):\n        lsb_facts = {}\n\n        if not os.path.exists(etc_lsb_release_location):\n            return lsb_facts\n\n        for line in get_file_lines(etc_lsb_release_location):\n            value = line.split('=', 1)[1].strip()\n\n            if 'DISTRIB_ID' in line:\n                lsb_facts['id'] = value\n            elif 'DISTRIB_RELEASE' in line:\n                lsb_facts['release'] = value\n            elif 'DISTRIB_DESCRIPTION' in line:\n                lsb_facts['description'] = value\n            elif 'DISTRIB_CODENAME' in line:\n                lsb_facts['codename'] = value\n\n        return lsb_facts",
        "begin_line": 60,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector.collect#80",
        "src_path": "lib/ansible/module_utils/facts/system/lsb.py",
        "class_name": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.lsb.LSBFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        lsb_facts = {}\n\n        if not module:\n            return facts_dict\n\n        lsb_path = module.get_bin_path('lsb_release')\n\n        # try the 'lsb_release' script first\n        if lsb_path:\n            lsb_facts = self._lsb_release_bin(lsb_path,\n                                              module=module)\n\n        # no lsb_release, try looking in /etc/lsb-release\n        if not lsb_facts:\n            lsb_facts = self._lsb_release_file('/etc/lsb-release')\n\n        if lsb_facts and 'release' in lsb_facts:\n            lsb_facts['major_release'] = lsb_facts['release'].split('.')[0]\n\n        for k, v in lsb_facts.items():\n            if v:\n                lsb_facts[k] = v.strip(LSBFactCollector.STRIP_QUOTES)\n\n        facts_dict['lsb'] = lsb_facts\n        return facts_dict",
        "begin_line": 80,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.fips.FipsFactCollector.collect#30",
        "src_path": "lib/ansible/module_utils/facts/system/fips.py",
        "class_name": "lib.ansible.module_utils.facts.system.fips.FipsFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.fips.FipsFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        # NOTE: this is populated even if it is not set\n        fips_facts = {}\n        fips_facts['fips'] = False\n        data = get_file_content('/proc/sys/crypto/fips_enabled')\n        if data and data == '1':\n            fips_facts['fips'] = True\n        return fips_facts",
        "begin_line": 30,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urlsplit.split_url#21",
        "src_path": "lib/ansible/plugins/filter/urlsplit.py",
        "class_name": "lib.ansible.plugins.filter.urlsplit",
        "signature": "lib.ansible.plugins.filter.urlsplit.split_url(value, query='', alias='urlsplit')",
        "snippet": "def split_url(value, query='', alias='urlsplit'):\n\n    results = helpers.object_to_dict(urlsplit(value), exclude=['count', 'index', 'geturl', 'encode'])\n\n    # If a query is supplied, make sure it's valid then return the results.\n    # If no option is supplied, return the entire dictionary.\n    if query:\n        if query not in results:\n            raise AnsibleFilterError(alias + ': unknown URL component: %s' % query)\n        return results[query]\n    else:\n        return results",
        "begin_line": 21,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urlsplit.FilterModule.filters#39",
        "src_path": "lib/ansible/plugins/filter/urlsplit.py",
        "class_name": "lib.ansible.plugins.filter.urlsplit.FilterModule",
        "signature": "lib.ansible.plugins.filter.urlsplit.FilterModule.filters(self)",
        "snippet": "    def filters(self):\n        return {\n            'urlsplit': split_url\n        }",
        "begin_line": 39,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006131207847946045,
            "pseudo_dstar_susp": 0.0006131207847946045,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0006131207847946045,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.__init__#22",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n\n        self._plugin = cache_loader.get(C.CACHE_PLUGIN)\n        if not self._plugin:\n            raise AnsibleError('Unable to load the facts cache plugin (%s).' % (C.CACHE_PLUGIN))\n\n        super(FactCache, self).__init__(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.__getitem__#30",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if not self._plugin.contains(key):\n            raise KeyError\n        return self._plugin.get(key)",
        "begin_line": 30,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.__setitem__#35",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        self._plugin.set(key, value)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.__delitem__#38",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        self._plugin.delete(key)",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.__contains__#41",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return self._plugin.contains(key)",
        "begin_line": 41,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.copy#50",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\" Return a primitive copy of the keys and values from the cache. \"\"\"\n        return dict(self)",
        "begin_line": 50,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.keys#54",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.keys(self)",
        "snippet": "    def keys(self):\n        return self._plugin.keys()",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.first_order_merge#61",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.first_order_merge(self, key, value)",
        "snippet": "    def first_order_merge(self, key, value):\n        host_facts = {key: value}\n\n        try:\n            host_cache = self._plugin.get(key)\n            if host_cache:\n                host_cache.update(value)\n                host_facts[key] = host_cache\n        except KeyError:\n            pass\n\n        super(FactCache, self).update(host_facts)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.fact_cache.FactCache.update#74",
        "src_path": "lib/ansible/vars/fact_cache.py",
        "class_name": "lib.ansible.vars.fact_cache.FactCache",
        "signature": "lib.ansible.vars.fact_cache.FactCache.update(self, *args)",
        "snippet": "    def update(self, *args):\n        \"\"\"\n        Backwards compat shim\n\n        We thought we needed this to ensure we always called the plugin's set() method but\n        MutableMapping.update() will call our __setitem__() just fine.  It's the calls to update\n        that we need to be careful of.  This contains a bug::\n\n            fact_cache[host.name].update(facts)\n\n        It retrieves a *copy* of the facts for host.name and then updates the copy.  So the changes\n        aren't persisted.\n\n        Instead we need to do::\n\n            fact_cache.update({host.name, facts})\n\n        Which will use FactCache's update() method.\n\n        We currently need this shim for backwards compat because the update() method that we had\n        implemented took key and value as arguments instead of taking a dict.  We can remove the\n        shim in 2.12 as MutableMapping.update() should do everything that we need.\n        \"\"\"\n        if len(args) == 2:\n            # Deprecated.  Call the new function with this name\n            display.deprecated('Calling FactCache().update(key, value) is deprecated.  Use'\n                               ' FactCache().first_order_merge(key, value) if you want the old'\n                               ' behaviour or use FactCache().update({key: value}) if you want'\n                               ' dict-like behaviour.', version='2.12')\n            return self.first_order_merge(*args)\n\n        elif len(args) == 1:\n            host_facts = args[0]\n\n        else:\n            raise TypeError('update expected at most 1 argument, got {0}'.format(len(args)))\n\n        super(FactCache, self).update(host_facts)",
        "begin_line": 74,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.file.is_executable#65",
        "src_path": "lib/ansible/module_utils/common/file.py",
        "class_name": "lib.ansible.module_utils.common.file",
        "signature": "lib.ansible.module_utils.common.file.is_executable(path)",
        "snippet": "def is_executable(path):\n    # This function's signature needs to be repeated\n    # as the first line of its docstring.\n    # This method is reused by the basic module,\n    # the repetion helps the basic module's html documentation come out right.\n    # http://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#confval-autodoc_docstring_signature\n    '''is_executable(path)\n\n    is the given path executable?\n\n    :arg path: The path of the file to check.\n\n    Limitations:\n\n    * Does not account for FSACLs.\n    * Most times we really want to know \"Can the current user execute this\n      file\".  This function does not tell us that, only if any execute bit is set.\n    '''\n    # These are all bitfields so first bitwise-or all the permissions we're\n    # looking for, then bitwise-and with the file's mode to determine if any\n    # execute bits are set.\n    return ((stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH) & os.stat(path)[stat.ST_MODE])",
        "begin_line": 65,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._lsblk_uuid#389",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._lsblk_uuid(self)",
        "snippet": "    def _lsblk_uuid(self):\n        uuids = {}\n        lsblk_path = self.module.get_bin_path(\"lsblk\")\n        if not lsblk_path:\n            return uuids\n\n        rc, out, err = self._run_lsblk(lsblk_path)\n        if rc != 0:\n            return uuids\n\n        # each line will be in format:\n        # <devicename><some whitespace><uuid>\n        # /dev/sda1  32caaec3-ef40-4691-a3b6-438c3f9bc1c0\n        for lsblk_line in out.splitlines():\n            if not lsblk_line:\n                continue\n\n            line = lsblk_line.strip()\n            fields = line.rsplit(None, 1)\n\n            if len(fields) < 2:\n                continue\n\n            device_name, uuid = fields[0].strip(), fields[1].strip()\n            if device_name in uuids:\n                continue\n            uuids[device_name] = uuid\n\n        return uuids",
        "begin_line": 389,
        "end_line": 417,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._udevadm_uuid#419",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._udevadm_uuid(self, device)",
        "snippet": "    def _udevadm_uuid(self, device):\n        # fallback for versions of lsblk <= 2.23 that don't have --paths, see _run_lsblk() above\n        uuid = 'N/A'\n\n        udevadm_path = self.module.get_bin_path('udevadm')\n        if not udevadm_path:\n            return uuid\n\n        cmd = [udevadm_path, 'info', '--query', 'property', '--name', device]\n        rc, out, err = self.module.run_command(cmd)\n        if rc != 0:\n            return uuid\n\n        # a snippet of the output of the udevadm command below will be:\n        # ...\n        # ID_FS_TYPE=ext4\n        # ID_FS_USAGE=filesystem\n        # ID_FS_UUID=57b1a3e7-9019-4747-9809-7ec52bba9179\n        # ...\n        m = re.search('ID_FS_UUID=(.*)\\n', out)\n        if m:\n            uuid = m.group(1)\n\n        return uuid",
        "begin_line": 419,
        "end_line": 442,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._find_bind_mounts#450",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._find_bind_mounts(self)",
        "snippet": "    def _find_bind_mounts(self):\n        bind_mounts = set()\n        findmnt_path = self.module.get_bin_path(\"findmnt\")\n        if not findmnt_path:\n            return bind_mounts\n\n        rc, out, err = self._run_findmnt(findmnt_path)\n        if rc != 0:\n            return bind_mounts\n\n        # find bind mounts, in case /etc/mtab is a symlink to /proc/mounts\n        for line in out.splitlines():\n            fields = line.split()\n            # fields[0] is the TARGET, fields[1] is the SOURCE\n            if len(fields) < 2:\n                continue\n\n            # bind mounts will have a [/directory_name] in the SOURCE column\n            if self.BIND_MOUNT_RE.match(fields[1]):\n                bind_mounts.add(fields[0])\n\n        return bind_mounts",
        "begin_line": 450,
        "end_line": 471,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._mtab_entries#473",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._mtab_entries(self)",
        "snippet": "    def _mtab_entries(self):\n        mtab_file = '/etc/mtab'\n        if not os.path.exists(mtab_file):\n            mtab_file = '/proc/mounts'\n\n        mtab = get_file_content(mtab_file, '')\n        mtab_entries = []\n        for line in mtab.splitlines():\n            fields = line.split()\n            if len(fields) < 4:\n                continue\n            mtab_entries.append(fields)\n        return mtab_entries",
        "begin_line": 473,
        "end_line": 485,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._replace_octal_escapes_helper#488",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._replace_octal_escapes_helper(match)",
        "snippet": "    def _replace_octal_escapes_helper(match):\n        # Convert to integer using base8 and then convert to character\n        return chr(int(match.group()[1:], 8))",
        "begin_line": 488,
        "end_line": 490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._replace_octal_escapes#492",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware._replace_octal_escapes(self, value)",
        "snippet": "    def _replace_octal_escapes(self, value):\n        return self.OCTAL_ESCAPE_RE.sub(self._replace_octal_escapes_helper, value)",
        "begin_line": 492,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware.get_mount_info#495",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware.get_mount_info(self, mount, device, uuids)",
        "snippet": "    def get_mount_info(self, mount, device, uuids):\n\n        mount_size = get_mount_size(mount)\n\n        # _udevadm_uuid is a fallback for versions of lsblk <= 2.23 that don't have --paths\n        # see _run_lsblk() above\n        # https://github.com/ansible/ansible/issues/36077\n        uuid = uuids.get(device, self._udevadm_uuid(device))\n\n        return mount_size, uuid",
        "begin_line": 495,
        "end_line": 504,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware.get_mount_facts#506",
        "src_path": "lib/ansible/module_utils/facts/hardware/linux.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware",
        "signature": "lib.ansible.module_utils.facts.hardware.linux.LinuxHardware.get_mount_facts(self)",
        "snippet": "    def get_mount_facts(self):\n\n        mounts = []\n\n        # gather system lists\n        bind_mounts = self._find_bind_mounts()\n        uuids = self._lsblk_uuid()\n        mtab_entries = self._mtab_entries()\n\n        # start threads to query each mount\n        results = {}\n        pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))\n        maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT\n        for fields in mtab_entries:\n            # Transform octal escape sequences\n            fields = [self._replace_octal_escapes(field) for field in fields]\n\n            device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n\n            if not device.startswith('/') and ':/' not in device or fstype == 'none':\n                continue\n\n            mount_info = {'mount': mount,\n                          'device': device,\n                          'fstype': fstype,\n                          'options': options}\n\n            if mount in bind_mounts:\n                # only add if not already there, we might have a plain /etc/mtab\n                if not self.MTAB_BIND_MOUNT_RE.match(options):\n                    mount_info['options'] += \",bind\"\n\n            results[mount] = {'info': mount_info,\n                              'extra': pool.apply_async(self.get_mount_info, (mount, device, uuids)),\n                              'timelimit': time.time() + maxtime}\n\n        pool.close()  # done with new workers, start gc\n\n        # wait for workers and get results\n        while results:\n            for mount in results:\n                res = results[mount]['extra']\n                if res.ready():\n                    if res.successful():\n                        mount_size, uuid = res.get()\n                        if mount_size:\n                            results[mount]['info'].update(mount_size)\n                        results[mount]['info']['uuid'] = uuid or 'N/A'\n                    else:\n                        # give incomplete data\n                        errmsg = to_text(res.get())\n                        self.module.warn(\"Error prevented getting extra info for mount %s: %s.\" % (mount, errmsg))\n                        results[mount]['info']['note'] = 'Could not get extra information: %s.' % (errmsg)\n\n                    mounts.append(results[mount]['info'])\n                    del results[mount]\n                    break\n                elif time.time() > results[mount]['timelimit']:\n                    results[mount]['info']['note'] = 'Timed out while attempting to get extra information.'\n                    mounts.append(results[mount]['info'])\n                    del results[mount]\n                    break\n            else:\n                # avoid cpu churn\n                time.sleep(0.1)\n\n        return {'mounts': mounts}",
        "begin_line": 506,
        "end_line": 572,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.powershell._parse_clixml#40",
        "src_path": "lib/ansible/plugins/shell/powershell.py",
        "class_name": "lib.ansible.plugins.shell.powershell",
        "signature": "lib.ansible.plugins.shell.powershell._parse_clixml(data, stream='Error')",
        "snippet": "def _parse_clixml(data, stream=\"Error\"):\n    \"\"\"\n    Takes a byte string like '#< CLIXML\\r\\n<Objs...' and extracts the stream\n    message encoded in the XML data. CLIXML is used by PowerShell to encode\n    multiple objects in stderr.\n    \"\"\"\n    clixml = ET.fromstring(data.split(b\"\\r\\n\", 1)[-1])\n    namespace_match = re.match(r'{(.*)}', clixml.tag)\n    namespace = \"{%s}\" % namespace_match.group(1) if namespace_match else \"\"\n\n    strings = clixml.findall(\"./%sS\" % namespace)\n    lines = [e.text.replace('_x000D__x000A_', '') for e in strings if e.attrib.get('S') == stream]\n    return to_bytes('\\r\\n'.join(lines))",
        "begin_line": 40,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.powershell.ShellModule.join_path#76",
        "src_path": "lib/ansible/plugins/shell/powershell.py",
        "class_name": "lib.ansible.plugins.shell.powershell.ShellModule",
        "signature": "lib.ansible.plugins.shell.powershell.ShellModule.join_path(self, *args)",
        "snippet": "    def join_path(self, *args):\n        # use normpath() to remove doubled slashed and convert forward to backslashes\n        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]\n\n        # Becuase ntpath.join treats any component that begins with a backslash as an absolute path,\n        # we have to strip slashes from at least the beginning, otherwise join will ignore all previous\n        # path components except for the drive.\n        return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])",
        "begin_line": 76,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.powershell.ShellModule._unquote#244",
        "src_path": "lib/ansible/plugins/shell/powershell.py",
        "class_name": "lib.ansible.plugins.shell.powershell.ShellModule",
        "signature": "lib.ansible.plugins.shell.powershell.ShellModule._unquote(self, value)",
        "snippet": "    def _unquote(self, value):\n        '''Remove any matching quotes that wrap the given value.'''\n        value = to_text(value or '')\n        m = re.match(r'^\\s*?\\'(.*?)\\'\\s*?$', value)\n        if m:\n            return m.group(1)\n        m = re.match(r'^\\s*?\"(.*?)\"\\s*?$', value)\n        if m:\n            return m.group(1)\n        return value",
        "begin_line": 244,
        "end_line": 253,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.metadata.RoleMetadata.__init__#46",
        "src_path": "lib/ansible/playbook/role/metadata.py",
        "class_name": "lib.ansible.playbook.role.metadata.RoleMetadata",
        "signature": "lib.ansible.playbook.role.metadata.RoleMetadata.__init__(self, owner=None)",
        "snippet": "    def __init__(self, owner=None):\n        self._owner = owner\n        super(RoleMetadata, self).__init__()",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.metadata.RoleMetadata.load#51",
        "src_path": "lib/ansible/playbook/role/metadata.py",
        "class_name": "lib.ansible.playbook.role.metadata.RoleMetadata",
        "signature": "lib.ansible.playbook.role.metadata.RoleMetadata.load(data, owner, variable_manager=None, loader=None)",
        "snippet": "    def load(data, owner, variable_manager=None, loader=None):\n        '''\n        Returns a new RoleMetadata object based on the datastructure passed in.\n        '''\n\n        if not isinstance(data, dict):\n            raise AnsibleParserError(\"the 'meta/main.yml' for role %s is not a dictionary\" % owner.get_name())\n\n        m = RoleMetadata(owner=owner).load_data(data, variable_manager=variable_manager, loader=loader)\n        return m",
        "begin_line": 51,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.metadata.RoleMetadata._load_dependencies#62",
        "src_path": "lib/ansible/playbook/role/metadata.py",
        "class_name": "lib.ansible.playbook.role.metadata.RoleMetadata",
        "signature": "lib.ansible.playbook.role.metadata.RoleMetadata._load_dependencies(self, attr, ds)",
        "snippet": "    def _load_dependencies(self, attr, ds):\n        '''\n        This is a helper loading function for the dependencies list,\n        which returns a list of RoleInclude objects\n        '''\n\n        roles = []\n        if ds:\n            if not isinstance(ds, list):\n                raise AnsibleParserError(\"Expected role dependencies to be a list.\", obj=self._ds)\n\n            for role_def in ds:\n                if isinstance(role_def, string_types) or 'role' in role_def or 'name' in role_def:\n                    roles.append(role_def)\n                    continue\n                try:\n                    # role_def is new style: { src: 'galaxy.role,version,name', other_vars: \"here\" }\n                    def_parsed = RoleRequirement.role_yaml_parse(role_def)\n                    if def_parsed.get('name'):\n                        role_def['name'] = def_parsed['name']\n                    roles.append(role_def)\n                except AnsibleError as exc:\n                    raise AnsibleParserError(to_native(exc), obj=role_def, orig_exc=exc)\n\n        current_role_path = None\n        collection_search_list = None\n\n        if self._owner:\n            current_role_path = os.path.dirname(self._owner._role_path)\n\n            # if the calling role has a collections search path defined, consult it\n            collection_search_list = self._owner.collections[:] or []\n\n            # if the calling role is a collection role, ensure that its containing collection is searched first\n            owner_collection = self._owner._role_collection\n            if owner_collection:\n                collection_search_list = [c for c in collection_search_list if c != owner_collection]\n                collection_search_list.insert(0, owner_collection)\n            # ensure fallback role search works\n            if 'ansible.legacy' not in collection_search_list:\n                collection_search_list.append('ansible.legacy')\n\n        try:\n            return load_list_of_roles(roles, play=self._owner._play, current_role_path=current_role_path,\n                                      variable_manager=self._variable_manager, loader=self._loader,\n                                      collection_search_list=collection_search_list)\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed list of role dependencies was encountered.\", obj=self._ds, orig_exc=e)",
        "begin_line": 62,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.metadata.RoleMetadata._load_galaxy_info#111",
        "src_path": "lib/ansible/playbook/role/metadata.py",
        "class_name": "lib.ansible.playbook.role.metadata.RoleMetadata",
        "signature": "lib.ansible.playbook.role.metadata.RoleMetadata._load_galaxy_info(self, attr, ds)",
        "snippet": "    def _load_galaxy_info(self, attr, ds):\n        '''\n        This is a helper loading function for the galaxy info entry\n        in the metadata, which returns a GalaxyInfo object rather than\n        a simple dictionary.\n        '''\n\n        return ds",
        "begin_line": 111,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_yaml#63",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_yaml(a, *args, **kw)",
        "snippet": "def to_yaml(a, *args, **kw):\n    '''Make verbose, human readable yaml'''\n    default_flow_style = kw.pop('default_flow_style', None)\n    transformed = yaml.dump(a, Dumper=AnsibleDumper, allow_unicode=True, default_flow_style=default_flow_style, **kw)\n    return to_text(transformed)",
        "begin_line": 63,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_nice_yaml#70",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_nice_yaml(a, indent=4, *args, **kw)",
        "snippet": "def to_nice_yaml(a, indent=4, *args, **kw):\n    '''Make verbose, human readable yaml'''\n    transformed = yaml.dump(a, Dumper=AnsibleDumper, indent=indent, allow_unicode=True, default_flow_style=False, **kw)\n    return to_text(transformed)",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_json#76",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_json(a, *args, **kw)",
        "snippet": "def to_json(a, *args, **kw):\n    ''' Convert the value to JSON '''\n    return json.dumps(a, cls=AnsibleJSONEncoder, *args, **kw)",
        "begin_line": 76,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_nice_json#81",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_nice_json(a, indent=4, sort_keys=True, *args, **kw)",
        "snippet": "def to_nice_json(a, indent=4, sort_keys=True, *args, **kw):\n    '''Make verbose, human readable JSON'''\n    return to_json(a, indent=indent, sort_keys=sort_keys, separators=(',', ': '), *args, **kw)",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_bool#86",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_bool(a)",
        "snippet": "def to_bool(a):\n    ''' return a bool for the arg '''\n    if a is None or isinstance(a, bool):\n        return a\n    if isinstance(a, string_types):\n        a = a.lower()\n    if a in ('yes', 'on', '1', 'true', 1):\n        return True\n    return False",
        "begin_line": 86,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_datetime#97",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_datetime(string, format='%Y-%m-%d %H:%M:%S')",
        "snippet": "def to_datetime(string, format=\"%Y-%m-%d %H:%M:%S\"):\n    return datetime.datetime.strptime(string, format)",
        "begin_line": 97,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.strftime#101",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.strftime(string_format, second=None)",
        "snippet": "def strftime(string_format, second=None):\n    ''' return a date string using string. See https://docs.python.org/2/library/time.html#time.strftime for format '''\n    if second is not None:\n        try:\n            second = int(second)\n        except Exception:\n            raise AnsibleFilterError('Invalid value for epoch value (%s)' % second)\n    return time.strftime(string_format, time.localtime(second))",
        "begin_line": 101,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.quote#111",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.quote(a)",
        "snippet": "def quote(a):\n    ''' return its argument quoted for shell usage '''\n    return shlex_quote(to_text(a))",
        "begin_line": 111,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.fileglob#116",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.fileglob(pathname)",
        "snippet": "def fileglob(pathname):\n    ''' return list of matched regular files for glob '''\n    return [g for g in glob.glob(pathname) if os.path.isfile(g)]",
        "begin_line": 116,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.regex_replace#121",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.regex_replace(value='', pattern='', replacement='', ignorecase=False, multiline=False)",
        "snippet": "def regex_replace(value='', pattern='', replacement='', ignorecase=False, multiline=False):\n    ''' Perform a `re.sub` returning a string '''\n\n    value = to_text(value, errors='surrogate_or_strict', nonstring='simplerepr')\n\n    flags = 0\n    if ignorecase:\n        flags |= re.I\n    if multiline:\n        flags |= re.M\n    _re = re.compile(pattern, flags=flags)\n    return _re.sub(replacement, value)",
        "begin_line": 121,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.regex_findall#135",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.regex_findall(value, regex, multiline=False, ignorecase=False)",
        "snippet": "def regex_findall(value, regex, multiline=False, ignorecase=False):\n    ''' Perform re.findall and return the list of matches '''\n    flags = 0\n    if ignorecase:\n        flags |= re.I\n    if multiline:\n        flags |= re.M\n    return re.findall(regex, value, flags)",
        "begin_line": 135,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.regex_search#145",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.regex_search(value, regex, *args, **kwargs)",
        "snippet": "def regex_search(value, regex, *args, **kwargs):\n    ''' Perform re.search and return the list of matches or a backref '''\n\n    groups = list()\n    for arg in args:\n        if arg.startswith('\\\\g'):\n            match = re.match(r'\\\\g<(\\S+)>', arg).group(1)\n            groups.append(match)\n        elif arg.startswith('\\\\'):\n            match = int(re.match(r'\\\\(\\d+)', arg).group(1))\n            groups.append(match)\n        else:\n            raise AnsibleFilterError('Unknown argument')\n\n    flags = 0\n    if kwargs.get('ignorecase'):\n        flags |= re.I\n    if kwargs.get('multiline'):\n        flags |= re.M\n\n    match = re.search(regex, value, flags)\n    if match:\n        if not groups:\n            return match.group()\n        else:\n            items = list()\n            for item in groups:\n                items.append(match.group(item))\n            return items",
        "begin_line": 145,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.ternary#176",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.ternary(value, true_val, false_val, none_val=None)",
        "snippet": "def ternary(value, true_val, false_val, none_val=None):\n    '''  value ? true_val : false_val '''\n    if value is None and none_val is not None:\n        return none_val\n    elif bool(value):\n        return true_val\n    else:\n        return false_val",
        "begin_line": 176,
        "end_line": 183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.regex_escape#186",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.regex_escape(string, re_type='python')",
        "snippet": "def regex_escape(string, re_type='python'):\n    '''Escape all regular expressions special characters from STRING.'''\n    if re_type == 'python':\n        return re.escape(string)\n    elif re_type == 'posix_basic':\n        # list of BRE special chars:\n        # https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions\n        return regex_replace(string, r'([].[^$*\\\\])', r'\\\\\\1')\n    # TODO: implement posix_extended\n    # It's similar to, but different from python regex, which is similar to,\n    # but different from PCRE.  It's possible that re.escape would work here.\n    # https://remram44.github.io/regex-cheatsheet/regex.html#programs\n    elif re_type == 'posix_extended':\n        raise AnsibleFilterError('Regex type (%s) not yet implemented' % re_type)\n    else:\n        raise AnsibleFilterError('Invalid regex type (%s)' % re_type)",
        "begin_line": 186,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.from_yaml#204",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.from_yaml(data)",
        "snippet": "def from_yaml(data):\n    if isinstance(data, string_types):\n        return yaml.safe_load(data)\n    return data",
        "begin_line": 204,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.from_yaml_all#210",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.from_yaml_all(data)",
        "snippet": "def from_yaml_all(data):\n    if isinstance(data, string_types):\n        return yaml.safe_load_all(data)\n    return data",
        "begin_line": 210,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.rand#217",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.rand(environment, end, start=None, step=None, seed=None)",
        "snippet": "def rand(environment, end, start=None, step=None, seed=None):\n    if seed is None:\n        r = SystemRandom()\n    else:\n        r = Random(seed)\n    if isinstance(end, integer_types):\n        if not start:\n            start = 0\n        if not step:\n            step = 1\n        return r.randrange(start, end, step)\n    elif hasattr(end, '__iter__'):\n        if start or step:\n            raise AnsibleFilterError('start and step can only be used with integer values')\n        return r.choice(end)\n    else:\n        raise AnsibleFilterError('random can only be used on sequences and integers')",
        "begin_line": 217,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.randomize_list#236",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.randomize_list(mylist, seed=None)",
        "snippet": "def randomize_list(mylist, seed=None):\n    try:\n        mylist = list(mylist)\n        if seed:\n            r = Random(seed)\n            r.shuffle(mylist)\n        else:\n            shuffle(mylist)\n    except Exception:\n        pass\n    return mylist",
        "begin_line": 236,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.get_hash#249",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.get_hash(data, hashtype='sha1')",
        "snippet": "def get_hash(data, hashtype='sha1'):\n\n    try:  # see if hash is supported\n        h = hashlib.new(hashtype)\n    except Exception:\n        return None\n\n    h.update(to_bytes(data, errors='surrogate_or_strict'))\n    return h.hexdigest()",
        "begin_line": 249,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.get_encrypted_password#260",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.get_encrypted_password(password, hashtype='sha512', salt=None, salt_size=None, rounds=None)",
        "snippet": "def get_encrypted_password(password, hashtype='sha512', salt=None, salt_size=None, rounds=None):\n    passlib_mapping = {\n        'md5': 'md5_crypt',\n        'blowfish': 'bcrypt',\n        'sha256': 'sha256_crypt',\n        'sha512': 'sha512_crypt',\n    }\n\n    hashtype = passlib_mapping.get(hashtype, hashtype)\n    try:\n        return passlib_or_crypt(password, hashtype, salt=salt, salt_size=salt_size, rounds=rounds)\n    except AnsibleError as e:\n        reraise(AnsibleFilterError, AnsibleFilterError(to_native(e), orig_exc=e), sys.exc_info()[2])",
        "begin_line": 260,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.to_uuid#275",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.to_uuid(string, namespace=UUID_NAMESPACE_ANSIBLE)",
        "snippet": "def to_uuid(string, namespace=UUID_NAMESPACE_ANSIBLE):\n    uuid_namespace = namespace\n    if not isinstance(uuid_namespace, uuid.UUID):\n        try:\n            uuid_namespace = uuid.UUID(namespace)\n        except (AttributeError, ValueError) as e:\n            raise AnsibleFilterError(\"Invalid value '%s' for 'namespace': %s\" % (to_native(namespace), to_native(e)))\n    # uuid.uuid5() requires bytes on Python 2 and bytes or text or Python 3\n    return to_text(uuid.uuid5(uuid_namespace, to_native(string, errors='surrogate_or_strict')))",
        "begin_line": 275,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.mandatory#286",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.mandatory(a, msg=None)",
        "snippet": "def mandatory(a, msg=None):\n    from jinja2.runtime import Undefined\n\n    ''' Make a variable mandatory '''\n    if isinstance(a, Undefined):\n        if a._undefined_name is not None:\n            name = \"'%s' \" % to_text(a._undefined_name)\n        else:\n            name = ''\n\n        if msg is not None:\n            raise AnsibleFilterError(to_native(msg))\n        else:\n            raise AnsibleFilterError(\"Mandatory variable %s not defined.\" % name)\n\n    return a",
        "begin_line": 286,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.combine#304",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.combine(*terms, **kwargs)",
        "snippet": "def combine(*terms, **kwargs):\n    recursive = kwargs.pop('recursive', False)\n    list_merge = kwargs.pop('list_merge', 'replace')\n    if kwargs:\n        raise AnsibleFilterError(\"'recursive' and 'list_merge' are the only valid keyword arguments\")\n\n    # allow the user to do `[dict1, dict2, ...] | combine`\n    dictionaries = flatten(terms, levels=1)\n\n    # recursively check that every elements are defined (for jinja2)\n    recursive_check_defined(dictionaries)\n\n    if not dictionaries:\n        return {}\n\n    if len(dictionaries) == 1:\n        return dictionaries[0]\n\n    # merge all the dicts so that the dict at the end of the array have precedence\n    # over the dict at the beginning.\n    # we merge the dicts from the highest to the lowest priority because there is\n    # a huge probability that the lowest priority dict will be the biggest in size\n    # (as the low prio dict will hold the \"default\" values and the others will be \"patches\")\n    # and merge_hash create a copy of it's first argument.\n    # so high/right -> low/left is more efficient than low/left -> high/right\n    high_to_low_prio_dict_iterator = reversed(dictionaries)\n    result = next(high_to_low_prio_dict_iterator)\n    for dictionary in high_to_low_prio_dict_iterator:\n        result = merge_hash(dictionary, result, recursive, list_merge)\n\n    return result",
        "begin_line": 304,
        "end_line": 334,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.comment#337",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.comment(text, style='plain', **kw)",
        "snippet": "def comment(text, style='plain', **kw):\n    # Predefined comment types\n    comment_styles = {\n        'plain': {\n            'decoration': '# '\n        },\n        'erlang': {\n            'decoration': '% '\n        },\n        'c': {\n            'decoration': '// '\n        },\n        'cblock': {\n            'beginning': '/*',\n            'decoration': ' * ',\n            'end': ' */'\n        },\n        'xml': {\n            'beginning': '<!--',\n            'decoration': ' - ',\n            'end': '-->'\n        }\n    }\n\n    # Pointer to the right comment type\n    style_params = comment_styles[style]\n\n    if 'decoration' in kw:\n        prepostfix = kw['decoration']\n    else:\n        prepostfix = style_params['decoration']\n\n    # Default params\n    p = {\n        'newline': '\\n',\n        'beginning': '',\n        'prefix': (prepostfix).rstrip(),\n        'prefix_count': 1,\n        'decoration': '',\n        'postfix': (prepostfix).rstrip(),\n        'postfix_count': 1,\n        'end': ''\n    }\n\n    # Update default params\n    p.update(style_params)\n    p.update(kw)\n\n    # Compose substrings for the final string\n    str_beginning = ''\n    if p['beginning']:\n        str_beginning = \"%s%s\" % (p['beginning'], p['newline'])\n    str_prefix = ''\n    if p['prefix']:\n        if p['prefix'] != p['newline']:\n            str_prefix = str(\n                \"%s%s\" % (p['prefix'], p['newline'])) * int(p['prefix_count'])\n        else:\n            str_prefix = str(\n                \"%s\" % (p['newline'])) * int(p['prefix_count'])\n    str_text = (\"%s%s\" % (\n        p['decoration'],\n        # Prepend each line of the text with the decorator\n        text.replace(\n            p['newline'], \"%s%s\" % (p['newline'], p['decoration'])))).replace(\n                # Remove trailing spaces when only decorator is on the line\n                \"%s%s\" % (p['decoration'], p['newline']),\n                \"%s%s\" % (p['decoration'].rstrip(), p['newline']))\n    str_postfix = p['newline'].join(\n        [''] + [p['postfix'] for x in range(p['postfix_count'])])\n    str_end = ''\n    if p['end']:\n        str_end = \"%s%s\" % (p['newline'], p['end'])\n\n    # Return the final string\n    return \"%s%s%s%s%s\" % (\n        str_beginning,\n        str_prefix,\n        str_text,\n        str_postfix,\n        str_end)",
        "begin_line": 337,
        "end_line": 417,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.extract#421",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.extract(environment, item, container, morekeys=None)",
        "snippet": "def extract(environment, item, container, morekeys=None):\n    if morekeys is None:\n        keys = [item]\n    elif isinstance(morekeys, list):\n        keys = [item] + morekeys\n    else:\n        keys = [item, morekeys]\n\n    value = container\n    for key in keys:\n        value = environment.getitem(value, key)\n\n    return value",
        "begin_line": 421,
        "end_line": 433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.do_groupby#437",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.do_groupby(environment, value, attribute)",
        "snippet": "def do_groupby(environment, value, attribute):\n    \"\"\"Overridden groupby filter for jinja2, to address an issue with\n    jinja2>=2.9.0,<2.9.5 where a namedtuple was returned which\n    has repr that prevents ansible.template.safe_eval.safe_eval from being\n    able to parse and eval the data.\n\n    jinja2<2.9.0,>=2.9.5 is not affected, as <2.9.0 uses a tuple, and\n    >=2.9.5 uses a standard tuple repr on the namedtuple.\n\n    The adaptation here, is to run the jinja2 `do_groupby` function, and\n    cast all of the namedtuples to a regular tuple.\n\n    See https://github.com/ansible/ansible/issues/20098\n\n    We may be able to remove this in the future.\n    \"\"\"\n    return [tuple(t) for t in _do_groupby(environment, value, attribute)]",
        "begin_line": 437,
        "end_line": 453,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.b64encode#456",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.b64encode(string, encoding='utf-8')",
        "snippet": "def b64encode(string, encoding='utf-8'):\n    return to_text(base64.b64encode(to_bytes(string, encoding=encoding, errors='surrogate_or_strict')))",
        "begin_line": 456,
        "end_line": 457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.b64decode#460",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.b64decode(string, encoding='utf-8')",
        "snippet": "def b64decode(string, encoding='utf-8'):\n    return to_text(base64.b64decode(to_bytes(string, errors='surrogate_or_strict')), encoding=encoding)",
        "begin_line": 460,
        "end_line": 461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.flatten#464",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.flatten(mylist, levels=None)",
        "snippet": "def flatten(mylist, levels=None):\n\n    ret = []\n    for element in mylist:\n        if element in (None, 'None', 'null'):\n            # ignore undefined items\n            break\n        elif is_sequence(element):\n            if levels is None:\n                ret.extend(flatten(element))\n            elif levels >= 1:\n                # decrement as we go down the stack\n                ret.extend(flatten(element, levels=(int(levels) - 1)))\n            else:\n                ret.append(element)\n        else:\n            ret.append(element)\n\n    return ret",
        "begin_line": 464,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.subelements#485",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.subelements(obj, subelements, skip_missing=False)",
        "snippet": "def subelements(obj, subelements, skip_missing=False):\n    '''Accepts a dict or list of dicts, and a dotted accessor and produces a product\n    of the element and the results of the dotted accessor\n\n    >>> obj = [{\"name\": \"alice\", \"groups\": [\"wheel\"], \"authorized\": [\"/tmp/alice/onekey.pub\"]}]\n    >>> subelements(obj, 'groups')\n    [({'name': 'alice', 'groups': ['wheel'], 'authorized': ['/tmp/alice/onekey.pub']}, 'wheel')]\n\n    '''\n    if isinstance(obj, dict):\n        element_list = list(obj.values())\n    elif isinstance(obj, list):\n        element_list = obj[:]\n    else:\n        raise AnsibleFilterError('obj must be a list of dicts or a nested dict')\n\n    if isinstance(subelements, list):\n        subelement_list = subelements[:]\n    elif isinstance(subelements, string_types):\n        subelement_list = subelements.split('.')\n    else:\n        raise AnsibleFilterError('subelements must be a list or a string')\n\n    results = []\n\n    for element in element_list:\n        values = element\n        for subelement in subelement_list:\n            try:\n                values = values[subelement]\n            except KeyError:\n                if skip_missing:\n                    values = []\n                    break\n                raise AnsibleFilterError(\"could not find %r key in iterated item %r\" % (subelement, values))\n            except TypeError:\n                raise AnsibleFilterError(\"the key %s should point to a dictionary, got '%s'\" % (subelement, values))\n        if not isinstance(values, list):\n            raise AnsibleFilterError(\"the key %r should point to a list, got %r\" % (subelement, values))\n\n        for value in values:\n            results.append((element, value))\n\n    return results",
        "begin_line": 485,
        "end_line": 528,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.dict_to_list_of_dict_key_value_elements#531",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.dict_to_list_of_dict_key_value_elements(mydict, key_name='key', value_name='value')",
        "snippet": "def dict_to_list_of_dict_key_value_elements(mydict, key_name='key', value_name='value'):\n    ''' takes a dictionary and transforms it into a list of dictionaries,\n        with each having a 'key' and 'value' keys that correspond to the keys and values of the original '''\n\n    if not isinstance(mydict, Mapping):\n        raise AnsibleFilterError(\"dict2items requires a dictionary, got %s instead.\" % type(mydict))\n\n    ret = []\n    for key in mydict:\n        ret.append({key_name: key, value_name: mydict[key]})\n    return ret",
        "begin_line": 531,
        "end_line": 541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.list_of_dict_key_value_elements_to_dict#544",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.list_of_dict_key_value_elements_to_dict(mylist, key_name='key', value_name='value')",
        "snippet": "def list_of_dict_key_value_elements_to_dict(mylist, key_name='key', value_name='value'):\n    ''' takes a list of dicts with each having a 'key' and 'value' keys, and transforms the list into a dictionary,\n        effectively as the reverse of dict2items '''\n\n    if not is_sequence(mylist):\n        raise AnsibleFilterError(\"items2dict requires a list, got %s instead.\" % type(mylist))\n\n    return dict((item[key_name], item[value_name]) for item in mylist)",
        "begin_line": 544,
        "end_line": 551,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.path_join#554",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core",
        "signature": "lib.ansible.plugins.filter.core.path_join(paths)",
        "snippet": "def path_join(paths):\n    ''' takes a sequence or a string, and return a concatenation\n        of the different members '''\n    if isinstance(paths, string_types):\n        return os.path.join(paths)\n    elif is_sequence(paths):\n        return os.path.join(*paths)\n    else:\n        raise AnsibleFilterError(\"|path_join expects string or sequence, got %s instead.\" % type(paths))",
        "begin_line": 554,
        "end_line": 562,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.core.FilterModule.filters#568",
        "src_path": "lib/ansible/plugins/filter/core.py",
        "class_name": "lib.ansible.plugins.filter.core.FilterModule",
        "signature": "lib.ansible.plugins.filter.core.FilterModule.filters(self)",
        "snippet": "    def filters(self):\n        return {\n            # jinja2 overrides\n            'groupby': do_groupby,\n\n            # base 64\n            'b64decode': b64decode,\n            'b64encode': b64encode,\n\n            # uuid\n            'to_uuid': to_uuid,\n\n            # json\n            'to_json': to_json,\n            'to_nice_json': to_nice_json,\n            'from_json': json.loads,\n\n            # yaml\n            'to_yaml': to_yaml,\n            'to_nice_yaml': to_nice_yaml,\n            'from_yaml': from_yaml,\n            'from_yaml_all': from_yaml_all,\n\n            # path\n            'basename': partial(unicode_wrap, os.path.basename),\n            'dirname': partial(unicode_wrap, os.path.dirname),\n            'expanduser': partial(unicode_wrap, os.path.expanduser),\n            'expandvars': partial(unicode_wrap, os.path.expandvars),\n            'path_join': path_join,\n            'realpath': partial(unicode_wrap, os.path.realpath),\n            'relpath': partial(unicode_wrap, os.path.relpath),\n            'splitext': partial(unicode_wrap, os.path.splitext),\n            'win_basename': partial(unicode_wrap, ntpath.basename),\n            'win_dirname': partial(unicode_wrap, ntpath.dirname),\n            'win_splitdrive': partial(unicode_wrap, ntpath.splitdrive),\n\n            # file glob\n            'fileglob': fileglob,\n\n            # types\n            'bool': to_bool,\n            'to_datetime': to_datetime,\n\n            # date formatting\n            'strftime': strftime,\n\n            # quote string for shell usage\n            'quote': quote,\n\n            # hash filters\n            # md5 hex digest of string\n            'md5': md5s,\n            # sha1 hex digest of string\n            'sha1': checksum_s,\n            # checksum of string as used by ansible for checksumming files\n            'checksum': checksum_s,\n            # generic hashing\n            'password_hash': get_encrypted_password,\n            'hash': get_hash,\n\n            # regex\n            'regex_replace': regex_replace,\n            'regex_escape': regex_escape,\n            'regex_search': regex_search,\n            'regex_findall': regex_findall,\n\n            # ? : ;\n            'ternary': ternary,\n\n            # random stuff\n            'random': rand,\n            'shuffle': randomize_list,\n\n            # undefined\n            'mandatory': mandatory,\n\n            # comment-style decoration\n            'comment': comment,\n\n            # debug\n            'type_debug': lambda o: o.__class__.__name__,\n\n            # Data structures\n            'combine': combine,\n            'extract': extract,\n            'flatten': flatten,\n            'dict2items': dict_to_list_of_dict_key_value_elements,\n            'items2dict': list_of_dict_key_value_elements_to_dict,\n            'subelements': subelements,\n        }",
        "begin_line": 568,
        "end_line": 657,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006131207847946045,
            "pseudo_dstar_susp": 0.0006131207847946045,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0006131207847946045,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.parsing.plugin_docs.read_docstring#18",
        "src_path": "lib/ansible/parsing/plugin_docs.py",
        "class_name": "lib.ansible.parsing.plugin_docs",
        "signature": "lib.ansible.parsing.plugin_docs.read_docstring(filename, verbose=True, ignore_errors=True)",
        "snippet": "def read_docstring(filename, verbose=True, ignore_errors=True):\n    \"\"\"\n    Search for assignment of the DOCUMENTATION and EXAMPLES variables in the given file.\n    Parse DOCUMENTATION from YAML and return the YAML doc or None together with EXAMPLES, as plain text.\n    \"\"\"\n\n    data = {\n        'doc': None,\n        'plainexamples': None,\n        'returndocs': None,\n        'metadata': None,\n        'seealso': None,\n    }\n\n    string_to_vars = {\n        'DOCUMENTATION': 'doc',\n        'EXAMPLES': 'plainexamples',\n        'RETURN': 'returndocs',\n    }\n\n    try:\n        with open(filename, 'rb') as b_module_data:\n            M = ast.parse(b_module_data.read())\n\n        for child in M.body:\n            if isinstance(child, ast.Assign):\n                for t in child.targets:\n                    try:\n                        theid = t.id\n                    except AttributeError:\n                        # skip errors can happen when trying to use the normal code\n                        display.warning(\"Failed to assign id for %s on %s, skipping\" % (t, filename))\n                        continue\n\n                    if theid in string_to_vars:\n                        varkey = string_to_vars[theid]\n                        if isinstance(child.value, ast.Dict):\n                            data[varkey] = ast.literal_eval(child.value)\n                        else:\n                            if theid == 'DOCUMENTATION':\n                                # string should be yaml\n                                data[varkey] = AnsibleLoader(child.value.s, file_name=filename).get_single_data()\n                            else:\n                                # not yaml, should be a simple string\n                                data[varkey] = to_text(child.value.s)\n                        display.debug('assigned :%s' % varkey)\n\n        # Metadata is per-file and a dict rather than per-plugin/function and yaml\n        data['metadata'] = extract_metadata(module_ast=M)[0]\n\n        if data['metadata']:\n            # remove version\n            for field in ('version', 'metadata_version'):\n                if field in data['metadata']:\n                    del data['metadata'][field]\n\n            if 'supported_by' not in data['metadata']:\n                data['metadata']['supported_by'] = 'community'\n\n            if 'status' not in data['metadata']:\n                data['metadata']['status'] = ['preview']\n\n        else:\n            # Add default metadata\n            data['metadata'] = {'supported_by': 'community',\n                                'status': ['preview']}\n    except Exception:\n        if verbose:\n            display.error(\"unable to parse %s\" % filename)\n        if not ignore_errors:\n            raise\n\n    return data",
        "begin_line": 18,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urls.unicode_urldecode#20",
        "src_path": "lib/ansible/plugins/filter/urls.py",
        "class_name": "lib.ansible.plugins.filter.urls",
        "signature": "lib.ansible.plugins.filter.urls.unicode_urldecode(string)",
        "snippet": "def unicode_urldecode(string):\n    if PY3:\n        return unquote_plus(string)\n    return to_text(unquote_plus(to_bytes(string)))",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urls.do_urldecode#26",
        "src_path": "lib/ansible/plugins/filter/urls.py",
        "class_name": "lib.ansible.plugins.filter.urls",
        "signature": "lib.ansible.plugins.filter.urls.do_urldecode(string)",
        "snippet": "def do_urldecode(string):\n    return unicode_urldecode(string)",
        "begin_line": 26,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urls.unicode_urlencode#31",
        "src_path": "lib/ansible/plugins/filter/urls.py",
        "class_name": "lib.ansible.plugins.filter.urls",
        "signature": "lib.ansible.plugins.filter.urls.unicode_urlencode(string, for_qs=False)",
        "snippet": "def unicode_urlencode(string, for_qs=False):\n    safe = b'' if for_qs else b'/'\n    if for_qs:\n        quote_func = quote_plus\n    else:\n        quote_func = quote\n    if PY3:\n        return quote_func(string, safe)\n    return to_text(quote_func(to_bytes(string), safe))",
        "begin_line": 31,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urls.do_urlencode#42",
        "src_path": "lib/ansible/plugins/filter/urls.py",
        "class_name": "lib.ansible.plugins.filter.urls",
        "signature": "lib.ansible.plugins.filter.urls.do_urlencode(value)",
        "snippet": "def do_urlencode(value):\n    itemiter = None\n    if isinstance(value, dict):\n        itemiter = iteritems(value)\n    elif not isinstance(value, string_types):\n        try:\n            itemiter = iter(value)\n        except TypeError:\n            pass\n    if itemiter is None:\n        return unicode_urlencode(value)\n    return u'&'.join(unicode_urlencode(k) + '=' +\n                     unicode_urlencode(v, for_qs=True)\n                     for k, v in itemiter)",
        "begin_line": 42,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.urls.FilterModule.filters#61",
        "src_path": "lib/ansible/plugins/filter/urls.py",
        "class_name": "lib.ansible.plugins.filter.urls.FilterModule",
        "signature": "lib.ansible.plugins.filter.urls.FilterModule.filters(self)",
        "snippet": "    def filters(self):\n        filters = {\n            'urldecode': do_urldecode,\n        }\n\n        if not HAS_URLENCODE:\n            filters['urlencode'] = do_urlencode\n\n        return filters",
        "begin_line": 61,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006131207847946045,
            "pseudo_dstar_susp": 0.0006131207847946045,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0006131207847946045,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.parsing.utils.addresses.parse_address#170",
        "src_path": "lib/ansible/parsing/utils/addresses.py",
        "class_name": "lib.ansible.parsing.utils.addresses",
        "signature": "lib.ansible.parsing.utils.addresses.parse_address(address, allow_ranges=False)",
        "snippet": "def parse_address(address, allow_ranges=False):\n    \"\"\"\n    Takes a string and returns a (host, port) tuple. If the host is None, then\n    the string could not be parsed as a host identifier with an optional port\n    specification. If the port is None, then no port was specified.\n\n    The host identifier may be a hostname (qualified or not), an IPv4 address,\n    or an IPv6 address. If allow_ranges is True, then any of those may contain\n    [x:y] range specifications, e.g. foo[1:3] or foo[0:5]-bar[x-z].\n\n    The port number is an optional :NN suffix on an IPv4 address or host name,\n    or a mandatory :NN suffix on any square-bracketed expression: IPv6 address,\n    IPv4 address, or host name. (This means the only way to specify a port for\n    an IPv6 address is to enclose it in square brackets.)\n    \"\"\"\n\n    # First, we extract the port number if one is specified.\n\n    port = None\n    for matching in ['bracketed_hostport', 'hostport']:\n        m = patterns[matching].match(address)\n        if m:\n            (address, port) = m.groups()\n            port = int(port)\n            continue\n\n    # What we're left with now must be an IPv4 or IPv6 address, possibly with\n    # numeric ranges, or a hostname with alphanumeric ranges.\n\n    host = None\n    for matching in ['ipv4', 'ipv6', 'hostname']:\n        m = patterns[matching].match(address)\n        if m:\n            host = address\n            continue\n\n    # If it isn't any of the above, we don't understand it.\n    if not host:\n        raise AnsibleError(\"Not a valid network hostname: %s\" % address)\n\n    # If we get to this point, we know that any included ranges are valid.\n    # If the caller is prepared to handle them, all is well.\n    # Otherwise we treat it as a parse failure.\n    if not allow_ranges and '[' in host:\n        raise AnsibleParserError(\"Detected range in host but was asked to ignore ranges\")\n\n    return (host, port)",
        "begin_line": 170,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution._file_exists#30",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution",
        "signature": "lib.ansible.module_utils.facts.system.distribution._file_exists(path, allow_empty=False)",
        "snippet": "def _file_exists(path, allow_empty=False):\n    # not finding the file, exit early\n    if not os.path.exists(path):\n        return False\n\n    # if just the path needs to exists (ie, it can be empty) we are done\n    if allow_empty:\n        return True\n\n    # file exists but is empty and we dont allow_empty\n    if os.path.getsize(path) == 0:\n        return False\n\n    # file exists with some content\n    return True",
        "begin_line": 30,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.__init__#93",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.__init__(self, module)",
        "snippet": "    def __init__(self, module):\n        self.module = module",
        "begin_line": 93,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._get_file_content#96",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._get_file_content(self, path)",
        "snippet": "    def _get_file_content(self, path):\n        return get_file_content(path)",
        "begin_line": 96,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._get_dist_file_content#99",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._get_dist_file_content(self, path, allow_empty=False)",
        "snippet": "    def _get_dist_file_content(self, path, allow_empty=False):\n        # cant find that dist file or it is incorrectly empty\n        if not _file_exists(path, allow_empty=allow_empty):\n            return False, None\n\n        data = self._get_file_content(path)\n        return True, data",
        "begin_line": 99,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._parse_dist_file#107",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._parse_dist_file(self, name, dist_file_content, path, collected_facts)",
        "snippet": "    def _parse_dist_file(self, name, dist_file_content, path, collected_facts):\n        dist_file_dict = {}\n        dist_file_content = dist_file_content.strip(DistributionFiles.STRIP_QUOTES)\n        if name in self.SEARCH_STRING:\n            # look for the distribution string in the data and replace according to RELEASE_NAME_MAP\n            # only the distribution name is set, the version is assumed to be correct from distro.linux_distribution()\n            if self.SEARCH_STRING[name] in dist_file_content:\n                # this sets distribution=RedHat if 'Red Hat' shows up in data\n                dist_file_dict['distribution'] = name\n                dist_file_dict['distribution_file_search_string'] = self.SEARCH_STRING[name]\n            else:\n                # this sets distribution to what's in the data, e.g. CentOS, Scientific, ...\n                dist_file_dict['distribution'] = dist_file_content.split()[0]\n\n            return True, dist_file_dict\n\n        if name in self.OS_RELEASE_ALIAS:\n            if self.OS_RELEASE_ALIAS[name] in dist_file_content:\n                dist_file_dict['distribution'] = name\n                return True, dist_file_dict\n            return False, dist_file_dict\n\n        # call a dedicated function for parsing the file content\n        # TODO: replace with a map or a class\n        try:\n            # FIXME: most of these dont actually look at the dist file contents, but random other stuff\n            distfunc_name = 'parse_distribution_file_' + name\n            distfunc = getattr(self, distfunc_name)\n            parsed, dist_file_dict = distfunc(name, dist_file_content, path, collected_facts)\n            return parsed, dist_file_dict\n        except AttributeError as exc:\n            print('exc: %s' % exc)\n            # this should never happen, but if it does fail quietly and not with a traceback\n            return False, dist_file_dict\n\n        return True, dist_file_dict",
        "begin_line": 107,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._guess_distribution#151",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles._guess_distribution(self)",
        "snippet": "    def _guess_distribution(self):\n        # try to find out which linux distribution this is\n        dist = (get_distribution(), get_distribution_version(), get_distribution_codename())\n        distribution_guess = {\n            'distribution': dist[0] or 'NA',\n            'distribution_version': dist[1] or 'NA',\n            # distribution_release can be the empty string\n            'distribution_release': 'NA' if dist[2] is None else dist[2]\n        }\n\n        distribution_guess['distribution_major_version'] = distribution_guess['distribution_version'].split('.')[0] or 'NA'\n        return distribution_guess",
        "begin_line": 151,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.process_dist_files#164",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.process_dist_files(self)",
        "snippet": "    def process_dist_files(self):\n        # Try to handle the exceptions now ...\n        # self.facts['distribution_debug'] = []\n        dist_file_facts = {}\n\n        dist_guess = self._guess_distribution()\n        dist_file_facts.update(dist_guess)\n\n        for ddict in self.OSDIST_LIST:\n            name = ddict['name']\n            path = ddict['path']\n            allow_empty = ddict.get('allowempty', False)\n\n            has_dist_file, dist_file_content = self._get_dist_file_content(path, allow_empty=allow_empty)\n\n            # but we allow_empty. For example, ArchLinux with an empty /etc/arch-release and a\n            # /etc/os-release with a different name\n            if has_dist_file and allow_empty:\n                dist_file_facts['distribution'] = name\n                dist_file_facts['distribution_file_path'] = path\n                dist_file_facts['distribution_file_variety'] = name\n                break\n\n            if not has_dist_file:\n                # keep looking\n                continue\n\n            parsed_dist_file, parsed_dist_file_facts = self._parse_dist_file(name, dist_file_content, path, dist_file_facts)\n\n            # finally found the right os dist file and were able to parse it\n            if parsed_dist_file:\n                dist_file_facts['distribution'] = name\n                dist_file_facts['distribution_file_path'] = path\n                # distribution and file_variety are the same here, but distribution\n                # will be changed/mapped to a more specific name.\n                # ie, dist=Fedora, file_variety=RedHat\n                dist_file_facts['distribution_file_variety'] = name\n                dist_file_facts['distribution_file_parsed'] = parsed_dist_file\n                dist_file_facts.update(parsed_dist_file_facts)\n                break\n\n        return dist_file_facts",
        "begin_line": 164,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_SUSE#247",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_SUSE(self, name, data, path, collected_facts)",
        "snippet": "    def parse_distribution_file_SUSE(self, name, data, path, collected_facts):\n        suse_facts = {}\n        if 'suse' not in data.lower():\n            return False, suse_facts  # TODO: remove if tested without this\n        if path == '/etc/os-release':\n            for line in data.splitlines():\n                distribution = re.search(\"^NAME=(.*)\", line)\n                if distribution:\n                    suse_facts['distribution'] = distribution.group(1).strip('\"')\n                # example pattern are 13.04 13.0 13\n                distribution_version = re.search(r'^VERSION_ID=\"?([0-9]+\\.?[0-9]*)\"?', line)\n                if distribution_version:\n                    suse_facts['distribution_version'] = distribution_version.group(1)\n                    suse_facts['distribution_major_version'] = distribution_version.group(1).split('.')[0]\n                if 'open' in data.lower():\n                    release = re.search(r'^VERSION_ID=\"?[0-9]+\\.?([0-9]*)\"?', line)\n                    if release:\n                        suse_facts['distribution_release'] = release.groups()[0]\n                elif 'enterprise' in data.lower() and 'VERSION_ID' in line:\n                    # SLES doesn't got funny release names\n                    release = re.search(r'^VERSION_ID=\"?[0-9]+\\.?([0-9]*)\"?', line)\n                    if release.group(1):\n                        release = release.group(1)\n                    else:\n                        release = \"0\"  # no minor number, so it is the first release\n                    suse_facts['distribution_release'] = release\n                # Starting with SLES4SAP12 SP3 NAME reports 'SLES' instead of 'SLES_SAP'\n                # According to SuSe Support (SR101182877871) we should use the CPE_NAME to detect SLES4SAP\n                if re.search(\"^CPE_NAME=.*sles_sap.*$\", line):\n                    suse_facts['distribution'] = 'SLES_SAP'\n        elif path == '/etc/SuSE-release':\n            if 'open' in data.lower():\n                data = data.splitlines()\n                distdata = get_file_content(path).splitlines()[0]\n                suse_facts['distribution'] = distdata.split()[0]\n                for line in data:\n                    release = re.search('CODENAME *= *([^\\n]+)', line)\n                    if release:\n                        suse_facts['distribution_release'] = release.groups()[0].strip()\n            elif 'enterprise' in data.lower():\n                lines = data.splitlines()\n                distribution = lines[0].split()[0]\n                if \"Server\" in data:\n                    suse_facts['distribution'] = \"SLES\"\n                elif \"Desktop\" in data:\n                    suse_facts['distribution'] = \"SLED\"\n                for line in lines:\n                    release = re.search('PATCHLEVEL = ([0-9]+)', line)  # SLES doesn't got funny release names\n                    if release:\n                        suse_facts['distribution_release'] = release.group(1)\n                        suse_facts['distribution_version'] = collected_facts['distribution_version'] + '.' + release.group(1)\n\n        return True, suse_facts",
        "begin_line": 247,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_Debian#301",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_Debian(self, name, data, path, collected_facts)",
        "snippet": "    def parse_distribution_file_Debian(self, name, data, path, collected_facts):\n        debian_facts = {}\n        if 'Debian' in data or 'Raspbian' in data:\n            debian_facts['distribution'] = 'Debian'\n            release = re.search(r\"PRETTY_NAME=[^(]+ \\(?([^)]+?)\\)\", data)\n            if release:\n                debian_facts['distribution_release'] = release.groups()[0]\n\n            # Last resort: try to find release from tzdata as either lsb is missing or this is very old debian\n            if collected_facts['distribution_release'] == 'NA' and 'Debian' in data:\n                dpkg_cmd = self.module.get_bin_path('dpkg')\n                if dpkg_cmd:\n                    cmd = \"%s --status tzdata|grep Provides|cut -f2 -d'-'\" % dpkg_cmd\n                    rc, out, err = self.module.run_command(cmd)\n                    if rc == 0:\n                        debian_facts['distribution_release'] = out.strip()\n        elif 'Ubuntu' in data:\n            debian_facts['distribution'] = 'Ubuntu'\n            # nothing else to do, Ubuntu gets correct info from python functions\n        elif 'SteamOS' in data:\n            debian_facts['distribution'] = 'SteamOS'\n            # nothing else to do, SteamOS gets correct info from python functions\n        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:\n            # Kali does not provide /etc/lsb-release anymore\n            debian_facts['distribution'] = 'Kali'\n            release = re.search('DISTRIB_RELEASE=(.*)', data)\n            if release:\n                debian_facts['distribution_release'] = release.groups()[0]\n        elif 'Devuan' in data:\n            debian_facts['distribution'] = 'Devuan'\n            release = re.search(r\"PRETTY_NAME=\\\"?[^(\\\"]+ \\(?([^) \\\"]+)\\)?\", data)\n            if release:\n                debian_facts['distribution_release'] = release.groups()[0]\n            version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n            if version:\n                debian_facts['distribution_version'] = version.group(1)\n                debian_facts['distribution_major_version'] = version.group(1)\n        elif 'Cumulus' in data:\n            debian_facts['distribution'] = 'Cumulus Linux'\n            version = re.search(r\"VERSION_ID=(.*)\", data)\n            if version:\n                major, _minor, _dummy_ver = version.group(1).split(\".\")\n                debian_facts['distribution_version'] = version.group(1)\n                debian_facts['distribution_major_version'] = major\n\n            release = re.search(r'VERSION=\"(.*)\"', data)\n            if release:\n                debian_facts['distribution_release'] = release.groups()[0]\n        elif \"Mint\" in data:\n            debian_facts['distribution'] = 'Linux Mint'\n            version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n            if version:\n                debian_facts['distribution_version'] = version.group(1)\n                debian_facts['distribution_major_version'] = version.group(1).split('.')[0]\n        else:\n            return False, debian_facts\n\n        return True, debian_facts",
        "begin_line": 301,
        "end_line": 358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_ClearLinux#405",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFiles.parse_distribution_file_ClearLinux(self, name, data, path, collected_facts)",
        "snippet": "    def parse_distribution_file_ClearLinux(self, name, data, path, collected_facts):\n        clear_facts = {}\n        if \"clearlinux\" not in name.lower():\n            return False, clear_facts\n\n        pname = re.search('NAME=\"(.*)\"', data)\n        if pname:\n            if 'Clear Linux' not in pname.groups()[0]:\n                return False, clear_facts\n            clear_facts['distribution'] = pname.groups()[0]\n        version = re.search('VERSION_ID=(.*)', data)\n        if version:\n            clear_facts['distribution_major_version'] = version.groups()[0]\n            clear_facts['distribution_version'] = version.groups()[0]\n        release = re.search('ID=(.*)', data)\n        if release:\n            clear_facts['distribution_release'] = release.groups()[0]\n        return True, clear_facts",
        "begin_line": 405,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.Distribution.__init__#495",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.Distribution",
        "signature": "lib.ansible.module_utils.facts.system.distribution.Distribution.__init__(self, module)",
        "snippet": "    def __init__(self, module):\n        self.module = module",
        "begin_line": 495,
        "end_line": 496,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.Distribution.get_distribution_facts#498",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.Distribution",
        "signature": "lib.ansible.module_utils.facts.system.distribution.Distribution.get_distribution_facts(self)",
        "snippet": "    def get_distribution_facts(self):\n        distribution_facts = {}\n\n        # The platform module provides information about the running\n        # system/distribution. Use this as a baseline and fix buggy systems\n        # afterwards\n        system = platform.system()\n        distribution_facts['distribution'] = system\n        distribution_facts['distribution_release'] = platform.release()\n        distribution_facts['distribution_version'] = platform.version()\n\n        systems_implemented = ('AIX', 'HP-UX', 'Darwin', 'FreeBSD', 'OpenBSD', 'SunOS', 'DragonFly', 'NetBSD')\n\n        if system in systems_implemented:\n            cleanedname = system.replace('-', '')\n            distfunc = getattr(self, 'get_distribution_' + cleanedname)\n            dist_func_facts = distfunc()\n            distribution_facts.update(dist_func_facts)\n        elif system == 'Linux':\n\n            distribution_files = DistributionFiles(module=self.module)\n\n            # linux_distribution_facts = LinuxDistribution(module).get_distribution_facts()\n            dist_file_facts = distribution_files.process_dist_files()\n\n            distribution_facts.update(dist_file_facts)\n\n        distro = distribution_facts['distribution']\n\n        # look for a os family alias for the 'distribution', if there isnt one, use 'distribution'\n        distribution_facts['os_family'] = self.OS_FAMILY.get(distro, None) or distro\n\n        return distribution_facts",
        "begin_line": 498,
        "end_line": 530,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.distribution.DistributionFactCollector.collect#654",
        "src_path": "lib/ansible/module_utils/facts/system/distribution.py",
        "class_name": "lib.ansible.module_utils.facts.system.distribution.DistributionFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.distribution.DistributionFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        collected_facts = collected_facts or {}\n        facts_dict = {}\n        if not module:\n            return facts_dict\n\n        distribution = Distribution(module=module)\n        distro_facts = distribution.get_distribution_facts()\n\n        return distro_facts",
        "begin_line": 654,
        "end_line": 663,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.python.PythonFactCollector.collect#37",
        "src_path": "lib/ansible/module_utils/facts/system/python.py",
        "class_name": "lib.ansible.module_utils.facts.system.python.PythonFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.python.PythonFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        python_facts = {}\n        python_facts['python'] = {\n            'version': {\n                'major': sys.version_info[0],\n                'minor': sys.version_info[1],\n                'micro': sys.version_info[2],\n                'releaselevel': sys.version_info[3],\n                'serial': sys.version_info[4]\n            },\n            'version_info': list(sys.version_info),\n            'executable': sys.executable,\n            'has_sslcontext': HAS_SSLCONTEXT\n        }\n\n        try:\n            python_facts['python']['type'] = sys.subversion[0]\n        except AttributeError:\n            try:\n                python_facts['python']['type'] = sys.implementation.name\n            except AttributeError:\n                python_facts['python']['type'] = None\n\n        return python_facts",
        "begin_line": 37,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.__init__#21",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.__init__(self, raw_version, revision=None, codename=None)",
        "snippet": "    def __init__(self, raw_version, revision=None, codename=None):\n        self._raw_version = raw_version\n        self._revision = revision\n        self._parsed_version = Version(raw_version)\n        self._codename = codename\n        self._parsed_regex_match = re.match(VERSION_PATTERN, raw_version, re.VERBOSE | re.IGNORECASE)",
        "begin_line": 21,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.619047619047618e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.deb_version#29",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.deb_version(self)",
        "snippet": "    def deb_version(self):\n        v = self._parsed_version\n\n        match = self._parsed_regex_match\n\n        # treat dev/post as prerelease for now; treat dev/post as equivalent and disallow together\n        if v.is_prerelease or match.group('dev') or match.group('post'):\n            if match.group('dev') and match.group('post'):\n                raise Exception(\"dev and post may not currently be used together\")\n            if match.group('pre'):\n                tag_value = match.group('pre')\n                tag_type = match.group('pre_l')\n                if match.group('dev'):\n                    tag_value += ('~%s' % match.group('dev').strip('.'))\n                if match.group('post'):\n                    tag_value += ('~%s' % match.group('post').strip('.'))\n            elif match.group('dev'):\n                tag_type = \"dev\"\n                tag_value = match.group('dev').strip('.')\n            elif match.group('post'):\n                tag_type = \"dev\"\n                tag_value = match.group('post').strip('.')\n            else:\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\n        else:\n            tag_type = None\n            tag_value = ''\n\n        # not a pre/post/dev release, just return base version\n        if not tag_type:\n            return '{base_version}'.format(base_version=self.base_version)\n\n        # it is a pre/dev release, include the tag value with a ~\n        return '{base_version}~{tag_value}'.format(base_version=self.base_version, tag_value=tag_value)",
        "begin_line": 29,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.deb_release#65",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.deb_release(self)",
        "snippet": "    def deb_release(self):\n        return '1' if self._revision is None else str(self._revision)",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.rpm_release#69",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.rpm_release(self)",
        "snippet": "    def rpm_release(self):\n        v = self._parsed_version\n        match = self._parsed_regex_match\n\n        # treat presence of dev/post as prerelease for now; treat dev/post the same and disallow together\n        if v.is_prerelease or match.group('dev') or match.group('post'):\n            if match.group('dev') and match.group('post'):\n                raise Exception(\"dev and post may not currently be used together\")\n            if match.group('pre'):\n                tag_value = match.group('pre')\n                tag_type = match.group('pre_l')\n                tag_ver = match.group('pre_n')\n                if match.group('dev'):\n                    tag_value += match.group('dev')\n                if match.group('post'):\n                    tag_value += match.group('post')\n            elif match.group('dev'):\n                tag_type = \"dev\"\n                tag_value = match.group('dev')\n                tag_ver = match.group('dev_n')\n            elif match.group('post'):\n                tag_type = \"dev\"\n                tag_value = match.group('post')\n                tag_ver = match.group('post_n')\n            else:\n                raise Exception(\"unknown prerelease type for version {0}\".format(self._raw_version))\n        else:\n            tag_type = None\n            tag_value = ''\n            tag_ver = 0\n\n        # not a pre/post/dev release, just append revision (default 1)\n        if not tag_type:\n            if self._revision is None:\n                self._revision = 1\n            return '{revision}'.format(revision=self._revision)\n\n        # cleanse tag value in case it starts with .\n        tag_value = tag_value.strip('.')\n\n        # coerce to int and None == 0\n        tag_ver = int(tag_ver if tag_ver else 0)\n\n        if self._revision is None:\n            tag_offset = self.tag_offsets.get(tag_type)\n            if tag_offset is None:\n                raise Exception('no tag offset defined for tag {0}'.format(tag_type))\n            pkgrel = '0.{0}'.format(tag_offset + tag_ver)\n        else:\n            pkgrel = self._revision\n\n        return '{pkgrel}.{tag_value}'.format(pkgrel=pkgrel, tag_value=tag_value)",
        "begin_line": 69,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.raw#123",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.raw(self)",
        "snippet": "    def raw(self):\n        return self._raw_version",
        "begin_line": 123,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.base_version#128",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.base_version(self)",
        "snippet": "    def base_version(self):\n        return self._parsed_version.base_version",
        "begin_line": 128,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.major_version#133",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.major_version(self)",
        "snippet": "    def major_version(self):\n        return re.match(r'^(\\d+.\\d+)', self._raw_version).group(1)",
        "begin_line": 133,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.codename#137",
        "src_path": "packaging/release/versionhelper/version_helper.py",
        "class_name": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger",
        "signature": "packaging.release.versionhelper.version_helper.AnsibleVersionMunger.codename(self)",
        "snippet": "    def codename(self):\n        return self._codename if self._codename else \"UNKNOWN\"",
        "begin_line": 137,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter._decode_escapes#42",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter._decode_escapes(s)",
        "snippet": "def _decode_escapes(s):\n    def decode_match(match):\n        return codecs.decode(match.group(0), 'unicode-escape')\n\n    return _ESCAPE_SEQUENCE_RE.sub(decode_match, s)",
        "begin_line": 42,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.988608568034104e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter.decode_match#43",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter.decode_match(match)",
        "snippet": "    def decode_match(match):\n        return codecs.decode(match.group(0), 'unicode-escape')",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter.parse_kv#49",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter.parse_kv(args, check_raw=False)",
        "snippet": "def parse_kv(args, check_raw=False):\n    '''\n    Convert a string of key/value items to a dict. If any free-form params\n    are found and the check_raw option is set to True, they will be added\n    to a new parameter called '_raw_params'. If check_raw is not enabled,\n    they will simply be ignored.\n    '''\n\n    args = to_text(args, nonstring='passthru')\n\n    options = {}\n    if args is not None:\n        try:\n            vargs = split_args(args)\n        except IndexError as e:\n            raise AnsibleParserError(\"Unable to parse argument string\", orig_exc=e)\n        except ValueError as ve:\n            if 'no closing quotation' in str(ve).lower():\n                raise AnsibleParserError(\"error parsing argument string, try quoting the entire line.\", orig_exc=ve)\n            else:\n                raise\n\n        raw_params = []\n        for orig_x in vargs:\n            x = _decode_escapes(orig_x)\n            if \"=\" in x:\n                pos = 0\n                try:\n                    while True:\n                        pos = x.index('=', pos + 1)\n                        if pos > 0 and x[pos - 1] != '\\\\':\n                            break\n                except ValueError:\n                    # ran out of string, but we must have some escaped equals,\n                    # so replace those and append this to the list of raw params\n                    raw_params.append(x.replace('\\\\=', '='))\n                    continue\n\n                k = x[:pos]\n                v = x[pos + 1:]\n\n                # FIXME: make the retrieval of this list of shell/command\n                #        options a function, so the list is centralized\n                if check_raw and k not in ('creates', 'removes', 'chdir', 'executable', 'warn'):\n                    raw_params.append(orig_x)\n                else:\n                    options[k.strip()] = unquote(v.strip())\n            else:\n                raw_params.append(orig_x)\n\n        # recombine the free-form params, if any were found, and assign\n        # them to a special option for use later by the shell/command module\n        if len(raw_params) > 0:\n            options[u'_raw_params'] = join_args(raw_params)\n\n    return options",
        "begin_line": 49,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.143367383384528e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter._get_quote_state#107",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter._get_quote_state(token, quote_char)",
        "snippet": "def _get_quote_state(token, quote_char):\n    '''\n    the goal of this block is to determine if the quoted string\n    is unterminated in which case it needs to be put back together\n    '''\n    # the char before the current one, used to see if\n    # the current character is escaped\n    prev_char = None\n    for idx, cur_char in enumerate(token):\n        if idx > 0:\n            prev_char = token[idx - 1]\n        if cur_char in '\"\\'' and prev_char != '\\\\':\n            if quote_char:\n                if cur_char == quote_char:\n                    quote_char = None\n            else:\n                quote_char = cur_char\n    return quote_char",
        "begin_line": 107,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.126060001425212e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter._count_jinja2_blocks#127",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter._count_jinja2_blocks(token, cur_depth, open_token, close_token)",
        "snippet": "def _count_jinja2_blocks(token, cur_depth, open_token, close_token):\n    '''\n    this function counts the number of opening/closing blocks for a\n    given opening/closing type and adjusts the current depth for that\n    block based on the difference\n    '''\n    num_open = token.count(open_token)\n    num_close = token.count(close_token)\n    if num_open != num_close:\n        cur_depth += (num_open - num_close)\n        if cur_depth < 0:\n            cur_depth = 0\n    return cur_depth",
        "begin_line": 127,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter.join_args#142",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter.join_args(s)",
        "snippet": "def join_args(s):\n    '''\n    Join the original cmd based on manipulations by split_args().\n    This retains the original newlines and whitespaces.\n    '''\n    result = ''\n    for p in s:\n        if len(result) == 0 or result.endswith('\\n'):\n            result += p\n        else:\n            result += ' ' + p\n    return result",
        "begin_line": 142,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.splitter.split_args#156",
        "src_path": "lib/ansible/parsing/splitter.py",
        "class_name": "lib.ansible.parsing.splitter",
        "signature": "lib.ansible.parsing.splitter.split_args(args)",
        "snippet": "def split_args(args):\n    '''\n    Splits args on whitespace, but intelligently reassembles\n    those that may have been split over a jinja2 block or quotes.\n\n    When used in a remote module, we won't ever have to be concerned about\n    jinja2 blocks, however this function is/will be used in the\n    core portions as well before the args are templated.\n\n    example input: a=b c=\"foo bar\"\n    example output: ['a=b', 'c=\"foo bar\"']\n\n    Basically this is a variation shlex that has some more intelligence for\n    how Ansible needs to use it.\n    '''\n\n    # the list of params parsed out of the arg string\n    # this is going to be the result value when we are done\n    params = []\n\n    # Initial split on newlines\n    items = args.split('\\n')\n\n    # iterate over the tokens, and reassemble any that may have been\n    # split on a space inside a jinja2 block.\n    # ex if tokens are \"{{\", \"foo\", \"}}\" these go together\n\n    # These variables are used\n    # to keep track of the state of the parsing, since blocks and quotes\n    # may be nested within each other.\n\n    quote_char = None\n    inside_quotes = False\n    print_depth = 0  # used to count nested jinja2 {{ }} blocks\n    block_depth = 0  # used to count nested jinja2 {% %} blocks\n    comment_depth = 0  # used to count nested jinja2 {# #} blocks\n\n    # now we loop over each split chunk, coalescing tokens if the white space\n    # split occurred within quotes or a jinja2 block of some kind\n    for (itemidx, item) in enumerate(items):\n\n        # we split on spaces and newlines separately, so that we\n        # can tell which character we split on for reassembly\n        # inside quotation characters\n        tokens = item.split(' ')\n\n        line_continuation = False\n        for (idx, token) in enumerate(tokens):\n\n            # Empty entries means we have subsequent spaces\n            # We want to hold onto them so we can reconstruct them later\n            if len(token) == 0 and idx != 0:\n                params[-1] += ' '\n                continue\n\n            # if we hit a line continuation character, but\n            # we're not inside quotes, ignore it and continue\n            # on to the next token while setting a flag\n            if token == '\\\\' and not inside_quotes:\n                line_continuation = True\n                continue\n\n            # store the previous quoting state for checking later\n            was_inside_quotes = inside_quotes\n            quote_char = _get_quote_state(token, quote_char)\n            inside_quotes = quote_char is not None\n\n            # multiple conditions may append a token to the list of params,\n            # so we keep track with this flag to make sure it only happens once\n            # append means add to the end of the list, don't append means concatenate\n            # it to the end of the last token\n            appended = False\n\n            # if we're inside quotes now, but weren't before, append the token\n            # to the end of the list, since we'll tack on more to it later\n            # otherwise, if we're inside any jinja2 block, inside quotes, or we were\n            # inside quotes (but aren't now) concat this token to the last param\n            if inside_quotes and not was_inside_quotes and not(print_depth or block_depth or comment_depth):\n                params.append(token)\n                appended = True\n            elif print_depth or block_depth or comment_depth or inside_quotes or was_inside_quotes:\n                if idx == 0 and was_inside_quotes:\n                    params[-1] = \"%s%s\" % (params[-1], token)\n                elif len(tokens) > 1:\n                    spacer = ''\n                    if idx > 0:\n                        spacer = ' '\n                    params[-1] = \"%s%s%s\" % (params[-1], spacer, token)\n                else:\n                    params[-1] = \"%s\\n%s\" % (params[-1], token)\n                appended = True\n\n            # if the number of paired block tags is not the same, the depth has changed, so we calculate that here\n            # and may append the current token to the params (if we haven't previously done so)\n            prev_print_depth = print_depth\n            print_depth = _count_jinja2_blocks(token, print_depth, \"{{\", \"}}\")\n            if print_depth != prev_print_depth and not appended:\n                params.append(token)\n                appended = True\n\n            prev_block_depth = block_depth\n            block_depth = _count_jinja2_blocks(token, block_depth, \"{%\", \"%}\")\n            if block_depth != prev_block_depth and not appended:\n                params.append(token)\n                appended = True\n\n            prev_comment_depth = comment_depth\n            comment_depth = _count_jinja2_blocks(token, comment_depth, \"{#\", \"#}\")\n            if comment_depth != prev_comment_depth and not appended:\n                params.append(token)\n                appended = True\n\n            # finally, if we're at zero depth for all blocks and not inside quotes, and have not\n            # yet appended anything to the list of params, we do so now\n            if not (print_depth or block_depth or comment_depth) and not inside_quotes and not appended and token != '':\n                params.append(token)\n\n        # if this was the last token in the list, and we have more than\n        # one item (meaning we split on newlines), add a newline back here\n        # to preserve the original structure\n        if len(items) > 1 and itemidx != len(items) - 1 and not line_continuation:\n            params[-1] += '\\n'\n\n        # always clear the line continuation flag\n        line_continuation = False\n\n    # If we're done and things are not at zero depth or we're still inside quotes,\n    # raise an error to indicate that the args were unbalanced\n    if print_depth or block_depth or comment_depth or inside_quotes:\n        raise AnsibleParserError(u\"failed at splitting arguments, either an unbalanced jinja2 block or quotes: {0}\".format(args))\n\n    return params",
        "begin_line": 156,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.get_all_plugin_loaders#39",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader",
        "signature": "lib.ansible.plugins.loader.get_all_plugin_loaders()",
        "snippet": "def get_all_plugin_loaders():\n    return [(name, obj) for (name, obj) in globals().items() if isinstance(obj, PluginLoader)]",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.add_all_plugin_dirs#43",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader",
        "signature": "lib.ansible.plugins.loader.add_all_plugin_dirs(path)",
        "snippet": "def add_all_plugin_dirs(path):\n    ''' add any existing plugin dirs in the path provided '''\n    b_path = to_bytes(path, errors='surrogate_or_strict')\n    if os.path.isdir(b_path):\n        for name, obj in get_all_plugin_loaders():\n            if obj.subdir:\n                plugin_path = os.path.join(b_path, to_bytes(obj.subdir))\n                if os.path.isdir(plugin_path):\n                    obj.add_directory(to_text(plugin_path))\n    else:\n        display.warning(\"Ignoring invalid path provided to plugin path: '%s' is not a directory\" % to_text(path))",
        "begin_line": 43,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.get_shell_plugin#56",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader",
        "signature": "lib.ansible.plugins.loader.get_shell_plugin(shell_type=None, executable=None)",
        "snippet": "def get_shell_plugin(shell_type=None, executable=None):\n\n    if not shell_type:\n        # default to sh\n        shell_type = 'sh'\n\n        # mostly for backwards compat\n        if executable:\n            if isinstance(executable, string_types):\n                shell_filename = os.path.basename(executable)\n                try:\n                    shell = shell_loader.get(shell_filename)\n                except Exception:\n                    shell = None\n\n                if shell is None:\n                    for shell in shell_loader.all():\n                        if shell_filename in shell.COMPATIBLE_SHELLS:\n                            shell_type = shell.SHELL_FAMILY\n                            break\n        else:\n            raise AnsibleError(\"Either a shell type or a shell executable must be provided \")\n\n    shell = shell_loader.get(shell_type)\n    if not shell:\n        raise AnsibleError(\"Could not find the shell plugin required (%s).\" % shell_type)\n\n    if executable:\n        setattr(shell, 'executable', executable)\n\n    return shell",
        "begin_line": 56,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.__init__#104",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.__init__(self, class_name, package, config, subdir, aliases=None, required_base_class=None)",
        "snippet": "    def __init__(self, class_name, package, config, subdir, aliases=None, required_base_class=None):\n        aliases = {} if aliases is None else aliases\n\n        self.class_name = class_name\n        self.base_class = required_base_class\n        self.package = package\n        self.subdir = subdir\n\n        # FIXME: remove alias dict in favor of alias by symlink?\n        self.aliases = aliases\n\n        if config and not isinstance(config, list):\n            config = [config]\n        elif not config:\n            config = []\n\n        self.config = config\n\n        if class_name not in MODULE_CACHE:\n            MODULE_CACHE[class_name] = {}\n        if class_name not in PATH_CACHE:\n            PATH_CACHE[class_name] = None\n        if class_name not in PLUGIN_PATH_CACHE:\n            PLUGIN_PATH_CACHE[class_name] = defaultdict(dict)\n\n        # hold dirs added at runtime outside of config\n        self._extra_dirs = []\n\n        # caches\n        self._module_cache = MODULE_CACHE[class_name]\n        self._paths = PATH_CACHE[class_name]\n        self._plugin_path_cache = PLUGIN_PATH_CACHE[class_name]\n\n        self._searched_paths = set()",
        "begin_line": 104,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.format_paths#192",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.format_paths(self, paths)",
        "snippet": "    def format_paths(self, paths):\n        ''' Returns a string suitable for printing of the search path '''\n\n        # Uses a list to get the order right\n        ret = []\n        for i in paths:\n            if i not in ret:\n                ret.append(i)\n        return os.pathsep.join(ret)",
        "begin_line": 192,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.print_paths#202",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.print_paths(self)",
        "snippet": "    def print_paths(self):\n        return self.format_paths(self._get_paths(subdirs=False))",
        "begin_line": 202,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._all_directories#205",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._all_directories(self, dir)",
        "snippet": "    def _all_directories(self, dir):\n        results = []\n        results.append(dir)\n        for root, subdirs, files in os.walk(dir, followlinks=True):\n            if '__init__.py' in files:\n                for x in subdirs:\n                    results.append(os.path.join(root, x))\n        return results",
        "begin_line": 205,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._get_package_paths#214",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._get_package_paths(self, subdirs=True)",
        "snippet": "    def _get_package_paths(self, subdirs=True):\n        ''' Gets the path of a Python package '''\n\n        if not self.package:\n            return []\n        if not hasattr(self, 'package_path'):\n            m = __import__(self.package)\n            parts = self.package.split('.')[1:]\n            for parent_mod in parts:\n                m = getattr(m, parent_mod)\n            self.package_path = os.path.dirname(m.__file__)\n        if subdirs:\n            return self._all_directories(self.package_path)\n        return [self.package_path]",
        "begin_line": 214,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._get_paths#229",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._get_paths(self, subdirs=True)",
        "snippet": "    def _get_paths(self, subdirs=True):\n        ''' Return a list of paths to search for plugins in '''\n\n        # FIXME: This is potentially buggy if subdirs is sometimes True and sometimes False.\n        # In current usage, everything calls this with subdirs=True except for module_utils_loader and ansible-doc\n        # which always calls it with subdirs=False. So there currently isn't a problem with this caching.\n        if self._paths is not None:\n            return self._paths\n\n        ret = self._extra_dirs[:]\n\n        # look in any configured plugin paths, allow one level deep for subcategories\n        if self.config is not None:\n            for path in self.config:\n                path = os.path.realpath(os.path.expanduser(path))\n                if subdirs:\n                    contents = glob.glob(\"%s/*\" % path) + glob.glob(\"%s/*/*\" % path)\n                    for c in contents:\n                        if os.path.isdir(c) and c not in ret:\n                            ret.append(c)\n                if path not in ret:\n                    ret.append(path)\n\n        # look for any plugins installed in the package subtree\n        # Note package path always gets added last so that every other type of\n        # path is searched before it.\n        ret.extend(self._get_package_paths(subdirs=subdirs))\n\n        # HACK: because powershell modules are in the same directory\n        # hierarchy as other modules we have to process them last.  This is\n        # because powershell only works on windows but the other modules work\n        # anywhere (possibly including windows if the correct language\n        # interpreter is installed).  the non-powershell modules can have any\n        # file extension and thus powershell modules are picked up in that.\n        # The non-hack way to fix this is to have powershell modules be\n        # a different PluginLoader/ModuleLoader.  But that requires changing\n        # other things too (known thing to change would be PATHS_CACHE,\n        # PLUGIN_PATHS_CACHE, and MODULE_CACHE.  Since those three dicts key\n        # on the class_name and neither regular modules nor powershell modules\n        # would have class_names, they would not work as written.\n        #\n        # The expected sort order is paths in the order in 'ret' with paths ending in '/windows' at the end,\n        # also in the original order they were found in 'ret'.\n        # The .sort() method is guaranteed to be stable, so original order is preserved.\n        ret.sort(key=lambda p: p.endswith('/windows'))\n\n        # cache and return the result\n        self._paths = ret\n        return ret",
        "begin_line": 229,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005698005698005698,
            "pseudo_dstar_susp": 0.0005698005698005698,
            "pseudo_tarantula_susp": 0.0005701254275940707,
            "pseudo_op2_susp": 0.0005698005698005698,
            "pseudo_barinel_susp": 0.0005701254275940707
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._load_config_defs#279",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._load_config_defs(self, name, module, path)",
        "snippet": "    def _load_config_defs(self, name, module, path):\n        ''' Reads plugin docs to find configuration setting definitions, to push to config manager for later use '''\n\n        # plugins w/o class name don't support config\n        if self.class_name:\n            type_name = get_plugin_class(self.class_name)\n\n            # if type name != 'module_doc_fragment':\n            if type_name in C.CONFIGURABLE_PLUGINS:\n                dstring = AnsibleLoader(getattr(module, 'DOCUMENTATION', ''), file_name=path).get_single_data()\n                if dstring:\n                    add_fragments(dstring, path, fragment_loader=fragment_loader)\n\n                if dstring and 'options' in dstring and isinstance(dstring['options'], dict):\n                    C.config.initialize_plugin_configuration_definitions(type_name, name, dstring['options'])\n                    display.debug('Loaded config def from plugin (%s/%s)' % (type_name, name))",
        "begin_line": 279,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.find_plugin#363",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.find_plugin(self, name, mod_type='', ignore_deprecated=False, check_aliases=False, collection_list=None)",
        "snippet": "    def find_plugin(self, name, mod_type='', ignore_deprecated=False, check_aliases=False, collection_list=None):\n        ''' Find a plugin named name '''\n        return self.find_plugin_with_name(name, mod_type, ignore_deprecated, check_aliases, collection_list)[1]",
        "begin_line": 363,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.21240533717995e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.find_plugin_with_name#367",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.find_plugin_with_name(self, name, mod_type='', ignore_deprecated=False, check_aliases=False, collection_list=None)",
        "snippet": "    def find_plugin_with_name(self, name, mod_type='', ignore_deprecated=False, check_aliases=False, collection_list=None):\n        ''' Find a plugin named name '''\n\n        global _PLUGIN_FILTERS\n        if name in _PLUGIN_FILTERS[self.package]:\n            return None, None\n\n        if mod_type:\n            suffix = mod_type\n        elif self.class_name:\n            # Ansible plugins that run in the controller process (most plugins)\n            suffix = '.py'\n        else:\n            # Only Ansible Modules.  Ansible modules can be any executable so\n            # they can have any suffix\n            suffix = ''\n\n        # FIXME: need this right now so we can still load shipped PS module_utils- come up with a more robust solution\n        if (AnsibleCollectionRef.is_valid_fqcr(name) or collection_list) and not name.startswith('Ansible'):\n            if '.' in name or not collection_list:\n                candidates = [name]\n            else:\n                candidates = ['{0}.{1}'.format(c, name) for c in collection_list]\n            # TODO: keep actual errors, not just assembled messages\n            errors = []\n            for candidate_name in candidates:\n                try:\n                    # HACK: refactor this properly\n                    if candidate_name.startswith('ansible.legacy'):\n                        # 'ansible.legacy' refers to the plugin finding behavior used before collections existed.\n                        # They need to search 'library' and the various '*_plugins' directories in order to find the file.\n                        full_name = name\n                        p = self._find_plugin_legacy(name.replace('ansible.legacy.', '', 1), ignore_deprecated, check_aliases, suffix)\n                    else:\n                        # 'ansible.builtin' should be handled here. This means only internal, or builtin, paths are searched.\n                        full_name, p = self._find_fq_plugin(candidate_name, suffix)\n                    if p:\n                        return full_name, p\n                except Exception as ex:\n                    errors.append(to_native(ex))\n\n            if errors:\n                display.debug(msg='plugin lookup for {0} failed; errors: {1}'.format(name, '; '.join(errors)))\n\n            return None, None\n\n        # if we got here, there's no collection list and it's not an FQ name, so do legacy lookup\n\n        return name, self._find_plugin_legacy(name, ignore_deprecated, check_aliases, suffix)",
        "begin_line": 367,
        "end_line": 415,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._find_plugin_legacy#417",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._find_plugin_legacy(self, name, ignore_deprecated=False, check_aliases=False, suffix=None)",
        "snippet": "    def _find_plugin_legacy(self, name, ignore_deprecated=False, check_aliases=False, suffix=None):\n        \"\"\"Search library and various *_plugins paths in order to find the file.\n        This was behavior prior to the existence of collections.\n        \"\"\"\n\n        if check_aliases:\n            name = self.aliases.get(name, name)\n\n        # The particular cache to look for modules within.  This matches the\n        # requested mod_type\n        pull_cache = self._plugin_path_cache[suffix]\n        try:\n            return pull_cache[name]\n        except KeyError:\n            # Cache miss.  Now let's find the plugin\n            pass\n\n        # TODO: Instead of using the self._paths cache (PATH_CACHE) and\n        #       self._searched_paths we could use an iterator.  Before enabling that\n        #       we need to make sure we don't want to add additional directories\n        #       (add_directory()) once we start using the iterator.\n        #       We can use _get_paths() since add_directory() forces a cache refresh.\n        for path in (p for p in self._get_paths() if p not in self._searched_paths and os.path.isdir(p)):\n            display.debug('trying %s' % path)\n            try:\n                full_paths = (os.path.join(path, f) for f in os.listdir(path))\n            except OSError as e:\n                display.warning(\"Error accessing plugin paths: %s\" % to_text(e))\n\n            for full_path in (f for f in full_paths if os.path.isfile(f) and not f.endswith('__init__.py')):\n                full_name = os.path.basename(full_path)\n\n                # HACK: We have no way of executing python byte compiled files as ansible modules so specifically exclude them\n                # FIXME: I believe this is only correct for modules and module_utils.\n                # For all other plugins we want .pyc and .pyo should be valid\n                if any(full_path.endswith(x) for x in C.MODULE_IGNORE_EXTS):\n                    continue\n\n                splitname = os.path.splitext(full_name)\n                base_name = splitname[0]\n                try:\n                    extension = splitname[1]\n                except IndexError:\n                    extension = ''\n\n                # Module found, now enter it into the caches that match this file\n                if base_name not in self._plugin_path_cache['']:\n                    self._plugin_path_cache[''][base_name] = full_path\n\n                if full_name not in self._plugin_path_cache['']:\n                    self._plugin_path_cache[''][full_name] = full_path\n\n                if base_name not in self._plugin_path_cache[extension]:\n                    self._plugin_path_cache[extension][base_name] = full_path\n\n                if full_name not in self._plugin_path_cache[extension]:\n                    self._plugin_path_cache[extension][full_name] = full_path\n\n            self._searched_paths.add(path)\n            try:\n                return pull_cache[name]\n            except KeyError:\n                # Didn't find the plugin in this directory. Load modules from the next one\n                pass\n\n        # if nothing is found, try finding alias/deprecated\n        if not name.startswith('_'):\n            alias_name = '_' + name\n            # We've already cached all the paths at this point\n            if alias_name in pull_cache:\n                if not ignore_deprecated and not os.path.islink(pull_cache[alias_name]):\n                    # FIXME: this is not always the case, some are just aliases\n                    display.deprecated('%s is kept for backwards compatibility but usage is discouraged. '  # pylint: disable=ansible-deprecated-no-version\n                                       'The module documentation details page may explain more about this rationale.' % name.lstrip('_'))\n                return pull_cache[alias_name]\n\n        return None",
        "begin_line": 417,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.has_plugin#495",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.has_plugin(self, name, collection_list=None)",
        "snippet": "    def has_plugin(self, name, collection_list=None):\n        ''' Checks if a plugin named name exists '''\n\n        try:\n            return self.find_plugin(name, collection_list=collection_list) is not None\n        except Exception as ex:\n            if isinstance(ex, AnsibleError):\n                raise\n            # log and continue, likely an innocuous type/package loading failure in collections import\n            display.debug('has_plugin error: {0}'.format(to_text(ex)))",
        "begin_line": 495,
        "end_line": 504,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.306202966318405e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._load_module_source#508",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._load_module_source(self, name, path)",
        "snippet": "    def _load_module_source(self, name, path):\n\n        # avoid collisions across plugins\n        if name.startswith('ansible_collections.'):\n            full_name = name\n        else:\n            full_name = '.'.join([self.package, name])\n\n        if full_name in sys.modules:\n            # Avoids double loading, See https://github.com/ansible/ansible/issues/13110\n            return sys.modules[full_name]\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            if imp is None:\n                spec = importlib.util.spec_from_file_location(to_native(full_name), to_native(path))\n                module = importlib.util.module_from_spec(spec)\n                spec.loader.exec_module(module)\n                sys.modules[full_name] = module\n            else:\n                with open(to_bytes(path), 'rb') as module_file:\n                    # to_native is used here because imp.load_source's path is for tracebacks and python's traceback formatting uses native strings\n                    module = imp.load_source(to_native(full_name), to_native(path), module_file)\n        return module",
        "begin_line": 508,
        "end_line": 531,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._update_object#533",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._update_object(self, obj, name, path)",
        "snippet": "    def _update_object(self, obj, name, path):\n\n        # set extra info on the module, in case we want it later\n        setattr(obj, '_original_path', path)\n        setattr(obj, '_load_name', name)",
        "begin_line": 533,
        "end_line": 537,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005571030640668524,
            "pseudo_dstar_susp": 0.0005571030640668524,
            "pseudo_tarantula_susp": 0.0005571030640668524,
            "pseudo_op2_susp": 0.0005571030640668524,
            "pseudo_barinel_susp": 0.0005571030640668524
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.get#539",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.get(self, name, *args, **kwargs)",
        "snippet": "    def get(self, name, *args, **kwargs):\n        ''' instantiates a plugin of the given name using arguments '''\n\n        found_in_cache = True\n        class_only = kwargs.pop('class_only', False)\n        collection_list = kwargs.pop('collection_list', None)\n        if name in self.aliases:\n            name = self.aliases[name]\n        name, path = self.find_plugin_with_name(name, collection_list=collection_list)\n        if path is None:\n            return None\n\n        if path not in self._module_cache:\n            self._module_cache[path] = self._load_module_source(name, path)\n            self._load_config_defs(name, self._module_cache[path], path)\n            found_in_cache = False\n\n        obj = getattr(self._module_cache[path], self.class_name)\n        if self.base_class:\n            # The import path is hardcoded and should be the right place,\n            # so we are not expecting an ImportError.\n            module = __import__(self.package, fromlist=[self.base_class])\n            # Check whether this obj has the required base class.\n            try:\n                plugin_class = getattr(module, self.base_class)\n            except AttributeError:\n                return None\n            if not issubclass(obj, plugin_class):\n                return None\n\n        self._display_plugin_load(self.class_name, name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n\n        if not class_only:\n            try:\n                # A plugin may need to use its _load_name in __init__ (for example, to set\n                # or get options from config), so update the object before using the constructor\n                instance = object.__new__(obj)\n                self._update_object(instance, name, path)\n                obj.__init__(instance, *args, **kwargs)\n                obj = instance\n            except TypeError as e:\n                if \"abstract\" in e.args[0]:\n                    # Abstract Base Class.  The found plugin file does not\n                    # fully implement the defined interface.\n                    return None\n                raise\n\n        self._update_object(obj, name, path)\n        return obj",
        "begin_line": 539,
        "end_line": 587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader._display_plugin_load#589",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader._display_plugin_load(self, class_name, name, searched_paths, path, found_in_cache=None, class_only=None)",
        "snippet": "    def _display_plugin_load(self, class_name, name, searched_paths, path, found_in_cache=None, class_only=None):\n        ''' formats data to display debug info for plugin loading, also avoids processing unless really needed '''\n        if C.DEFAULT_DEBUG:\n            msg = 'Loading %s \\'%s\\' from %s' % (class_name, os.path.basename(name), path)\n\n            if len(searched_paths) > 1:\n                msg = '%s (searched paths: %s)' % (msg, self.format_paths(searched_paths))\n\n            if found_in_cache or class_only:\n                msg = '%s (found_in_cache=%s, class_only=%s)' % (msg, found_in_cache, class_only)\n\n            display.debug(msg)",
        "begin_line": 589,
        "end_line": 600,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005571030640668524,
            "pseudo_dstar_susp": 0.0005571030640668524,
            "pseudo_tarantula_susp": 0.0005571030640668524,
            "pseudo_op2_susp": 0.0005571030640668524,
            "pseudo_barinel_susp": 0.0005571030640668524
        }
    },
    {
        "name": "lib.ansible.plugins.loader.PluginLoader.all#602",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.PluginLoader",
        "signature": "lib.ansible.plugins.loader.PluginLoader.all(self, *args, **kwargs)",
        "snippet": "    def all(self, *args, **kwargs):\n        '''\n        Iterate through all plugins of this type\n\n        A plugin loader is initialized with a specific type.  This function is an iterator returning\n        all of the plugins of that type to the caller.\n\n        :kwarg path_only: If this is set to True, then we return the paths to where the plugins reside\n            instead of an instance of the plugin.  This conflicts with class_only and both should\n            not be set.\n        :kwarg class_only: If this is set to True then we return the python class which implements\n            a plugin rather than an instance of the plugin.  This conflicts with path_only and both\n            should not be set.\n        :kwarg _dedupe: By default, we only return one plugin per plugin name.  Deduplication happens\n            in the same way as the :meth:`get` and :meth:`find_plugin` methods resolve which plugin\n            should take precedence.  If this is set to False, then we return all of the plugins\n            found, including those with duplicate names.  In the case of duplicates, the order in\n            which they are returned is the one that would take precedence first, followed by the\n            others  in decreasing precedence order.  This should only be used by subclasses which\n            want to manage their own deduplication of the plugins.\n        :*args: Any extra arguments are passed to each plugin when it is instantiated.\n        :**kwargs: Any extra keyword arguments are passed to each plugin when it is instantiated.\n        '''\n        # TODO: Change the signature of this method to:\n        # def all(return_type='instance', args=None, kwargs=None):\n        #     if args is None: args = []\n        #     if kwargs is None: kwargs = {}\n        #     return_type can be instance, class, or path.\n        #     These changes will mean that plugin parameters won't conflict with our params and\n        #     will also make it impossible to request both a path and a class at the same time.\n        #\n        #     Move _dedupe to be a class attribute, CUSTOM_DEDUPE, with subclasses for filters and\n        #     tests setting it to True\n\n        global _PLUGIN_FILTERS\n\n        dedupe = kwargs.pop('_dedupe', True)\n        path_only = kwargs.pop('path_only', False)\n        class_only = kwargs.pop('class_only', False)\n        # Having both path_only and class_only is a coding bug\n        if path_only and class_only:\n            raise AnsibleError('Do not set both path_only and class_only when calling PluginLoader.all()')\n\n        all_matches = []\n        found_in_cache = True\n\n        for i in self._get_paths():\n            all_matches.extend(glob.glob(os.path.join(i, \"*.py\")))\n\n        loaded_modules = set()\n        for path in sorted(all_matches, key=os.path.basename):\n            name = os.path.splitext(path)[0]\n            basename = os.path.basename(name)\n\n            if basename == '__init__' or basename in _PLUGIN_FILTERS[self.package]:\n                continue\n\n            if dedupe and basename in loaded_modules:\n                continue\n            loaded_modules.add(basename)\n\n            if path_only:\n                yield path\n                continue\n\n            if path not in self._module_cache:\n                try:\n                    if self.subdir in ('filter_plugins', 'test_plugins'):\n                        # filter and test plugin files can contain multiple plugins\n                        # they must have a unique python module name to prevent them from shadowing each other\n                        full_name = '{0}_{1}'.format(abs(hash(path)), basename)\n                    else:\n                        full_name = basename\n                    module = self._load_module_source(full_name, path)\n                    self._load_config_defs(basename, module, path)\n                except Exception as e:\n                    display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n                    continue\n                self._module_cache[path] = module\n                found_in_cache = False\n\n            try:\n                obj = getattr(self._module_cache[path], self.class_name)\n            except AttributeError as e:\n                display.warning(\"Skipping plugin (%s) as it seems to be invalid: %s\" % (path, to_text(e)))\n                continue\n\n            if self.base_class:\n                # The import path is hardcoded and should be the right place,\n                # so we are not expecting an ImportError.\n                module = __import__(self.package, fromlist=[self.base_class])\n                # Check whether this obj has the required base class.\n                try:\n                    plugin_class = getattr(module, self.base_class)\n                except AttributeError:\n                    continue\n                if not issubclass(obj, plugin_class):\n                    continue\n\n            self._display_plugin_load(self.class_name, basename, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n\n            if not class_only:\n                try:\n                    obj = obj(*args, **kwargs)\n                except TypeError as e:\n                    display.warning(\"Skipping plugin (%s) as it seems to be incomplete: %s\" % (path, to_text(e)))\n\n            self._update_object(obj, basename, path)\n            yield obj",
        "begin_line": 602,
        "end_line": 710,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000580046403712297,
            "pseudo_dstar_susp": 0.000580046403712297,
            "pseudo_tarantula_susp": 0.0005803830528148578,
            "pseudo_op2_susp": 0.000580046403712297,
            "pseudo_barinel_susp": 0.0005803830528148578
        }
    },
    {
        "name": "lib.ansible.plugins.loader.Jinja2Loader.all#736",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader.Jinja2Loader",
        "signature": "lib.ansible.plugins.loader.Jinja2Loader.all(self, *args, **kwargs)",
        "snippet": "    def all(self, *args, **kwargs):\n        \"\"\"\n        Differences with :meth:`PluginLoader.all`:\n\n        * We do not deduplicate ansible plugin names.  This is because we don't care about our\n          plugin names, here.  We care about the names of the actual jinja2 plugins which are inside\n          of our plugins.\n        * We reverse the order of the list of plugins compared to other PluginLoaders.  This is\n          because of how calling code chooses to sync the plugins from the list.  It adds all the\n          Jinja2 plugins from one of our Ansible plugins into a dict.  Then it adds the Jinja2\n          plugins from the next Ansible plugin, overwriting any Jinja2 plugins that had the same\n          name.  This is an encapsulation violation (the PluginLoader should not know about what\n          calling code does with the data) but we're pushing the common code here.  We'll fix\n          this in the future by moving more of the common code into this PluginLoader.\n        * We return a list.  We could iterate the list instead but that's extra work for no gain because\n          the API receiving this doesn't care.  It just needs an iterable\n        \"\"\"\n        # We don't deduplicate ansible plugin names.  Instead, calling code deduplicates jinja2\n        # plugin names.\n        kwargs['_dedupe'] = False\n\n        # We have to instantiate a list of all plugins so that we can reverse it.  We reverse it so\n        # that calling code will deduplicate this correctly.\n        plugins = [p for p in super(Jinja2Loader, self).all(*args, **kwargs)]\n        plugins.reverse()\n\n        return plugins",
        "begin_line": 736,
        "end_line": 762,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007363770250368188,
            "pseudo_dstar_susp": 0.0012224938875305623,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0012224938875305623,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.plugins.loader._configure_collection_loader#821",
        "src_path": "lib/ansible/plugins/loader.py",
        "class_name": "lib.ansible.plugins.loader",
        "signature": "lib.ansible.plugins.loader._configure_collection_loader()",
        "snippet": "def _configure_collection_loader():\n    if not any((isinstance(l, AnsibleCollectionLoader) for l in sys.meta_path)):\n        sys.meta_path.insert(0, AnsibleCollectionLoader(C.config))",
        "begin_line": 821,
        "end_line": 823,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005235602094240838,
            "pseudo_dstar_susp": 0.0013123359580052493,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0013123359580052493,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.virtual.base.Virtual.__init__#39",
        "src_path": "lib/ansible/module_utils/facts/virtual/base.py",
        "class_name": "lib.ansible.module_utils.facts.virtual.base.Virtual",
        "signature": "lib.ansible.module_utils.facts.virtual.base.Virtual.__init__(self, module, load_on_init=False)",
        "snippet": "    def __init__(self, module, load_on_init=False):\n        self.module = module",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.784524365561264e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.virtual.base.Virtual.populate#43",
        "src_path": "lib/ansible/module_utils/facts/virtual/base.py",
        "class_name": "lib.ansible.module_utils.facts.virtual.base.Virtual",
        "signature": "lib.ansible.module_utils.facts.virtual.base.Virtual.populate(self, collected_facts=None)",
        "snippet": "    def populate(self, collected_facts=None):\n        virtual_facts = self.get_virtual_facts()\n\n        return virtual_facts",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.virtual.base.Virtual.get_virtual_facts#48",
        "src_path": "lib/ansible/module_utils/facts/virtual/base.py",
        "class_name": "lib.ansible.module_utils.facts.virtual.base.Virtual",
        "signature": "lib.ansible.module_utils.facts.virtual.base.Virtual.get_virtual_facts(self)",
        "snippet": "    def get_virtual_facts(self):\n        virtual_facts = {'virtualization_type': '',\n                         'virtualization_role': ''}\n        return virtual_facts",
        "begin_line": 48,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.virtual.base.VirtualCollector.collect#60",
        "src_path": "lib/ansible/module_utils/facts/virtual/base.py",
        "class_name": "lib.ansible.module_utils.facts.virtual.base.VirtualCollector",
        "signature": "lib.ansible.module_utils.facts.virtual.base.VirtualCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        collected_facts = collected_facts or {}\n        if not module:\n            return {}\n\n        # Network munges cached_facts by side effect, so give it a copy\n        facts_obj = self._fact_class(module)\n\n        facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n        return facts_dict",
        "begin_line": 60,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.role.GalaxyRole.__init__#53",
        "src_path": "lib/ansible/galaxy/role.py",
        "class_name": "lib.ansible.galaxy.role.GalaxyRole",
        "signature": "lib.ansible.galaxy.role.GalaxyRole.__init__(self, galaxy, api, name, src=None, version=None, scm=None, path=None)",
        "snippet": "    def __init__(self, galaxy, api, name, src=None, version=None, scm=None, path=None):\n\n        self._metadata = None\n        self._requirements = None\n        self._install_info = None\n        self._validate_certs = not context.CLIARGS['ignore_certs']\n\n        display.debug('Validate TLS certificates: %s' % self._validate_certs)\n\n        self.galaxy = galaxy\n        self.api = api\n\n        self.name = name\n        self.version = version\n        self.src = src or name\n        self.scm = scm\n        self.paths = [os.path.join(x, self.name) for x in galaxy.roles_paths]\n\n        if path is not None:\n            if not path.endswith(os.path.join(os.path.sep, self.name)):\n                path = os.path.join(path, self.name)\n            else:\n                # Look for a meta/main.ya?ml inside the potential role dir in case\n                #  the role name is the same as parent directory of the role.\n                #\n                # Example:\n                #   ./roles/testing/testing/meta/main.yml\n                for meta_main in self.META_MAIN:\n                    if os.path.exists(os.path.join(path, name, meta_main)):\n                        path = os.path.join(path, self.name)\n                        break\n            self.path = path\n        else:\n            # use the first path by default\n            self.path = os.path.join(galaxy.roles_paths[0], self.name)",
        "begin_line": 53,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.taggable.Taggable._load_tags#33",
        "src_path": "lib/ansible/playbook/taggable.py",
        "class_name": "lib.ansible.playbook.taggable.Taggable",
        "signature": "lib.ansible.playbook.taggable.Taggable._load_tags(self, attr, ds)",
        "snippet": "    def _load_tags(self, attr, ds):\n        if isinstance(ds, list):\n            return ds\n        elif isinstance(ds, string_types):\n            value = ds.split(',')\n            if isinstance(value, list):\n                return [x.strip() for x in value]\n            else:\n                return [ds]\n        else:\n            raise AnsibleError('tags must be specified as a list', obj=ds)",
        "begin_line": 33,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.taggable.Taggable.evaluate_tags#45",
        "src_path": "lib/ansible/playbook/taggable.py",
        "class_name": "lib.ansible.playbook.taggable.Taggable",
        "signature": "lib.ansible.playbook.taggable.Taggable.evaluate_tags(self, only_tags, skip_tags, all_vars)",
        "snippet": "    def evaluate_tags(self, only_tags, skip_tags, all_vars):\n        ''' this checks if the current item should be executed depending on tag options '''\n\n        if self.tags:\n            templar = Templar(loader=self._loader, variables=all_vars)\n            tags = templar.template(self.tags)\n\n            _temp_tags = set()\n            for tag in tags:\n                if isinstance(tag, list):\n                    _temp_tags.update(tag)\n                else:\n                    _temp_tags.add(tag)\n            tags = _temp_tags\n            self.tags = list(tags)\n        else:\n            # this makes isdisjoint work for untagged\n            tags = self.untagged\n\n        should_run = True  # default, tasks to run\n\n        if only_tags:\n            if 'always' in tags:\n                should_run = True\n            elif ('all' in only_tags and 'never' not in tags):\n                should_run = True\n            elif not tags.isdisjoint(only_tags):\n                should_run = True\n            elif 'tagged' in only_tags and tags != self.untagged and 'never' not in tags:\n                should_run = True\n            else:\n                should_run = False\n\n        if should_run and skip_tags:\n\n            # Check for tags that we need to skip\n            if 'all' in skip_tags:\n                if 'always' not in tags or 'always' in skip_tags:\n                    should_run = False\n            elif not tags.isdisjoint(skip_tags):\n                should_run = False\n            elif 'tagged' in skip_tags and tags != self.untagged:\n                should_run = False\n\n        return should_run",
        "begin_line": 45,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.vars.host_group_vars.VarsModule.get_vars#71",
        "src_path": "lib/ansible/plugins/vars/host_group_vars.py",
        "class_name": "lib.ansible.plugins.vars.host_group_vars.VarsModule",
        "signature": "lib.ansible.plugins.vars.host_group_vars.VarsModule.get_vars(self, loader, path, entities, cache=True)",
        "snippet": "    def get_vars(self, loader, path, entities, cache=True):\n        ''' parses the inventory file '''\n\n        if not isinstance(entities, list):\n            entities = [entities]\n\n        super(VarsModule, self).get_vars(loader, path, entities)\n\n        data = {}\n        for entity in entities:\n            if isinstance(entity, Host):\n                subdir = 'host_vars'\n            elif isinstance(entity, Group):\n                subdir = 'group_vars'\n            else:\n                raise AnsibleParserError(\"Supplied entity must be Host or Group, got %s instead\" % (type(entity)))\n\n            # avoid 'chroot' type inventory hostnames /path/to/chroot\n            if not entity.name.startswith(os.path.sep):\n                try:\n                    found_files = []\n                    # load vars\n                    b_opath = os.path.realpath(to_bytes(os.path.join(self._basedir, subdir)))\n                    opath = to_text(b_opath)\n                    key = '%s.%s' % (entity.name, opath)\n                    if cache and key in FOUND:\n                        found_files = FOUND[key]\n                    else:\n                        # no need to do much if path does not exist for basedir\n                        if os.path.exists(b_opath):\n                            if os.path.isdir(b_opath):\n                                self._display.debug(\"\\tprocessing dir %s\" % opath)\n                                found_files = loader.find_vars_files(opath, entity.name)\n                                FOUND[key] = found_files\n                            else:\n                                self._display.warning(\"Found %s that is not a directory, skipping: %s\" % (subdir, opath))\n\n                    for found in found_files:\n                        new_data = loader.load_from_file(found, cache=True, unsafe=True)\n                        if new_data:  # ignore empty files\n                            data = combine_vars(data, new_data)\n\n                except Exception as e:\n                    raise AnsibleParserError(to_native(e))\n        return data",
        "begin_line": 71,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role_include.IncludeRole.__init__#56",
        "src_path": "lib/ansible/playbook/role_include.py",
        "class_name": "lib.ansible.playbook.role_include.IncludeRole",
        "signature": "lib.ansible.playbook.role_include.IncludeRole.__init__(self, block=None, role=None, task_include=None)",
        "snippet": "    def __init__(self, block=None, role=None, task_include=None):\n\n        super(IncludeRole, self).__init__(block=block, role=role, task_include=task_include)\n\n        self._from_files = {}\n        self._parent_role = role\n        self._role_name = None\n        self._role_path = None",
        "begin_line": 56,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role_include.IncludeRole.get_block_list#69",
        "src_path": "lib/ansible/playbook/role_include.py",
        "class_name": "lib.ansible.playbook.role_include.IncludeRole",
        "signature": "lib.ansible.playbook.role_include.IncludeRole.get_block_list(self, play=None, variable_manager=None, loader=None)",
        "snippet": "    def get_block_list(self, play=None, variable_manager=None, loader=None):\n\n        # only need play passed in when dynamic\n        if play is None:\n            myplay = self._parent._play\n        else:\n            myplay = play\n\n        ri = RoleInclude.load(self._role_name, play=myplay, variable_manager=variable_manager, loader=loader, collection_list=self.collections)\n        ri.vars.update(self.vars)\n\n        # build role\n        actual_role = Role.load(ri, myplay, parent_role=self._parent_role, from_files=self._from_files,\n                                from_include=True)\n        actual_role._metadata.allow_duplicates = self.allow_duplicates\n\n        if self.statically_loaded or self.public:\n            myplay.roles.append(actual_role)\n\n        # save this for later use\n        self._role_path = actual_role._role_path\n\n        # compile role with parent roles as dependencies to ensure they inherit\n        # variables\n        if not self._parent_role:\n            dep_chain = []\n        else:\n            dep_chain = list(self._parent_role._parents)\n            dep_chain.append(self._parent_role)\n\n        p_block = self.build_parent_block()\n\n        # collections value is not inherited; override with the value we calculated during role setup\n        p_block.collections = actual_role.collections\n\n        blocks = actual_role.compile(play=myplay, dep_chain=dep_chain)\n        for b in blocks:\n            b._parent = p_block\n            # HACK: parent inheritance doesn't seem to have a way to handle this intermediate override until squashed/finalized\n            b.collections = actual_role.collections\n\n        # updated available handlers in play\n        handlers = actual_role.get_handler_blocks(play=myplay)\n        for h in handlers:\n            h._parent = p_block\n        myplay.handlers = myplay.handlers + handlers\n        return blocks, handlers",
        "begin_line": 69,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role_include.IncludeRole.load#118",
        "src_path": "lib/ansible/playbook/role_include.py",
        "class_name": "lib.ansible.playbook.role_include.IncludeRole",
        "signature": "lib.ansible.playbook.role_include.IncludeRole.load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None)",
        "snippet": "    def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n\n        ir = IncludeRole(block, role, task_include=task_include).load_data(data, variable_manager=variable_manager, loader=loader)\n\n        # Validate options\n        my_arg_names = frozenset(ir.args.keys())\n\n        # name is needed, or use role as alias\n        ir._role_name = ir.args.get('name', ir.args.get('role'))\n        if ir._role_name is None:\n            raise AnsibleParserError(\"'name' is a required field for %s.\" % ir.action, obj=data)\n\n        if 'public' in ir.args and ir.action != 'include_role':\n            raise AnsibleParserError('Invalid options for %s: public' % ir.action, obj=data)\n\n        # validate bad args, otherwise we silently ignore\n        bad_opts = my_arg_names.difference(IncludeRole.VALID_ARGS)\n        if bad_opts:\n            raise AnsibleParserError('Invalid options for %s: %s' % (ir.action, ','.join(list(bad_opts))), obj=data)\n\n        # build options for role includes\n        for key in my_arg_names.intersection(IncludeRole.FROM_ARGS):\n            from_key = key.replace('_from', '')\n            args_value = ir.args.get(key)\n            if not isinstance(args_value, string_types):\n                raise AnsibleParserError('Expected a string for %s but got %s instead' % (key, type(args_value)))\n            ir._from_files[from_key] = basename(args_value)\n\n        apply_attrs = ir.args.get('apply', {})\n        if apply_attrs and ir.action != 'include_role':\n            raise AnsibleParserError('Invalid options for %s: apply' % ir.action, obj=data)\n        elif not isinstance(apply_attrs, dict):\n            raise AnsibleParserError('Expected a dict for apply but got %s instead' % type(apply_attrs), obj=data)\n\n        # manual list as otherwise the options would set other task parameters we don't want.\n        for option in my_arg_names.intersection(IncludeRole.OTHER_ARGS):\n            setattr(ir, option, ir.args.get(option))\n\n        return ir",
        "begin_line": 118,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role_include.IncludeRole.copy#158",
        "src_path": "lib/ansible/playbook/role_include.py",
        "class_name": "lib.ansible.playbook.role_include.IncludeRole",
        "signature": "lib.ansible.playbook.role_include.IncludeRole.copy(self, exclude_parent=False, exclude_tasks=False)",
        "snippet": "    def copy(self, exclude_parent=False, exclude_tasks=False):\n\n        new_me = super(IncludeRole, self).copy(exclude_parent=exclude_parent, exclude_tasks=exclude_tasks)\n        new_me.statically_loaded = self.statically_loaded\n        new_me._from_files = self._from_files.copy()\n        new_me._parent_role = self._parent_role\n        new_me._role_name = self._role_name\n        new_me._role_path = self._role_path\n\n        return new_me",
        "begin_line": 158,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role_include.IncludeRole.get_include_params#169",
        "src_path": "lib/ansible/playbook/role_include.py",
        "class_name": "lib.ansible.playbook.role_include.IncludeRole",
        "signature": "lib.ansible.playbook.role_include.IncludeRole.get_include_params(self)",
        "snippet": "    def get_include_params(self):\n        v = super(IncludeRole, self).get_include_params()\n        if self._parent_role:\n            v.update(self._parent_role.get_role_params())\n            v.setdefault('ansible_parent_role_names', []).insert(0, self._parent_role.get_name())\n            v.setdefault('ansible_parent_role_paths', []).insert(0, self._parent_role._role_path)\n        return v",
        "begin_line": 169,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.selinux.SelinuxFactCollector.collect#40",
        "src_path": "lib/ansible/module_utils/facts/system/selinux.py",
        "class_name": "lib.ansible.module_utils.facts.system.selinux.SelinuxFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.selinux.SelinuxFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        selinux_facts = {}\n\n        # If selinux library is missing, only set the status and selinux_python_present since\n        # there is no way to tell if SELinux is enabled or disabled on the system\n        # without the library.\n        if not HAVE_SELINUX:\n            selinux_facts['status'] = 'Missing selinux Python library'\n            facts_dict['selinux'] = selinux_facts\n            facts_dict['selinux_python_present'] = False\n            return facts_dict\n\n        # Set a boolean for testing whether the Python library is present\n        facts_dict['selinux_python_present'] = True\n\n        if not selinux.is_selinux_enabled():\n            selinux_facts['status'] = 'disabled'\n        else:\n            selinux_facts['status'] = 'enabled'\n\n            try:\n                selinux_facts['policyvers'] = selinux.security_policyvers()\n            except (AttributeError, OSError):\n                selinux_facts['policyvers'] = 'unknown'\n\n            try:\n                (rc, configmode) = selinux.selinux_getenforcemode()\n                if rc == 0:\n                    selinux_facts['config_mode'] = SELINUX_MODE_DICT.get(configmode, 'unknown')\n                else:\n                    selinux_facts['config_mode'] = 'unknown'\n            except (AttributeError, OSError):\n                selinux_facts['config_mode'] = 'unknown'\n\n            try:\n                mode = selinux.security_getenforce()\n                selinux_facts['mode'] = SELINUX_MODE_DICT.get(mode, 'unknown')\n            except (AttributeError, OSError):\n                selinux_facts['mode'] = 'unknown'\n\n            try:\n                (rc, policytype) = selinux.selinux_getpolicytype()\n                if rc == 0:\n                    selinux_facts['type'] = policytype\n                else:\n                    selinux_facts['type'] = 'unknown'\n            except (AttributeError, OSError):\n                selinux_facts['type'] = 'unknown'\n\n        facts_dict['selinux'] = selinux_facts\n        return facts_dict",
        "begin_line": 40,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.utils.get_file_content#19",
        "src_path": "lib/ansible/module_utils/facts/utils.py",
        "class_name": "lib.ansible.module_utils.facts.utils",
        "signature": "lib.ansible.module_utils.facts.utils.get_file_content(path, default=None, strip=True)",
        "snippet": "def get_file_content(path, default=None, strip=True):\n    data = default\n    if os.path.exists(path) and os.access(path, os.R_OK):\n        try:\n            try:\n                datafile = open(path)\n                data = datafile.read()\n                if strip:\n                    data = data.strip()\n                if len(data) == 0:\n                    data = default\n            finally:\n                datafile.close()\n        except Exception:\n            # ignore errors as some jails/containers might have readable permissions but not allow reads to proc\n            # done in 2 blocks for 2.4 compat\n            pass\n    return data",
        "begin_line": 19,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.112273870365864e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.utils.get_file_lines#39",
        "src_path": "lib/ansible/module_utils/facts/utils.py",
        "class_name": "lib.ansible.module_utils.facts.utils",
        "signature": "lib.ansible.module_utils.facts.utils.get_file_lines(path, strip=True, line_sep=None)",
        "snippet": "def get_file_lines(path, strip=True, line_sep=None):\n    '''get list of lines from file'''\n    data = get_file_content(path, strip=strip)\n    if data:\n        if line_sep is None:\n            ret = data.splitlines()\n        else:\n            if len(line_sep) == 1:\n                ret = data.rstrip(line_sep).split(line_sep)\n            else:\n                ret = data.split(line_sep)\n    else:\n        ret = []\n    return ret",
        "begin_line": 39,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.utils.get_mount_size#55",
        "src_path": "lib/ansible/module_utils/facts/utils.py",
        "class_name": "lib.ansible.module_utils.facts.utils",
        "signature": "lib.ansible.module_utils.facts.utils.get_mount_size(mountpoint)",
        "snippet": "def get_mount_size(mountpoint):\n    mount_size = {}\n\n    try:\n        statvfs_result = os.statvfs(mountpoint)\n        mount_size['size_total'] = statvfs_result.f_frsize * statvfs_result.f_blocks\n        mount_size['size_available'] = statvfs_result.f_frsize * (statvfs_result.f_bavail)\n\n        # Block total/available/used\n        mount_size['block_size'] = statvfs_result.f_bsize\n        mount_size['block_total'] = statvfs_result.f_blocks\n        mount_size['block_available'] = statvfs_result.f_bavail\n        mount_size['block_used'] = mount_size['block_total'] - mount_size['block_available']\n\n        # Inode total/available/used\n        mount_size['inode_total'] = statvfs_result.f_files\n        mount_size['inode_available'] = statvfs_result.f_favail\n        mount_size['inode_used'] = mount_size['inode_total'] - mount_size['inode_available']\n    except OSError:\n        pass\n\n    return mount_size",
        "begin_line": 55,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.__init__#39",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.__init__(self, blocks)",
        "snippet": "    def __init__(self, blocks):\n        self._blocks = blocks[:]\n\n        self.cur_block = 0\n        self.cur_regular_task = 0\n        self.cur_rescue_task = 0\n        self.cur_always_task = 0\n        self.cur_dep_chain = None\n        self.run_state = PlayIterator.ITERATING_SETUP\n        self.fail_state = PlayIterator.FAILED_NONE\n        self.pending_setup = False\n        self.tasks_child_state = None\n        self.rescue_child_state = None\n        self.always_child_state = None\n        self.did_rescue = False\n        self.did_start_at_task = False",
        "begin_line": 39,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.__repr__#56",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return \"HostState(%r)\" % self._blocks",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.__str__#59",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.__str__(self)",
        "snippet": "    def __str__(self):\n        def _run_state_to_string(n):\n            states = [\"ITERATING_SETUP\", \"ITERATING_TASKS\", \"ITERATING_RESCUE\", \"ITERATING_ALWAYS\", \"ITERATING_COMPLETE\"]\n            try:\n                return states[n]\n            except IndexError:\n                return \"UNKNOWN STATE\"\n\n        def _failed_state_to_string(n):\n            states = {1: \"FAILED_SETUP\", 2: \"FAILED_TASKS\", 4: \"FAILED_RESCUE\", 8: \"FAILED_ALWAYS\"}\n            if n == 0:\n                return \"FAILED_NONE\"\n            else:\n                ret = []\n                for i in (1, 2, 4, 8):\n                    if n & i:\n                        ret.append(states[i])\n                return \"|\".join(ret)\n\n        return (\"HOST STATE: block=%d, task=%d, rescue=%d, always=%d, run_state=%s, fail_state=%s, pending_setup=%s, tasks child state? (%s), \"\n                \"rescue child state? (%s), always child state? (%s), did rescue? %s, did start at task? %s\" % (\n                    self.cur_block,\n                    self.cur_regular_task,\n                    self.cur_rescue_task,\n                    self.cur_always_task,\n                    _run_state_to_string(self.run_state),\n                    _failed_state_to_string(self.fail_state),\n                    self.pending_setup,\n                    self.tasks_child_state,\n                    self.rescue_child_state,\n                    self.always_child_state,\n                    self.did_rescue,\n                    self.did_start_at_task,\n                ))",
        "begin_line": 59,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState._run_state_to_string#60",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState._run_state_to_string(n)",
        "snippet": "        def _run_state_to_string(n):\n            states = [\"ITERATING_SETUP\", \"ITERATING_TASKS\", \"ITERATING_RESCUE\", \"ITERATING_ALWAYS\", \"ITERATING_COMPLETE\"]\n            try:\n                return states[n]\n            except IndexError:\n                return \"UNKNOWN STATE\"",
        "begin_line": 60,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState._failed_state_to_string#67",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState._failed_state_to_string(n)",
        "snippet": "        def _failed_state_to_string(n):\n            states = {1: \"FAILED_SETUP\", 2: \"FAILED_TASKS\", 4: \"FAILED_RESCUE\", 8: \"FAILED_ALWAYS\"}\n            if n == 0:\n                return \"FAILED_NONE\"\n            else:\n                ret = []\n                for i in (1, 2, 4, 8):\n                    if n & i:\n                        ret.append(states[i])\n                return \"|\".join(ret)",
        "begin_line": 67,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.__eq__#94",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        if not isinstance(other, HostState):\n            return False\n\n        for attr in ('_blocks', 'cur_block', 'cur_regular_task', 'cur_rescue_task', 'cur_always_task',\n                     'run_state', 'fail_state', 'pending_setup', 'cur_dep_chain',\n                     'tasks_child_state', 'rescue_child_state', 'always_child_state'):\n            if getattr(self, attr) != getattr(other, attr):\n                return False\n\n        return True",
        "begin_line": 94,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.get_current_block#106",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.get_current_block(self)",
        "snippet": "    def get_current_block(self):\n        return self._blocks[self.cur_block]",
        "begin_line": 106,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.HostState.copy#109",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.HostState",
        "signature": "lib.ansible.executor.play_iterator.HostState.copy(self)",
        "snippet": "    def copy(self):\n        new_state = HostState(self._blocks)\n        new_state.cur_block = self.cur_block\n        new_state.cur_regular_task = self.cur_regular_task\n        new_state.cur_rescue_task = self.cur_rescue_task\n        new_state.cur_always_task = self.cur_always_task\n        new_state.run_state = self.run_state\n        new_state.fail_state = self.fail_state\n        new_state.pending_setup = self.pending_setup\n        new_state.did_rescue = self.did_rescue\n        new_state.did_start_at_task = self.did_start_at_task\n        if self.cur_dep_chain is not None:\n            new_state.cur_dep_chain = self.cur_dep_chain[:]\n        if self.tasks_child_state is not None:\n            new_state.tasks_child_state = self.tasks_child_state.copy()\n        if self.rescue_child_state is not None:\n            new_state.rescue_child_state = self.rescue_child_state.copy()\n        if self.always_child_state is not None:\n            new_state.always_child_state = self.always_child_state.copy()\n        return new_state",
        "begin_line": 109,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.__init__#148",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.__init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False)",
        "snippet": "    def __init__(self, inventory, play, play_context, variable_manager, all_vars, start_at_done=False):\n        self._play = play\n        self._blocks = []\n        self._variable_manager = variable_manager\n\n        # Default options to gather\n        gather_subset = self._play.gather_subset\n        gather_timeout = self._play.gather_timeout\n        fact_path = self._play.fact_path\n\n        setup_block = Block(play=self._play)\n        # Gathering facts with run_once would copy the facts from one host to\n        # the others.\n        setup_block.run_once = False\n        setup_task = Task(block=setup_block)\n        setup_task.action = 'gather_facts'\n        setup_task.name = 'Gathering Facts'\n        setup_task.args = {\n            'gather_subset': gather_subset,\n        }\n\n        # Unless play is specifically tagged, gathering should 'always' run\n        if not self._play.tags:\n            setup_task.tags = ['always']\n\n        if gather_timeout:\n            setup_task.args['gather_timeout'] = gather_timeout\n        if fact_path:\n            setup_task.args['fact_path'] = fact_path\n        setup_task.set_loader(self._play._loader)\n        # short circuit fact gathering if the entire playbook is conditional\n        if self._play._included_conditional is not None:\n            setup_task.when = self._play._included_conditional[:]\n        setup_block.block = [setup_task]\n\n        setup_block = setup_block.filter_tagged_tasks(all_vars)\n        self._blocks.append(setup_block)\n\n        for block in self._play.compile():\n            new_block = block.filter_tagged_tasks(all_vars)\n            if new_block.has_tasks():\n                self._blocks.append(new_block)\n\n        self._host_states = {}\n        start_at_matched = False\n        batch = inventory.get_hosts(self._play.hosts, order=self._play.order)\n        self.batch_size = len(batch)\n        for host in batch:\n            self._host_states[host.name] = HostState(blocks=self._blocks)\n            # if we're looking to start at a specific task, iterate through\n            # the tasks for this host until we find the specified task\n            if play_context.start_at_task is not None and not start_at_done:\n                while True:\n                    (s, task) = self.get_next_task_for_host(host, peek=True)\n                    if s.run_state == self.ITERATING_COMPLETE:\n                        break\n                    if task.name == play_context.start_at_task or (task.name and fnmatch.fnmatch(task.name, play_context.start_at_task)) or \\\n                       task.get_name() == play_context.start_at_task or fnmatch.fnmatch(task.get_name(), play_context.start_at_task):\n                        start_at_matched = True\n                        break\n                    else:\n                        self.get_next_task_for_host(host)\n\n                # finally, reset the host's state to ITERATING_SETUP\n                if start_at_matched:\n                    self._host_states[host.name].did_start_at_task = True\n                    self._host_states[host.name].run_state = self.ITERATING_SETUP\n\n        if start_at_matched:\n            # we have our match, so clear the start_at_task field on the\n            # play context to flag that we've started at a task (and future\n            # plays won't try to advance)\n            play_context.start_at_task = None",
        "begin_line": 148,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.get_host_state#222",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.get_host_state(self, host)",
        "snippet": "    def get_host_state(self, host):\n        # Since we're using the PlayIterator to carry forward failed hosts,\n        # in the event that a previous host was not in the current inventory\n        # we create a stub state for it now\n        if host.name not in self._host_states:\n            self._host_states[host.name] = HostState(blocks=[])\n\n        return self._host_states[host.name].copy()",
        "begin_line": 222,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.get_next_task_for_host#237",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.get_next_task_for_host(self, host, peek=False)",
        "snippet": "    def get_next_task_for_host(self, host, peek=False):\n\n        display.debug(\"getting the next task for host %s\" % host.name)\n        s = self.get_host_state(host)\n\n        task = None\n        if s.run_state == self.ITERATING_COMPLETE:\n            display.debug(\"host %s is done iterating, returning\" % host.name)\n            return (s, None)\n\n        (s, task) = self._get_next_task_from_state(s, host=host, peek=peek)\n\n        if not peek:\n            self._host_states[host.name] = s\n\n        display.debug(\"done getting next task for host %s\" % host.name)\n        display.debug(\" ^ task is: %s\" % task)\n        display.debug(\" ^ state is: %s\" % s)\n        return (s, task)",
        "begin_line": 237,
        "end_line": 255,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator._get_next_task_from_state#257",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator._get_next_task_from_state(self, state, host, peek, in_child=False)",
        "snippet": "    def _get_next_task_from_state(self, state, host, peek, in_child=False):\n\n        task = None\n\n        # try and find the next task, given the current state.\n        while True:\n            # try to get the current block from the list of blocks, and\n            # if we run past the end of the list we know we're done with\n            # this block\n            try:\n                block = state._blocks[state.cur_block]\n            except IndexError:\n                state.run_state = self.ITERATING_COMPLETE\n                return (state, None)\n\n            if state.run_state == self.ITERATING_SETUP:\n                # First, we check to see if we were pending setup. If not, this is\n                # the first trip through ITERATING_SETUP, so we set the pending_setup\n                # flag and try to determine if we do in fact want to gather facts for\n                # the specified host.\n                if not state.pending_setup:\n                    state.pending_setup = True\n\n                    # Gather facts if the default is 'smart' and we have not yet\n                    # done it for this host; or if 'explicit' and the play sets\n                    # gather_facts to True; or if 'implicit' and the play does\n                    # NOT explicitly set gather_facts to False.\n\n                    gathering = C.DEFAULT_GATHERING\n                    implied = self._play.gather_facts is None or boolean(self._play.gather_facts, strict=False)\n\n                    if (gathering == 'implicit' and implied) or \\\n                       (gathering == 'explicit' and boolean(self._play.gather_facts, strict=False)) or \\\n                       (gathering == 'smart' and implied and not (self._variable_manager._fact_cache.get(host.name, {}).get('_ansible_facts_gathered', False))):\n                        # The setup block is always self._blocks[0], as we inject it\n                        # during the play compilation in __init__ above.\n                        setup_block = self._blocks[0]\n                        if setup_block.has_tasks() and len(setup_block.block) > 0:\n                            task = setup_block.block[0]\n                else:\n                    # This is the second trip through ITERATING_SETUP, so we clear\n                    # the flag and move onto the next block in the list while setting\n                    # the run state to ITERATING_TASKS\n                    state.pending_setup = False\n\n                    state.run_state = self.ITERATING_TASKS\n                    if not state.did_start_at_task:\n                        state.cur_block += 1\n                        state.cur_regular_task = 0\n                        state.cur_rescue_task = 0\n                        state.cur_always_task = 0\n                        state.tasks_child_state = None\n                        state.rescue_child_state = None\n                        state.always_child_state = None\n\n            elif state.run_state == self.ITERATING_TASKS:\n                # clear the pending setup flag, since we're past that and it didn't fail\n                if state.pending_setup:\n                    state.pending_setup = False\n\n                # First, we check for a child task state that is not failed, and if we\n                # have one recurse into it for the next task. If we're done with the child\n                # state, we clear it and drop back to getting the next task from the list.\n                if state.tasks_child_state:\n                    (state.tasks_child_state, task) = self._get_next_task_from_state(state.tasks_child_state, host=host, peek=peek, in_child=True)\n                    if self._check_failed_state(state.tasks_child_state):\n                        # failed child state, so clear it and move into the rescue portion\n                        state.tasks_child_state = None\n                        self._set_failed_state(state)\n                    else:\n                        # get the next task recursively\n                        if task is None or state.tasks_child_state.run_state == self.ITERATING_COMPLETE:\n                            # we're done with the child state, so clear it and continue\n                            # back to the top of the loop to get the next task\n                            state.tasks_child_state = None\n                            continue\n                else:\n                    # First here, we check to see if we've failed anywhere down the chain\n                    # of states we have, and if so we move onto the rescue portion. Otherwise,\n                    # we check to see if we've moved past the end of the list of tasks. If so,\n                    # we move into the always portion of the block, otherwise we get the next\n                    # task from the list.\n                    if self._check_failed_state(state):\n                        state.run_state = self.ITERATING_RESCUE\n                    elif state.cur_regular_task >= len(block.block):\n                        state.run_state = self.ITERATING_ALWAYS\n                    else:\n                        task = block.block[state.cur_regular_task]\n                        # if the current task is actually a child block, create a child\n                        # state for us to recurse into on the next pass\n                        if isinstance(task, Block):\n                            state.tasks_child_state = HostState(blocks=[task])\n                            state.tasks_child_state.run_state = self.ITERATING_TASKS\n                            # since we've created the child state, clear the task\n                            # so we can pick up the child state on the next pass\n                            task = None\n                        state.cur_regular_task += 1\n\n            elif state.run_state == self.ITERATING_RESCUE:\n                # The process here is identical to ITERATING_TASKS, except instead\n                # we move into the always portion of the block.\n                if host.name in self._play._removed_hosts:\n                    self._play._removed_hosts.remove(host.name)\n\n                if state.rescue_child_state:\n                    (state.rescue_child_state, task) = self._get_next_task_from_state(state.rescue_child_state, host=host, peek=peek, in_child=True)\n                    if self._check_failed_state(state.rescue_child_state):\n                        state.rescue_child_state = None\n                        self._set_failed_state(state)\n                    else:\n                        if task is None or state.rescue_child_state.run_state == self.ITERATING_COMPLETE:\n                            state.rescue_child_state = None\n                            continue\n                else:\n                    if state.fail_state & self.FAILED_RESCUE == self.FAILED_RESCUE:\n                        state.run_state = self.ITERATING_ALWAYS\n                    elif state.cur_rescue_task >= len(block.rescue):\n                        if len(block.rescue) > 0:\n                            state.fail_state = self.FAILED_NONE\n                        state.run_state = self.ITERATING_ALWAYS\n                        state.did_rescue = True\n                    else:\n                        task = block.rescue[state.cur_rescue_task]\n                        if isinstance(task, Block):\n                            state.rescue_child_state = HostState(blocks=[task])\n                            state.rescue_child_state.run_state = self.ITERATING_TASKS\n                            task = None\n                        state.cur_rescue_task += 1\n\n            elif state.run_state == self.ITERATING_ALWAYS:\n                # And again, the process here is identical to ITERATING_TASKS, except\n                # instead we either move onto the next block in the list, or we set the\n                # run state to ITERATING_COMPLETE in the event of any errors, or when we\n                # have hit the end of the list of blocks.\n                if state.always_child_state:\n                    (state.always_child_state, task) = self._get_next_task_from_state(state.always_child_state, host=host, peek=peek, in_child=True)\n                    if self._check_failed_state(state.always_child_state):\n                        state.always_child_state = None\n                        self._set_failed_state(state)\n                    else:\n                        if task is None or state.always_child_state.run_state == self.ITERATING_COMPLETE:\n                            state.always_child_state = None\n                            continue\n                else:\n                    if state.cur_always_task >= len(block.always):\n                        if state.fail_state != self.FAILED_NONE:\n                            state.run_state = self.ITERATING_COMPLETE\n                        else:\n                            state.cur_block += 1\n                            state.cur_regular_task = 0\n                            state.cur_rescue_task = 0\n                            state.cur_always_task = 0\n                            state.run_state = self.ITERATING_TASKS\n                            state.tasks_child_state = None\n                            state.rescue_child_state = None\n                            state.always_child_state = None\n                            state.did_rescue = False\n\n                            # we're advancing blocks, so if this was an end-of-role block we\n                            # mark the current role complete\n                            if block._eor and host.name in block._role._had_task_run and not in_child and not peek:\n                                block._role._completed[host.name] = True\n                    else:\n                        task = block.always[state.cur_always_task]\n                        if isinstance(task, Block):\n                            state.always_child_state = HostState(blocks=[task])\n                            state.always_child_state.run_state = self.ITERATING_TASKS\n                            task = None\n                        state.cur_always_task += 1\n\n            elif state.run_state == self.ITERATING_COMPLETE:\n                return (state, None)\n\n            # if something above set the task, break out of the loop now\n            if task:\n                break\n\n        return (state, task)",
        "begin_line": 257,
        "end_line": 434,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator._set_failed_state#436",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator._set_failed_state(self, state)",
        "snippet": "    def _set_failed_state(self, state):\n        if state.run_state == self.ITERATING_SETUP:\n            state.fail_state |= self.FAILED_SETUP\n            state.run_state = self.ITERATING_COMPLETE\n        elif state.run_state == self.ITERATING_TASKS:\n            if state.tasks_child_state is not None:\n                state.tasks_child_state = self._set_failed_state(state.tasks_child_state)\n            else:\n                state.fail_state |= self.FAILED_TASKS\n                if state._blocks[state.cur_block].rescue:\n                    state.run_state = self.ITERATING_RESCUE\n                elif state._blocks[state.cur_block].always:\n                    state.run_state = self.ITERATING_ALWAYS\n                else:\n                    state.run_state = self.ITERATING_COMPLETE\n        elif state.run_state == self.ITERATING_RESCUE:\n            if state.rescue_child_state is not None:\n                state.rescue_child_state = self._set_failed_state(state.rescue_child_state)\n            else:\n                state.fail_state |= self.FAILED_RESCUE\n                if state._blocks[state.cur_block].always:\n                    state.run_state = self.ITERATING_ALWAYS\n                else:\n                    state.run_state = self.ITERATING_COMPLETE\n        elif state.run_state == self.ITERATING_ALWAYS:\n            if state.always_child_state is not None:\n                state.always_child_state = self._set_failed_state(state.always_child_state)\n            else:\n                state.fail_state |= self.FAILED_ALWAYS\n                state.run_state = self.ITERATING_COMPLETE\n        return state",
        "begin_line": 436,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.mark_host_failed#468",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.mark_host_failed(self, host)",
        "snippet": "    def mark_host_failed(self, host):\n        s = self.get_host_state(host)\n        display.debug(\"marking host %s failed, current state: %s\" % (host, s))\n        s = self._set_failed_state(s)\n        display.debug(\"^ failed state is now: %s\" % s)\n        self._host_states[host.name] = s\n        self._play._removed_hosts.append(host.name)",
        "begin_line": 468,
        "end_line": 474,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.get_failed_hosts#476",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.get_failed_hosts(self)",
        "snippet": "    def get_failed_hosts(self):\n        return dict((host, True) for (host, state) in iteritems(self._host_states) if self._check_failed_state(state))",
        "begin_line": 476,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator._check_failed_state#479",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator._check_failed_state(self, state)",
        "snippet": "    def _check_failed_state(self, state):\n        if state is None:\n            return False\n        elif state.run_state == self.ITERATING_RESCUE and self._check_failed_state(state.rescue_child_state):\n            return True\n        elif state.run_state == self.ITERATING_ALWAYS and self._check_failed_state(state.always_child_state):\n            return True\n        elif state.fail_state != self.FAILED_NONE:\n            if state.run_state == self.ITERATING_RESCUE and state.fail_state & self.FAILED_RESCUE == 0:\n                return False\n            elif state.run_state == self.ITERATING_ALWAYS and state.fail_state & self.FAILED_ALWAYS == 0:\n                return False\n            else:\n                return not state.did_rescue\n        elif state.run_state == self.ITERATING_TASKS and self._check_failed_state(state.tasks_child_state):\n            cur_block = self._blocks[state.cur_block]\n            if len(cur_block.rescue) > 0 and state.fail_state & self.FAILED_RESCUE == 0:\n                return False\n            else:\n                return True\n        return False",
        "begin_line": 479,
        "end_line": 499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.get_active_state#505",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.get_active_state(self, state)",
        "snippet": "    def get_active_state(self, state):\n        '''\n        Finds the active state, recursively if necessary when there are child states.\n        '''\n        if state.run_state == self.ITERATING_TASKS and state.tasks_child_state is not None:\n            return self.get_active_state(state.tasks_child_state)\n        elif state.run_state == self.ITERATING_RESCUE and state.rescue_child_state is not None:\n            return self.get_active_state(state.rescue_child_state)\n        elif state.run_state == self.ITERATING_ALWAYS and state.always_child_state is not None:\n            return self.get_active_state(state.always_child_state)\n        return state",
        "begin_line": 505,
        "end_line": 515,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator._insert_tasks_into_state#521",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator._insert_tasks_into_state(self, state, task_list)",
        "snippet": "    def _insert_tasks_into_state(self, state, task_list):\n        # if we've failed at all, or if the task list is empty, just return the current state\n        if state.fail_state != self.FAILED_NONE and state.run_state not in (self.ITERATING_RESCUE, self.ITERATING_ALWAYS) or not task_list:\n            return state\n\n        if state.run_state == self.ITERATING_TASKS:\n            if state.tasks_child_state:\n                state.tasks_child_state = self._insert_tasks_into_state(state.tasks_child_state, task_list)\n            else:\n                target_block = state._blocks[state.cur_block].copy()\n                before = target_block.block[:state.cur_regular_task]\n                after = target_block.block[state.cur_regular_task:]\n                target_block.block = before + task_list + after\n                state._blocks[state.cur_block] = target_block\n        elif state.run_state == self.ITERATING_RESCUE:\n            if state.rescue_child_state:\n                state.rescue_child_state = self._insert_tasks_into_state(state.rescue_child_state, task_list)\n            else:\n                target_block = state._blocks[state.cur_block].copy()\n                before = target_block.rescue[:state.cur_rescue_task]\n                after = target_block.rescue[state.cur_rescue_task:]\n                target_block.rescue = before + task_list + after\n                state._blocks[state.cur_block] = target_block\n        elif state.run_state == self.ITERATING_ALWAYS:\n            if state.always_child_state:\n                state.always_child_state = self._insert_tasks_into_state(state.always_child_state, task_list)\n            else:\n                target_block = state._blocks[state.cur_block].copy()\n                before = target_block.always[:state.cur_always_task]\n                after = target_block.always[state.cur_always_task:]\n                target_block.always = before + task_list + after\n                state._blocks[state.cur_block] = target_block\n        return state",
        "begin_line": 521,
        "end_line": 553,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.play_iterator.PlayIterator.add_tasks#555",
        "src_path": "lib/ansible/executor/play_iterator.py",
        "class_name": "lib.ansible.executor.play_iterator.PlayIterator",
        "signature": "lib.ansible.executor.play_iterator.PlayIterator.add_tasks(self, host, task_list)",
        "snippet": "    def add_tasks(self, host, task_list):\n        self._host_states[host.name] = self._insert_tasks_into_state(self.get_host_state(host), task_list)",
        "begin_line": 555,
        "end_line": 556,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.packaging.os.apt.package_split#413",
        "src_path": "lib/ansible/modules/packaging/os/apt.py",
        "class_name": "lib.ansible.modules.packaging.os.apt",
        "signature": "lib.ansible.modules.packaging.os.apt.package_split(pkgspec)",
        "snippet": "def package_split(pkgspec):\n    parts = pkgspec.split('=', 1)\n    version = None\n    if len(parts) > 1:\n        version = parts[1]\n    return parts[0], version",
        "begin_line": 413,
        "end_line": 418,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.packaging.os.apt.expand_pkgspec_from_fnmatches#529",
        "src_path": "lib/ansible/modules/packaging/os/apt.py",
        "class_name": "lib.ansible.modules.packaging.os.apt",
        "signature": "lib.ansible.modules.packaging.os.apt.expand_pkgspec_from_fnmatches(m, pkgspec, cache)",
        "snippet": "def expand_pkgspec_from_fnmatches(m, pkgspec, cache):\n    # Note: apt-get does implicit regex matching when an exact package name\n    # match is not found.  Something like this:\n    # matches = [pkg.name for pkg in cache if re.match(pkgspec, pkg.name)]\n    # (Should also deal with the ':' for multiarch like the fnmatch code below)\n    #\n    # We have decided not to do similar implicit regex matching but might take\n    # a PR to add some sort of explicit regex matching:\n    # https://github.com/ansible/ansible-modules-core/issues/1258\n    new_pkgspec = []\n    if pkgspec:\n        for pkgspec_pattern in pkgspec:\n            pkgname_pattern, version = package_split(pkgspec_pattern)\n\n            # note that none of these chars is allowed in a (debian) pkgname\n            if frozenset('*?[]!').intersection(pkgname_pattern):\n                # handle multiarch pkgnames, the idea is that \"apt*\" should\n                # only select native packages. But \"apt*:i386\" should still work\n                if \":\" not in pkgname_pattern:\n                    # Filter the multiarch packages from the cache only once\n                    try:\n                        pkg_name_cache = _non_multiarch\n                    except NameError:\n                        pkg_name_cache = _non_multiarch = [pkg.name for pkg in cache if ':' not in pkg.name]  # noqa: F841\n                else:\n                    # Create a cache of pkg_names including multiarch only once\n                    try:\n                        pkg_name_cache = _all_pkg_names\n                    except NameError:\n                        pkg_name_cache = _all_pkg_names = [pkg.name for pkg in cache]  # noqa: F841\n\n                matches = fnmatch.filter(pkg_name_cache, pkgname_pattern)\n\n                if not matches:\n                    m.fail_json(msg=\"No package(s) matching '%s' available\" % str(pkgname_pattern))\n                else:\n                    new_pkgspec.extend(matches)\n            else:\n                # No wildcards in name\n                new_pkgspec.append(pkgspec_pattern)\n    return new_pkgspec",
        "begin_line": 529,
        "end_line": 569,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.yaml.InventoryModule.__init__#83",
        "src_path": "lib/ansible/plugins/inventory/yaml.py",
        "class_name": "lib.ansible.plugins.inventory.yaml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.yaml.InventoryModule.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(InventoryModule, self).__init__()",
        "begin_line": 83,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.yaml.InventoryModule.verify_file#87",
        "src_path": "lib/ansible/plugins/inventory/yaml.py",
        "class_name": "lib.ansible.plugins.inventory.yaml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.yaml.InventoryModule.verify_file(self, path)",
        "snippet": "    def verify_file(self, path):\n\n        valid = False\n        if super(InventoryModule, self).verify_file(path):\n            file_name, ext = os.path.splitext(path)\n            if not ext or ext in self.get_option('yaml_extensions'):\n                valid = True\n        return valid",
        "begin_line": 87,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.yaml.InventoryModule.parse#96",
        "src_path": "lib/ansible/plugins/inventory/yaml.py",
        "class_name": "lib.ansible.plugins.inventory.yaml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.yaml.InventoryModule.parse(self, inventory, loader, path, cache=True)",
        "snippet": "    def parse(self, inventory, loader, path, cache=True):\n        ''' parses the inventory file '''\n\n        super(InventoryModule, self).parse(inventory, loader, path)\n        self.set_options()\n\n        try:\n            data = self.loader.load_from_file(path, cache=False)\n        except Exception as e:\n            raise AnsibleParserError(e)\n\n        if not data:\n            raise AnsibleParserError('Parsed empty YAML file')\n        elif not isinstance(data, MutableMapping):\n            raise AnsibleParserError('YAML inventory has invalid structure, it should be a dictionary, got: %s' % type(data))\n        elif data.get('plugin'):\n            raise AnsibleParserError('Plugin configuration YAML file, not YAML inventory')\n\n        # We expect top level keys to correspond to groups, iterate over them\n        # to get host, vars and subgroups (which we iterate over recursivelly)\n        if isinstance(data, MutableMapping):\n            for group_name in data:\n                self._parse_group(group_name, data[group_name])\n        else:\n            raise AnsibleParserError(\"Invalid data from file, expected dictionary and got:\\n\\n%s\" % to_native(data))",
        "begin_line": 96,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.yaml.InventoryModule._parse_group#122",
        "src_path": "lib/ansible/plugins/inventory/yaml.py",
        "class_name": "lib.ansible.plugins.inventory.yaml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.yaml.InventoryModule._parse_group(self, group, group_data)",
        "snippet": "    def _parse_group(self, group, group_data):\n\n        if isinstance(group_data, (MutableMapping, NoneType)):\n\n            try:\n                group = self.inventory.add_group(group)\n            except AnsibleError as e:\n                raise AnsibleParserError(\"Unable to add group %s: %s\" % (group, to_text(e)))\n\n            if group_data is not None:\n                # make sure they are dicts\n                for section in ['vars', 'children', 'hosts']:\n                    if section in group_data:\n                        # convert strings to dicts as these are allowed\n                        if isinstance(group_data[section], string_types):\n                            group_data[section] = {group_data[section]: None}\n\n                        if not isinstance(group_data[section], (MutableMapping, NoneType)):\n                            raise AnsibleParserError('Invalid \"%s\" entry for \"%s\" group, requires a dictionary, found \"%s\" instead.' %\n                                                     (section, group, type(group_data[section])))\n\n                for key in group_data:\n\n                    if not isinstance(group_data[key], (MutableMapping, NoneType)):\n                        self.display.warning('Skipping key (%s) in group (%s) as it is not a mapping, it is a %s' % (key, group, type(group_data[key])))\n                        continue\n\n                    if isinstance(group_data[key], NoneType):\n                        self.display.vvv('Skipping empty key (%s) in group (%s)' % (key, group))\n                    elif key == 'vars':\n                        for var in group_data[key]:\n                            self.inventory.set_variable(group, var, group_data[key][var])\n                    elif key == 'children':\n                        for subgroup in group_data[key]:\n                            subgroup = self._parse_group(subgroup, group_data[key][subgroup])\n                            self.inventory.add_child(group, subgroup)\n\n                    elif key == 'hosts':\n                        for host_pattern in group_data[key]:\n                            hosts, port = self._parse_host(host_pattern)\n                            self._populate_host_vars(hosts, group_data[key][host_pattern] or {}, group, port)\n                    else:\n                        self.display.warning('Skipping unexpected key (%s) in group (%s), only \"vars\", \"children\" and \"hosts\" are valid' % (key, group))\n\n        else:\n            self.display.warning(\"Skipping '%s' as this is not a valid group definition\" % group)\n\n        return group",
        "begin_line": 122,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.yaml.InventoryModule._parse_host#171",
        "src_path": "lib/ansible/plugins/inventory/yaml.py",
        "class_name": "lib.ansible.plugins.inventory.yaml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.yaml.InventoryModule._parse_host(self, host_pattern)",
        "snippet": "    def _parse_host(self, host_pattern):\n        '''\n        Each host key can be a pattern, try to process it and add variables as needed\n        '''\n        (hostnames, port) = self._expand_hostpattern(host_pattern)\n\n        return hostnames, port",
        "begin_line": 171,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.utils.yaml._handle_error#25",
        "src_path": "lib/ansible/parsing/utils/yaml.py",
        "class_name": "lib.ansible.parsing.utils.yaml",
        "signature": "lib.ansible.parsing.utils.yaml._handle_error(json_exc, yaml_exc, file_name, show_content)",
        "snippet": "def _handle_error(json_exc, yaml_exc, file_name, show_content):\n    '''\n    Optionally constructs an object (AnsibleBaseYAMLObject) to encapsulate the\n    file name/position where a YAML exception occurred, and raises an AnsibleParserError\n    to display the syntax exception information.\n    '''\n\n    # if the YAML exception contains a problem mark, use it to construct\n    # an object the error class can use to display the faulty line\n    err_obj = None\n    if hasattr(yaml_exc, 'problem_mark'):\n        err_obj = AnsibleBaseYAMLObject()\n        err_obj.ansible_pos = (file_name, yaml_exc.problem_mark.line + 1, yaml_exc.problem_mark.column + 1)\n\n    err_msg = 'We were unable to read either as JSON nor YAML, these are the errors we got from each:\\n' \\\n              'JSON: %s\\n\\n' % to_text(json_exc) + YAML_SYNTAX_ERROR % getattr(yaml_exc, 'problem', '')\n\n    raise AnsibleParserError(to_native(err_msg), obj=err_obj, show_content=show_content, orig_exc=yaml_exc)",
        "begin_line": 25,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.utils.yaml._safe_load#45",
        "src_path": "lib/ansible/parsing/utils/yaml.py",
        "class_name": "lib.ansible.parsing.utils.yaml",
        "signature": "lib.ansible.parsing.utils.yaml._safe_load(stream, file_name=None, vault_secrets=None)",
        "snippet": "def _safe_load(stream, file_name=None, vault_secrets=None):\n    ''' Implements yaml.safe_load(), except using our custom loader class. '''\n\n    loader = AnsibleLoader(stream, file_name, vault_secrets)\n    try:\n        return loader.get_single_data()\n    finally:\n        try:\n            loader.dispose()\n        except AttributeError:\n            pass  # older versions of yaml don't have dispose function, ignore",
        "begin_line": 45,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.utils.yaml.from_yaml#58",
        "src_path": "lib/ansible/parsing/utils/yaml.py",
        "class_name": "lib.ansible.parsing.utils.yaml",
        "signature": "lib.ansible.parsing.utils.yaml.from_yaml(data, file_name='<string>', show_content=True, vault_secrets=None, json_only=False)",
        "snippet": "def from_yaml(data, file_name='<string>', show_content=True, vault_secrets=None, json_only=False):\n    '''\n    Creates a python datastructure from the given data, which can be either\n    a JSON or YAML string.\n    '''\n    new_data = None\n\n    try:\n        # in case we have to deal with vaults\n        AnsibleJSONDecoder.set_secrets(vault_secrets)\n\n        # we first try to load this data as JSON.\n        # Fixes issues with extra vars json strings not being parsed correctly by the yaml parser\n        new_data = json.loads(data, cls=AnsibleJSONDecoder)\n    except Exception as json_exc:\n\n        if json_only:\n            raise AnsibleParserError(to_native(json_exc), orig_exc=json_exc)\n\n        # must not be JSON, let the rest try\n        try:\n            new_data = _safe_load(data, file_name=file_name, vault_secrets=vault_secrets)\n        except YAMLError as yaml_exc:\n            _handle_error(json_exc, yaml_exc, file_name, show_content)\n\n    return new_data",
        "begin_line": 58,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.removed.removed_module#14",
        "src_path": "lib/ansible/module_utils/common/removed.py",
        "class_name": "lib.ansible.module_utils.common.removed",
        "signature": "lib.ansible.module_utils.common.removed.removed_module(removed_in, msg='This module has been removed. The module documentation for Ansible-%(version)s may contain hints for porting')",
        "snippet": "def removed_module(removed_in, msg='This module has been removed. The module documentation for'\n                                   ' Ansible-%(version)s may contain hints for porting'):\n    \"\"\"\n    This function is deprecated and should not be used. Instead the module should just be\n    actually removed. This function is scheduled for removal for the 2.12 release.\n\n    Returns module failure along with a message about the module being removed\n\n    :arg removed_in: The version that the module was removed in\n    :kwarg msg: Message to use in the module's failure message. The default says that the module\n        has been removed and what version of the Ansible documentation to search for porting help.\n\n    Remove the actual code and instead have boilerplate like this::\n\n        from ansible.module_utils.common.removed import removed_module\n\n        if __name__ == '__main__':\n            removed_module(\"2.4\")\n    \"\"\"\n    results = {\n        'failed': True,\n        'deprecations': [\n            {\n                'msg': 'The removed_module function is deprecated',\n                'version': '2.12',\n            },\n        ]\n    }\n\n    # Convert numbers into strings\n    removed_in = to_native(removed_in)\n\n    version = removed_in.split('.')\n    try:\n        numeric_minor = int(version[-1])\n    except Exception:\n        last_version = None\n    else:\n        version = version[:-1]\n        version.append(to_native(numeric_minor - 1))\n        last_version = '.'.join(version)\n\n    if last_version is None:\n        results['warnings'] = ['removed modules should specify the version they were removed in']\n        results['msg'] = 'This module has been removed'\n    else:\n        results['msg'] = msg % {'version': last_version}\n\n    print('\\n{0}\\n'.format(json.dumps(results)))\n    sys.exit(1)",
        "begin_line": 14,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.handler.Handler.__init__#31",
        "src_path": "lib/ansible/playbook/handler.py",
        "class_name": "lib.ansible.playbook.handler.Handler",
        "signature": "lib.ansible.playbook.handler.Handler.__init__(self, block=None, role=None, task_include=None)",
        "snippet": "    def __init__(self, block=None, role=None, task_include=None):\n        self.notified_hosts = []\n\n        self.cached_name = False\n\n        super(Handler, self).__init__(block=block, role=role, task_include=task_include)",
        "begin_line": 31,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.handler.Handler.load#43",
        "src_path": "lib/ansible/playbook/handler.py",
        "class_name": "lib.ansible.playbook.handler.Handler",
        "signature": "lib.ansible.playbook.handler.Handler.load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None)",
        "snippet": "    def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n        t = Handler(block=block, role=role, task_include=task_include)\n        return t.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 43,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.handler.Handler.notify_host#47",
        "src_path": "lib/ansible/playbook/handler.py",
        "class_name": "lib.ansible.playbook.handler.Handler",
        "signature": "lib.ansible.playbook.handler.Handler.notify_host(self, host)",
        "snippet": "    def notify_host(self, host):\n        if not self.is_host_notified(host):\n            self.notified_hosts.append(host)\n            return True\n        return False",
        "begin_line": 47,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.handler.Handler.is_host_notified#53",
        "src_path": "lib/ansible/playbook/handler.py",
        "class_name": "lib.ansible.playbook.handler.Handler",
        "signature": "lib.ansible.playbook.handler.Handler.is_host_notified(self, host)",
        "snippet": "    def is_host_notified(self, host):\n        return host in self.notified_hosts",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh._handle_error#318",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh",
        "signature": "lib.ansible.plugins.connection.ssh._handle_error(remaining_retries, command, return_tuple, no_log, host, display=display)",
        "snippet": "def _handle_error(remaining_retries, command, return_tuple, no_log, host, display=display):\n\n    # sshpass errors\n    if command == b'sshpass':\n        # Error 5 is invalid/incorrect password. Raise an exception to prevent retries from locking the account.\n        if return_tuple[0] == 5:\n            msg = 'Invalid/incorrect username/password. Skipping remaining {0} retries to prevent account lockout:'.format(remaining_retries)\n            if remaining_retries <= 0:\n                msg = 'Invalid/incorrect password:'\n            if no_log:\n                msg = '{0} <error censored due to no log>'.format(msg)\n            else:\n                msg = '{0} {1}'.format(msg, to_native(return_tuple[2]).rstrip())\n            raise AnsibleAuthenticationFailure(msg)\n\n        # sshpass returns codes are 1-6. We handle 5 previously, so this catches other scenarios.\n        # No exception is raised, so the connection is retried.\n        elif return_tuple[0] in [1, 2, 3, 4, 6]:\n            msg = 'sshpass error:'\n            if no_log:\n                msg = '{0} <error censored due to no log>'.format(msg)\n            else:\n                msg = '{0} {1}'.format(msg, to_native(return_tuple[2]).rstrip())\n\n    if return_tuple[0] == 255:\n        SSH_ERROR = True\n        for signature in b_NOT_SSH_ERRORS:\n            if signature in return_tuple[1]:\n                SSH_ERROR = False\n                break\n\n        if SSH_ERROR:\n            msg = \"Failed to connect to the host via ssh:\"\n            if no_log:\n                msg = '{0} <error censored due to no log>'.format(msg)\n            else:\n                msg = '{0} {1}'.format(msg, to_native(return_tuple[2]).rstrip())\n            raise AnsibleConnectionFailure(msg)\n\n    # For other errors, no exception is raised so the connection is retried and we only log the messages\n    if 1 <= return_tuple[0] <= 254:\n        msg = u\"Failed to connect to the host via ssh:\"\n        if no_log:\n            msg = u'{0} <error censored due to no log>'.format(msg)\n        else:\n            msg = u'{0} {1}'.format(msg, to_text(return_tuple[2]).rstrip())\n        display.vvv(msg, host=host)",
        "begin_line": 318,
        "end_line": 364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh._ssh_retry#367",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh",
        "signature": "lib.ansible.plugins.connection.ssh._ssh_retry(func)",
        "snippet": "def _ssh_retry(func):\n    \"\"\"\n    Decorator to retry ssh/scp/sftp in the case of a connection failure\n\n    Will retry if:\n    * an exception is caught\n    * ssh returns 255\n    Will not retry if\n    * sshpass returns 5 (invalid password, to prevent account lockouts)\n    * remaining_tries is < 2\n    * retries limit reached\n    \"\"\"\n    @wraps(func)\n    def wrapped(self, *args, **kwargs):\n        remaining_tries = int(C.ANSIBLE_SSH_RETRIES) + 1\n        cmd_summary = u\"%s...\" % to_text(args[0])\n        for attempt in range(remaining_tries):\n            cmd = args[0]\n            if attempt != 0 and self._play_context.password and isinstance(cmd, list):\n                # If this is a retry, the fd/pipe for sshpass is closed, and we need a new one\n                self.sshpass_pipe = os.pipe()\n                cmd[1] = b'-d' + to_bytes(self.sshpass_pipe[0], nonstring='simplerepr', errors='surrogate_or_strict')\n\n            try:\n                try:\n                    return_tuple = func(self, *args, **kwargs)\n                    if self._play_context.no_log:\n                        display.vvv(u'rc=%s, stdout and stderr censored due to no log' % return_tuple[0], host=self.host)\n                    else:\n                        display.vvv(return_tuple, host=self.host)\n                    # 0 = success\n                    # 1-254 = remote command return code\n                    # 255 could be a failure from the ssh command itself\n                except (AnsibleControlPersistBrokenPipeError):\n                    # Retry one more time because of the ControlPersist broken pipe (see #16731)\n                    cmd = args[0]\n                    if self._play_context.password and isinstance(cmd, list):\n                        # This is a retry, so the fd/pipe for sshpass is closed, and we need a new one\n                        self.sshpass_pipe = os.pipe()\n                        cmd[1] = b'-d' + to_bytes(self.sshpass_pipe[0], nonstring='simplerepr', errors='surrogate_or_strict')\n                    display.vvv(u\"RETRYING BECAUSE OF CONTROLPERSIST BROKEN PIPE\")\n                    return_tuple = func(self, *args, **kwargs)\n\n                remaining_retries = remaining_tries - attempt - 1\n                _handle_error(remaining_retries, cmd[0], return_tuple, self._play_context.no_log, self.host)\n\n                break\n\n            # 5 = Invalid/incorrect password from sshpass\n            except AnsibleAuthenticationFailure:\n                # Raising this exception, which is subclassed from AnsibleConnectionFailure, prevents further retries\n                raise\n\n            except (AnsibleConnectionFailure, Exception) as e:\n\n                if attempt == remaining_tries - 1:\n                    raise\n                else:\n                    pause = 2 ** attempt - 1\n                    if pause > 30:\n                        pause = 30\n\n                    if isinstance(e, AnsibleConnectionFailure):\n                        msg = u\"ssh_retry: attempt: %d, ssh return code is 255. cmd (%s), pausing for %d seconds\" % (attempt + 1, cmd_summary, pause)\n                    else:\n                        msg = (u\"ssh_retry: attempt: %d, caught exception(%s) from cmd (%s), \"\n                               u\"pausing for %d seconds\" % (attempt + 1, to_text(e), cmd_summary, pause))\n\n                    display.vv(msg, host=self.host)\n\n                    time.sleep(pause)\n                    continue\n\n        return return_tuple\n    return wrapped",
        "begin_line": 367,
        "end_line": 441,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.wrapped#380",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh",
        "signature": "lib.ansible.plugins.connection.ssh.wrapped(self, *args, **kwargs)",
        "snippet": "    def wrapped(self, *args, **kwargs):\n        remaining_tries = int(C.ANSIBLE_SSH_RETRIES) + 1\n        cmd_summary = u\"%s...\" % to_text(args[0])\n        for attempt in range(remaining_tries):\n            cmd = args[0]\n            if attempt != 0 and self._play_context.password and isinstance(cmd, list):\n                # If this is a retry, the fd/pipe for sshpass is closed, and we need a new one\n                self.sshpass_pipe = os.pipe()\n                cmd[1] = b'-d' + to_bytes(self.sshpass_pipe[0], nonstring='simplerepr', errors='surrogate_or_strict')\n\n            try:\n                try:\n                    return_tuple = func(self, *args, **kwargs)\n                    if self._play_context.no_log:\n                        display.vvv(u'rc=%s, stdout and stderr censored due to no log' % return_tuple[0], host=self.host)\n                    else:\n                        display.vvv(return_tuple, host=self.host)\n                    # 0 = success\n                    # 1-254 = remote command return code\n                    # 255 could be a failure from the ssh command itself\n                except (AnsibleControlPersistBrokenPipeError):\n                    # Retry one more time because of the ControlPersist broken pipe (see #16731)\n                    cmd = args[0]\n                    if self._play_context.password and isinstance(cmd, list):\n                        # This is a retry, so the fd/pipe for sshpass is closed, and we need a new one\n                        self.sshpass_pipe = os.pipe()\n                        cmd[1] = b'-d' + to_bytes(self.sshpass_pipe[0], nonstring='simplerepr', errors='surrogate_or_strict')\n                    display.vvv(u\"RETRYING BECAUSE OF CONTROLPERSIST BROKEN PIPE\")\n                    return_tuple = func(self, *args, **kwargs)\n\n                remaining_retries = remaining_tries - attempt - 1\n                _handle_error(remaining_retries, cmd[0], return_tuple, self._play_context.no_log, self.host)\n\n                break\n\n            # 5 = Invalid/incorrect password from sshpass\n            except AnsibleAuthenticationFailure:\n                # Raising this exception, which is subclassed from AnsibleConnectionFailure, prevents further retries\n                raise\n\n            except (AnsibleConnectionFailure, Exception) as e:\n\n                if attempt == remaining_tries - 1:\n                    raise\n                else:\n                    pause = 2 ** attempt - 1\n                    if pause > 30:\n                        pause = 30\n\n                    if isinstance(e, AnsibleConnectionFailure):\n                        msg = u\"ssh_retry: attempt: %d, ssh return code is 255. cmd (%s), pausing for %d seconds\" % (attempt + 1, cmd_summary, pause)\n                    else:\n                        msg = (u\"ssh_retry: attempt: %d, caught exception(%s) from cmd (%s), \"\n                               u\"pausing for %d seconds\" % (attempt + 1, to_text(e), cmd_summary, pause))\n\n                    display.vv(msg, host=self.host)\n\n                    time.sleep(pause)\n                    continue\n\n        return return_tuple",
        "begin_line": 380,
        "end_line": 440,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection.__init__#450",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(Connection, self).__init__(*args, **kwargs)\n\n        self.host = self._play_context.remote_addr\n        self.port = self._play_context.port\n        self.user = self._play_context.remote_user\n        self.control_path = C.ANSIBLE_SSH_CONTROL_PATH\n        self.control_path_dir = C.ANSIBLE_SSH_CONTROL_PATH_DIR\n\n        # Windows operates differently from a POSIX connection/shell plugin,\n        # we need to set various properties to ensure SSH on Windows continues\n        # to work\n        if getattr(self._shell, \"_IS_WINDOWS\", False):\n            self.has_native_async = True\n            self.always_pipeline_modules = True\n            self.module_implementation_preferences = ('.ps1', '.exe', '')\n            self.allow_executable = False",
        "begin_line": 450,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._connect#472",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._connect(self)",
        "snippet": "    def _connect(self):\n        return self",
        "begin_line": 472,
        "end_line": 473,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._create_control_path#476",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._create_control_path(host, port, user, connection=None, pid=None)",
        "snippet": "    def _create_control_path(host, port, user, connection=None, pid=None):\n        '''Make a hash for the controlpath based on con attributes'''\n        pstring = '%s-%s-%s' % (host, port, user)\n        if connection:\n            pstring += '-%s' % connection\n        if pid:\n            pstring += '-%s' % to_text(pid)\n        m = hashlib.sha1()\n        m.update(to_bytes(pstring))\n        digest = m.hexdigest()\n        cpath = '%(directory)s/' + digest[:10]\n        return cpath",
        "begin_line": 476,
        "end_line": 487,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._sshpass_available#490",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._sshpass_available()",
        "snippet": "    def _sshpass_available():\n        global SSHPASS_AVAILABLE\n\n        # We test once if sshpass is available, and remember the result. It\n        # would be nice to use distutils.spawn.find_executable for this, but\n        # distutils isn't always available; shutils.which() is Python3-only.\n\n        if SSHPASS_AVAILABLE is None:\n            try:\n                p = subprocess.Popen([\"sshpass\"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                p.communicate()\n                SSHPASS_AVAILABLE = True\n            except OSError:\n                SSHPASS_AVAILABLE = False\n\n        return SSHPASS_AVAILABLE",
        "begin_line": 490,
        "end_line": 505,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._persistence_controls#508",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._persistence_controls(b_command)",
        "snippet": "    def _persistence_controls(b_command):\n        '''\n        Takes a command array and scans it for ControlPersist and ControlPath\n        settings and returns two booleans indicating whether either was found.\n        This could be smarter, e.g. returning false if ControlPersist is 'no',\n        but for now we do it simple way.\n        '''\n\n        controlpersist = False\n        controlpath = False\n\n        for b_arg in (a.lower() for a in b_command):\n            if b'controlpersist' in b_arg:\n                controlpersist = True\n            elif b'controlpath' in b_arg:\n                controlpath = True\n\n        return controlpersist, controlpath",
        "begin_line": 508,
        "end_line": 525,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._add_args#527",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._add_args(self, b_command, b_args, explanation)",
        "snippet": "    def _add_args(self, b_command, b_args, explanation):\n        \"\"\"\n        Adds arguments to the ssh command and displays a caller-supplied explanation of why.\n\n        :arg b_command: A list containing the command to add the new arguments to.\n            This list will be modified by this method.\n        :arg b_args: An iterable of new arguments to add.  This iterable is used\n            more than once so it must be persistent (ie: a list is okay but a\n            StringIO would not)\n        :arg explanation: A text string containing explaining why the arguments\n            were added.  It will be displayed with a high enough verbosity.\n        .. note:: This function does its work via side-effect.  The b_command list has the new arguments appended.\n        \"\"\"\n        display.vvvvv(u'SSH: %s: (%s)' % (explanation, ')('.join(to_text(a) for a in b_args)), host=self._play_context.remote_addr)\n        b_command += b_args",
        "begin_line": 527,
        "end_line": 541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._build_command#543",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._build_command(self, binary, *other_args)",
        "snippet": "    def _build_command(self, binary, *other_args):\n        '''\n        Takes a binary (ssh, scp, sftp) and optional extra arguments and returns\n        a command line as an array that can be passed to subprocess.Popen.\n        '''\n\n        b_command = []\n\n        #\n        # First, the command to invoke\n        #\n\n        # If we want to use password authentication, we have to set up a pipe to\n        # write the password to sshpass.\n\n        if self._play_context.password:\n            if not self._sshpass_available():\n                raise AnsibleError(\"to use the 'ssh' connection type with passwords, you must install the sshpass program\")\n\n            self.sshpass_pipe = os.pipe()\n            b_command += [b'sshpass', b'-d' + to_bytes(self.sshpass_pipe[0], nonstring='simplerepr', errors='surrogate_or_strict')]\n\n        if binary == 'ssh':\n            b_command += [to_bytes(self._play_context.ssh_executable, errors='surrogate_or_strict')]\n        else:\n            b_command += [to_bytes(binary, errors='surrogate_or_strict')]\n\n        #\n        # Next, additional arguments based on the configuration.\n        #\n\n        # sftp batch mode allows us to correctly catch failed transfers, but can\n        # be disabled if the client side doesn't support the option. However,\n        # sftp batch mode does not prompt for passwords so it must be disabled\n        # if not using controlpersist and using sshpass\n        if binary == 'sftp' and C.DEFAULT_SFTP_BATCH_MODE:\n            if self._play_context.password:\n                b_args = [b'-o', b'BatchMode=no']\n                self._add_args(b_command, b_args, u'disable batch mode for sshpass')\n            b_command += [b'-b', b'-']\n\n        if self._play_context.verbosity > 3:\n            b_command.append(b'-vvv')\n\n        #\n        # Next, we add [ssh_connection]ssh_args from ansible.cfg.\n        #\n\n        if self._play_context.ssh_args:\n            b_args = [to_bytes(a, errors='surrogate_or_strict') for a in\n                      self._split_ssh_args(self._play_context.ssh_args)]\n            self._add_args(b_command, b_args, u\"ansible.cfg set ssh_args\")\n\n        # Now we add various arguments controlled by configuration file settings\n        # (e.g. host_key_checking) or inventory variables (ansible_ssh_port) or\n        # a combination thereof.\n\n        if not C.HOST_KEY_CHECKING:\n            b_args = (b\"-o\", b\"StrictHostKeyChecking=no\")\n            self._add_args(b_command, b_args, u\"ANSIBLE_HOST_KEY_CHECKING/host_key_checking disabled\")\n\n        if self._play_context.port is not None:\n            b_args = (b\"-o\", b\"Port=\" + to_bytes(self._play_context.port, nonstring='simplerepr', errors='surrogate_or_strict'))\n            self._add_args(b_command, b_args, u\"ANSIBLE_REMOTE_PORT/remote_port/ansible_port set\")\n\n        key = self._play_context.private_key_file\n        if key:\n            b_args = (b\"-o\", b'IdentityFile=\"' + to_bytes(os.path.expanduser(key), errors='surrogate_or_strict') + b'\"')\n            self._add_args(b_command, b_args, u\"ANSIBLE_PRIVATE_KEY_FILE/private_key_file/ansible_ssh_private_key_file set\")\n\n        if not self._play_context.password:\n            self._add_args(\n                b_command, (\n                    b\"-o\", b\"KbdInteractiveAuthentication=no\",\n                    b\"-o\", b\"PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey\",\n                    b\"-o\", b\"PasswordAuthentication=no\"\n                ),\n                u\"ansible_password/ansible_ssh_password not set\"\n            )\n\n        user = self._play_context.remote_user\n        if user:\n            self._add_args(\n                b_command,\n                (b\"-o\", b'User=\"%s\"' % to_bytes(self._play_context.remote_user, errors='surrogate_or_strict')),\n                u\"ANSIBLE_REMOTE_USER/remote_user/ansible_user/user/-u set\"\n            )\n\n        self._add_args(\n            b_command,\n            (b\"-o\", b\"ConnectTimeout=\" + to_bytes(self._play_context.timeout, errors='surrogate_or_strict', nonstring='simplerepr')),\n            u\"ANSIBLE_TIMEOUT/timeout set\"\n        )\n\n        # Add in any common or binary-specific arguments from the PlayContext\n        # (i.e. inventory or task settings or overrides on the command line).\n\n        for opt in (u'ssh_common_args', u'{0}_extra_args'.format(binary)):\n            attr = getattr(self._play_context, opt, None)\n            if attr is not None:\n                b_args = [to_bytes(a, errors='surrogate_or_strict') for a in self._split_ssh_args(attr)]\n                self._add_args(b_command, b_args, u\"PlayContext set %s\" % opt)\n\n        # Check if ControlPersist is enabled and add a ControlPath if one hasn't\n        # already been set.\n\n        controlpersist, controlpath = self._persistence_controls(b_command)\n\n        if controlpersist:\n            self._persistent = True\n\n            if not controlpath:\n                cpdir = unfrackpath(self.control_path_dir)\n                b_cpdir = to_bytes(cpdir, errors='surrogate_or_strict')\n\n                # The directory must exist and be writable.\n                makedirs_safe(b_cpdir, 0o700)\n                if not os.access(b_cpdir, os.W_OK):\n                    raise AnsibleError(\"Cannot write to ControlPath %s\" % to_native(cpdir))\n\n                if not self.control_path:\n                    self.control_path = self._create_control_path(\n                        self.host,\n                        self.port,\n                        self.user\n                    )\n                b_args = (b\"-o\", b\"ControlPath=\" + to_bytes(self.control_path % dict(directory=cpdir), errors='surrogate_or_strict'))\n                self._add_args(b_command, b_args, u\"found only ControlPersist; added ControlPath\")\n\n        # Finally, we add any caller-supplied extras.\n        if other_args:\n            b_command += [to_bytes(a) for a in other_args]\n\n        return b_command",
        "begin_line": 543,
        "end_line": 676,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._examine_output#714",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._examine_output(self, source, state, b_chunk, sudoable)",
        "snippet": "    def _examine_output(self, source, state, b_chunk, sudoable):\n        '''\n        Takes a string, extracts complete lines from it, tests to see if they\n        are a prompt, error message, etc., and sets appropriate flags in self.\n        Prompt and success lines are removed.\n\n        Returns the processed (i.e. possibly-edited) output and the unprocessed\n        remainder (to be processed with the next chunk) as strings.\n        '''\n\n        output = []\n        for b_line in b_chunk.splitlines(True):\n            display_line = to_text(b_line).rstrip('\\r\\n')\n            suppress_output = False\n\n            # display.debug(\"Examining line (source=%s, state=%s): '%s'\" % (source, state, display_line))\n            if self.become.expect_prompt() and self.become.check_password_prompt(b_line):\n                display.debug(u\"become_prompt: (source=%s, state=%s): '%s'\" % (source, state, display_line))\n                self._flags['become_prompt'] = True\n                suppress_output = True\n            elif self.become.success and self.become.check_success(b_line):\n                display.debug(u\"become_success: (source=%s, state=%s): '%s'\" % (source, state, display_line))\n                self._flags['become_success'] = True\n                suppress_output = True\n            elif sudoable and self.become.check_incorrect_password(b_line):\n                display.debug(u\"become_error: (source=%s, state=%s): '%s'\" % (source, state, display_line))\n                self._flags['become_error'] = True\n            elif sudoable and self.become.check_missing_password(b_line):\n                display.debug(u\"become_nopasswd_error: (source=%s, state=%s): '%s'\" % (source, state, display_line))\n                self._flags['become_nopasswd_error'] = True\n\n            if not suppress_output:\n                output.append(b_line)\n\n        # The chunk we read was most likely a series of complete lines, but just\n        # in case the last line was incomplete (and not a prompt, which we would\n        # have removed from the output), we retain it to be processed with the\n        # next chunk.\n\n        remainder = b''\n        if output and not output[-1].endswith(b'\\n'):\n            remainder = output[-1]\n            output = output[:-1]\n\n        return b''.join(output), remainder",
        "begin_line": 714,
        "end_line": 758,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection._file_transport_command#1056",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection._file_transport_command(self, in_path, out_path, sftp_action)",
        "snippet": "    def _file_transport_command(self, in_path, out_path, sftp_action):\n        # scp and sftp require square brackets for IPv6 addresses, but\n        # accept them for hostnames and IPv4 addresses too.\n        host = '[%s]' % self.host\n\n        smart_methods = ['sftp', 'scp', 'piped']\n\n        # Windows does not support dd so we cannot use the piped method\n        if getattr(self._shell, \"_IS_WINDOWS\", False):\n            smart_methods.remove('piped')\n\n        # Transfer methods to try\n        methods = []\n\n        # Use the transfer_method option if set, otherwise use scp_if_ssh\n        ssh_transfer_method = self._play_context.ssh_transfer_method\n        if ssh_transfer_method is not None:\n            if not (ssh_transfer_method in ('smart', 'sftp', 'scp', 'piped')):\n                raise AnsibleOptionsError('transfer_method needs to be one of [smart|sftp|scp|piped]')\n            if ssh_transfer_method == 'smart':\n                methods = smart_methods\n            else:\n                methods = [ssh_transfer_method]\n        else:\n            # since this can be a non-bool now, we need to handle it correctly\n            scp_if_ssh = C.DEFAULT_SCP_IF_SSH\n            if not isinstance(scp_if_ssh, bool):\n                scp_if_ssh = scp_if_ssh.lower()\n                if scp_if_ssh in BOOLEANS:\n                    scp_if_ssh = boolean(scp_if_ssh, strict=False)\n                elif scp_if_ssh != 'smart':\n                    raise AnsibleOptionsError('scp_if_ssh needs to be one of [smart|True|False]')\n            if scp_if_ssh == 'smart':\n                methods = smart_methods\n            elif scp_if_ssh is True:\n                methods = ['scp']\n            else:\n                methods = ['sftp']\n\n        for method in methods:\n            returncode = stdout = stderr = None\n            if method == 'sftp':\n                cmd = self._build_command(self.get_option('sftp_executable'), to_bytes(host))\n                in_data = u\"{0} {1} {2}\\n\".format(sftp_action, shlex_quote(in_path), shlex_quote(out_path))\n                in_data = to_bytes(in_data, nonstring='passthru')\n                (returncode, stdout, stderr) = self._bare_run(cmd, in_data, checkrc=False)\n            elif method == 'scp':\n                scp = self.get_option('scp_executable')\n\n                if sftp_action == 'get':\n                    cmd = self._build_command(scp, u'{0}:{1}'.format(host, self._shell.quote(in_path)), out_path)\n                else:\n                    cmd = self._build_command(scp, in_path, u'{0}:{1}'.format(host, self._shell.quote(out_path)))\n                in_data = None\n                (returncode, stdout, stderr) = self._bare_run(cmd, in_data, checkrc=False)\n            elif method == 'piped':\n                if sftp_action == 'get':\n                    # we pass sudoable=False to disable pty allocation, which\n                    # would end up mixing stdout/stderr and screwing with newlines\n                    (returncode, stdout, stderr) = self.exec_command('dd if=%s bs=%s' % (in_path, BUFSIZE), sudoable=False)\n                    with open(to_bytes(out_path, errors='surrogate_or_strict'), 'wb+') as out_file:\n                        out_file.write(stdout)\n                else:\n                    with open(to_bytes(in_path, errors='surrogate_or_strict'), 'rb') as f:\n                        in_data = to_bytes(f.read(), nonstring='passthru')\n                    if not in_data:\n                        count = ' count=0'\n                    else:\n                        count = ''\n                    (returncode, stdout, stderr) = self.exec_command('dd of=%s bs=%s%s' % (out_path, BUFSIZE, count), in_data=in_data, sudoable=False)\n\n            # Check the return code and rollover to next method if failed\n            if returncode == 0:\n                return (returncode, stdout, stderr)\n            else:\n                # If not in smart mode, the data will be printed by the raise below\n                if len(methods) > 1:\n                    display.warning(u'%s transfer mechanism failed on %s. Use ANSIBLE_DEBUG=1 to see detailed information' % (method, host))\n                    display.debug(u'%s' % to_text(stdout))\n                    display.debug(u'%s' % to_text(stderr))\n\n        if returncode == 255:\n            raise AnsibleConnectionFailure(\"Failed to connect to the host via %s: %s\" % (method, to_native(stderr)))\n        else:\n            raise AnsibleError(\"failed to transfer file to %s %s:\\n%s\\n%s\" %\n                               (to_native(in_path), to_native(out_path), to_native(stdout), to_native(stderr)))",
        "begin_line": 1056,
        "end_line": 1141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection.exec_command#1156",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection.exec_command(self, cmd, in_data=None, sudoable=True)",
        "snippet": "    def exec_command(self, cmd, in_data=None, sudoable=True):\n        ''' run a command on the remote host '''\n\n        super(Connection, self).exec_command(cmd, in_data=in_data, sudoable=sudoable)\n\n        display.vvv(u\"ESTABLISH SSH CONNECTION FOR USER: {0}\".format(self._play_context.remote_user), host=self._play_context.remote_addr)\n\n        if getattr(self._shell, \"_IS_WINDOWS\", False):\n            # Become method 'runas' is done in the wrapper that is executed,\n            # need to disable sudoable so the bare_run is not waiting for a\n            # prompt that will not occur\n            sudoable = False\n\n            # Make sure our first command is to set the console encoding to\n            # utf-8, this must be done via chcp to get utf-8 (65001)\n            cmd_parts = [\"chcp.com\", \"65001\", self._shell._SHELL_REDIRECT_ALLNULL, self._shell._SHELL_AND]\n            cmd_parts.extend(self._shell._encode_script(cmd, as_list=True, strict_mode=False, preserve_rc=False))\n            cmd = ' '.join(cmd_parts)\n\n        # we can only use tty when we are not pipelining the modules. piping\n        # data into /usr/bin/python inside a tty automatically invokes the\n        # python interactive-mode but the modules are not compatible with the\n        # interactive-mode (\"unexpected indent\" mainly because of empty lines)\n\n        ssh_executable = self._play_context.ssh_executable\n\n        # -tt can cause various issues in some environments so allow the user\n        # to disable it as a troubleshooting method.\n        use_tty = self.get_option('use_tty')\n\n        if not in_data and sudoable and use_tty:\n            args = (ssh_executable, '-tt', self.host, cmd)\n        else:\n            args = (ssh_executable, self.host, cmd)\n\n        cmd = self._build_command(*args)\n        (returncode, stdout, stderr) = self._run(cmd, in_data, sudoable=sudoable)\n\n        # When running on Windows, stderr may contain CLIXML encoded output\n        if getattr(self._shell, \"_IS_WINDOWS\", False) and stderr.startswith(b\"#< CLIXML\"):\n            stderr = _parse_clixml(stderr)\n\n        return (returncode, stdout, stderr)",
        "begin_line": 1156,
        "end_line": 1198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection.put_file#1200",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection.put_file(self, in_path, out_path)",
        "snippet": "    def put_file(self, in_path, out_path):\n        ''' transfer a file from local to remote '''\n\n        super(Connection, self).put_file(in_path, out_path)\n\n        display.vvv(u\"PUT {0} TO {1}\".format(in_path, out_path), host=self.host)\n        if not os.path.exists(to_bytes(in_path, errors='surrogate_or_strict')):\n            raise AnsibleFileNotFound(\"file or module does not exist: {0}\".format(to_native(in_path)))\n\n        if getattr(self._shell, \"_IS_WINDOWS\", False):\n            out_path = self._escape_win_path(out_path)\n\n        return self._file_transport_command(in_path, out_path, 'put')",
        "begin_line": 1200,
        "end_line": 1212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection.fetch_file#1214",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection.fetch_file(self, in_path, out_path)",
        "snippet": "    def fetch_file(self, in_path, out_path):\n        ''' fetch a file from remote to local '''\n\n        super(Connection, self).fetch_file(in_path, out_path)\n\n        display.vvv(u\"FETCH {0} TO {1}\".format(in_path, out_path), host=self.host)\n\n        # need to add / if path is rooted\n        if getattr(self._shell, \"_IS_WINDOWS\", False):\n            in_path = self._escape_win_path(in_path)\n\n        return self._file_transport_command(in_path, out_path, 'get')",
        "begin_line": 1214,
        "end_line": 1225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.ssh.Connection.close#1253",
        "src_path": "lib/ansible/plugins/connection/ssh.py",
        "class_name": "lib.ansible.plugins.connection.ssh.Connection",
        "signature": "lib.ansible.plugins.connection.ssh.Connection.close(self)",
        "snippet": "    def close(self):\n        self._connected = False",
        "begin_line": 1253,
        "end_line": 1254,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.user_agent.user_agent#13",
        "src_path": "lib/ansible/galaxy/user_agent.py",
        "class_name": "lib.ansible.galaxy.user_agent",
        "signature": "lib.ansible.galaxy.user_agent.user_agent()",
        "snippet": "def user_agent():\n    \"\"\"Returns a user agent used by ansible-galaxy to include the Ansible version, platform and python version.\"\"\"\n\n    python_version = sys.version_info\n    return u\"ansible-galaxy/{ansible_version} ({platform}; python:{py_major}.{py_minor}.{py_micro})\".format(\n        ansible_version=ansible_version,\n        platform=platform.system(),\n        py_major=python_version.major,\n        py_minor=python_version.minor,\n        py_micro=python_version.micro,\n    )",
        "begin_line": 13,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006060606060606061,
            "pseudo_dstar_susp": 0.008064516129032258,
            "pseudo_tarantula_susp": 0.0035842293906810036,
            "pseudo_op2_susp": 0.008064516129032258,
            "pseudo_barinel_susp": 0.0035842293906810036
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.loader.AnsibleLoader.__init__#46",
        "src_path": "lib/ansible/parsing/yaml/loader.py",
        "class_name": "lib.ansible.parsing.yaml.loader.AnsibleLoader",
        "signature": "lib.ansible.parsing.yaml.loader.AnsibleLoader.__init__(self, stream, file_name=None, vault_secrets=None)",
        "snippet": "        def __init__(self, stream, file_name=None, vault_secrets=None):\n            Reader.__init__(self, stream)\n            Scanner.__init__(self)\n            Parser.__init__(self)\n            Composer.__init__(self)\n            AnsibleConstructor.__init__(self, file_name=file_name, vault_secrets=vault_secrets)\n            Resolver.__init__(self)",
        "begin_line": 46,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.998880179171333e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.base.Hardware.__init__#39",
        "src_path": "lib/ansible/module_utils/facts/hardware/base.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.base.Hardware",
        "signature": "lib.ansible.module_utils.facts.hardware.base.Hardware.__init__(self, module, load_on_init=False)",
        "snippet": "    def __init__(self, module, load_on_init=False):\n        self.module = module",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.21240533717995e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.base.Hardware.populate#42",
        "src_path": "lib/ansible/module_utils/facts/hardware/base.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.base.Hardware",
        "signature": "lib.ansible.module_utils.facts.hardware.base.Hardware.populate(self, collected_facts=None)",
        "snippet": "    def populate(self, collected_facts=None):\n        return {}",
        "begin_line": 42,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.hardware.base.HardwareCollector.collect#56",
        "src_path": "lib/ansible/module_utils/facts/hardware/base.py",
        "class_name": "lib.ansible.module_utils.facts.hardware.base.HardwareCollector",
        "signature": "lib.ansible.module_utils.facts.hardware.base.HardwareCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        collected_facts = collected_facts or {}\n        if not module:\n            return {}\n\n        # Network munges cached_facts by side effect, so give it a copy\n        facts_obj = self._fact_class(module)\n\n        facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n        return facts_dict",
        "begin_line": 56,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.process.get_bin_path#12",
        "src_path": "lib/ansible/module_utils/common/process.py",
        "class_name": "lib.ansible.module_utils.common.process",
        "signature": "lib.ansible.module_utils.common.process.get_bin_path(arg, opt_dirs=None, required=None)",
        "snippet": "def get_bin_path(arg, opt_dirs=None, required=None):\n    '''\n    Find system executable in PATH. Raises ValueError if executable is not found.\n    Optional arguments:\n       - required:  [Deprecated] Prior to 2.10, if executable is not found and required is true it raises an Exception.\n                    In 2.10 and later, an Exception is always raised. This parameter will be removed in 2.14.\n       - opt_dirs:  optional list of directories to search in addition to PATH\n    If found return full path, otherwise raise ValueError.\n    '''\n    opt_dirs = [] if opt_dirs is None else opt_dirs\n\n    sbin_paths = ['/sbin', '/usr/sbin', '/usr/local/sbin']\n    paths = []\n    for d in opt_dirs:\n        if d is not None and os.path.exists(d):\n            paths.append(d)\n    paths += os.environ.get('PATH', '').split(os.pathsep)\n    bin_path = None\n    # mangle PATH to include /sbin dirs\n    for p in sbin_paths:\n        if p not in paths and os.path.exists(p):\n            paths.append(p)\n    for d in paths:\n        if not d:\n            continue\n        path = os.path.join(d, arg)\n        if os.path.exists(path) and not os.path.isdir(path) and is_executable(path):\n            bin_path = path\n            break\n    if bin_path is None:\n        raise ValueError('Failed to find required executable %s in paths: %s' % (arg, os.pathsep.join(paths)))\n\n    return bin_path",
        "begin_line": 12,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__init__#16",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._store = dict(*args, **kwargs)",
        "begin_line": 16,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011299435028248588,
            "pseudo_dstar_susp": 0.001483679525222552,
            "pseudo_tarantula_susp": 0.0009250693802035153,
            "pseudo_op2_susp": 0.001483679525222552,
            "pseudo_barinel_susp": 0.0009250693802035153
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__getitem__#19",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._store[key]",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003389830508474576,
            "pseudo_dstar_susp": 0.25,
            "pseudo_tarantula_susp": 0.0009643201542912247,
            "pseudo_op2_susp": 0.25,
            "pseudo_barinel_susp": 0.0009643201542912247
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__iter__#22",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return self._store.__iter__()",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__len__#25",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__len__(self)",
        "snippet": "    def __len__(self):\n        return self._store.__len__()",
        "begin_line": 25,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__hash__#28",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(frozenset(self.items()))",
        "begin_line": 28,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.ImmutableDict.__repr__#31",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections.ImmutableDict",
        "signature": "lib.ansible.module_utils.common.collections.ImmutableDict.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'ImmutableDict({0})'.format(repr(self._store))",
        "begin_line": 31,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.043478260869565216,
            "pseudo_dstar_susp": 0.00390625,
            "pseudo_tarantula_susp": 0.008771929824561403,
            "pseudo_op2_susp": 0.00390625,
            "pseudo_barinel_susp": 0.008771929824561403
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.is_string#59",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections",
        "signature": "lib.ansible.module_utils.common.collections.is_string(seq)",
        "snippet": "def is_string(seq):\n    \"\"\"Identify whether the input has a string-like type (inclding bytes).\"\"\"\n    return isinstance(seq, (text_type, binary_type))",
        "begin_line": 59,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.862946949420081e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.is_iterable#64",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections",
        "signature": "lib.ansible.module_utils.common.collections.is_iterable(seq, include_strings=False)",
        "snippet": "def is_iterable(seq, include_strings=False):\n    \"\"\"Identify whether the input is an iterable.\"\"\"\n    if not include_strings and is_string(seq):\n        return False\n\n    try:\n        iter(seq)\n        return True\n    except TypeError:\n        return False",
        "begin_line": 64,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.collections.is_sequence#76",
        "src_path": "lib/ansible/module_utils/common/collections.py",
        "class_name": "lib.ansible.module_utils.common.collections",
        "signature": "lib.ansible.module_utils.common.collections.is_sequence(seq, include_strings=False)",
        "snippet": "def is_sequence(seq, include_strings=False):\n    \"\"\"Identify whether the input is a sequence.\n\n    Strings and bytes are not sequences here,\n    unless ``include_string`` is ``True``.\n\n    Non-indexable things are never of a sequence type.\n    \"\"\"\n    if not include_strings and is_string(seq):\n        return False\n\n    return isinstance(seq, Sequence)",
        "begin_line": 76,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.993985172751433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.FactCache.__init__#47",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.FactCache",
        "signature": "lib.ansible.plugins.cache.__init__.FactCache.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        display.deprecated('ansible.plugins.cache.FactCache has been moved to'\n                           ' ansible.vars.fact_cache.FactCache.  If you are looking for the class'\n                           ' to subclass for a cache plugin, you want'\n                           ' ansible.plugins.cache.BaseCacheModule or one of its subclasses.',\n                           version='2.12')\n        super(FactCache, self).__init__(*args, **kwargs)",
        "begin_line": 47,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.BaseCacheModule.__init__#61",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.BaseCacheModule",
        "signature": "lib.ansible.plugins.cache.__init__.BaseCacheModule.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        # Third party code is not using cache_loader to load plugin - fall back to previous behavior\n        if not hasattr(self, '_load_name'):\n            display.deprecated('Rather than importing custom CacheModules directly, use ansible.plugins.loader.cache_loader', version='2.14')\n            self._load_name = self.__module__.split('.')[-1]\n        super(BaseCacheModule, self).__init__()\n        self.set_options(var_options=args, direct=kwargs)",
        "begin_line": 61,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator._do_load_key#309",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator._do_load_key(self, key)",
        "snippet": "    def _do_load_key(self, key):\n        load = False\n        if key not in self._cache and key not in self._retrieved and self._plugin_name != 'memory':\n            if isinstance(self._plugin, BaseFileCacheModule):\n                load = True\n            elif not isinstance(self._plugin, BaseFileCacheModule) and self._plugin.contains(key):\n                # Database-backed caches don't raise KeyError for expired keys, so only load if the key is valid by checking contains()\n                load = True\n        return load",
        "begin_line": 309,
        "end_line": 317,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.__getitem__#319",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if self._do_load_key(key):\n            try:\n                self._cache[key] = self._plugin.get(key)\n            except KeyError:\n                pass\n            else:\n                self._retrieved[key] = self._cache[key]\n        return self._cache[key]",
        "begin_line": 319,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.get#329",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.get(self, key, default=None)",
        "snippet": "    def get(self, key, default=None):\n        if self._do_load_key(key):\n            try:\n                self._cache[key] = self._plugin.get(key)\n            except KeyError as e:\n                pass\n            else:\n                self._retrieved[key] = self._cache[key]\n        return self._cache.get(key, default)",
        "begin_line": 329,
        "end_line": 337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.pop#348",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        if args:\n            return self._cache.pop(key, args[0])\n        return self._cache.pop(key)",
        "begin_line": 348,
        "end_line": 351,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.__setitem__#356",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        self._cache[key] = value",
        "begin_line": 356,
        "end_line": 357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.update#364",
        "src_path": "lib/ansible/plugins/cache/__init__.py",
        "class_name": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator",
        "signature": "lib.ansible.plugins.cache.__init__.CachePluginAdjudicator.update(self, value)",
        "snippet": "    def update(self, value):\n        self._cache.update(value)",
        "begin_line": 364,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.stats.AggregateStats.__init__#29",
        "src_path": "lib/ansible/executor/stats.py",
        "class_name": "lib.ansible.executor.stats.AggregateStats",
        "signature": "lib.ansible.executor.stats.AggregateStats.__init__(self)",
        "snippet": "    def __init__(self):\n\n        self.processed = {}\n        self.failures = {}\n        self.ok = {}\n        self.dark = {}\n        self.changed = {}\n        self.skipped = {}\n        self.rescued = {}\n        self.ignored = {}\n\n        # user defined stats, which can be per host or global\n        self.custom = {}",
        "begin_line": 29,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.get_platform#274",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.get_platform()",
        "snippet": "def get_platform():\n    '''\n    **Deprecated** Use :py:func:`platform.system` directly.\n\n    :returns: Name of the platform the module is running on in a native string\n\n    Returns a native string that labels the platform (\"Linux\", \"Solaris\", etc). Currently, this is\n    the result of calling :py:func:`platform.system`.\n    '''\n    return platform.system()",
        "begin_line": 274,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.load_platform_subclass#292",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.load_platform_subclass(cls, *args, **kwargs)",
        "snippet": "def load_platform_subclass(cls, *args, **kwargs):\n    \"\"\"**Deprecated**: Use ansible.module_utils.common.sys_info.get_platform_subclass instead\"\"\"\n    platform_cls = get_platform_subclass(cls)\n    return super(cls, platform_cls).__new__(platform_cls)",
        "begin_line": 292,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.get_all_subclasses#298",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.get_all_subclasses(cls)",
        "snippet": "def get_all_subclasses(cls):\n    \"\"\"**Deprecated**: Use ansible.module_utils.common._utils.get_all_subclasses instead\"\"\"\n    return list(_get_all_subclasses(cls))",
        "begin_line": 298,
        "end_line": 300,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic._remove_values_conditions#306",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic._remove_values_conditions(value, no_log_strings, deferred_removals)",
        "snippet": "def _remove_values_conditions(value, no_log_strings, deferred_removals):\n    \"\"\"\n    Helper function for :meth:`remove_values`.\n\n    :arg value: The value to check for strings that need to be stripped\n    :arg no_log_strings: set of strings which must be stripped out of any values\n    :arg deferred_removals: List which holds information about nested\n        containers that have to be iterated for removals.  It is passed into\n        this function so that more entries can be added to it if value is\n        a container type.  The format of each entry is a 2-tuple where the first\n        element is the ``value`` parameter and the second value is a new\n        container to copy the elements of ``value`` into once iterated.\n    :returns: if ``value`` is a scalar, returns ``value`` with two exceptions:\n        1. :class:`~datetime.datetime` objects which are changed into a string representation.\n        2. objects which are in no_log_strings are replaced with a placeholder\n            so that no sensitive data is leaked.\n        If ``value`` is a container type, returns a new empty container.\n\n    ``deferred_removals`` is added to as a side-effect of this function.\n\n    .. warning:: It is up to the caller to make sure the order in which value\n        is passed in is correct.  For instance, higher level containers need\n        to be passed in before lower level containers. For example, given\n        ``{'level1': {'level2': 'level3': [True]} }`` first pass in the\n        dictionary for ``level1``, then the dict for ``level2``, and finally\n        the list for ``level3``.\n    \"\"\"\n    if isinstance(value, (text_type, binary_type)):\n        # Need native str type\n        native_str_value = value\n        if isinstance(value, text_type):\n            value_is_text = True\n            if PY2:\n                native_str_value = to_bytes(value, errors='surrogate_or_strict')\n        elif isinstance(value, binary_type):\n            value_is_text = False\n            if PY3:\n                native_str_value = to_text(value, errors='surrogate_or_strict')\n\n        if native_str_value in no_log_strings:\n            return 'VALUE_SPECIFIED_IN_NO_LOG_PARAMETER'\n        for omit_me in no_log_strings:\n            native_str_value = native_str_value.replace(omit_me, '*' * 8)\n\n        if value_is_text and isinstance(native_str_value, binary_type):\n            value = to_text(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n        elif not value_is_text and isinstance(native_str_value, text_type):\n            value = to_bytes(native_str_value, encoding='utf-8', errors='surrogate_then_replace')\n        else:\n            value = native_str_value\n\n    elif isinstance(value, Sequence):\n        if isinstance(value, MutableSequence):\n            new_value = type(value)()\n        else:\n            new_value = []  # Need a mutable value\n        deferred_removals.append((value, new_value))\n        value = new_value\n\n    elif isinstance(value, Set):\n        if isinstance(value, MutableSet):\n            new_value = type(value)()\n        else:\n            new_value = set()  # Need a mutable value\n        deferred_removals.append((value, new_value))\n        value = new_value\n\n    elif isinstance(value, Mapping):\n        if isinstance(value, MutableMapping):\n            new_value = type(value)()\n        else:\n            new_value = {}  # Need a mutable value\n        deferred_removals.append((value, new_value))\n        value = new_value\n\n    elif isinstance(value, tuple(chain(integer_types, (float, bool, NoneType)))):\n        stringy_value = to_native(value, encoding='utf-8', errors='surrogate_or_strict')\n        if stringy_value in no_log_strings:\n            return 'VALUE_SPECIFIED_IN_NO_LOG_PARAMETER'\n        for omit_me in no_log_strings:\n            if omit_me in stringy_value:\n                return 'VALUE_SPECIFIED_IN_NO_LOG_PARAMETER'\n\n    elif isinstance(value, datetime.datetime):\n        value = value.isoformat()\n    else:\n        raise TypeError('Value of unknown type: %s, %s' % (type(value), value))\n\n    return value",
        "begin_line": 306,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.remove_values#397",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.remove_values(value, no_log_strings)",
        "snippet": "def remove_values(value, no_log_strings):\n    \"\"\" Remove strings in no_log_strings from value.  If value is a container\n    type, then remove a lot more\"\"\"\n    deferred_removals = deque()\n\n    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n    new_value = _remove_values_conditions(value, no_log_strings, deferred_removals)\n\n    while deferred_removals:\n        old_data, new_data = deferred_removals.popleft()\n        if isinstance(new_data, Mapping):\n            for old_key, old_elem in old_data.items():\n                new_elem = _remove_values_conditions(old_elem, no_log_strings, deferred_removals)\n                new_data[old_key] = new_elem\n        else:\n            for elem in old_data:\n                new_elem = _remove_values_conditions(elem, no_log_strings, deferred_removals)\n                if isinstance(new_data, MutableSequence):\n                    new_data.append(new_elem)\n                elif isinstance(new_data, MutableSet):\n                    new_data.add(new_elem)\n                else:\n                    raise TypeError('Unknown container type encountered when removing private values from output')\n\n    return new_value",
        "begin_line": 397,
        "end_line": 421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.heuristic_log_sanitize#424",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.heuristic_log_sanitize(data, no_log_values=None)",
        "snippet": "def heuristic_log_sanitize(data, no_log_values=None):\n    ''' Remove strings that look like passwords from log messages '''\n    # Currently filters:\n    # user:pass@foo/whatever and http://username:pass@wherever/foo\n    # This code has false positives and consumes parts of logs that are\n    # not passwds\n\n    # begin: start of a passwd containing string\n    # end: end of a passwd containing string\n    # sep: char between user and passwd\n    # prev_begin: where in the overall string to start a search for\n    #   a passwd\n    # sep_search_end: where in the string to end a search for the sep\n    data = to_native(data)\n\n    output = []\n    begin = len(data)\n    prev_begin = begin\n    sep = 1\n    while sep:\n        # Find the potential end of a passwd\n        try:\n            end = data.rindex('@', 0, begin)\n        except ValueError:\n            # No passwd in the rest of the data\n            output.insert(0, data[0:begin])\n            break\n\n        # Search for the beginning of a passwd\n        sep = None\n        sep_search_end = end\n        while not sep:\n            # URL-style username+password\n            try:\n                begin = data.rindex('://', 0, sep_search_end)\n            except ValueError:\n                # No url style in the data, check for ssh style in the\n                # rest of the string\n                begin = 0\n            # Search for separator\n            try:\n                sep = data.index(':', begin + 3, end)\n            except ValueError:\n                # No separator; choices:\n                if begin == 0:\n                    # Searched the whole string so there's no password\n                    # here.  Return the remaining data\n                    output.insert(0, data[0:begin])\n                    break\n                # Search for a different beginning of the password field.\n                sep_search_end = begin\n                continue\n        if sep:\n            # Password was found; remove it.\n            output.insert(0, data[end:prev_begin])\n            output.insert(0, '********')\n            output.insert(0, data[begin:sep + 1])\n            prev_begin = begin\n\n    output = ''.join(output)\n    if no_log_values:\n        output = remove_values(output, no_log_values)\n    return output",
        "begin_line": 424,
        "end_line": 486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic._load_params#489",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic._load_params()",
        "snippet": "def _load_params():\n    ''' read the modules parameters and store them globally.\n\n    This function may be needed for certain very dynamic custom modules which\n    want to process the parameters that are being handed the module.  Since\n    this is so closely tied to the implementation of modules we cannot\n    guarantee API stability for it (it may change between versions) however we\n    will try not to break it gratuitously.  It is certainly more future-proof\n    to call this function and consume its outputs than to implement the logic\n    inside it as a copy in your own code.\n    '''\n    global _ANSIBLE_ARGS\n    if _ANSIBLE_ARGS is not None:\n        buffer = _ANSIBLE_ARGS\n    else:\n        # debug overrides to read args from file or cmdline\n\n        # Avoid tracebacks when locale is non-utf8\n        # We control the args and we pass them as utf8\n        if len(sys.argv) > 1:\n            if os.path.isfile(sys.argv[1]):\n                fd = open(sys.argv[1], 'rb')\n                buffer = fd.read()\n                fd.close()\n            else:\n                buffer = sys.argv[1]\n                if PY3:\n                    buffer = buffer.encode('utf-8', errors='surrogateescape')\n        # default case, read from stdin\n        else:\n            if PY2:\n                buffer = sys.stdin.read()\n            else:\n                buffer = sys.stdin.buffer.read()\n        _ANSIBLE_ARGS = buffer\n\n    try:\n        params = json.loads(buffer.decode('utf-8'))\n    except ValueError:\n        # This helper used too early for fail_json to work.\n        print('\\n{\"msg\": \"Error: Module unable to decode valid JSON on stdin.  Unable to figure out what parameters were passed\", \"failed\": true}')\n        sys.exit(1)\n\n    if PY2:\n        params = json_dict_unicode_to_bytes(params)\n\n    try:\n        return params['ANSIBLE_MODULE_ARGS']\n    except KeyError:\n        # This helper does not have access to fail_json so we have to print\n        # json output on our own.\n        print('\\n{\"msg\": \"Error: Module unable to locate ANSIBLE_MODULE_ARGS in json data from stdin.  Unable to figure out what parameters were passed\", '\n              '\"failed\": true}')\n        sys.exit(1)",
        "begin_line": 489,
        "end_line": 542,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.env_fallback#545",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.env_fallback(*args, **kwargs)",
        "snippet": "def env_fallback(*args, **kwargs):\n    ''' Load value from environment '''\n    for arg in args:\n        if arg in os.environ:\n            return os.environ[arg]\n    raise AnsibleFallbackNotFound",
        "begin_line": 545,
        "end_line": 550,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.missing_required_lib#553",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.missing_required_lib(library, reason=None, url=None)",
        "snippet": "def missing_required_lib(library, reason=None, url=None):\n    hostname = platform.node()\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\n    if reason:\n        msg += \" This is required %s.\" % reason\n    if url:\n        msg += \" See %s for more info.\" % url\n\n    msg += (\" Please read the module documentation and install it in the appropriate location.\"\n            \" If the required library is installed, but Ansible is using the wrong Python interpreter,\"\n            \" please consult the documentation on ansible_python_interpreter\")\n    return msg",
        "begin_line": 553,
        "end_line": 564,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.__init__#572",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.__init__(self, argument_spec, bypass_checks=False, no_log=False, mutually_exclusive=None, required_together=None, required_one_of=None, add_file_common_args=False, supports_check_mode=False, required_if=None, required_by=None)",
        "snippet": "    def __init__(self, argument_spec, bypass_checks=False, no_log=False,\n                 mutually_exclusive=None, required_together=None,\n                 required_one_of=None, add_file_common_args=False,\n                 supports_check_mode=False, required_if=None, required_by=None):\n\n        '''\n        Common code for quickly building an ansible module in Python\n        (although you can write modules with anything that can return JSON).\n\n        See :ref:`developing_modules_general` for a general introduction\n        and :ref:`developing_program_flow_modules` for more detailed explanation.\n        '''\n\n        self._name = os.path.basename(__file__)  # initialize name until we can parse from options\n        self.argument_spec = argument_spec\n        self.supports_check_mode = supports_check_mode\n        self.check_mode = False\n        self.bypass_checks = bypass_checks\n        self.no_log = no_log\n\n        self.mutually_exclusive = mutually_exclusive\n        self.required_together = required_together\n        self.required_one_of = required_one_of\n        self.required_if = required_if\n        self.required_by = required_by\n        self.cleanup_files = []\n        self._debug = False\n        self._diff = False\n        self._socket_path = None\n        self._shell = None\n        self._verbosity = 0\n        # May be used to set modifications to the environment for any\n        # run_command invocation\n        self.run_command_environ_update = {}\n        self._clean = {}\n        self._string_conversion_action = ''\n\n        self.aliases = {}\n        self._legal_inputs = []\n        self._options_context = list()\n        self._tmpdir = None\n\n        if add_file_common_args:\n            for k, v in FILE_COMMON_ARGUMENTS.items():\n                if k not in self.argument_spec:\n                    self.argument_spec[k] = v\n\n        self._load_params()\n        self._set_fallbacks()\n\n        # append to legal_inputs and then possibly check against them\n        try:\n            self.aliases = self._handle_aliases()\n        except (ValueError, TypeError) as e:\n            # Use exceptions here because it isn't safe to call fail_json until no_log is processed\n            print('\\n{\"failed\": true, \"msg\": \"Module alias error: %s\"}' % to_native(e))\n            sys.exit(1)\n\n        # Save parameter values that should never be logged\n        self.no_log_values = set()\n        self._handle_no_log_values()\n\n        # check the locale as set by the current environment, and reset to\n        # a known valid (LANG=C) if it's an invalid/unavailable locale\n        self._check_locale()\n\n        self._check_arguments()\n\n        # check exclusive early\n        if not bypass_checks:\n            self._check_mutually_exclusive(mutually_exclusive)\n\n        self._set_defaults(pre=True)\n\n        self._CHECK_ARGUMENT_TYPES_DISPATCHER = {\n            'str': self._check_type_str,\n            'list': self._check_type_list,\n            'dict': self._check_type_dict,\n            'bool': self._check_type_bool,\n            'int': self._check_type_int,\n            'float': self._check_type_float,\n            'path': self._check_type_path,\n            'raw': self._check_type_raw,\n            'jsonarg': self._check_type_jsonarg,\n            'json': self._check_type_jsonarg,\n            'bytes': self._check_type_bytes,\n            'bits': self._check_type_bits,\n        }\n        if not bypass_checks:\n            self._check_required_arguments()\n            self._check_argument_types()\n            self._check_argument_values()\n            self._check_required_together(required_together)\n            self._check_required_one_of(required_one_of)\n            self._check_required_if(required_if)\n            self._check_required_by(required_by)\n\n        self._set_defaults(pre=False)\n\n        # deal with options sub-spec\n        self._handle_options()\n\n        if not self.no_log:\n            self._log_invocation()\n\n        # finally, make sure we're in a sane working dir\n        self._set_cwd()",
        "begin_line": 572,
        "end_line": 678,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.tmpdir#681",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.tmpdir(self)",
        "snippet": "    def tmpdir(self):\n        # if _ansible_tmpdir was not set and we have a remote_tmp,\n        # the module needs to create it and clean it up once finished.\n        # otherwise we create our own module tmp dir from the system defaults\n        if self._tmpdir is None:\n            basedir = None\n\n            if self._remote_tmp is not None:\n                basedir = os.path.expanduser(os.path.expandvars(self._remote_tmp))\n\n            if basedir is not None and not os.path.exists(basedir):\n                try:\n                    os.makedirs(basedir, mode=0o700)\n                except (OSError, IOError) as e:\n                    self.warn(\"Unable to use %s as temporary directory, \"\n                              \"failing back to system: %s\" % (basedir, to_native(e)))\n                    basedir = None\n                else:\n                    self.warn(\"Module remote_tmp %s did not exist and was \"\n                              \"created with a mode of 0700, this may cause\"\n                              \" issues when running as another user. To \"\n                              \"avoid this, create the remote_tmp dir with \"\n                              \"the correct permissions manually\" % basedir)\n\n            basefile = \"ansible-moduletmp-%s-\" % time.time()\n            try:\n                tmpdir = tempfile.mkdtemp(prefix=basefile, dir=basedir)\n            except (OSError, IOError) as e:\n                self.fail_json(\n                    msg=\"Failed to create remote module tmp path at dir %s \"\n                        \"with prefix %s: %s\" % (basedir, basefile, to_native(e))\n                )\n            if not self._keep_remote_files:\n                atexit.register(shutil.rmtree, tmpdir)\n            self._tmpdir = tmpdir\n\n        return self._tmpdir",
        "begin_line": 681,
        "end_line": 717,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.warn#719",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.warn(self, warning)",
        "snippet": "    def warn(self, warning):\n        warn(warning)\n        self.log('[WARNING] %s' % warning)",
        "begin_line": 719,
        "end_line": 721,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.deprecate#723",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.deprecate(self, msg, version=None)",
        "snippet": "    def deprecate(self, msg, version=None):\n        deprecate(msg, version)\n        self.log('[DEPRECATION WARNING] %s %s' % (msg, version))",
        "begin_line": 723,
        "end_line": 725,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.load_file_common_arguments#727",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.load_file_common_arguments(self, params, path=None)",
        "snippet": "    def load_file_common_arguments(self, params, path=None):\n        '''\n        many modules deal with files, this encapsulates common\n        options that the file module accepts such that it is directly\n        available to all modules and they can share code.\n\n        Allows to overwrite the path/dest module argument by providing path.\n        '''\n\n        if path is None:\n            path = params.get('path', params.get('dest', None))\n        if path is None:\n            return {}\n        else:\n            path = os.path.expanduser(os.path.expandvars(path))\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        # if the path is a symlink, and we're following links, get\n        # the target of the link instead for testing\n        if params.get('follow', False) and os.path.islink(b_path):\n            b_path = os.path.realpath(b_path)\n            path = to_native(b_path)\n\n        mode = params.get('mode', None)\n        owner = params.get('owner', None)\n        group = params.get('group', None)\n\n        # selinux related options\n        seuser = params.get('seuser', None)\n        serole = params.get('serole', None)\n        setype = params.get('setype', None)\n        selevel = params.get('selevel', None)\n        secontext = [seuser, serole, setype]\n\n        if self.selinux_mls_enabled():\n            secontext.append(selevel)\n\n        default_secontext = self.selinux_default_context(path)\n        for i in range(len(default_secontext)):\n            if i is not None and secontext[i] == '_default':\n                secontext[i] = default_secontext[i]\n\n        attributes = params.get('attributes', None)\n        return dict(\n            path=path, mode=mode, owner=owner, group=group,\n            seuser=seuser, serole=serole, setype=setype,\n            selevel=selevel, secontext=secontext, attributes=attributes,\n        )",
        "begin_line": 727,
        "end_line": 774,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.selinux_mls_enabled#782",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.selinux_mls_enabled(self)",
        "snippet": "    def selinux_mls_enabled(self):\n        if not HAVE_SELINUX:\n            return False\n        if selinux.is_selinux_mls_enabled() == 1:\n            return True\n        else:\n            return False",
        "begin_line": 782,
        "end_line": 788,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.selinux_enabled#790",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.selinux_enabled(self)",
        "snippet": "    def selinux_enabled(self):\n        if not HAVE_SELINUX:\n            seenabled = self.get_bin_path('selinuxenabled')\n            if seenabled is not None:\n                (rc, out, err) = self.run_command(seenabled)\n                if rc == 0:\n                    self.fail_json(msg=\"Aborting, target uses selinux but python bindings (libselinux-python) aren't installed!\")\n            return False\n        if selinux.is_selinux_enabled() == 1:\n            return True\n        else:\n            return False",
        "begin_line": 790,
        "end_line": 801,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.selinux_initial_context#804",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.selinux_initial_context(self)",
        "snippet": "    def selinux_initial_context(self):\n        context = [None, None, None]\n        if self.selinux_mls_enabled():\n            context.append(None)\n        return context",
        "begin_line": 804,
        "end_line": 808,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.selinux_default_context#811",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.selinux_default_context(self, path, mode=0)",
        "snippet": "    def selinux_default_context(self, path, mode=0):\n        context = self.selinux_initial_context()\n        if not HAVE_SELINUX or not self.selinux_enabled():\n            return context\n        try:\n            ret = selinux.matchpathcon(to_native(path, errors='surrogate_or_strict'), mode)\n        except OSError:\n            return context\n        if ret[0] == -1:\n            return context\n        # Limit split to 4 because the selevel, the last in the list,\n        # may contain ':' characters\n        context = ret[1].split(':', 3)\n        return context",
        "begin_line": 811,
        "end_line": 824,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.selinux_context#826",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.selinux_context(self, path)",
        "snippet": "    def selinux_context(self, path):\n        context = self.selinux_initial_context()\n        if not HAVE_SELINUX or not self.selinux_enabled():\n            return context\n        try:\n            ret = selinux.lgetfilecon_raw(to_native(path, errors='surrogate_or_strict'))\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                self.fail_json(path=path, msg='path %s does not exist' % path)\n            else:\n                self.fail_json(path=path, msg='failed to retrieve selinux context')\n        if ret[0] == -1:\n            return context\n        # Limit split to 4 because the selevel, the last in the list,\n        # may contain ':' characters\n        context = ret[1].split(':', 3)\n        return context",
        "begin_line": 826,
        "end_line": 842,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.user_and_group#844",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.user_and_group(self, path, expand=True)",
        "snippet": "    def user_and_group(self, path, expand=True):\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if expand:\n            b_path = os.path.expanduser(os.path.expandvars(b_path))\n        st = os.lstat(b_path)\n        uid = st.st_uid\n        gid = st.st_gid\n        return (uid, gid)",
        "begin_line": 844,
        "end_line": 851,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.find_mount_point#853",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.find_mount_point(self, path)",
        "snippet": "    def find_mount_point(self, path):\n        path_is_bytes = False\n        if isinstance(path, binary_type):\n            path_is_bytes = True\n\n        b_path = os.path.realpath(to_bytes(os.path.expanduser(os.path.expandvars(path)), errors='surrogate_or_strict'))\n        while not os.path.ismount(b_path):\n            b_path = os.path.dirname(b_path)\n\n        if path_is_bytes:\n            return b_path\n\n        return to_text(b_path, errors='surrogate_or_strict')",
        "begin_line": 853,
        "end_line": 865,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.is_special_selinux_path#867",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.is_special_selinux_path(self, path)",
        "snippet": "    def is_special_selinux_path(self, path):\n        \"\"\"\n        Returns a tuple containing (True, selinux_context) if the given path is on a\n        NFS or other 'special' fs  mount point, otherwise the return will be (False, None).\n        \"\"\"\n        try:\n            f = open('/proc/mounts', 'r')\n            mount_data = f.readlines()\n            f.close()\n        except Exception:\n            return (False, None)\n        path_mount_point = self.find_mount_point(path)\n        for line in mount_data:\n            (device, mount_point, fstype, options, rest) = line.split(' ', 4)\n\n            if path_mount_point == mount_point:\n                for fs in self._selinux_special_fs:\n                    if fs in fstype:\n                        special_context = self.selinux_context(path_mount_point)\n                        return (True, special_context)\n\n        return (False, None)",
        "begin_line": 867,
        "end_line": 888,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_default_selinux_context#890",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_default_selinux_context(self, path, changed)",
        "snippet": "    def set_default_selinux_context(self, path, changed):\n        if not HAVE_SELINUX or not self.selinux_enabled():\n            return changed\n        context = self.selinux_default_context(path)\n        return self.set_context_if_different(path, context, False)",
        "begin_line": 890,
        "end_line": 894,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_context_if_different#896",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_context_if_different(self, path, context, changed, diff=None)",
        "snippet": "    def set_context_if_different(self, path, context, changed, diff=None):\n\n        if not HAVE_SELINUX or not self.selinux_enabled():\n            return changed\n\n        if self.check_file_absent_if_check_mode(path):\n            return True\n\n        cur_context = self.selinux_context(path)\n        new_context = list(cur_context)\n        # Iterate over the current context instead of the\n        # argument context, which may have selevel.\n\n        (is_special_se, sp_context) = self.is_special_selinux_path(path)\n        if is_special_se:\n            new_context = sp_context\n        else:\n            for i in range(len(cur_context)):\n                if len(context) > i:\n                    if context[i] is not None and context[i] != cur_context[i]:\n                        new_context[i] = context[i]\n                    elif context[i] is None:\n                        new_context[i] = cur_context[i]\n\n        if cur_context != new_context:\n            if diff is not None:\n                if 'before' not in diff:\n                    diff['before'] = {}\n                diff['before']['secontext'] = cur_context\n                if 'after' not in diff:\n                    diff['after'] = {}\n                diff['after']['secontext'] = new_context\n\n            try:\n                if self.check_mode:\n                    return True\n                rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))\n            except OSError as e:\n                self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),\n                               new_context=new_context, cur_context=cur_context, input_was=context)\n            if rc != 0:\n                self.fail_json(path=path, msg='set selinux context failed')\n            changed = True\n        return changed",
        "begin_line": 896,
        "end_line": 939,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_owner_if_different#941",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_owner_if_different(self, path, owner, changed, diff=None, expand=True)",
        "snippet": "    def set_owner_if_different(self, path, owner, changed, diff=None, expand=True):\n\n        if owner is None:\n            return changed\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if expand:\n            b_path = os.path.expanduser(os.path.expandvars(b_path))\n\n        if self.check_file_absent_if_check_mode(b_path):\n            return True\n\n        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n        try:\n            uid = int(owner)\n        except ValueError:\n            try:\n                uid = pwd.getpwnam(owner).pw_uid\n            except KeyError:\n                path = to_text(b_path)\n                self.fail_json(path=path, msg='chown failed: failed to look up user %s' % owner)\n\n        if orig_uid != uid:\n            if diff is not None:\n                if 'before' not in diff:\n                    diff['before'] = {}\n                diff['before']['owner'] = orig_uid\n                if 'after' not in diff:\n                    diff['after'] = {}\n                diff['after']['owner'] = uid\n\n            if self.check_mode:\n                return True\n            try:\n                os.lchown(b_path, uid, -1)\n            except (IOError, OSError) as e:\n                path = to_text(b_path)\n                self.fail_json(path=path, msg='chown failed: %s' % (to_text(e)))\n            changed = True\n        return changed",
        "begin_line": 941,
        "end_line": 980,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_group_if_different#982",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_group_if_different(self, path, group, changed, diff=None, expand=True)",
        "snippet": "    def set_group_if_different(self, path, group, changed, diff=None, expand=True):\n\n        if group is None:\n            return changed\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if expand:\n            b_path = os.path.expanduser(os.path.expandvars(b_path))\n\n        if self.check_file_absent_if_check_mode(b_path):\n            return True\n\n        orig_uid, orig_gid = self.user_and_group(b_path, expand)\n        try:\n            gid = int(group)\n        except ValueError:\n            try:\n                gid = grp.getgrnam(group).gr_gid\n            except KeyError:\n                path = to_text(b_path)\n                self.fail_json(path=path, msg='chgrp failed: failed to look up group %s' % group)\n\n        if orig_gid != gid:\n            if diff is not None:\n                if 'before' not in diff:\n                    diff['before'] = {}\n                diff['before']['group'] = orig_gid\n                if 'after' not in diff:\n                    diff['after'] = {}\n                diff['after']['group'] = gid\n\n            if self.check_mode:\n                return True\n            try:\n                os.lchown(b_path, -1, gid)\n            except OSError:\n                path = to_text(b_path)\n                self.fail_json(path=path, msg='chgrp failed')\n            changed = True\n        return changed",
        "begin_line": 982,
        "end_line": 1021,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_mode_if_different#1023",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_mode_if_different(self, path, mode, changed, diff=None, expand=True)",
        "snippet": "    def set_mode_if_different(self, path, mode, changed, diff=None, expand=True):\n\n        if mode is None:\n            return changed\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if expand:\n            b_path = os.path.expanduser(os.path.expandvars(b_path))\n        path_stat = os.lstat(b_path)\n\n        if self.check_file_absent_if_check_mode(b_path):\n            return True\n\n        if not isinstance(mode, int):\n            try:\n                mode = int(mode, 8)\n            except Exception:\n                try:\n                    mode = self._symbolic_mode_to_octal(path_stat, mode)\n                except Exception as e:\n                    path = to_text(b_path)\n                    self.fail_json(path=path,\n                                   msg=\"mode must be in octal or symbolic form\",\n                                   details=to_native(e))\n\n                if mode != stat.S_IMODE(mode):\n                    # prevent mode from having extra info orbeing invalid long number\n                    path = to_text(b_path)\n                    self.fail_json(path=path, msg=\"Invalid mode supplied, only permission info is allowed\", details=mode)\n\n        prev_mode = stat.S_IMODE(path_stat.st_mode)\n\n        if prev_mode != mode:\n\n            if diff is not None:\n                if 'before' not in diff:\n                    diff['before'] = {}\n                diff['before']['mode'] = '0%03o' % prev_mode\n                if 'after' not in diff:\n                    diff['after'] = {}\n                diff['after']['mode'] = '0%03o' % mode\n\n            if self.check_mode:\n                return True\n            # FIXME: comparison against string above will cause this to be executed\n            # every time\n            try:\n                if hasattr(os, 'lchmod'):\n                    os.lchmod(b_path, mode)\n                else:\n                    if not os.path.islink(b_path):\n                        os.chmod(b_path, mode)\n                    else:\n                        # Attempt to set the perms of the symlink but be\n                        # careful not to change the perms of the underlying\n                        # file while trying\n                        underlying_stat = os.stat(b_path)\n                        os.chmod(b_path, mode)\n                        new_underlying_stat = os.stat(b_path)\n                        if underlying_stat.st_mode != new_underlying_stat.st_mode:\n                            os.chmod(b_path, stat.S_IMODE(underlying_stat.st_mode))\n            except OSError as e:\n                if os.path.islink(b_path) and e.errno in (errno.EPERM, errno.EROFS):  # Can't set mode on symbolic links\n                    pass\n                elif e.errno in (errno.ENOENT, errno.ELOOP):  # Can't set mode on broken symbolic links\n                    pass\n                else:\n                    raise\n            except Exception as e:\n                path = to_text(b_path)\n                self.fail_json(path=path, msg='chmod failed', details=to_native(e),\n                               exception=traceback.format_exc())\n\n            path_stat = os.lstat(b_path)\n            new_mode = stat.S_IMODE(path_stat.st_mode)\n\n            if new_mode != prev_mode:\n                changed = True\n        return changed",
        "begin_line": 1023,
        "end_line": 1101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_attributes_if_different#1103",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_attributes_if_different(self, path, attributes, changed, diff=None, expand=True)",
        "snippet": "    def set_attributes_if_different(self, path, attributes, changed, diff=None, expand=True):\n\n        if attributes is None:\n            return changed\n\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if expand:\n            b_path = os.path.expanduser(os.path.expandvars(b_path))\n\n        if self.check_file_absent_if_check_mode(b_path):\n            return True\n\n        existing = self.get_file_attributes(b_path)\n\n        attr_mod = '='\n        if attributes.startswith(('-', '+')):\n            attr_mod = attributes[0]\n            attributes = attributes[1:]\n\n        if existing.get('attr_flags', '') != attributes or attr_mod == '-':\n            attrcmd = self.get_bin_path('chattr')\n            if attrcmd:\n                attrcmd = [attrcmd, '%s%s' % (attr_mod, attributes), b_path]\n                changed = True\n\n                if diff is not None:\n                    if 'before' not in diff:\n                        diff['before'] = {}\n                    diff['before']['attributes'] = existing.get('attr_flags')\n                    if 'after' not in diff:\n                        diff['after'] = {}\n                    diff['after']['attributes'] = '%s%s' % (attr_mod, attributes)\n\n                if not self.check_mode:\n                    try:\n                        rc, out, err = self.run_command(attrcmd)\n                        if rc != 0 or err:\n                            raise Exception(\"Error while setting attributes: %s\" % (out + err))\n                    except Exception as e:\n                        self.fail_json(path=to_text(b_path), msg='chattr failed',\n                                       details=to_native(e), exception=traceback.format_exc())\n        return changed",
        "begin_line": 1103,
        "end_line": 1144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.get_file_attributes#1146",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.get_file_attributes(self, path)",
        "snippet": "    def get_file_attributes(self, path):\n        output = {}\n        attrcmd = self.get_bin_path('lsattr', False)\n        if attrcmd:\n            attrcmd = [attrcmd, '-vd', path]\n            try:\n                rc, out, err = self.run_command(attrcmd)\n                if rc == 0:\n                    res = out.split()\n                    output['attr_flags'] = res[1].replace('-', '').strip()\n                    output['version'] = res[0].strip()\n                    output['attributes'] = format_attributes(output['attr_flags'])\n            except Exception:\n                pass\n        return output",
        "begin_line": 1146,
        "end_line": 1160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._symbolic_mode_to_octal#1163",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._symbolic_mode_to_octal(cls, path_stat, symbolic_mode)",
        "snippet": "    def _symbolic_mode_to_octal(cls, path_stat, symbolic_mode):\n        \"\"\"\n        This enables symbolic chmod string parsing as stated in the chmod man-page\n\n        This includes things like: \"u=rw-x+X,g=r-x+X,o=r-x+X\"\n        \"\"\"\n\n        new_mode = stat.S_IMODE(path_stat.st_mode)\n\n        # Now parse all symbolic modes\n        for mode in symbolic_mode.split(','):\n            # Per single mode. This always contains a '+', '-' or '='\n            # Split it on that\n            permlist = MODE_OPERATOR_RE.split(mode)\n\n            # And find all the operators\n            opers = MODE_OPERATOR_RE.findall(mode)\n\n            # The user(s) where it's all about is the first element in the\n            # 'permlist' list. Take that and remove it from the list.\n            # An empty user or 'a' means 'all'.\n            users = permlist.pop(0)\n            use_umask = (users == '')\n            if users == 'a' or users == '':\n                users = 'ugo'\n\n            # Check if there are illegal characters in the user list\n            # They can end up in 'users' because they are not split\n            if USERS_RE.match(users):\n                raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n\n            # Now we have two list of equal length, one contains the requested\n            # permissions and one with the corresponding operators.\n            for idx, perms in enumerate(permlist):\n                # Check if there are illegal characters in the permissions\n                if PERMS_RE.match(perms):\n                    raise ValueError(\"bad symbolic permission for mode: %s\" % mode)\n\n                for user in users:\n                    mode_to_apply = cls._get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask)\n                    new_mode = cls._apply_operation_to_mode(user, opers[idx], mode_to_apply, new_mode)\n\n        return new_mode",
        "begin_line": 1163,
        "end_line": 1205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._apply_operation_to_mode#1208",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._apply_operation_to_mode(user, operator, mode_to_apply, current_mode)",
        "snippet": "    def _apply_operation_to_mode(user, operator, mode_to_apply, current_mode):\n        if operator == '=':\n            if user == 'u':\n                mask = stat.S_IRWXU | stat.S_ISUID\n            elif user == 'g':\n                mask = stat.S_IRWXG | stat.S_ISGID\n            elif user == 'o':\n                mask = stat.S_IRWXO | stat.S_ISVTX\n\n            # mask out u, g, or o permissions from current_mode and apply new permissions\n            inverse_mask = mask ^ PERM_BITS\n            new_mode = (current_mode & inverse_mask) | mode_to_apply\n        elif operator == '+':\n            new_mode = current_mode | mode_to_apply\n        elif operator == '-':\n            new_mode = current_mode - (current_mode & mode_to_apply)\n        return new_mode",
        "begin_line": 1208,
        "end_line": 1224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._get_octal_mode_from_symbolic_perms#1227",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask)",
        "snippet": "    def _get_octal_mode_from_symbolic_perms(path_stat, user, perms, use_umask):\n        prev_mode = stat.S_IMODE(path_stat.st_mode)\n\n        is_directory = stat.S_ISDIR(path_stat.st_mode)\n        has_x_permissions = (prev_mode & EXEC_PERM_BITS) > 0\n        apply_X_permission = is_directory or has_x_permissions\n\n        # Get the umask, if the 'user' part is empty, the effect is as if (a) were\n        # given, but bits that are set in the umask are not affected.\n        # We also need the \"reversed umask\" for masking\n        umask = os.umask(0)\n        os.umask(umask)\n        rev_umask = umask ^ PERM_BITS\n\n        # Permission bits constants documented at:\n        # http://docs.python.org/2/library/stat.html#stat.S_ISUID\n        if apply_X_permission:\n            X_perms = {\n                'u': {'X': stat.S_IXUSR},\n                'g': {'X': stat.S_IXGRP},\n                'o': {'X': stat.S_IXOTH},\n            }\n        else:\n            X_perms = {\n                'u': {'X': 0},\n                'g': {'X': 0},\n                'o': {'X': 0},\n            }\n\n        user_perms_to_modes = {\n            'u': {\n                'r': rev_umask & stat.S_IRUSR if use_umask else stat.S_IRUSR,\n                'w': rev_umask & stat.S_IWUSR if use_umask else stat.S_IWUSR,\n                'x': rev_umask & stat.S_IXUSR if use_umask else stat.S_IXUSR,\n                's': stat.S_ISUID,\n                't': 0,\n                'u': prev_mode & stat.S_IRWXU,\n                'g': (prev_mode & stat.S_IRWXG) << 3,\n                'o': (prev_mode & stat.S_IRWXO) << 6},\n            'g': {\n                'r': rev_umask & stat.S_IRGRP if use_umask else stat.S_IRGRP,\n                'w': rev_umask & stat.S_IWGRP if use_umask else stat.S_IWGRP,\n                'x': rev_umask & stat.S_IXGRP if use_umask else stat.S_IXGRP,\n                's': stat.S_ISGID,\n                't': 0,\n                'u': (prev_mode & stat.S_IRWXU) >> 3,\n                'g': prev_mode & stat.S_IRWXG,\n                'o': (prev_mode & stat.S_IRWXO) << 3},\n            'o': {\n                'r': rev_umask & stat.S_IROTH if use_umask else stat.S_IROTH,\n                'w': rev_umask & stat.S_IWOTH if use_umask else stat.S_IWOTH,\n                'x': rev_umask & stat.S_IXOTH if use_umask else stat.S_IXOTH,\n                's': 0,\n                't': stat.S_ISVTX,\n                'u': (prev_mode & stat.S_IRWXU) >> 6,\n                'g': (prev_mode & stat.S_IRWXG) >> 3,\n                'o': prev_mode & stat.S_IRWXO},\n        }\n\n        # Insert X_perms into user_perms_to_modes\n        for key, value in X_perms.items():\n            user_perms_to_modes[key].update(value)\n\n        def or_reduce(mode, perm):\n            return mode | user_perms_to_modes[user][perm]\n\n        return reduce(or_reduce, perms, 0)",
        "begin_line": 1227,
        "end_line": 1293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_fs_attributes_if_different#1295",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_fs_attributes_if_different(self, file_args, changed, diff=None, expand=True)",
        "snippet": "    def set_fs_attributes_if_different(self, file_args, changed, diff=None, expand=True):\n        # set modes owners and context as needed\n        changed = self.set_context_if_different(\n            file_args['path'], file_args['secontext'], changed, diff\n        )\n        changed = self.set_owner_if_different(\n            file_args['path'], file_args['owner'], changed, diff, expand\n        )\n        changed = self.set_group_if_different(\n            file_args['path'], file_args['group'], changed, diff, expand\n        )\n        changed = self.set_mode_if_different(\n            file_args['path'], file_args['mode'], changed, diff, expand\n        )\n        changed = self.set_attributes_if_different(\n            file_args['path'], file_args['attributes'], changed, diff, expand\n        )\n        return changed",
        "begin_line": 1295,
        "end_line": 1312,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.check_file_absent_if_check_mode#1314",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.check_file_absent_if_check_mode(self, file_path)",
        "snippet": "    def check_file_absent_if_check_mode(self, file_path):\n        return self.check_mode and not os.path.exists(file_path)",
        "begin_line": 1314,
        "end_line": 1315,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_directory_attributes_if_different#1317",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_directory_attributes_if_different(self, file_args, changed, diff=None, expand=True)",
        "snippet": "    def set_directory_attributes_if_different(self, file_args, changed, diff=None, expand=True):\n        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)",
        "begin_line": 1317,
        "end_line": 1318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.set_file_attributes_if_different#1320",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.set_file_attributes_if_different(self, file_args, changed, diff=None, expand=True)",
        "snippet": "    def set_file_attributes_if_different(self, file_args, changed, diff=None, expand=True):\n        return self.set_fs_attributes_if_different(file_args, changed, diff, expand)",
        "begin_line": 1320,
        "end_line": 1321,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.add_path_info#1323",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.add_path_info(self, kwargs)",
        "snippet": "    def add_path_info(self, kwargs):\n        '''\n        for results that are files, supplement the info about the file\n        in the return path with stats about the file path.\n        '''\n\n        path = kwargs.get('path', kwargs.get('dest', None))\n        if path is None:\n            return kwargs\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        if os.path.exists(b_path):\n            (uid, gid) = self.user_and_group(path)\n            kwargs['uid'] = uid\n            kwargs['gid'] = gid\n            try:\n                user = pwd.getpwuid(uid)[0]\n            except KeyError:\n                user = str(uid)\n            try:\n                group = grp.getgrgid(gid)[0]\n            except KeyError:\n                group = str(gid)\n            kwargs['owner'] = user\n            kwargs['group'] = group\n            st = os.lstat(b_path)\n            kwargs['mode'] = '0%03o' % stat.S_IMODE(st[stat.ST_MODE])\n            # secontext not yet supported\n            if os.path.islink(b_path):\n                kwargs['state'] = 'link'\n            elif os.path.isdir(b_path):\n                kwargs['state'] = 'directory'\n            elif os.stat(b_path).st_nlink > 1:\n                kwargs['state'] = 'hard'\n            else:\n                kwargs['state'] = 'file'\n            if HAVE_SELINUX and self.selinux_enabled():\n                kwargs['secontext'] = ':'.join(self.selinux_context(path))\n            kwargs['size'] = st[stat.ST_SIZE]\n        return kwargs",
        "begin_line": 1323,
        "end_line": 1361,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_locale#1363",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_locale(self)",
        "snippet": "    def _check_locale(self):\n        '''\n        Uses the locale module to test the currently set locale\n        (per the LANG and LC_CTYPE environment settings)\n        '''\n        try:\n            # setting the locale to '' uses the default locale\n            # as it would be returned by locale.getdefaultlocale()\n            locale.setlocale(locale.LC_ALL, '')\n        except locale.Error:\n            # fallback to the 'C' locale, which may cause unicode\n            # issues but is preferable to simply failing because\n            # of an unknown locale\n            locale.setlocale(locale.LC_ALL, 'C')\n            os.environ['LANG'] = 'C'\n            os.environ['LC_ALL'] = 'C'\n            os.environ['LC_MESSAGES'] = 'C'\n        except Exception as e:\n            self.fail_json(msg=\"An unknown error was encountered while attempting to validate the locale: %s\" %\n                           to_native(e), exception=traceback.format_exc())",
        "begin_line": 1363,
        "end_line": 1382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._handle_aliases#1384",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._handle_aliases(self, spec=None, param=None, option_prefix='')",
        "snippet": "    def _handle_aliases(self, spec=None, param=None, option_prefix=''):\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n\n        # this uses exceptions as it happens before we can safely call fail_json\n        alias_warnings = []\n        alias_results, self._legal_inputs = handle_aliases(spec, param, alias_warnings=alias_warnings)\n        for option, alias in alias_warnings:\n            warn('Both option %s and its alias %s are set.' % (option_prefix + option, option_prefix + alias))\n\n        deprecated_aliases = []\n        for i in spec.keys():\n            if 'deprecated_aliases' in spec[i].keys():\n                for alias in spec[i]['deprecated_aliases']:\n                    deprecated_aliases.append(alias)\n\n        for deprecation in deprecated_aliases:\n            if deprecation['name'] in param.keys():\n                deprecate(\"Alias '%s' is deprecated. See the module docs for more information\" % deprecation['name'], deprecation['version'])\n        return alias_results",
        "begin_line": 1384,
        "end_line": 1405,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._handle_no_log_values#1407",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._handle_no_log_values(self, spec=None, param=None)",
        "snippet": "    def _handle_no_log_values(self, spec=None, param=None):\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n\n        try:\n            self.no_log_values.update(list_no_log_values(spec, param))\n        except TypeError as te:\n            self.fail_json(msg=\"Failure when processing no_log parameters. Module invocation will be hidden. \"\n                               \"%s\" % to_native(te), invocation={'module_args': 'HIDDEN DUE TO FAILURE'})\n\n        for message in list_deprecations(spec, param):\n            deprecate(message['msg'], message['version'])",
        "begin_line": 1407,
        "end_line": 1420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_arguments#1422",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_arguments(self, spec=None, param=None, legal_inputs=None)",
        "snippet": "    def _check_arguments(self, spec=None, param=None, legal_inputs=None):\n        self._syslog_facility = 'LOG_USER'\n        unsupported_parameters = set()\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n        if legal_inputs is None:\n            legal_inputs = self._legal_inputs\n\n        for k in list(param.keys()):\n\n            if k not in legal_inputs:\n                unsupported_parameters.add(k)\n\n        for k in PASS_VARS:\n            # handle setting internal properties from internal ansible vars\n            param_key = '_ansible_%s' % k\n            if param_key in param:\n                if k in PASS_BOOLS:\n                    setattr(self, PASS_VARS[k][0], self.boolean(param[param_key]))\n                else:\n                    setattr(self, PASS_VARS[k][0], param[param_key])\n\n                # clean up internal top level params:\n                if param_key in self.params:\n                    del self.params[param_key]\n            else:\n                # use defaults if not already set\n                if not hasattr(self, PASS_VARS[k][0]):\n                    setattr(self, PASS_VARS[k][0], PASS_VARS[k][1])\n\n        if unsupported_parameters:\n            msg = \"Unsupported parameters for (%s) module: %s\" % (self._name, ', '.join(sorted(list(unsupported_parameters))))\n            if self._options_context:\n                msg += \" found in %s.\" % \" -> \".join(self._options_context)\n            msg += \" Supported parameters include: %s\" % (', '.join(sorted(spec.keys())))\n            self.fail_json(msg=msg)\n        if self.check_mode and not self.supports_check_mode:\n            self.exit_json(skipped=True, msg=\"remote module (%s) does not support check mode\" % self._name)",
        "begin_line": 1422,
        "end_line": 1461,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._count_terms#1463",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._count_terms(self, check, param=None)",
        "snippet": "    def _count_terms(self, check, param=None):\n        if param is None:\n            param = self.params\n        return count_terms(check, param)",
        "begin_line": 1463,
        "end_line": 1466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_mutually_exclusive#1468",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_mutually_exclusive(self, spec, param=None)",
        "snippet": "    def _check_mutually_exclusive(self, spec, param=None):\n        if param is None:\n            param = self.params\n\n        try:\n            check_mutually_exclusive(spec, param)\n        except TypeError as e:\n            msg = to_native(e)\n            if self._options_context:\n                msg += \" found in %s\" % \" -> \".join(self._options_context)\n            self.fail_json(msg=msg)",
        "begin_line": 1468,
        "end_line": 1478,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_required_one_of#1480",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_required_one_of(self, spec, param=None)",
        "snippet": "    def _check_required_one_of(self, spec, param=None):\n        if spec is None:\n            return\n\n        if param is None:\n            param = self.params\n\n        try:\n            check_required_one_of(spec, param)\n        except TypeError as e:\n            msg = to_native(e)\n            if self._options_context:\n                msg += \" found in %s\" % \" -> \".join(self._options_context)\n            self.fail_json(msg=msg)",
        "begin_line": 1480,
        "end_line": 1493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_required_together#1495",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_required_together(self, spec, param=None)",
        "snippet": "    def _check_required_together(self, spec, param=None):\n        if spec is None:\n            return\n        if param is None:\n            param = self.params\n\n        try:\n            check_required_together(spec, param)\n        except TypeError as e:\n            msg = to_native(e)\n            if self._options_context:\n                msg += \" found in %s\" % \" -> \".join(self._options_context)\n            self.fail_json(msg=msg)",
        "begin_line": 1495,
        "end_line": 1507,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_required_by#1509",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_required_by(self, spec, param=None)",
        "snippet": "    def _check_required_by(self, spec, param=None):\n        if spec is None:\n            return\n        if param is None:\n            param = self.params\n\n        try:\n            check_required_by(spec, param)\n        except TypeError as e:\n            self.fail_json(msg=to_native(e))",
        "begin_line": 1509,
        "end_line": 1518,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_required_arguments#1520",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_required_arguments(self, spec=None, param=None)",
        "snippet": "    def _check_required_arguments(self, spec=None, param=None):\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n\n        try:\n            check_required_arguments(spec, param)\n        except TypeError as e:\n            msg = to_native(e)\n            if self._options_context:\n                msg += \" found in %s\" % \" -> \".join(self._options_context)\n            self.fail_json(msg=msg)",
        "begin_line": 1520,
        "end_line": 1532,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_required_if#1534",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_required_if(self, spec, param=None)",
        "snippet": "    def _check_required_if(self, spec, param=None):\n        ''' ensure that parameters which conditionally required are present '''\n        if spec is None:\n            return\n        if param is None:\n            param = self.params\n\n        try:\n            check_required_if(spec, param)\n        except TypeError as e:\n            msg = to_native(e)\n            if self._options_context:\n                msg += \" found in %s\" % \" -> \".join(self._options_context)\n            self.fail_json(msg=msg)",
        "begin_line": 1534,
        "end_line": 1547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_argument_values#1549",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_argument_values(self, spec=None, param=None)",
        "snippet": "    def _check_argument_values(self, spec=None, param=None):\n        ''' ensure all arguments have the requested values, and there are no stray arguments '''\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n        for (k, v) in spec.items():\n            choices = v.get('choices', None)\n            if choices is None:\n                continue\n            if isinstance(choices, SEQUENCETYPE) and not isinstance(choices, (binary_type, text_type)):\n                if k in param:\n                    # Allow one or more when type='list' param with choices\n                    if isinstance(param[k], list):\n                        diff_list = \", \".join([item for item in param[k] if item not in choices])\n                        if diff_list:\n                            choices_str = \", \".join([to_native(c) for c in choices])\n                            msg = \"value of %s must be one or more of: %s. Got no match for: %s\" % (k, choices_str, diff_list)\n                            if self._options_context:\n                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n                            self.fail_json(msg=msg)\n                    elif param[k] not in choices:\n                        # PyYaml converts certain strings to bools.  If we can unambiguously convert back, do so before checking\n                        # the value.  If we can't figure this out, module author is responsible.\n                        lowered_choices = None\n                        if param[k] == 'False':\n                            lowered_choices = lenient_lowercase(choices)\n                            overlap = BOOLEANS_FALSE.intersection(choices)\n                            if len(overlap) == 1:\n                                # Extract from a set\n                                (param[k],) = overlap\n\n                        if param[k] == 'True':\n                            if lowered_choices is None:\n                                lowered_choices = lenient_lowercase(choices)\n                            overlap = BOOLEANS_TRUE.intersection(choices)\n                            if len(overlap) == 1:\n                                (param[k],) = overlap\n\n                        if param[k] not in choices:\n                            choices_str = \", \".join([to_native(c) for c in choices])\n                            msg = \"value of %s must be one of: %s, got: %s\" % (k, choices_str, param[k])\n                            if self._options_context:\n                                msg += \" found in %s\" % \" -> \".join(self._options_context)\n                            self.fail_json(msg=msg)\n            else:\n                msg = \"internal error: choices for argument %s are not iterable: %s\" % (k, choices)\n                if self._options_context:\n                    msg += \" found in %s\" % \" -> \".join(self._options_context)\n                self.fail_json(msg=msg)",
        "begin_line": 1549,
        "end_line": 1598,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.safe_eval#1600",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.safe_eval(self, value, locals=None, include_exceptions=False)",
        "snippet": "    def safe_eval(self, value, locals=None, include_exceptions=False):\n        return safe_eval(value, locals, include_exceptions)",
        "begin_line": 1600,
        "end_line": 1601,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_str#1603",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_str(self, value)",
        "snippet": "    def _check_type_str(self, value):\n        opts = {\n            'error': False,\n            'warn': False,\n            'ignore': True\n        }\n\n        # Ignore, warn, or error when converting to a string.\n        allow_conversion = opts.get(self._string_conversion_action, True)\n        try:\n            return check_type_str(value, allow_conversion)\n        except TypeError:\n            common_msg = 'quote the entire value to ensure it does not change.'\n            if self._string_conversion_action == 'error':\n                msg = common_msg.capitalize()\n                raise TypeError(to_native(msg))\n            elif self._string_conversion_action == 'warn':\n                msg = ('The value {0!r} (type {0.__class__.__name__}) in a string field was converted to {1!r} (type string). '\n                       'If this does not look like what you expect, {2}').format(value, to_text(value), common_msg)\n                self.warn(to_native(msg))\n                return to_native(value, errors='surrogate_or_strict')",
        "begin_line": 1603,
        "end_line": 1623,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_list#1625",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_list(self, value)",
        "snippet": "    def _check_type_list(self, value):\n        return check_type_list(value)",
        "begin_line": 1625,
        "end_line": 1626,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_dict#1628",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_dict(self, value)",
        "snippet": "    def _check_type_dict(self, value):\n        return check_type_dict(value)",
        "begin_line": 1628,
        "end_line": 1629,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bool#1631",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bool(self, value)",
        "snippet": "    def _check_type_bool(self, value):\n        return check_type_bool(value)",
        "begin_line": 1631,
        "end_line": 1632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_int#1634",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_int(self, value)",
        "snippet": "    def _check_type_int(self, value):\n        return check_type_int(value)",
        "begin_line": 1634,
        "end_line": 1635,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_float#1637",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_float(self, value)",
        "snippet": "    def _check_type_float(self, value):\n        return check_type_float(value)",
        "begin_line": 1637,
        "end_line": 1638,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_path#1640",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_path(self, value)",
        "snippet": "    def _check_type_path(self, value):\n        return check_type_path(value)",
        "begin_line": 1640,
        "end_line": 1641,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_jsonarg#1643",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_jsonarg(self, value)",
        "snippet": "    def _check_type_jsonarg(self, value):\n        return check_type_jsonarg(value)",
        "begin_line": 1643,
        "end_line": 1644,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_raw#1646",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_raw(self, value)",
        "snippet": "    def _check_type_raw(self, value):\n        return check_type_raw(value)",
        "begin_line": 1646,
        "end_line": 1647,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bytes#1649",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bytes(self, value)",
        "snippet": "    def _check_type_bytes(self, value):\n        return check_type_bytes(value)",
        "begin_line": 1649,
        "end_line": 1650,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bits#1652",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_type_bits(self, value)",
        "snippet": "    def _check_type_bits(self, value):\n        return check_type_bits(value)",
        "begin_line": 1652,
        "end_line": 1653,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._handle_options#1655",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._handle_options(self, argument_spec=None, params=None, prefix='')",
        "snippet": "    def _handle_options(self, argument_spec=None, params=None, prefix=''):\n        ''' deal with options to create sub spec '''\n        if argument_spec is None:\n            argument_spec = self.argument_spec\n        if params is None:\n            params = self.params\n\n        for (k, v) in argument_spec.items():\n            wanted = v.get('type', None)\n            if wanted == 'dict' or (wanted == 'list' and v.get('elements', '') == 'dict'):\n                spec = v.get('options', None)\n                if v.get('apply_defaults', False):\n                    if spec is not None:\n                        if params.get(k) is None:\n                            params[k] = {}\n                    else:\n                        continue\n                elif spec is None or k not in params or params[k] is None:\n                    continue\n\n                self._options_context.append(k)\n\n                if isinstance(params[k], dict):\n                    elements = [params[k]]\n                else:\n                    elements = params[k]\n\n                for idx, param in enumerate(elements):\n                    if not isinstance(param, dict):\n                        self.fail_json(msg=\"value of %s must be of type dict or list of dict\" % k)\n\n                    new_prefix = prefix + k\n                    if wanted == 'list':\n                        new_prefix += '[%d]' % idx\n                    new_prefix += '.'\n\n                    self._set_fallbacks(spec, param)\n                    options_aliases = self._handle_aliases(spec, param, option_prefix=new_prefix)\n\n                    options_legal_inputs = list(spec.keys()) + list(options_aliases.keys())\n\n                    self._check_arguments(spec, param, options_legal_inputs)\n\n                    # check exclusive early\n                    if not self.bypass_checks:\n                        self._check_mutually_exclusive(v.get('mutually_exclusive', None), param)\n\n                    self._set_defaults(pre=True, spec=spec, param=param)\n\n                    if not self.bypass_checks:\n                        self._check_required_arguments(spec, param)\n                        self._check_argument_types(spec, param)\n                        self._check_argument_values(spec, param)\n\n                        self._check_required_together(v.get('required_together', None), param)\n                        self._check_required_one_of(v.get('required_one_of', None), param)\n                        self._check_required_if(v.get('required_if', None), param)\n                        self._check_required_by(v.get('required_by', None), param)\n\n                    self._set_defaults(pre=False, spec=spec, param=param)\n\n                    # handle multi level options (sub argspec)\n                    self._handle_options(spec, param, new_prefix)\n                self._options_context.pop()",
        "begin_line": 1655,
        "end_line": 1718,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._get_wanted_type#1720",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._get_wanted_type(self, wanted, k)",
        "snippet": "    def _get_wanted_type(self, wanted, k):\n        if not callable(wanted):\n            if wanted is None:\n                # Mostly we want to default to str.\n                # For values set to None explicitly, return None instead as\n                # that allows a user to unset a parameter\n                wanted = 'str'\n            try:\n                type_checker = self._CHECK_ARGUMENT_TYPES_DISPATCHER[wanted]\n            except KeyError:\n                self.fail_json(msg=\"implementation error: unknown type %s requested for %s\" % (wanted, k))\n        else:\n            # set the type_checker to the callable, and reset wanted to the callable's name (or type if it doesn't have one, ala MagicMock)\n            type_checker = wanted\n            wanted = getattr(wanted, '__name__', to_native(type(wanted)))\n\n        return type_checker, wanted",
        "begin_line": 1720,
        "end_line": 1736,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._handle_elements#1738",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._handle_elements(self, wanted, param, values)",
        "snippet": "    def _handle_elements(self, wanted, param, values):\n        type_checker, wanted_name = self._get_wanted_type(wanted, param)\n        validated_params = []\n        for value in values:\n            try:\n                validated_params.append(type_checker(value))\n            except (TypeError, ValueError) as e:\n                msg = \"Elements value for option %s\" % param\n                if self._options_context:\n                    msg += \" found in '%s'\" % \" -> \".join(self._options_context)\n                msg += \" is of type %s and we were unable to convert to %s: %s\" % (type(value), wanted_name, to_native(e))\n                self.fail_json(msg=msg)\n        return validated_params",
        "begin_line": 1738,
        "end_line": 1750,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._check_argument_types#1752",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._check_argument_types(self, spec=None, param=None)",
        "snippet": "    def _check_argument_types(self, spec=None, param=None):\n        ''' ensure all arguments have the requested type '''\n\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n\n        for (k, v) in spec.items():\n            wanted = v.get('type', None)\n            if k not in param:\n                continue\n\n            value = param[k]\n            if value is None:\n                continue\n\n            type_checker, wanted_name = self._get_wanted_type(wanted, k)\n            try:\n                param[k] = type_checker(value)\n                wanted_elements = v.get('elements', None)\n                if wanted_elements:\n                    if wanted != 'list' or not isinstance(param[k], list):\n                        msg = \"Invalid type %s for option '%s'\" % (wanted_name, param)\n                        if self._options_context:\n                            msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n                        msg += \", elements value check is supported only with 'list' type\"\n                        self.fail_json(msg=msg)\n                    param[k] = self._handle_elements(wanted_elements, k, param[k])\n\n            except (TypeError, ValueError) as e:\n                msg = \"argument %s is of type %s\" % (k, type(value))\n                if self._options_context:\n                    msg += \" found in '%s'.\" % \" -> \".join(self._options_context)\n                msg += \" and we were unable to convert to %s: %s\" % (wanted_name, to_native(e))\n                self.fail_json(msg=msg)",
        "begin_line": 1752,
        "end_line": 1787,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._set_defaults#1789",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._set_defaults(self, pre=True, spec=None, param=None)",
        "snippet": "    def _set_defaults(self, pre=True, spec=None, param=None):\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n        for (k, v) in spec.items():\n            default = v.get('default', None)\n            if pre is True:\n                # this prevents setting defaults on required items\n                if default is not None and k not in param:\n                    param[k] = default\n            else:\n                # make sure things without a default still get set None\n                if k not in param:\n                    param[k] = default",
        "begin_line": 1789,
        "end_line": 1803,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._set_fallbacks#1805",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._set_fallbacks(self, spec=None, param=None)",
        "snippet": "    def _set_fallbacks(self, spec=None, param=None):\n        if spec is None:\n            spec = self.argument_spec\n        if param is None:\n            param = self.params\n\n        for (k, v) in spec.items():\n            fallback = v.get('fallback', (None,))\n            fallback_strategy = fallback[0]\n            fallback_args = []\n            fallback_kwargs = {}\n            if k not in param and fallback_strategy is not None:\n                for item in fallback[1:]:\n                    if isinstance(item, dict):\n                        fallback_kwargs = item\n                    else:\n                        fallback_args = item\n                try:\n                    param[k] = fallback_strategy(*fallback_args, **fallback_kwargs)\n                except AnsibleFallbackNotFound:\n                    continue",
        "begin_line": 1805,
        "end_line": 1825,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._load_params#1827",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._load_params(self)",
        "snippet": "    def _load_params(self):\n        ''' read the input and set the params attribute.\n\n        This method is for backwards compatibility.  The guts of the function\n        were moved out in 2.1 so that custom modules could read the parameters.\n        '''\n        # debug overrides to read args from file or cmdline\n        self.params = _load_params()",
        "begin_line": 1827,
        "end_line": 1834,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.365939893930465e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._log_to_syslog#1836",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._log_to_syslog(self, msg)",
        "snippet": "    def _log_to_syslog(self, msg):\n        if HAS_SYSLOG:\n            module = 'ansible-%s' % self._name\n            facility = getattr(syslog, self._syslog_facility, syslog.LOG_USER)\n            syslog.openlog(str(module), 0, facility)\n            syslog.syslog(syslog.LOG_INFO, msg)",
        "begin_line": 1836,
        "end_line": 1841,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.debug#1843",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.debug(self, msg)",
        "snippet": "    def debug(self, msg):\n        if self._debug:\n            self.log('[debug] %s' % msg)",
        "begin_line": 1843,
        "end_line": 1845,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.log#1847",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.log(self, msg, log_args=None)",
        "snippet": "    def log(self, msg, log_args=None):\n\n        if not self.no_log:\n\n            if log_args is None:\n                log_args = dict()\n\n            module = 'ansible-%s' % self._name\n            if isinstance(module, binary_type):\n                module = module.decode('utf-8', 'replace')\n\n            # 6655 - allow for accented characters\n            if not isinstance(msg, (binary_type, text_type)):\n                raise TypeError(\"msg should be a string (got %s)\" % type(msg))\n\n            # We want journal to always take text type\n            # syslog takes bytes on py2, text type on py3\n            if isinstance(msg, binary_type):\n                journal_msg = remove_values(msg.decode('utf-8', 'replace'), self.no_log_values)\n            else:\n                # TODO: surrogateescape is a danger here on Py3\n                journal_msg = remove_values(msg, self.no_log_values)\n\n            if PY3:\n                syslog_msg = journal_msg\n            else:\n                syslog_msg = journal_msg.encode('utf-8', 'replace')\n\n            if has_journal:\n                journal_args = [(\"MODULE\", os.path.basename(__file__))]\n                for arg in log_args:\n                    journal_args.append((arg.upper(), str(log_args[arg])))\n                try:\n                    if HAS_SYSLOG:\n                        # If syslog_facility specified, it needs to convert\n                        #  from the facility name to the facility code, and\n                        #  set it as SYSLOG_FACILITY argument of journal.send()\n                        facility = getattr(syslog,\n                                           self._syslog_facility,\n                                           syslog.LOG_USER) >> 3\n                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n                                     SYSLOG_FACILITY=facility,\n                                     **dict(journal_args))\n                    else:\n                        journal.send(MESSAGE=u\"%s %s\" % (module, journal_msg),\n                                     **dict(journal_args))\n                except IOError:\n                    # fall back to syslog since logging to journal failed\n                    self._log_to_syslog(syslog_msg)\n            else:\n                self._log_to_syslog(syslog_msg)",
        "begin_line": 1847,
        "end_line": 1897,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._log_invocation#1899",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._log_invocation(self)",
        "snippet": "    def _log_invocation(self):\n        ''' log that ansible ran the module '''\n        # TODO: generalize a separate log function and make log_invocation use it\n        # Sanitize possible password argument when logging.\n        log_args = dict()\n\n        for param in self.params:\n            canon = self.aliases.get(param, param)\n            arg_opts = self.argument_spec.get(canon, {})\n            no_log = arg_opts.get('no_log', None)\n\n            # try to proactively capture password/passphrase fields\n            if no_log is None and PASSWORD_MATCH.search(param):\n                log_args[param] = 'NOT_LOGGING_PASSWORD'\n                self.warn('Module did not set no_log for %s' % param)\n            elif self.boolean(no_log):\n                log_args[param] = 'NOT_LOGGING_PARAMETER'\n            else:\n                param_val = self.params[param]\n                if not isinstance(param_val, (text_type, binary_type)):\n                    param_val = str(param_val)\n                elif isinstance(param_val, text_type):\n                    param_val = param_val.encode('utf-8')\n                log_args[param] = heuristic_log_sanitize(param_val, self.no_log_values)\n\n        msg = ['%s=%s' % (to_native(arg), to_native(val)) for arg, val in log_args.items()]\n        if msg:\n            msg = 'Invoked with %s' % ' '.join(msg)\n        else:\n            msg = 'Invoked'\n\n        self.log(msg, log_args=log_args)",
        "begin_line": 1899,
        "end_line": 1930,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.258320257659592e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._set_cwd#1932",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._set_cwd(self)",
        "snippet": "    def _set_cwd(self):\n        try:\n            cwd = os.getcwd()\n            if not os.access(cwd, os.F_OK | os.R_OK):\n                raise Exception()\n            return cwd\n        except Exception:\n            # we don't have access to the cwd, probably because of sudo.\n            # Try and move to a neutral location to prevent errors\n            for cwd in [self.tmpdir, os.path.expandvars('$HOME'), tempfile.gettempdir()]:\n                try:\n                    if os.access(cwd, os.F_OK | os.R_OK):\n                        os.chdir(cwd)\n                        return cwd\n                except Exception:\n                    pass\n        # we won't error here, as it may *not* be a problem,\n        # and we don't want to break modules unnecessarily\n        return None",
        "begin_line": 1932,
        "end_line": 1950,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.get_bin_path#1952",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.get_bin_path(self, arg, required=False, opt_dirs=None)",
        "snippet": "    def get_bin_path(self, arg, required=False, opt_dirs=None):\n        '''\n        Find system executable in PATH.\n\n        :param arg: The executable to find.\n        :param required: if executable is not found and required is ``True``, fail_json\n        :param opt_dirs: optional list of directories to search in addition to ``PATH``\n        :returns: if found return full path; otherwise return None\n        '''\n\n        bin_path = None\n        try:\n            bin_path = get_bin_path(arg=arg, opt_dirs=opt_dirs)\n        except ValueError as e:\n            if required:\n                self.fail_json(msg=to_text(e))\n            else:\n                return bin_path\n\n        return bin_path",
        "begin_line": 1952,
        "end_line": 1971,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.boolean#1973",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.boolean(self, arg)",
        "snippet": "    def boolean(self, arg):\n        '''Convert the argument to a boolean'''\n        if arg is None:\n            return arg\n\n        try:\n            return boolean(arg)\n        except TypeError as e:\n            self.fail_json(msg=to_native(e))",
        "begin_line": 1973,
        "end_line": 1981,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.jsonify#1983",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.jsonify(self, data)",
        "snippet": "    def jsonify(self, data):\n        try:\n            return jsonify(data)\n        except UnicodeError as e:\n            self.fail_json(msg=to_text(e))",
        "begin_line": 1983,
        "end_line": 1987,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.from_json#1989",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.from_json(self, data)",
        "snippet": "    def from_json(self, data):\n        return json.loads(data)",
        "begin_line": 1989,
        "end_line": 1990,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.add_cleanup_file#1992",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.add_cleanup_file(self, path)",
        "snippet": "    def add_cleanup_file(self, path):\n        if path not in self.cleanup_files:\n            self.cleanup_files.append(path)",
        "begin_line": 1992,
        "end_line": 1994,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.do_cleanup_files#1996",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.do_cleanup_files(self)",
        "snippet": "    def do_cleanup_files(self):\n        for path in self.cleanup_files:\n            self.cleanup(path)",
        "begin_line": 1996,
        "end_line": 1998,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._return_formatted#2000",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._return_formatted(self, kwargs)",
        "snippet": "    def _return_formatted(self, kwargs):\n\n        self.add_path_info(kwargs)\n\n        if 'invocation' not in kwargs:\n            kwargs['invocation'] = {'module_args': self.params}\n\n        if 'warnings' in kwargs:\n            if isinstance(kwargs['warnings'], list):\n                for w in kwargs['warnings']:\n                    self.warn(w)\n            else:\n                self.warn(kwargs['warnings'])\n\n        warnings = get_warning_messages()\n        if warnings:\n            kwargs['warnings'] = warnings\n\n        if 'deprecations' in kwargs:\n            if isinstance(kwargs['deprecations'], list):\n                for d in kwargs['deprecations']:\n                    if isinstance(d, SEQUENCETYPE) and len(d) == 2:\n                        self.deprecate(d[0], version=d[1])\n                    elif isinstance(d, Mapping):\n                        self.deprecate(d['msg'], version=d.get('version', None))\n                    else:\n                        self.deprecate(d)  # pylint: disable=ansible-deprecated-no-version\n            else:\n                self.deprecate(kwargs['deprecations'])  # pylint: disable=ansible-deprecated-no-version\n\n        deprecations = get_deprecation_messages()\n        if deprecations:\n            kwargs['deprecations'] = deprecations\n\n        kwargs = remove_values(kwargs, self.no_log_values)\n        print('\\n%s' % self.jsonify(kwargs))",
        "begin_line": 2000,
        "end_line": 2035,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.exit_json#2037",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.exit_json(self, **kwargs)",
        "snippet": "    def exit_json(self, **kwargs):\n        ''' return from the module, without error '''\n\n        self.do_cleanup_files()\n        self._return_formatted(kwargs)\n        sys.exit(0)",
        "begin_line": 2037,
        "end_line": 2042,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.fail_json#2044",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.fail_json(self, msg, **kwargs)",
        "snippet": "    def fail_json(self, msg, **kwargs):\n        ''' return from the module, with an error message '''\n\n        kwargs['failed'] = True\n        kwargs['msg'] = msg\n\n        # Add traceback if debug or high verbosity and it is missing\n        # NOTE: Badly named as exception, it really always has been a traceback\n        if 'exception' not in kwargs and sys.exc_info()[2] and (self._debug or self._verbosity >= 3):\n            if PY2:\n                # On Python 2 this is the last (stack frame) exception and as such may be unrelated to the failure\n                kwargs['exception'] = 'WARNING: The below traceback may *not* be related to the actual failure.\\n' +\\\n                                      ''.join(traceback.format_tb(sys.exc_info()[2]))\n            else:\n                kwargs['exception'] = ''.join(traceback.format_tb(sys.exc_info()[2]))\n\n        self.do_cleanup_files()\n        self._return_formatted(kwargs)\n        sys.exit(1)",
        "begin_line": 2044,
        "end_line": 2062,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.fail_on_missing_params#2064",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.fail_on_missing_params(self, required_params=None)",
        "snippet": "    def fail_on_missing_params(self, required_params=None):\n        if not required_params:\n            return\n        try:\n            check_missing_parameters(self.params, required_params)\n        except TypeError as e:\n            self.fail_json(msg=to_native(e))",
        "begin_line": 2064,
        "end_line": 2070,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.digest_from_file#2072",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.digest_from_file(self, filename, algorithm)",
        "snippet": "    def digest_from_file(self, filename, algorithm):\n        ''' Return hex digest of local file for a digest_method specified by name, or None if file is not present. '''\n        b_filename = to_bytes(filename, errors='surrogate_or_strict')\n\n        if not os.path.exists(b_filename):\n            return None\n        if os.path.isdir(b_filename):\n            self.fail_json(msg=\"attempted to take checksum of directory: %s\" % filename)\n\n        # preserve old behaviour where the third parameter was a hash algorithm object\n        if hasattr(algorithm, 'hexdigest'):\n            digest_method = algorithm\n        else:\n            try:\n                digest_method = AVAILABLE_HASH_ALGORITHMS[algorithm]()\n            except KeyError:\n                self.fail_json(msg=\"Could not hash file '%s' with algorithm '%s'. Available algorithms: %s\" %\n                                   (filename, algorithm, ', '.join(AVAILABLE_HASH_ALGORITHMS)))\n\n        blocksize = 64 * 1024\n        infile = open(os.path.realpath(b_filename), 'rb')\n        block = infile.read(blocksize)\n        while block:\n            digest_method.update(block)\n            block = infile.read(blocksize)\n        infile.close()\n        return digest_method.hexdigest()",
        "begin_line": 2072,
        "end_line": 2098,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.md5#2100",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.md5(self, filename)",
        "snippet": "    def md5(self, filename):\n        ''' Return MD5 hex digest of local file using digest_from_file().\n\n        Do not use this function unless you have no other choice for:\n            1) Optional backwards compatibility\n            2) Compatibility with a third party protocol\n\n        This function will not work on systems complying with FIPS-140-2.\n\n        Most uses of this function can use the module.sha1 function instead.\n        '''\n        if 'md5' not in AVAILABLE_HASH_ALGORITHMS:\n            raise ValueError('MD5 not available.  Possibly running in FIPS mode')\n        return self.digest_from_file(filename, 'md5')",
        "begin_line": 2100,
        "end_line": 2113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.sha1#2115",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.sha1(self, filename)",
        "snippet": "    def sha1(self, filename):\n        ''' Return SHA1 hex digest of local file using digest_from_file(). '''\n        return self.digest_from_file(filename, 'sha1')",
        "begin_line": 2115,
        "end_line": 2117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.sha256#2119",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.sha256(self, filename)",
        "snippet": "    def sha256(self, filename):\n        ''' Return SHA-256 hex digest of local file using digest_from_file(). '''\n        return self.digest_from_file(filename, 'sha256')",
        "begin_line": 2119,
        "end_line": 2121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.backup_local#2123",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.backup_local(self, fn)",
        "snippet": "    def backup_local(self, fn):\n        '''make a date-marked backup of the specified file, return True or False on success or failure'''\n\n        backupdest = ''\n        if os.path.exists(fn):\n            # backups named basename.PID.YYYY-MM-DD@HH:MM:SS~\n            ext = time.strftime(\"%Y-%m-%d@%H:%M:%S~\", time.localtime(time.time()))\n            backupdest = '%s.%s.%s' % (fn, os.getpid(), ext)\n\n            try:\n                self.preserved_copy(fn, backupdest)\n            except (shutil.Error, IOError) as e:\n                self.fail_json(msg='Could not make backup of %s to %s: %s' % (fn, backupdest, to_native(e)))\n\n        return backupdest",
        "begin_line": 2123,
        "end_line": 2137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.cleanup#2139",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.cleanup(self, tmpfile)",
        "snippet": "    def cleanup(self, tmpfile):\n        if os.path.exists(tmpfile):\n            try:\n                os.unlink(tmpfile)\n            except OSError as e:\n                sys.stderr.write(\"could not cleanup %s: %s\" % (tmpfile, to_native(e)))",
        "begin_line": 2139,
        "end_line": 2144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.preserved_copy#2146",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.preserved_copy(self, src, dest)",
        "snippet": "    def preserved_copy(self, src, dest):\n        \"\"\"Copy a file with preserved ownership, permissions and context\"\"\"\n\n        # shutil.copy2(src, dst)\n        #   Similar to shutil.copy(), but metadata is copied as well - in fact,\n        #   this is just shutil.copy() followed by copystat(). This is similar\n        #   to the Unix command cp -p.\n        #\n        # shutil.copystat(src, dst)\n        #   Copy the permission bits, last access time, last modification time,\n        #   and flags from src to dst. The file contents, owner, and group are\n        #   unaffected. src and dst are path names given as strings.\n\n        shutil.copy2(src, dest)\n\n        # Set the context\n        if self.selinux_enabled():\n            context = self.selinux_context(src)\n            self.set_context_if_different(dest, context, False)\n\n        # chown it\n        try:\n            dest_stat = os.stat(src)\n            tmp_stat = os.stat(dest)\n            if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n                os.chown(dest, dest_stat.st_uid, dest_stat.st_gid)\n        except OSError as e:\n            if e.errno != errno.EPERM:\n                raise\n\n        # Set the attributes\n        current_attribs = self.get_file_attributes(src)\n        current_attribs = current_attribs.get('attr_flags', '')\n        self.set_attributes_if_different(dest, current_attribs, True)",
        "begin_line": 2146,
        "end_line": 2179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.atomic_move#2181",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.atomic_move(self, src, dest, unsafe_writes=False)",
        "snippet": "    def atomic_move(self, src, dest, unsafe_writes=False):\n        '''atomically move src to dest, copying attributes from dest, returns true on success\n        it uses os.rename to ensure this as it is an atomic operation, rest of the function is\n        to work around limitations, corner cases and ensure selinux context is saved if possible'''\n        context = None\n        dest_stat = None\n        b_src = to_bytes(src, errors='surrogate_or_strict')\n        b_dest = to_bytes(dest, errors='surrogate_or_strict')\n        if os.path.exists(b_dest):\n            try:\n                dest_stat = os.stat(b_dest)\n\n                # copy mode and ownership\n                os.chmod(b_src, dest_stat.st_mode & PERM_BITS)\n                os.chown(b_src, dest_stat.st_uid, dest_stat.st_gid)\n\n                # try to copy flags if possible\n                if hasattr(os, 'chflags') and hasattr(dest_stat, 'st_flags'):\n                    try:\n                        os.chflags(b_src, dest_stat.st_flags)\n                    except OSError as e:\n                        for err in 'EOPNOTSUPP', 'ENOTSUP':\n                            if hasattr(errno, err) and e.errno == getattr(errno, err):\n                                break\n                        else:\n                            raise\n            except OSError as e:\n                if e.errno != errno.EPERM:\n                    raise\n            if self.selinux_enabled():\n                context = self.selinux_context(dest)\n        else:\n            if self.selinux_enabled():\n                context = self.selinux_default_context(dest)\n\n        creating = not os.path.exists(b_dest)\n\n        try:\n            # Optimistically try a rename, solves some corner cases and can avoid useless work, throws exception if not atomic.\n            os.rename(b_src, b_dest)\n        except (IOError, OSError) as e:\n            if e.errno not in [errno.EPERM, errno.EXDEV, errno.EACCES, errno.ETXTBSY, errno.EBUSY]:\n                # only try workarounds for errno 18 (cross device), 1 (not permitted),  13 (permission denied)\n                # and 26 (text file busy) which happens on vagrant synced folders and other 'exotic' non posix file systems\n                self.fail_json(msg='Could not replace file: %s to %s: %s' % (src, dest, to_native(e)),\n                               exception=traceback.format_exc())\n            else:\n                # Use bytes here.  In the shippable CI, this fails with\n                # a UnicodeError with surrogateescape'd strings for an unknown\n                # reason (doesn't happen in a local Ubuntu16.04 VM)\n                b_dest_dir = os.path.dirname(b_dest)\n                b_suffix = os.path.basename(b_dest)\n                error_msg = None\n                tmp_dest_name = None\n                try:\n                    tmp_dest_fd, tmp_dest_name = tempfile.mkstemp(prefix=b'.ansible_tmp',\n                                                                  dir=b_dest_dir, suffix=b_suffix)\n                except (OSError, IOError) as e:\n                    error_msg = 'The destination directory (%s) is not writable by the current user. Error was: %s' % (os.path.dirname(dest), to_native(e))\n                except TypeError:\n                    # We expect that this is happening because python3.4.x and\n                    # below can't handle byte strings in mkstemp().  Traceback\n                    # would end in something like:\n                    #     file = _os.path.join(dir, pre + name + suf)\n                    # TypeError: can't concat bytes to str\n                    error_msg = ('Failed creating tmp file for atomic move.  This usually happens when using Python3 less than Python3.5. '\n                                 'Please use Python2.x or Python3.5 or greater.')\n                finally:\n                    if error_msg:\n                        if unsafe_writes:\n                            self._unsafe_writes(b_src, b_dest)\n                        else:\n                            self.fail_json(msg=error_msg, exception=traceback.format_exc())\n\n                if tmp_dest_name:\n                    b_tmp_dest_name = to_bytes(tmp_dest_name, errors='surrogate_or_strict')\n\n                    try:\n                        try:\n                            # close tmp file handle before file operations to prevent text file busy errors on vboxfs synced folders (windows host)\n                            os.close(tmp_dest_fd)\n                            # leaves tmp file behind when sudo and not root\n                            try:\n                                shutil.move(b_src, b_tmp_dest_name)\n                            except OSError:\n                                # cleanup will happen by 'rm' of tmpdir\n                                # copy2 will preserve some metadata\n                                shutil.copy2(b_src, b_tmp_dest_name)\n\n                            if self.selinux_enabled():\n                                self.set_context_if_different(\n                                    b_tmp_dest_name, context, False)\n                            try:\n                                tmp_stat = os.stat(b_tmp_dest_name)\n                                if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n                                    os.chown(b_tmp_dest_name, dest_stat.st_uid, dest_stat.st_gid)\n                            except OSError as e:\n                                if e.errno != errno.EPERM:\n                                    raise\n                            try:\n                                os.rename(b_tmp_dest_name, b_dest)\n                            except (shutil.Error, OSError, IOError) as e:\n                                if unsafe_writes and e.errno == errno.EBUSY:\n                                    self._unsafe_writes(b_tmp_dest_name, b_dest)\n                                else:\n                                    self.fail_json(msg='Unable to make %s into to %s, failed final rename from %s: %s' %\n                                                       (src, dest, b_tmp_dest_name, to_native(e)),\n                                                   exception=traceback.format_exc())\n                        except (shutil.Error, OSError, IOError) as e:\n                            self.fail_json(msg='Failed to replace file: %s to %s: %s' % (src, dest, to_native(e)),\n                                           exception=traceback.format_exc())\n                    finally:\n                        self.cleanup(b_tmp_dest_name)\n\n        if creating:\n            # make sure the file has the correct permissions\n            # based on the current value of umask\n            umask = os.umask(0)\n            os.umask(umask)\n            os.chmod(b_dest, DEFAULT_PERM & ~umask)\n            try:\n                os.chown(b_dest, os.geteuid(), os.getegid())\n            except OSError:\n                # We're okay with trying our best here.  If the user is not\n                # root (or old Unices) they won't be able to chown.\n                pass\n\n        if self.selinux_enabled():\n            # rename might not preserve context\n            self.set_context_if_different(dest, context, False)",
        "begin_line": 2181,
        "end_line": 2310,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._unsafe_writes#2312",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._unsafe_writes(self, src, dest)",
        "snippet": "    def _unsafe_writes(self, src, dest):\n        # sadly there are some situations where we cannot ensure atomicity, but only if\n        # the user insists and we get the appropriate error we update the file unsafely\n        try:\n            out_dest = in_src = None\n            try:\n                out_dest = open(dest, 'wb')\n                in_src = open(src, 'rb')\n                shutil.copyfileobj(in_src, out_dest)\n            finally:  # assuring closed files in 2.4 compatible way\n                if out_dest:\n                    out_dest.close()\n                if in_src:\n                    in_src.close()\n        except (shutil.Error, OSError, IOError) as e:\n            self.fail_json(msg='Could not write data to file (%s) from (%s): %s' % (dest, src, to_native(e)),\n                           exception=traceback.format_exc())",
        "begin_line": 2312,
        "end_line": 2328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._read_from_pipes#2330",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._read_from_pipes(self, rpipes, rfds, file_descriptor)",
        "snippet": "    def _read_from_pipes(self, rpipes, rfds, file_descriptor):\n        data = b('')\n        if file_descriptor in rfds:\n            data = os.read(file_descriptor.fileno(), self.get_buffer_size(file_descriptor))\n            if data == b(''):\n                rpipes.remove(file_descriptor)\n\n        return data",
        "begin_line": 2330,
        "end_line": 2337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._clean_args#2339",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._clean_args(self, args)",
        "snippet": "    def _clean_args(self, args):\n\n        if not self._clean:\n            # create a printable version of the command for use in reporting later,\n            # which strips out things like passwords from the args list\n            to_clean_args = args\n            if PY2:\n                if isinstance(args, text_type):\n                    to_clean_args = to_bytes(args)\n            else:\n                if isinstance(args, binary_type):\n                    to_clean_args = to_text(args)\n            if isinstance(args, (text_type, binary_type)):\n                to_clean_args = shlex.split(to_clean_args)\n\n            clean_args = []\n            is_passwd = False\n            for arg in (to_native(a) for a in to_clean_args):\n                if is_passwd:\n                    is_passwd = False\n                    clean_args.append('********')\n                    continue\n                if PASSWD_ARG_RE.match(arg):\n                    sep_idx = arg.find('=')\n                    if sep_idx > -1:\n                        clean_args.append('%s=********' % arg[:sep_idx])\n                        continue\n                    else:\n                        is_passwd = True\n                arg = heuristic_log_sanitize(arg, self.no_log_values)\n                clean_args.append(arg)\n            self._clean = ' '.join(shlex_quote(arg) for arg in clean_args)\n\n        return self._clean",
        "begin_line": 2339,
        "end_line": 2372,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule._restore_signal_handlers#2374",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule._restore_signal_handlers(self)",
        "snippet": "    def _restore_signal_handlers(self):\n        # Reset SIGPIPE to SIG_DFL, otherwise in Python2.7 it gets ignored in subprocesses.\n        if PY2 and sys.platform != 'win32':\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)",
        "begin_line": 2374,
        "end_line": 2377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.run_command#2379",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.run_command(self, args, check_rc=False, close_fds=True, executable=None, data=None, binary_data=False, path_prefix=None, cwd=None, use_unsafe_shell=False, prompt_regex=None, environ_update=None, umask=None, encoding='utf-8', errors='surrogate_or_strict', expand_user_and_vars=True, pass_fds=None, before_communicate_callback=None)",
        "snippet": "    def run_command(self, args, check_rc=False, close_fds=True, executable=None, data=None, binary_data=False, path_prefix=None, cwd=None,\n                    use_unsafe_shell=False, prompt_regex=None, environ_update=None, umask=None, encoding='utf-8', errors='surrogate_or_strict',\n                    expand_user_and_vars=True, pass_fds=None, before_communicate_callback=None):\n        '''\n        Execute a command, returns rc, stdout, and stderr.\n\n        :arg args: is the command to run\n            * If args is a list, the command will be run with shell=False.\n            * If args is a string and use_unsafe_shell=False it will split args to a list and run with shell=False\n            * If args is a string and use_unsafe_shell=True it runs with shell=True.\n        :kw check_rc: Whether to call fail_json in case of non zero RC.\n            Default False\n        :kw close_fds: See documentation for subprocess.Popen(). Default True\n        :kw executable: See documentation for subprocess.Popen(). Default None\n        :kw data: If given, information to write to the stdin of the command\n        :kw binary_data: If False, append a newline to the data.  Default False\n        :kw path_prefix: If given, additional path to find the command in.\n            This adds to the PATH environment variable so helper commands in\n            the same directory can also be found\n        :kw cwd: If given, working directory to run the command inside\n        :kw use_unsafe_shell: See `args` parameter.  Default False\n        :kw prompt_regex: Regex string (not a compiled regex) which can be\n            used to detect prompts in the stdout which would otherwise cause\n            the execution to hang (especially if no input data is specified)\n        :kw environ_update: dictionary to *update* os.environ with\n        :kw umask: Umask to be used when running the command. Default None\n        :kw encoding: Since we return native strings, on python3 we need to\n            know the encoding to use to transform from bytes to text.  If you\n            want to always get bytes back, use encoding=None.  The default is\n            \"utf-8\".  This does not affect transformation of strings given as\n            args.\n        :kw errors: Since we return native strings, on python3 we need to\n            transform stdout and stderr from bytes to text.  If the bytes are\n            undecodable in the ``encoding`` specified, then use this error\n            handler to deal with them.  The default is ``surrogate_or_strict``\n            which means that the bytes will be decoded using the\n            surrogateescape error handler if available (available on all\n            python3 versions we support) otherwise a UnicodeError traceback\n            will be raised.  This does not affect transformations of strings\n            given as args.\n        :kw expand_user_and_vars: When ``use_unsafe_shell=False`` this argument\n            dictates whether ``~`` is expanded in paths and environment variables\n            are expanded before running the command. When ``True`` a string such as\n            ``$SHELL`` will be expanded regardless of escaping. When ``False`` and\n            ``use_unsafe_shell=False`` no path or variable expansion will be done.\n        :kw pass_fds: When running on Python 3 this argument\n            dictates which file descriptors should be passed\n            to an underlying ``Popen`` constructor. On Python 2, this will\n            set ``close_fds`` to False.\n        :kw before_communicate_callback: This function will be called\n            after ``Popen`` object will be created\n            but before communicating to the process.\n            (``Popen`` object will be passed to callback as a first argument)\n        :returns: A 3-tuple of return code (integer), stdout (native string),\n            and stderr (native string).  On python2, stdout and stderr are both\n            byte strings.  On python3, stdout and stderr are text strings converted\n            according to the encoding and errors parameters.  If you want byte\n            strings on python3, use encoding=None to turn decoding to text off.\n        '''\n        # used by clean args later on\n        self._clean = None\n\n        if not isinstance(args, (list, binary_type, text_type)):\n            msg = \"Argument 'args' to run_command must be list or string\"\n            self.fail_json(rc=257, cmd=args, msg=msg)\n\n        shell = False\n        if use_unsafe_shell:\n\n            # stringify args for unsafe/direct shell usage\n            if isinstance(args, list):\n                args = b\" \".join([to_bytes(shlex_quote(x), errors='surrogate_or_strict') for x in args])\n            else:\n                args = to_bytes(args, errors='surrogate_or_strict')\n\n            # not set explicitly, check if set by controller\n            if executable:\n                executable = to_bytes(executable, errors='surrogate_or_strict')\n                args = [executable, b'-c', args]\n            elif self._shell not in (None, '/bin/sh'):\n                args = [to_bytes(self._shell, errors='surrogate_or_strict'), b'-c', args]\n            else:\n                shell = True\n        else:\n            # ensure args are a list\n            if isinstance(args, (binary_type, text_type)):\n                # On python2.6 and below, shlex has problems with text type\n                # On python3, shlex needs a text type.\n                if PY2:\n                    args = to_bytes(args, errors='surrogate_or_strict')\n                elif PY3:\n                    args = to_text(args, errors='surrogateescape')\n                args = shlex.split(args)\n\n            # expand ``~`` in paths, and all environment vars\n            if expand_user_and_vars:\n                args = [to_bytes(os.path.expanduser(os.path.expandvars(x)), errors='surrogate_or_strict') for x in args if x is not None]\n            else:\n                args = [to_bytes(x, errors='surrogate_or_strict') for x in args if x is not None]\n\n        prompt_re = None\n        if prompt_regex:\n            if isinstance(prompt_regex, text_type):\n                if PY3:\n                    prompt_regex = to_bytes(prompt_regex, errors='surrogateescape')\n                elif PY2:\n                    prompt_regex = to_bytes(prompt_regex, errors='surrogate_or_strict')\n            try:\n                prompt_re = re.compile(prompt_regex, re.MULTILINE)\n            except re.error:\n                self.fail_json(msg=\"invalid prompt regular expression given to run_command\")\n\n        rc = 0\n        msg = None\n        st_in = None\n\n        # Manipulate the environ we'll send to the new process\n        old_env_vals = {}\n        # We can set this from both an attribute and per call\n        for key, val in self.run_command_environ_update.items():\n            old_env_vals[key] = os.environ.get(key, None)\n            os.environ[key] = val\n        if environ_update:\n            for key, val in environ_update.items():\n                old_env_vals[key] = os.environ.get(key, None)\n                os.environ[key] = val\n        if path_prefix:\n            old_env_vals['PATH'] = os.environ['PATH']\n            os.environ['PATH'] = \"%s:%s\" % (path_prefix, os.environ['PATH'])\n\n        # If using test-module.py and explode, the remote lib path will resemble:\n        #   /tmp/test_module_scratch/debug_dir/ansible/module_utils/basic.py\n        # If using ansible or ansible-playbook with a remote system:\n        #   /tmp/ansible_vmweLQ/ansible_modlib.zip/ansible/module_utils/basic.py\n\n        # Clean out python paths set by ansiballz\n        if 'PYTHONPATH' in os.environ:\n            pypaths = os.environ['PYTHONPATH'].split(':')\n            pypaths = [x for x in pypaths\n                       if not x.endswith('/ansible_modlib.zip') and\n                       not x.endswith('/debug_dir')]\n            os.environ['PYTHONPATH'] = ':'.join(pypaths)\n            if not os.environ['PYTHONPATH']:\n                del os.environ['PYTHONPATH']\n\n        if data:\n            st_in = subprocess.PIPE\n\n        kwargs = dict(\n            executable=executable,\n            shell=shell,\n            close_fds=close_fds,\n            stdin=st_in,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            preexec_fn=self._restore_signal_handlers,\n        )\n        if PY3 and pass_fds:\n            kwargs[\"pass_fds\"] = pass_fds\n        elif PY2 and pass_fds:\n            kwargs['close_fds'] = False\n\n        # store the pwd\n        prev_dir = os.getcwd()\n\n        # make sure we're in the right working directory\n        if cwd and os.path.isdir(cwd):\n            cwd = to_bytes(os.path.abspath(os.path.expanduser(cwd)), errors='surrogate_or_strict')\n            kwargs['cwd'] = cwd\n            try:\n                os.chdir(cwd)\n            except (OSError, IOError) as e:\n                self.fail_json(rc=e.errno, msg=\"Could not open %s, %s\" % (cwd, to_native(e)),\n                               exception=traceback.format_exc())\n\n        old_umask = None\n        if umask:\n            old_umask = os.umask(umask)\n\n        try:\n            if self._debug:\n                self.log('Executing: ' + self._clean_args(args))\n            cmd = subprocess.Popen(args, **kwargs)\n            if before_communicate_callback:\n                before_communicate_callback(cmd)\n\n            # the communication logic here is essentially taken from that\n            # of the _communicate() function in ssh.py\n\n            stdout = b('')\n            stderr = b('')\n            rpipes = [cmd.stdout, cmd.stderr]\n\n            if data:\n                if not binary_data:\n                    data += '\\n'\n                if isinstance(data, text_type):\n                    data = to_bytes(data)\n                cmd.stdin.write(data)\n                cmd.stdin.close()\n\n            while True:\n                rfds, wfds, efds = select.select(rpipes, [], rpipes, 1)\n                stdout += self._read_from_pipes(rpipes, rfds, cmd.stdout)\n                stderr += self._read_from_pipes(rpipes, rfds, cmd.stderr)\n                # if we're checking for prompts, do it now\n                if prompt_re:\n                    if prompt_re.search(stdout) and not data:\n                        if encoding:\n                            stdout = to_native(stdout, encoding=encoding, errors=errors)\n                        return (257, stdout, \"A prompt was encountered while running a command, but no input data was specified\")\n                # only break out if no pipes are left to read or\n                # the pipes are completely read and\n                # the process is terminated\n                if (not rpipes or not rfds) and cmd.poll() is not None:\n                    break\n                # No pipes are left to read but process is not yet terminated\n                # Only then it is safe to wait for the process to be finished\n                # NOTE: Actually cmd.poll() is always None here if rpipes is empty\n                elif not rpipes and cmd.poll() is None:\n                    cmd.wait()\n                    # The process is terminated. Since no pipes to read from are\n                    # left, there is no need to call select() again.\n                    break\n\n            cmd.stdout.close()\n            cmd.stderr.close()\n\n            rc = cmd.returncode\n        except (OSError, IOError) as e:\n            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(e)))\n            self.fail_json(rc=e.errno, msg=to_native(e), cmd=self._clean_args(args))\n        except Exception as e:\n            self.log(\"Error Executing CMD:%s Exception:%s\" % (self._clean_args(args), to_native(traceback.format_exc())))\n            self.fail_json(rc=257, msg=to_native(e), exception=traceback.format_exc(), cmd=self._clean_args(args))\n\n        # Restore env settings\n        for key, val in old_env_vals.items():\n            if val is None:\n                del os.environ[key]\n            else:\n                os.environ[key] = val\n\n        if old_umask:\n            os.umask(old_umask)\n\n        if rc != 0 and check_rc:\n            msg = heuristic_log_sanitize(stderr.rstrip(), self.no_log_values)\n            self.fail_json(cmd=self._clean_args(args), rc=rc, stdout=stdout, stderr=stderr, msg=msg)\n\n        # reset the pwd\n        os.chdir(prev_dir)\n\n        if encoding is not None:\n            return (rc, to_native(stdout, encoding=encoding, errors=errors),\n                    to_native(stderr, encoding=encoding, errors=errors))\n\n        return (rc, stdout, stderr)",
        "begin_line": 2379,
        "end_line": 2636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.append_to_file#2638",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.append_to_file(self, filename, str)",
        "snippet": "    def append_to_file(self, filename, str):\n        filename = os.path.expandvars(os.path.expanduser(filename))\n        fh = open(filename, 'a')\n        fh.write(str)\n        fh.close()",
        "begin_line": 2638,
        "end_line": 2642,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.bytes_to_human#2644",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.bytes_to_human(self, size)",
        "snippet": "    def bytes_to_human(self, size):\n        return bytes_to_human(size)",
        "begin_line": 2644,
        "end_line": 2645,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.human_to_bytes#2650",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.human_to_bytes(self, number, isbits=False)",
        "snippet": "    def human_to_bytes(self, number, isbits=False):\n        return human_to_bytes(number, isbits)",
        "begin_line": 2650,
        "end_line": 2651,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.AnsibleModule.get_buffer_size#2661",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic.AnsibleModule",
        "signature": "lib.ansible.module_utils.basic.AnsibleModule.get_buffer_size(fd)",
        "snippet": "    def get_buffer_size(fd):\n        try:\n            # 1032 == FZ_GETPIPE_SZ\n            buffer_size = fcntl.fcntl(fd, 1032)\n        except Exception:\n            try:\n                # not as exact as above, but should be good enough for most platforms that fail the previous call\n                buffer_size = select.PIPE_BUF\n            except Exception:\n                buffer_size = 9000  # use sane default JIC\n\n        return buffer_size",
        "begin_line": 2661,
        "end_line": 2672,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.basic.get_module_path#2675",
        "src_path": "lib/ansible/module_utils/basic.py",
        "class_name": "lib.ansible.module_utils.basic",
        "signature": "lib.ansible.module_utils.basic.get_module_path()",
        "snippet": "def get_module_path():\n    return os.path.dirname(os.path.realpath(__file__))",
        "begin_line": 2675,
        "end_line": 2676,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule.__init__#97",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(InventoryModule, self).__init__()\n\n        self.patterns = {}\n        self._filename = None",
        "begin_line": 97,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule.parse#104",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule.parse(self, inventory, loader, path, cache=True)",
        "snippet": "    def parse(self, inventory, loader, path, cache=True):\n\n        super(InventoryModule, self).parse(inventory, loader, path)\n\n        self._filename = path\n\n        try:\n            # Read in the hosts, groups, and variables defined in the inventory file.\n            if self.loader:\n                (b_data, private) = self.loader._get_file_contents(path)\n            else:\n                b_path = to_bytes(path, errors='surrogate_or_strict')\n                with open(b_path, 'rb') as fh:\n                    b_data = fh.read()\n\n            try:\n                # Faster to do to_text once on a long string than many\n                # times on smaller strings\n                data = to_text(b_data, errors='surrogate_or_strict').splitlines()\n            except UnicodeError:\n                # Handle non-utf8 in comment lines: https://github.com/ansible/ansible/issues/17593\n                data = []\n                for line in b_data.splitlines():\n                    if line and line[0] in self.b_COMMENT_MARKERS:\n                        # Replace is okay for comment lines\n                        # data.append(to_text(line, errors='surrogate_then_replace'))\n                        # Currently we only need these lines for accurate lineno in errors\n                        data.append(u'')\n                    else:\n                        # Non-comment lines still have to be valid uf-8\n                        data.append(to_text(line, errors='surrogate_or_strict'))\n\n            self._parse(path, data)\n        except Exception as e:\n            raise AnsibleParserError(e)",
        "begin_line": 104,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._raise_error#140",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._raise_error(self, message)",
        "snippet": "    def _raise_error(self, message):\n        raise AnsibleError(\"%s:%d: \" % (self._filename, self.lineno) + message)",
        "begin_line": 140,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._parse#143",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._parse(self, path, lines)",
        "snippet": "    def _parse(self, path, lines):\n        '''\n        Populates self.groups from the given array of lines. Raises an error on\n        any parse failure.\n        '''\n\n        self._compile_patterns()\n\n        # We behave as though the first line of the inventory is '[ungrouped]',\n        # and begin to look for host definitions. We make a single pass through\n        # each line of the inventory, building up self.groups and adding hosts,\n        # subgroups, and setting variables as we go.\n\n        pending_declarations = {}\n        groupname = 'ungrouped'\n        state = 'hosts'\n        self.lineno = 0\n        for line in lines:\n            self.lineno += 1\n\n            line = line.strip()\n            # Skip empty lines and comments\n            if not line or line[0] in self._COMMENT_MARKERS:\n                continue\n\n            # Is this a [section] header? That tells us what group we're parsing\n            # definitions for, and what kind of definitions to expect.\n\n            m = self.patterns['section'].match(line)\n            if m:\n                (groupname, state) = m.groups()\n\n                groupname = to_safe_group_name(groupname)\n\n                state = state or 'hosts'\n                if state not in ['hosts', 'children', 'vars']:\n                    title = \":\".join(m.groups())\n                    self._raise_error(\"Section [%s] has unknown type: %s\" % (title, state))\n\n                # If we haven't seen this group before, we add a new Group.\n                if groupname not in self.inventory.groups:\n                    # Either [groupname] or [groupname:children] is sufficient to declare a group,\n                    # but [groupname:vars] is allowed only if the # group is declared elsewhere.\n                    # We add the group anyway, but make a note in pending_declarations to check at the end.\n                    #\n                    # It's possible that a group is previously pending due to being defined as a child\n                    # group, in that case we simply pass so that the logic below to process pending\n                    # declarations will take the appropriate action for a pending child group instead of\n                    # incorrectly handling it as a var state pending declaration\n                    if state == 'vars' and groupname not in pending_declarations:\n                        pending_declarations[groupname] = dict(line=self.lineno, state=state, name=groupname)\n\n                    self.inventory.add_group(groupname)\n\n                # When we see a declaration that we've been waiting for, we process and delete.\n                if groupname in pending_declarations and state != 'vars':\n                    if pending_declarations[groupname]['state'] == 'children':\n                        self._add_pending_children(groupname, pending_declarations)\n                    elif pending_declarations[groupname]['state'] == 'vars':\n                        del pending_declarations[groupname]\n\n                continue\n            elif line.startswith('[') and line.endswith(']'):\n                self._raise_error(\"Invalid section entry: '%s'. Please make sure that there are no spaces\" % line + \" \" +\n                                  \"in the section entry, and that there are no other invalid characters\")\n\n            # It's not a section, so the current state tells us what kind of\n            # definition it must be. The individual parsers will raise an\n            # error if we feed them something they can't digest.\n\n            # [groupname] contains host definitions that must be added to\n            # the current group.\n            if state == 'hosts':\n                hosts, port, variables = self._parse_host_definition(line)\n                self._populate_host_vars(hosts, variables, groupname, port)\n\n            # [groupname:vars] contains variable definitions that must be\n            # applied to the current group.\n            elif state == 'vars':\n                (k, v) = self._parse_variable_definition(line)\n                self.inventory.set_variable(groupname, k, v)\n\n            # [groupname:children] contains subgroup names that must be\n            # added as children of the current group. The subgroup names\n            # must themselves be declared as groups, but as before, they\n            # may only be declared later.\n            elif state == 'children':\n                child = self._parse_group_name(line)\n                if child not in self.inventory.groups:\n                    if child not in pending_declarations:\n                        pending_declarations[child] = dict(line=self.lineno, state=state, name=child, parents=[groupname])\n                    else:\n                        pending_declarations[child]['parents'].append(groupname)\n                else:\n                    self.inventory.add_child(groupname, child)\n            else:\n                # This can happen only if the state checker accepts a state that isn't handled above.\n                self._raise_error(\"Entered unhandled state: %s\" % (state))\n\n        # Any entries in pending_declarations not removed by a group declaration above mean that there was an unresolved reference.\n        # We report only the first such error here.\n        for g in pending_declarations:\n            decl = pending_declarations[g]\n            if decl['state'] == 'vars':\n                raise AnsibleError(\"%s:%d: Section [%s:vars] not valid for undefined group: %s\" % (path, decl['line'], decl['name'], decl['name']))\n            elif decl['state'] == 'children':\n                raise AnsibleError(\"%s:%d: Section [%s:children] includes undefined group: %s\" % (path, decl['line'], decl['parents'].pop(), decl['name']))",
        "begin_line": 143,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._add_pending_children#251",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._add_pending_children(self, group, pending)",
        "snippet": "    def _add_pending_children(self, group, pending):\n        for parent in pending[group]['parents']:\n            self.inventory.add_child(parent, group)\n            if parent in pending and pending[parent]['state'] == 'children':\n                self._add_pending_children(parent, pending)\n        del pending[group]",
        "begin_line": 251,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_group_name#258",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_group_name(self, line)",
        "snippet": "    def _parse_group_name(self, line):\n        '''\n        Takes a single line and tries to parse it as a group name. Returns the\n        group name if successful, or raises an error.\n        '''\n\n        m = self.patterns['groupname'].match(line)\n        if m:\n            return m.group(1)\n\n        self._raise_error(\"Expected group name, got: %s\" % (line))",
        "begin_line": 258,
        "end_line": 268,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_variable_definition#270",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_variable_definition(self, line)",
        "snippet": "    def _parse_variable_definition(self, line):\n        '''\n        Takes a string and tries to parse it as a variable definition. Returns\n        the key and value if successful, or raises an error.\n        '''\n\n        # TODO: We parse variable assignments as a key (anything to the left of\n        # an '='\"), an '=', and a value (anything left) and leave the value to\n        # _parse_value to sort out. We should be more systematic here about\n        # defining what is acceptable, how quotes work, and so on.\n\n        if '=' in line:\n            (k, v) = [e.strip() for e in line.split(\"=\", 1)]\n            return (k, self._parse_value(v))\n\n        self._raise_error(\"Expected key=value, got: %s\" % (line))",
        "begin_line": 270,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_host_definition#287",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_host_definition(self, line)",
        "snippet": "    def _parse_host_definition(self, line):\n        '''\n        Takes a single line and tries to parse it as a host definition. Returns\n        a list of Hosts if successful, or raises an error.\n        '''\n\n        # A host definition comprises (1) a non-whitespace hostname or range,\n        # optionally followed by (2) a series of key=\"some value\" assignments.\n        # We ignore any trailing whitespace and/or comments. For example, here\n        # are a series of host definitions in a group:\n        #\n        # [groupname]\n        # alpha\n        # beta:2345 user=admin      # we'll tell shlex\n        # gamma sudo=True user=root # to ignore comments\n\n        try:\n            tokens = shlex_split(line, comments=True)\n        except ValueError as e:\n            self._raise_error(\"Error parsing host definition '%s': %s\" % (line, e))\n\n        (hostnames, port) = self._expand_hostpattern(tokens[0])\n\n        # Try to process anything remaining as a series of key=value pairs.\n        variables = {}\n        for t in tokens[1:]:\n            if '=' not in t:\n                self._raise_error(\"Expected key=value host variable assignment, got: %s\" % (t))\n            (k, v) = t.split('=', 1)\n            variables[k] = self._parse_value(v)\n\n        return hostnames, port, variables",
        "begin_line": 287,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._expand_hostpattern#320",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._expand_hostpattern(self, hostpattern)",
        "snippet": "    def _expand_hostpattern(self, hostpattern):\n        '''\n        do some extra checks over normal processing\n        '''\n        # specification?\n\n        hostnames, port = super(InventoryModule, self)._expand_hostpattern(hostpattern)\n\n        if hostpattern.strip().endswith(':') and port is None:\n            raise AnsibleParserError(\"Invalid host pattern '%s' supplied, ending in ':' is not allowed, this character is reserved to provide a port.\" %\n                                     hostpattern)\n        for pattern in hostnames:\n            # some YAML parsing prevention checks\n            if pattern.strip() == '---':\n                raise AnsibleParserError(\"Invalid host pattern '%s' supplied, '---' is normally a sign this is a YAML file.\" % hostpattern)\n\n        return (hostnames, port)",
        "begin_line": 320,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_value#339",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._parse_value(v)",
        "snippet": "    def _parse_value(v):\n        '''\n        Attempt to transform the string value from an ini file into a basic python object\n        (int, dict, list, unicode string, etc).\n        '''\n        try:\n            v = ast.literal_eval(v)\n        # Using explicit exceptions.\n        # Likely a string that literal_eval does not like. We wil then just set it.\n        except ValueError:\n            # For some reason this was thought to be malformed.\n            pass\n        except SyntaxError:\n            # Is this a hash with an equals at the end?\n            pass\n        return to_text(v, nonstring='passthru', errors='surrogate_or_strict')",
        "begin_line": 339,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.ini.InventoryModule._compile_patterns#356",
        "src_path": "lib/ansible/plugins/inventory/ini.py",
        "class_name": "lib.ansible.plugins.inventory.ini.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.ini.InventoryModule._compile_patterns(self)",
        "snippet": "    def _compile_patterns(self):\n        '''\n        Compiles the regular expressions required to parse the inventory and\n        stores them in self.patterns.\n        '''\n\n        # Section names are square-bracketed expressions at the beginning of a\n        # line, comprising (1) a group name optionally followed by (2) a tag\n        # that specifies the contents of the section. We ignore any trailing\n        # whitespace and/or comments. For example:\n        #\n        # [groupname]\n        # [somegroup:vars]\n        # [naughty:children] # only get coal in their stockings\n\n        self.patterns['section'] = re.compile(\n            to_text(r'''^\\[\n                    ([^:\\]\\s]+)             # group name (see groupname below)\n                    (?::(\\w+))?             # optional : and tag name\n                \\]\n                \\s*                         # ignore trailing whitespace\n                (?:\\#.*)?                   # and/or a comment till the\n                $                           # end of the line\n            ''', errors='surrogate_or_strict'), re.X\n        )\n\n        # FIXME: What are the real restrictions on group names, or rather, what\n        # should they be? At the moment, they must be non-empty sequences of non\n        # whitespace characters excluding ':' and ']', but we should define more\n        # precise rules in order to support better diagnostics.\n\n        self.patterns['groupname'] = re.compile(\n            to_text(r'''^\n                ([^:\\]\\s]+)\n                \\s*                         # ignore trailing whitespace\n                (?:\\#.*)?                   # and/or a comment till the\n                $                           # end of the line\n            ''', errors='surrogate_or_strict'), re.X\n        )",
        "begin_line": 356,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.hashing.secure_hash_s#45",
        "src_path": "lib/ansible/utils/hashing.py",
        "class_name": "lib.ansible.utils.hashing",
        "signature": "lib.ansible.utils.hashing.secure_hash_s(data, hash_func=sha1)",
        "snippet": "def secure_hash_s(data, hash_func=sha1):\n    ''' Return a secure hash hex digest of data. '''\n\n    digest = hash_func()\n    data = to_bytes(data, errors='surrogate_or_strict')\n    digest.update(data)\n    return digest.hexdigest()",
        "begin_line": 45,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008833922261484099,
            "pseudo_dstar_susp": 0.0008764241893076249,
            "pseudo_tarantula_susp": 0.0009337068160597573,
            "pseudo_op2_susp": 0.0008764241893076249,
            "pseudo_barinel_susp": 0.0009337068160597573
        }
    },
    {
        "name": "lib.ansible.utils.hashing.secure_hash#54",
        "src_path": "lib/ansible/utils/hashing.py",
        "class_name": "lib.ansible.utils.hashing",
        "signature": "lib.ansible.utils.hashing.secure_hash(filename, hash_func=sha1)",
        "snippet": "def secure_hash(filename, hash_func=sha1):\n    ''' Return a secure hash hex digest of local file, None if file is not present or a directory. '''\n\n    if not os.path.exists(to_bytes(filename, errors='surrogate_or_strict')) or os.path.isdir(to_bytes(filename, errors='strict')):\n        return None\n    digest = hash_func()\n    blocksize = 64 * 1024\n    try:\n        infile = open(to_bytes(filename, errors='surrogate_or_strict'), 'rb')\n        block = infile.read(blocksize)\n        while block:\n            digest.update(block)\n            block = infile.read(blocksize)\n        infile.close()\n    except IOError as e:\n        raise AnsibleError(\"error while accessing the file %s, error was: %s\" % (filename, e))\n    return digest.hexdigest()",
        "begin_line": 54,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008488964346349745,
            "pseudo_dstar_susp": 0.0008453085376162299,
            "pseudo_tarantula_susp": 0.00089126559714795,
            "pseudo_op2_susp": 0.0008453085376162299,
            "pseudo_barinel_susp": 0.00089126559714795
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.__init__#48",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.__init__(self, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, implicit=False)",
        "snippet": "    def __init__(self, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, implicit=False):\n        self._play = play\n        self._role = role\n        self._parent = None\n        self._dep_chain = None\n        self._use_handlers = use_handlers\n        self._implicit = implicit\n\n        # end of role flag\n        self._eor = False\n\n        if task_include:\n            self._parent = task_include\n        elif parent_block:\n            self._parent = parent_block\n\n        super(Block, self).__init__()",
        "begin_line": 48,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.__eq__#69",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        '''object comparison based on _uuid'''\n        return self._uuid == other._uuid",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.get_vars#77",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.get_vars(self)",
        "snippet": "    def get_vars(self):\n        '''\n        Blocks do not store variables directly, however they may be a member\n        of a role or task include which does, so return those if present.\n        '''\n\n        all_vars = self.vars.copy()\n\n        if self._parent:\n            all_vars.update(self._parent.get_vars())\n\n        return all_vars",
        "begin_line": 77,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.load#91",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.load(data, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None)",
        "snippet": "    def load(data, play=None, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\n        implicit = not Block.is_block(data)\n        b = Block(play=play, parent_block=parent_block, role=role, task_include=task_include, use_handlers=use_handlers, implicit=implicit)\n        return b.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 91,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.21240533717995e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.is_block#97",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.is_block(ds)",
        "snippet": "    def is_block(ds):\n        is_block = False\n        if isinstance(ds, dict):\n            for attr in ('block', 'rescue', 'always'):\n                if attr in ds:\n                    is_block = True\n                    break\n        return is_block",
        "begin_line": 97,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.preprocess_data#106",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        '''\n        If a simple task is given, an implicit block for that single task\n        is created, which goes in the main portion of the block\n        '''\n\n        if not Block.is_block(ds):\n            if isinstance(ds, list):\n                return super(Block, self).preprocess_data(dict(block=ds))\n            else:\n                return super(Block, self).preprocess_data(dict(block=[ds]))\n\n        return super(Block, self).preprocess_data(ds)",
        "begin_line": 106,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._load_block#120",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._load_block(self, attr, ds)",
        "snippet": "    def _load_block(self, attr, ds):\n        try:\n            return load_list_of_tasks(\n                ds,\n                play=self._play,\n                block=self,\n                role=self._role,\n                task_include=None,\n                variable_manager=self._variable_manager,\n                loader=self._loader,\n                use_handlers=self._use_handlers,\n            )\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading a block\", obj=self._ds, orig_exc=e)",
        "begin_line": 120,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._load_rescue#135",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._load_rescue(self, attr, ds)",
        "snippet": "    def _load_rescue(self, attr, ds):\n        try:\n            return load_list_of_tasks(\n                ds,\n                play=self._play,\n                block=self,\n                role=self._role,\n                task_include=None,\n                variable_manager=self._variable_manager,\n                loader=self._loader,\n                use_handlers=self._use_handlers,\n            )\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading rescue.\", obj=self._ds, orig_exc=e)",
        "begin_line": 135,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._load_always#150",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._load_always(self, attr, ds)",
        "snippet": "    def _load_always(self, attr, ds):\n        try:\n            return load_list_of_tasks(\n                ds,\n                play=self._play,\n                block=self,\n                role=self._role,\n                task_include=None,\n                variable_manager=self._variable_manager,\n                loader=self._loader,\n                use_handlers=self._use_handlers,\n            )\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading always\", obj=self._ds, orig_exc=e)",
        "begin_line": 150,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._validate_always#165",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._validate_always(self, attr, name, value)",
        "snippet": "    def _validate_always(self, attr, name, value):\n        if value and not self.block:\n            raise AnsibleParserError(\"'%s' keyword cannot be used without 'block'\" % name, obj=self._ds)",
        "begin_line": 165,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.get_dep_chain#171",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.get_dep_chain(self)",
        "snippet": "    def get_dep_chain(self):\n        if self._dep_chain is None:\n            if self._parent:\n                return self._parent.get_dep_chain()\n            else:\n                return None\n        else:\n            return self._dep_chain[:]",
        "begin_line": 171,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.copy#180",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.copy(self, exclude_parent=False, exclude_tasks=False)",
        "snippet": "    def copy(self, exclude_parent=False, exclude_tasks=False):\n        def _dupe_task_list(task_list, new_block):\n            new_task_list = []\n            for task in task_list:\n                new_task = task.copy(exclude_parent=True)\n                if task._parent:\n                    new_task._parent = task._parent.copy(exclude_tasks=True)\n                    if task._parent == new_block:\n                        # If task._parent is the same as new_block, just replace it\n                        new_task._parent = new_block\n                    else:\n                        # task may not be a direct child of new_block, search for the correct place to insert new_block\n                        cur_obj = new_task._parent\n                        while cur_obj._parent and cur_obj._parent != new_block:\n                            cur_obj = cur_obj._parent\n\n                        cur_obj._parent = new_block\n                else:\n                    new_task._parent = new_block\n                new_task_list.append(new_task)\n            return new_task_list\n\n        new_me = super(Block, self).copy()\n        new_me._play = self._play\n        new_me._use_handlers = self._use_handlers\n        new_me._eor = self._eor\n\n        if self._dep_chain is not None:\n            new_me._dep_chain = self._dep_chain[:]\n\n        new_me._parent = None\n        if self._parent and not exclude_parent:\n            new_me._parent = self._parent.copy(exclude_tasks=True)\n\n        if not exclude_tasks:\n            new_me.block = _dupe_task_list(self.block or [], new_me)\n            new_me.rescue = _dupe_task_list(self.rescue or [], new_me)\n            new_me.always = _dupe_task_list(self.always or [], new_me)\n\n        new_me._role = None\n        if self._role:\n            new_me._role = self._role\n\n        new_me.validate()\n        return new_me",
        "begin_line": 180,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._dupe_task_list#181",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._dupe_task_list(task_list, new_block)",
        "snippet": "        def _dupe_task_list(task_list, new_block):\n            new_task_list = []\n            for task in task_list:\n                new_task = task.copy(exclude_parent=True)\n                if task._parent:\n                    new_task._parent = task._parent.copy(exclude_tasks=True)\n                    if task._parent == new_block:\n                        # If task._parent is the same as new_block, just replace it\n                        new_task._parent = new_block\n                    else:\n                        # task may not be a direct child of new_block, search for the correct place to insert new_block\n                        cur_obj = new_task._parent\n                        while cur_obj._parent and cur_obj._parent != new_block:\n                            cur_obj = cur_obj._parent\n\n                        cur_obj._parent = new_block\n                else:\n                    new_task._parent = new_block\n                new_task_list.append(new_task)\n            return new_task_list",
        "begin_line": 181,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.deserialize#248",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.deserialize(self, data)",
        "snippet": "    def deserialize(self, data):\n        '''\n        Override of the default deserialize method, to match the above overridden\n        serialize method\n        '''\n\n        # import is here to avoid import loops\n        from ansible.playbook.task_include import TaskInclude\n        from ansible.playbook.handler_task_include import HandlerTaskInclude\n\n        # we don't want the full set of attributes (the task lists), as that\n        # would lead to a serialize/deserialize loop\n        for attr in self._valid_attrs:\n            if attr in data and attr not in ('block', 'rescue', 'always'):\n                setattr(self, attr, data.get(attr))\n\n        self._dep_chain = data.get('dep_chain', None)\n        self._eor = data.get('eor', False)\n\n        # if there was a serialized role, unpack it too\n        role_data = data.get('role')\n        if role_data:\n            r = Role()\n            r.deserialize(role_data)\n            self._role = r\n\n        parent_data = data.get('parent')\n        if parent_data:\n            parent_type = data.get('parent_type')\n            if parent_type == 'Block':\n                p = Block()\n            elif parent_type == 'TaskInclude':\n                p = TaskInclude()\n            elif parent_type == 'HandlerTaskInclude':\n                p = HandlerTaskInclude()\n            p.deserialize(parent_data)\n            self._parent = p\n            self._dep_chain = self._parent.get_dep_chain()",
        "begin_line": 248,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.set_loader#287",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.set_loader(self, loader)",
        "snippet": "    def set_loader(self, loader):\n        self._loader = loader\n        if self._parent:\n            self._parent.set_loader(loader)\n        elif self._role:\n            self._role.set_loader(loader)\n\n        dep_chain = self.get_dep_chain()\n        if dep_chain:\n            for dep in dep_chain:\n                dep.set_loader(loader)",
        "begin_line": 287,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block._get_parent_attribute#299",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block._get_parent_attribute(self, attr, extend=False, prepend=False)",
        "snippet": "    def _get_parent_attribute(self, attr, extend=False, prepend=False):\n        '''\n        Generic logic to get the attribute or parent attribute for a block value.\n        '''\n\n        extend = self._valid_attrs[attr].extend\n        prepend = self._valid_attrs[attr].prepend\n        try:\n            value = self._attributes[attr]\n            # If parent is static, we can grab attrs from the parent\n            # otherwise, defer to the grandparent\n            if getattr(self._parent, 'statically_loaded', True):\n                _parent = self._parent\n            else:\n                _parent = self._parent._parent\n\n            if _parent and (value is Sentinel or extend):\n                try:\n                    if getattr(_parent, 'statically_loaded', True):\n                        if hasattr(_parent, '_get_parent_attribute'):\n                            parent_value = _parent._get_parent_attribute(attr)\n                        else:\n                            parent_value = _parent._attributes.get(attr, Sentinel)\n                        if extend:\n                            value = self._extend_value(value, parent_value, prepend)\n                        else:\n                            value = parent_value\n                except AttributeError:\n                    pass\n            if self._role and (value is Sentinel or extend):\n                try:\n                    parent_value = self._role._attributes.get(attr, Sentinel)\n                    if extend:\n                        value = self._extend_value(value, parent_value, prepend)\n                    else:\n                        value = parent_value\n\n                    dep_chain = self.get_dep_chain()\n                    if dep_chain and (value is Sentinel or extend):\n                        dep_chain.reverse()\n                        for dep in dep_chain:\n                            dep_value = dep._attributes.get(attr, Sentinel)\n                            if extend:\n                                value = self._extend_value(value, dep_value, prepend)\n                            else:\n                                value = dep_value\n\n                            if value is not Sentinel and not extend:\n                                break\n                except AttributeError:\n                    pass\n            if self._play and (value is Sentinel or extend):\n                try:\n                    play_value = self._play._attributes.get(attr, Sentinel)\n                    if play_value is not Sentinel:\n                        if extend:\n                            value = self._extend_value(value, play_value, prepend)\n                        else:\n                            value = play_value\n                except AttributeError:\n                    pass\n        except KeyError:\n            pass\n\n        return value",
        "begin_line": 299,
        "end_line": 363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.filter_tagged_tasks#365",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.filter_tagged_tasks(self, all_vars)",
        "snippet": "    def filter_tagged_tasks(self, all_vars):\n        '''\n        Creates a new block, with task lists filtered based on the tags.\n        '''\n\n        def evaluate_and_append_task(target):\n            tmp_list = []\n            for task in target:\n                if isinstance(task, Block):\n                    tmp_list.append(evaluate_block(task))\n                elif (task.action == 'meta' or\n                        (task.action == 'include' and task.evaluate_tags([], self._play.skip_tags, all_vars=all_vars)) or\n                        task.evaluate_tags(self._play.only_tags, self._play.skip_tags, all_vars=all_vars)):\n                    tmp_list.append(task)\n            return tmp_list\n\n        def evaluate_block(block):\n            new_block = block.copy(exclude_parent=True, exclude_tasks=True)\n            new_block._parent = block._parent\n            new_block.block = evaluate_and_append_task(block.block)\n            new_block.rescue = evaluate_and_append_task(block.rescue)\n            new_block.always = evaluate_and_append_task(block.always)\n            return new_block\n\n        return evaluate_block(self)",
        "begin_line": 365,
        "end_line": 389,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.evaluate_and_append_task#370",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.evaluate_and_append_task(target)",
        "snippet": "        def evaluate_and_append_task(target):\n            tmp_list = []\n            for task in target:\n                if isinstance(task, Block):\n                    tmp_list.append(evaluate_block(task))\n                elif (task.action == 'meta' or\n                        (task.action == 'include' and task.evaluate_tags([], self._play.skip_tags, all_vars=all_vars)) or\n                        task.evaluate_tags(self._play.only_tags, self._play.skip_tags, all_vars=all_vars)):\n                    tmp_list.append(task)\n            return tmp_list",
        "begin_line": 370,
        "end_line": 379,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.evaluate_block#381",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.evaluate_block(block)",
        "snippet": "        def evaluate_block(block):\n            new_block = block.copy(exclude_parent=True, exclude_tasks=True)\n            new_block._parent = block._parent\n            new_block.block = evaluate_and_append_task(block.block)\n            new_block.rescue = evaluate_and_append_task(block.rescue)\n            new_block.always = evaluate_and_append_task(block.always)\n            return new_block",
        "begin_line": 381,
        "end_line": 387,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.has_tasks#391",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.has_tasks(self)",
        "snippet": "    def has_tasks(self):\n        return len(self.block) > 0 or len(self.rescue) > 0 or len(self.always) > 0",
        "begin_line": 391,
        "end_line": 392,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.get_include_params#394",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.get_include_params(self)",
        "snippet": "    def get_include_params(self):\n        if self._parent:\n            return self._parent.get_include_params()\n        else:\n            return dict()",
        "begin_line": 394,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.block.Block.all_parents_static#400",
        "src_path": "lib/ansible/playbook/block.py",
        "class_name": "lib.ansible.playbook.block.Block",
        "signature": "lib.ansible.playbook.block.Block.all_parents_static(self)",
        "snippet": "    def all_parents_static(self):\n        '''\n        Determine if all of the parents of this block were statically loaded\n        or not. Since Task/TaskInclude objects may be in the chain, they simply\n        call their parents all_parents_static() method. Only Block objects in\n        the chain check the statically_loaded value of the parent.\n        '''\n        from ansible.playbook.task_include import TaskInclude\n        if self._parent:\n            if isinstance(self._parent, TaskInclude) and not self._parent.statically_loaded:\n                return False\n            return self._parent.all_parents_static()\n\n        return True",
        "begin_line": 400,
        "end_line": 413,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject._get_ansible_position#39",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject._get_ansible_position(self)",
        "snippet": "    def _get_ansible_position(self):\n        return (self._data_source, self._line_number, self._column_number)",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject._set_ansible_position#42",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleBaseYAMLObject._set_ansible_position(self, obj)",
        "snippet": "    def _set_ansible_position(self, obj):\n        try:\n            (src, line, col) = obj\n        except (TypeError, ValueError):\n            raise AssertionError(\n                'ansible_pos can only be set with a tuple/list '\n                'of three values: source, line number, column number'\n            )\n        self._data_source = src\n        self._line_number = line\n        self._column_number = col",
        "begin_line": 42,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.008199593524424e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.from_plaintext#90",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.from_plaintext(cls, seq, vault, secret)",
        "snippet": "    def from_plaintext(cls, seq, vault, secret):\n        if not vault:\n            raise vault.AnsibleVaultError('Error creating AnsibleVaultEncryptedUnicode, invalid vault (%s) provided' % vault)\n\n        ciphertext = vault.encrypt(seq, secret)\n        avu = cls(ciphertext)\n        avu.vault = vault\n        return avu",
        "begin_line": 90,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__init__#99",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__init__(self, ciphertext)",
        "snippet": "    def __init__(self, ciphertext):\n        '''A AnsibleUnicode with a Vault attribute that can decrypt it.\n\n        ciphertext is a byte string (str on PY2, bytestring on PY3).\n\n        The .data attribute is a property that returns the decrypted plaintext\n        of the ciphertext as a PY2 unicode or PY3 string object.\n        '''\n        super(AnsibleVaultEncryptedUnicode, self).__init__()\n\n        # after construction, calling code has to set the .vault attribute to a vaultlib object\n        self.vault = None\n        self._ciphertext = to_bytes(ciphertext)",
        "begin_line": 99,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.data#114",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.data(self)",
        "snippet": "    def data(self):\n        if not self.vault:\n            # FIXME: raise exception?\n            return self._ciphertext\n        return to_text(self.vault.decrypt(self._ciphertext))",
        "begin_line": 114,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.258320257659592e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__eq__#128",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        if self.vault:\n            return other == self.data\n        return False",
        "begin_line": 128,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__hash__#133",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return id(self)",
        "begin_line": 133,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__ne__#136",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        if self.vault:\n            return other != self.data\n        return True",
        "begin_line": 136,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__str__#141",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.__str__(self)",
        "snippet": "    def __str__(self):\n        return to_native(self.data, errors='surrogate_or_strict')",
        "begin_line": 141,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.encode#147",
        "src_path": "lib/ansible/parsing/yaml/objects.py",
        "class_name": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode",
        "signature": "lib.ansible.parsing.yaml.objects.AnsibleVaultEncryptedUnicode.encode(self, encoding=None, errors=None)",
        "snippet": "    def encode(self, encoding=None, errors=None):\n        return self.data.encode(encoding, errors)",
        "begin_line": 147,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.gather_facts.ActionModule._get_module_args#18",
        "src_path": "lib/ansible/plugins/action/gather_facts.py",
        "class_name": "lib.ansible.plugins.action.gather_facts.ActionModule",
        "signature": "lib.ansible.plugins.action.gather_facts.ActionModule._get_module_args(self, fact_module, task_vars)",
        "snippet": "    def _get_module_args(self, fact_module, task_vars):\n\n        mod_args = self._task.args.copy()\n\n        # deal with 'setup specific arguments'\n        if fact_module != 'setup':\n\n            # network facts modules must support gather_subset\n            if self._connection._load_name not in ('network_cli', 'httpapi', 'netconf'):\n                subset = mod_args.pop('gather_subset', None)\n                if subset not in ('all', ['all']):\n                    self._display.warning('Ignoring subset(%s) for %s' % (subset, fact_module))\n\n            timeout = mod_args.pop('gather_timeout', None)\n            if timeout is not None:\n                self._display.warning('Ignoring timeout(%s) for %s' % (timeout, fact_module))\n\n            fact_filter = mod_args.pop('filter', None)\n            if fact_filter is not None:\n                self._display.warning('Ignoring filter(%s) for %s' % (fact_filter, fact_module))\n\n        # Strip out keys with ``None`` values, effectively mimicking ``omit`` behavior\n        # This ensures we don't pass a ``None`` value as an argument expecting a specific type\n        mod_args = dict((k, v) for k, v in mod_args.items() if v is not None)\n\n        # handle module defaults\n        mod_args = get_action_args_with_defaults(fact_module, mod_args, self._task.module_defaults, self._templar)\n\n        return mod_args",
        "begin_line": 18,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.gather_facts.ActionModule.run#57",
        "src_path": "lib/ansible/plugins/action/gather_facts.py",
        "class_name": "lib.ansible.plugins.action.gather_facts.ActionModule",
        "signature": "lib.ansible.plugins.action.gather_facts.ActionModule.run(self, tmp=None, task_vars=None)",
        "snippet": "    def run(self, tmp=None, task_vars=None):\n\n        self._supports_check_mode = True\n\n        result = super(ActionModule, self).run(tmp, task_vars)\n        result['ansible_facts'] = {}\n\n        modules = C.config.get_config_value('FACTS_MODULES', variables=task_vars)\n        parallel = task_vars.pop('ansible_facts_parallel', self._task.args.pop('parallel', None))\n        if 'smart' in modules:\n            connection_map = C.config.get_config_value('CONNECTION_FACTS_MODULES', variables=task_vars)\n            network_os = self._task.args.get('network_os', task_vars.get('ansible_network_os', task_vars.get('ansible_facts', {}).get('network_os')))\n            modules.extend([connection_map.get(network_os or self._connection._load_name, 'setup')])\n            modules.pop(modules.index('smart'))\n\n        failed = {}\n        skipped = {}\n        if parallel is False or (len(modules) == 1 and parallel is None):\n            # serially execute each module\n            for fact_module in modules:\n                # just one module, no need for fancy async\n                mod_args = self._get_module_args(fact_module, task_vars)\n                res = self._execute_module(module_name=fact_module, module_args=mod_args, task_vars=task_vars, wrap_async=False)\n                if res.get('failed', False):\n                    failed[fact_module] = res\n                elif res.get('skipped', False):\n                    skipped[fact_module] = res\n                else:\n                    result = self._combine_task_result(result, res)\n\n            self._remove_tmp_path(self._connection._shell.tmpdir)\n        else:\n            # do it async\n            jobs = {}\n            for fact_module in modules:\n\n                mod_args = self._get_module_args(fact_module, task_vars)\n                self._display.vvvv(\"Running %s\" % fact_module)\n                jobs[fact_module] = (self._execute_module(module_name=fact_module, module_args=mod_args, task_vars=task_vars, wrap_async=True))\n\n            while jobs:\n                for module in jobs:\n                    poll_args = {'jid': jobs[module]['ansible_job_id'], '_async_dir': os.path.dirname(jobs[module]['results_file'])}\n                    res = self._execute_module(module_name='async_status', module_args=poll_args, task_vars=task_vars, wrap_async=False)\n                    if res.get('finished', 0) == 1:\n                        if res.get('failed', False):\n                            failed[module] = res\n                        elif res.get('skipped', False):\n                            skipped[module] = res\n                        else:\n                            result = self._combine_task_result(result, res)\n                        del jobs[module]\n                        break\n                    else:\n                        time.sleep(0.1)\n                else:\n                    time.sleep(0.5)\n\n        if skipped:\n            result['msg'] = \"The following modules were skipped: %s\\n\" % (', '.join(skipped.keys()))\n            result['skipped_modules'] = skipped\n            if len(skipped) == len(modules):\n                result['skipped'] = True\n\n        if failed:\n            result['failed'] = True\n            result['msg'] = \"The following modules failed to execute: %s\\n\" % (', '.join(failed.keys()))\n            result['failed_modules'] = failed\n\n        # tell executor facts were gathered\n        result['ansible_facts']['_ansible_facts_gathered'] = True\n\n        # hack to keep --verbose from showing all the setup module result\n        result['_ansible_verbose_override'] = True\n\n        return result",
        "begin_line": 57,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.linear.StrategyModule._get_next_task_lockstep#81",
        "src_path": "lib/ansible/plugins/strategy/linear.py",
        "class_name": "lib.ansible.plugins.strategy.linear.StrategyModule",
        "signature": "lib.ansible.plugins.strategy.linear.StrategyModule._get_next_task_lockstep(self, hosts, iterator)",
        "snippet": "    def _get_next_task_lockstep(self, hosts, iterator):\n        '''\n        Returns a list of (host, task) tuples, where the task may\n        be a noop task to keep the iterator in lock step across\n        all hosts.\n        '''\n\n        noop_task = Task()\n        noop_task.action = 'meta'\n        noop_task.args['_raw_params'] = 'noop'\n        noop_task.set_loader(iterator._play._loader)\n\n        host_tasks = {}\n        display.debug(\"building list of next tasks for hosts\")\n        for host in hosts:\n            host_tasks[host.name] = iterator.get_next_task_for_host(host, peek=True)\n        display.debug(\"done building task lists\")\n\n        num_setups = 0\n        num_tasks = 0\n        num_rescue = 0\n        num_always = 0\n\n        display.debug(\"counting tasks in each state of execution\")\n        host_tasks_to_run = [(host, state_task)\n                             for host, state_task in iteritems(host_tasks)\n                             if state_task and state_task[1]]\n\n        if host_tasks_to_run:\n            try:\n                lowest_cur_block = min(\n                    (iterator.get_active_state(s).cur_block for h, (s, t) in host_tasks_to_run\n                     if s.run_state != PlayIterator.ITERATING_COMPLETE))\n            except ValueError:\n                lowest_cur_block = None\n        else:\n            # empty host_tasks_to_run will just run till the end of the function\n            # without ever touching lowest_cur_block\n            lowest_cur_block = None\n\n        for (k, v) in host_tasks_to_run:\n            (s, t) = v\n\n            s = iterator.get_active_state(s)\n            if s.cur_block > lowest_cur_block:\n                # Not the current block, ignore it\n                continue\n\n            if s.run_state == PlayIterator.ITERATING_SETUP:\n                num_setups += 1\n            elif s.run_state == PlayIterator.ITERATING_TASKS:\n                num_tasks += 1\n            elif s.run_state == PlayIterator.ITERATING_RESCUE:\n                num_rescue += 1\n            elif s.run_state == PlayIterator.ITERATING_ALWAYS:\n                num_always += 1\n        display.debug(\"done counting tasks in each state of execution:\\n\\tnum_setups: %s\\n\\tnum_tasks: %s\\n\\tnum_rescue: %s\\n\\tnum_always: %s\" % (num_setups,\n                                                                                                                                                  num_tasks,\n                                                                                                                                                  num_rescue,\n                                                                                                                                                  num_always))\n\n        def _advance_selected_hosts(hosts, cur_block, cur_state):\n            '''\n            This helper returns the task for all hosts in the requested\n            state, otherwise they get a noop dummy task. This also advances\n            the state of the host, since the given states are determined\n            while using peek=True.\n            '''\n            # we return the values in the order they were originally\n            # specified in the given hosts array\n            rvals = []\n            display.debug(\"starting to advance hosts\")\n            for host in hosts:\n                host_state_task = host_tasks.get(host.name)\n                if host_state_task is None:\n                    continue\n                (s, t) = host_state_task\n                s = iterator.get_active_state(s)\n                if t is None:\n                    continue\n                if s.run_state == cur_state and s.cur_block == cur_block:\n                    new_t = iterator.get_next_task_for_host(host)\n                    rvals.append((host, t))\n                else:\n                    rvals.append((host, noop_task))\n            display.debug(\"done advancing hosts to next task\")\n            return rvals\n\n        # if any hosts are in ITERATING_SETUP, return the setup task\n        # while all other hosts get a noop\n        if num_setups:\n            display.debug(\"advancing hosts in ITERATING_SETUP\")\n            return _advance_selected_hosts(hosts, lowest_cur_block, PlayIterator.ITERATING_SETUP)\n\n        # if any hosts are in ITERATING_TASKS, return the next normal\n        # task for these hosts, while all other hosts get a noop\n        if num_tasks:\n            display.debug(\"advancing hosts in ITERATING_TASKS\")\n            return _advance_selected_hosts(hosts, lowest_cur_block, PlayIterator.ITERATING_TASKS)\n\n        # if any hosts are in ITERATING_RESCUE, return the next rescue\n        # task for these hosts, while all other hosts get a noop\n        if num_rescue:\n            display.debug(\"advancing hosts in ITERATING_RESCUE\")\n            return _advance_selected_hosts(hosts, lowest_cur_block, PlayIterator.ITERATING_RESCUE)\n\n        # if any hosts are in ITERATING_ALWAYS, return the next always\n        # task for these hosts, while all other hosts get a noop\n        if num_always:\n            display.debug(\"advancing hosts in ITERATING_ALWAYS\")\n            return _advance_selected_hosts(hosts, lowest_cur_block, PlayIterator.ITERATING_ALWAYS)\n\n        # at this point, everything must be ITERATING_COMPLETE, so we\n        # return None for all hosts in the list\n        display.debug(\"all hosts are done, so returning None's for all hosts\")\n        return [(host, None) for host in hosts]",
        "begin_line": 81,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.strategy.linear.StrategyModule._advance_selected_hosts#142",
        "src_path": "lib/ansible/plugins/strategy/linear.py",
        "class_name": "lib.ansible.plugins.strategy.linear.StrategyModule",
        "signature": "lib.ansible.plugins.strategy.linear.StrategyModule._advance_selected_hosts(hosts, cur_block, cur_state)",
        "snippet": "        def _advance_selected_hosts(hosts, cur_block, cur_state):\n            '''\n            This helper returns the task for all hosts in the requested\n            state, otherwise they get a noop dummy task. This also advances\n            the state of the host, since the given states are determined\n            while using peek=True.\n            '''\n            # we return the values in the order they were originally\n            # specified in the given hosts array\n            rvals = []\n            display.debug(\"starting to advance hosts\")\n            for host in hosts:\n                host_state_task = host_tasks.get(host.name)\n                if host_state_task is None:\n                    continue\n                (s, t) = host_state_task\n                s = iterator.get_active_state(s)\n                if t is None:\n                    continue\n                if s.run_state == cur_state and s.cur_block == cur_block:\n                    new_t = iterator.get_next_task_for_host(host)\n                    rvals.append((host, t))\n                else:\n                    rvals.append((host, noop_task))\n            display.debug(\"done advancing hosts to next task\")\n            return rvals",
        "begin_line": 142,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.linux_distribution#153",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro",
        "signature": "lib.ansible.module_utils.distro._distro.linux_distribution(full_distribution_name=True)",
        "snippet": "def linux_distribution(full_distribution_name=True):\n    \"\"\"\n    Return information about the current OS distribution as a tuple\n    ``(id_name, version, codename)`` with items as follows:\n\n    * ``id_name``:  If *full_distribution_name* is false, the result of\n      :func:`distro.id`. Otherwise, the result of :func:`distro.name`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``codename``:  The result of :func:`distro.codename`.\n\n    The interface of this function is compatible with the original\n    :py:func:`platform.linux_distribution` function, supporting a subset of\n    its parameters.\n\n    The data it returns may not exactly be the same, because it uses more data\n    sources than the original function, and that may lead to different data if\n    the OS distribution is not consistent across multiple data sources it\n    provides (there are indeed such distributions ...).\n\n    Another reason for differences is the fact that the :func:`distro.id`\n    method normalizes the distro ID string to a reliable machine-readable value\n    for a number of popular OS distributions.\n    \"\"\"\n    return _distro.linux_distribution(full_distribution_name)",
        "begin_line": 153,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.id#181",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro",
        "signature": "lib.ansible.module_utils.distro._distro.id()",
        "snippet": "def id():\n    \"\"\"\n    Return the distro ID of the current distribution, as a\n    machine-readable string.\n\n    For a number of OS distributions, the returned distro ID value is\n    *reliable*, in the sense that it is documented and that it does not change\n    across releases of the distribution.\n\n    This package maintains the following reliable distro ID values:\n\n    ==============  =========================================\n    Distro ID       Distribution\n    ==============  =========================================\n    \"ubuntu\"        Ubuntu\n    \"debian\"        Debian\n    \"rhel\"          RedHat Enterprise Linux\n    \"centos\"        CentOS\n    \"fedora\"        Fedora\n    \"sles\"          SUSE Linux Enterprise Server\n    \"opensuse\"      openSUSE\n    \"amazon\"        Amazon Linux\n    \"arch\"          Arch Linux\n    \"cloudlinux\"    CloudLinux OS\n    \"exherbo\"       Exherbo Linux\n    \"gentoo\"        GenToo Linux\n    \"ibm_powerkvm\"  IBM PowerKVM\n    \"kvmibm\"        KVM for IBM z Systems\n    \"linuxmint\"     Linux Mint\n    \"mageia\"        Mageia\n    \"mandriva\"      Mandriva Linux\n    \"parallels\"     Parallels\n    \"pidora\"        Pidora\n    \"raspbian\"      Raspbian\n    \"oracle\"        Oracle Linux (and Oracle Enterprise Linux)\n    \"scientific\"    Scientific Linux\n    \"slackware\"     Slackware\n    \"xenserver\"     XenServer\n    \"openbsd\"       OpenBSD\n    \"netbsd\"        NetBSD\n    \"freebsd\"       FreeBSD\n    ==============  =========================================\n\n    If you have a need to get distros for reliable IDs added into this set,\n    or if you find that the :func:`distro.id` function returns a different\n    distro ID for one of the listed distros, please create an issue in the\n    `distro issue tracker`_.\n\n    **Lookup hierarchy and transformations:**\n\n    First, the ID is obtained from the following sources, in the specified\n    order. The first available and non-empty value is used:\n\n    * the value of the \"ID\" attribute of the os-release file,\n\n    * the value of the \"Distributor ID\" attribute returned by the lsb_release\n      command,\n\n    * the first part of the file name of the distro release file,\n\n    The so determined ID value then passes the following transformations,\n    before it is returned by this method:\n\n    * it is translated to lower case,\n\n    * blanks (which should not be there anyway) are translated to underscores,\n\n    * a normalization of the ID is performed, based upon\n      `normalization tables`_. The purpose of this normalization is to ensure\n      that the ID is as reliable as possible, even across incompatible changes\n      in the OS distributions. A common reason for an incompatible change is\n      the addition of an os-release file, or the addition of the lsb_release\n      command, with ID values that differ from what was previously determined\n      from the distro release file name.\n    \"\"\"\n    return _distro.id()",
        "begin_line": 181,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.version#298",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro",
        "signature": "lib.ansible.module_utils.distro._distro.version(pretty=False, best=False)",
        "snippet": "def version(pretty=False, best=False):\n    \"\"\"\n    Return the version of the current OS distribution, as a human-readable\n    string.\n\n    If *pretty* is false, the version is returned without codename (e.g.\n    \"7.0\").\n\n    If *pretty* is true, the codename in parenthesis is appended, if the\n    codename is non-empty (e.g. \"7.0 (Maipo)\").\n\n    Some distributions provide version numbers with different precisions in\n    the different sources of distribution information. Examining the different\n    sources in a fixed priority order does not always yield the most precise\n    version (e.g. for Debian 8.2, or CentOS 7.1).\n\n    The *best* parameter can be used to control the approach for the returned\n    version:\n\n    If *best* is false, the first non-empty version number in priority order of\n    the examined sources is returned.\n\n    If *best* is true, the most precise version number out of all examined\n    sources is returned.\n\n    **Lookup hierarchy:**\n\n    In all cases, the version number is obtained from the following sources.\n    If *best* is false, this order represents the priority order:\n\n    * the value of the \"VERSION_ID\" attribute of the os-release file,\n    * the value of the \"Release\" attribute returned by the lsb_release\n      command,\n    * the version number parsed from the \"<version_id>\" field of the first line\n      of the distro release file,\n    * the version number parsed from the \"PRETTY_NAME\" attribute of the\n      os-release file, if it follows the format of the distro release files.\n    * the version number parsed from the \"Description\" attribute returned by\n      the lsb_release command, if it follows the format of the distro release\n      files.\n    \"\"\"\n    return _distro.version(pretty, best)",
        "begin_line": 298,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.info#439",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro",
        "signature": "lib.ansible.module_utils.distro._distro.info(pretty=False, best=False)",
        "snippet": "def info(pretty=False, best=False):\n    \"\"\"\n    Return certain machine-readable information items about the current OS\n    distribution in a dictionary, as shown in the following example:\n\n    .. sourcecode:: python\n\n        {\n            'id': 'rhel',\n            'version': '7.0',\n            'version_parts': {\n                'major': '7',\n                'minor': '0',\n                'build_number': ''\n            },\n            'like': 'fedora',\n            'codename': 'Maipo'\n        }\n\n    The dictionary structure and keys are always the same, regardless of which\n    information items are available in the underlying data sources. The values\n    for the various keys are as follows:\n\n    * ``id``:  The result of :func:`distro.id`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``version_parts -> major``:  The result of :func:`distro.major_version`.\n\n    * ``version_parts -> minor``:  The result of :func:`distro.minor_version`.\n\n    * ``version_parts -> build_number``:  The result of\n      :func:`distro.build_number`.\n\n    * ``like``:  The result of :func:`distro.like`.\n\n    * ``codename``:  The result of :func:`distro.codename`.\n\n    For a description of the *pretty* and *best* parameters, see the\n    :func:`distro.version` method.\n    \"\"\"\n    return _distro.info(pretty, best)",
        "begin_line": 439,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.os_release_info#483",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro",
        "signature": "lib.ansible.module_utils.distro._distro.os_release_info()",
        "snippet": "def os_release_info():\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the os-release file data source of the current OS distribution.\n\n    See `os-release file`_ for details about these information items.\n    \"\"\"\n    return _distro.os_release_info()",
        "begin_line": 483,
        "end_line": 490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.cached_property.__get__#606",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.cached_property",
        "signature": "lib.ansible.module_utils.distro._distro.cached_property.__get__(self, obj, owner)",
        "snippet": "    def __get__(self, obj, owner):\n        assert obj is not None, 'call {0} on an instance'.format(self._fname)\n        ret = obj.__dict__[self._fname] = self._f(obj)\n        return ret",
        "begin_line": 606,
        "end_line": 609,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.linux_distribution#723",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.linux_distribution(self, full_distribution_name=True)",
        "snippet": "    def linux_distribution(self, full_distribution_name=True):\n        \"\"\"\n        Return information about the OS distribution that is compatible\n        with Python's :func:`platform.linux_distribution`, supporting a subset\n        of its parameters.\n\n        For details, see :func:`distro.linux_distribution`.\n        \"\"\"\n        return (\n            self.name() if full_distribution_name else self.id(),\n            self.version(),\n            self.codename()\n        )",
        "begin_line": 723,
        "end_line": 735,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.id#737",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.id(self)",
        "snippet": "    def id(self):\n        \"\"\"Return the distro ID of the OS distribution, as a string.\n\n        For details, see :func:`distro.id`.\n        \"\"\"\n        def normalize(distro_id, table):\n            distro_id = distro_id.lower().replace(' ', '_')\n            return table.get(distro_id, distro_id)\n\n        distro_id = self.os_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_OS_ID)\n\n        distro_id = self.lsb_release_attr('distributor_id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_LSB_ID)\n\n        distro_id = self.distro_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        distro_id = self.uname_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        return ''",
        "begin_line": 737,
        "end_line": 762,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.normalize#742",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.normalize(distro_id, table)",
        "snippet": "        def normalize(distro_id, table):\n            distro_id = distro_id.lower().replace(' ', '_')\n            return table.get(distro_id, distro_id)",
        "begin_line": 742,
        "end_line": 744,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.name#764",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.name(self, pretty=False)",
        "snippet": "    def name(self, pretty=False):\n        \"\"\"\n        Return the name of the OS distribution, as a string.\n\n        For details, see :func:`distro.name`.\n        \"\"\"\n        name = self.os_release_attr('name') \\\n            or self.lsb_release_attr('distributor_id') \\\n            or self.distro_release_attr('name') \\\n            or self.uname_attr('name')\n        if pretty:\n            name = self.os_release_attr('pretty_name') \\\n                or self.lsb_release_attr('description')\n            if not name:\n                name = self.distro_release_attr('name') \\\n                       or self.uname_attr('name')\n                version = self.version(pretty=True)\n                if version:\n                    name = name + ' ' + version\n        return name or ''",
        "begin_line": 764,
        "end_line": 783,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.version#785",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.version(self, pretty=False, best=False)",
        "snippet": "    def version(self, pretty=False, best=False):\n        \"\"\"\n        Return the version of the OS distribution, as a string.\n\n        For details, see :func:`distro.version`.\n        \"\"\"\n        versions = [\n            self.os_release_attr('version_id'),\n            self.lsb_release_attr('release'),\n            self.distro_release_attr('version_id'),\n            self._parse_distro_release_content(\n                self.os_release_attr('pretty_name')).get('version_id', ''),\n            self._parse_distro_release_content(\n                self.lsb_release_attr('description')).get('version_id', ''),\n            self.uname_attr('release')\n        ]\n        version = ''\n        if best:\n            # This algorithm uses the last version in priority order that has\n            # the best precision. If the versions are not in conflict, that\n            # does not matter; otherwise, using the last one instead of the\n            # first one might be considered a surprise.\n            for v in versions:\n                if v.count(\".\") > version.count(\".\") or version == '':\n                    version = v\n        else:\n            for v in versions:\n                if v != '':\n                    version = v\n                    break\n        if pretty and version and self.codename():\n            version = u'{0} ({1})'.format(version, self.codename())\n        return version",
        "begin_line": 785,
        "end_line": 817,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.version_parts#819",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.version_parts(self, best=False)",
        "snippet": "    def version_parts(self, best=False):\n        \"\"\"\n        Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.\n        \"\"\"\n        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r'(\\d+)\\.?(\\d+)?\\.?(\\d+)?')\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or '', build_number or ''\n        return '', '', ''",
        "begin_line": 819,
        "end_line": 833,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.major_version#835",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.major_version(self, best=False)",
        "snippet": "    def major_version(self, best=False):\n        \"\"\"\n        Return the major version number of the current distribution.\n\n        For details, see :func:`distro.major_version`.\n        \"\"\"\n        return self.version_parts(best)[0]",
        "begin_line": 835,
        "end_line": 841,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.minor_version#843",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.minor_version(self, best=False)",
        "snippet": "    def minor_version(self, best=False):\n        \"\"\"\n        Return the minor version number of the current distribution.\n\n        For details, see :func:`distro.minor_version`.\n        \"\"\"\n        return self.version_parts(best)[1]",
        "begin_line": 843,
        "end_line": 849,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.build_number#851",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.build_number(self, best=False)",
        "snippet": "    def build_number(self, best=False):\n        \"\"\"\n        Return the build number of the current distribution.\n\n        For details, see :func:`distro.build_number`.\n        \"\"\"\n        return self.version_parts(best)[2]",
        "begin_line": 851,
        "end_line": 857,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.like#859",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.like(self)",
        "snippet": "    def like(self):\n        \"\"\"\n        Return the IDs of distributions that are like the OS distribution.\n\n        For details, see :func:`distro.like`.\n        \"\"\"\n        return self.os_release_attr('id_like') or ''",
        "begin_line": 859,
        "end_line": 865,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.codename#867",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.codename(self)",
        "snippet": "    def codename(self):\n        \"\"\"\n        Return the codename of the OS distribution.\n\n        For details, see :func:`distro.codename`.\n        \"\"\"\n        try:\n            # Handle os_release specially since distros might purposefully set\n            # this to empty string to have no codename\n            return self._os_release_info['codename']\n        except KeyError:\n            return self.lsb_release_attr('codename') \\\n                or self.distro_release_attr('codename') \\\n                or ''",
        "begin_line": 867,
        "end_line": 880,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.info#882",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.info(self, pretty=False, best=False)",
        "snippet": "    def info(self, pretty=False, best=False):\n        \"\"\"\n        Return certain machine-readable information about the OS\n        distribution.\n\n        For details, see :func:`distro.info`.\n        \"\"\"\n        return dict(\n            id=self.id(),\n            version=self.version(pretty, best),\n            version_parts=dict(\n                major=self.major_version(best),\n                minor=self.minor_version(best),\n                build_number=self.build_number(best)\n            ),\n            like=self.like(),\n            codename=self.codename(),\n        )",
        "begin_line": 882,
        "end_line": 899,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.os_release_info#901",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.os_release_info(self)",
        "snippet": "    def os_release_info(self):\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the os-release file data source of the OS distribution.\n\n        For details, see :func:`distro.os_release_info`.\n        \"\"\"\n        return self._os_release_info",
        "begin_line": 901,
        "end_line": 908,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.os_release_attr#939",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.os_release_attr(self, attribute)",
        "snippet": "    def os_release_attr(self, attribute):\n        \"\"\"\n        Return a single named information item from the os-release file data\n        source of the OS distribution.\n\n        For details, see :func:`distro.os_release_attr`.\n        \"\"\"\n        return self._os_release_info.get(attribute, '')",
        "begin_line": 939,
        "end_line": 946,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.lsb_release_attr#948",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.lsb_release_attr(self, attribute)",
        "snippet": "    def lsb_release_attr(self, attribute):\n        \"\"\"\n        Return a single named information item from the lsb_release command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.lsb_release_attr`.\n        \"\"\"\n        return self._lsb_release_info.get(attribute, '')",
        "begin_line": 948,
        "end_line": 955,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.distro_release_attr#957",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.distro_release_attr(self, attribute)",
        "snippet": "    def distro_release_attr(self, attribute):\n        \"\"\"\n        Return a single named information item from the distro release file\n        data source of the OS distribution.\n\n        For details, see :func:`distro.distro_release_attr`.\n        \"\"\"\n        return self._distro_release_info.get(attribute, '')",
        "begin_line": 957,
        "end_line": 964,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution.uname_attr#966",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution.uname_attr(self, attribute)",
        "snippet": "    def uname_attr(self, attribute):\n        \"\"\"\n        Return a single named information item from the uname command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.uname_release_attr`.\n        \"\"\"\n        return self._uname_info.get(attribute, '')",
        "begin_line": 966,
        "end_line": 973,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._os_release_info#976",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._os_release_info(self)",
        "snippet": "    def _os_release_info(self):\n        \"\"\"\n        Get the information items from the specified os-release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if os.path.isfile(self.os_release_file):\n            with open(self.os_release_file) as release_file:\n                return self._parse_os_release_content(release_file)\n        return {}",
        "begin_line": 976,
        "end_line": 986,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_os_release_content#989",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_os_release_content(lines)",
        "snippet": "    def _parse_os_release_content(lines):\n        \"\"\"\n        Parse the lines of an os-release file.\n\n        Parameters:\n\n        * lines: Iterable through the lines in the os-release file.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        lexer = shlex.shlex(lines, posix=True)\n        lexer.whitespace_split = True\n\n        # The shlex module defines its `wordchars` variable using literals,\n        # making it dependent on the encoding of the Python source file.\n        # In Python 2.6 and 2.7, the shlex source file is encoded in\n        # 'iso-8859-1', and the `wordchars` variable is defined as a byte\n        # string. This causes a UnicodeDecodeError to be raised when the\n        # parsed content is a unicode object. The following fix resolves that\n        # (... but it should be fixed in shlex...):\n        if sys.version_info[0] == 2 and isinstance(lexer.wordchars, bytes):\n            lexer.wordchars = lexer.wordchars.decode('iso-8859-1')\n\n        tokens = list(lexer)\n        for token in tokens:\n            # At this point, all shell-like parsing has been done (i.e.\n            # comments processed, quotes and backslash escape sequences\n            # processed, multi-line values assembled, trailing newlines\n            # stripped, etc.), so the tokens are now either:\n            # * variable assignments: var=value\n            # * commands or their arguments (not allowed in os-release)\n            if '=' in token:\n                k, v = token.split('=', 1)\n                if isinstance(v, bytes):\n                    v = v.decode('utf-8')\n                props[k.lower()] = v\n            else:\n                # Ignore any tokens that are not variable assignments\n                pass\n\n        if 'version_codename' in props:\n            # os-release added a version_codename field.  Use that in\n            # preference to anything else Note that some distros purposefully\n            # do not have code names.  They should be setting\n            # version_codename=\"\"\n            props['codename'] = props['version_codename']\n        elif 'ubuntu_codename' in props:\n            # Same as above but a non-standard field name used on older Ubuntus\n            props['codename'] = props['ubuntu_codename']\n        elif 'version' in props:\n            # If there is no version_codename, parse it from the version\n            codename = re.search(r'(\\(\\D+\\))|,(\\s+)?\\D+', props['version'])\n            if codename:\n                codename = codename.group()\n                codename = codename.strip('()')\n                codename = codename.strip(',')\n                codename = codename.strip()\n                # codename appears within paranthese.\n                props['codename'] = codename\n\n        return props",
        "begin_line": 989,
        "end_line": 1053,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._lsb_release_info#1056",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._lsb_release_info(self)",
        "snippet": "    def _lsb_release_info(self):\n        \"\"\"\n        Get the information items from the lsb_release command output.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if not self.include_lsb:\n            return {}\n        with open(os.devnull, 'w') as devnull:\n            try:\n                cmd = ('lsb_release', '-a')\n                stdout = _check_output(cmd, stderr=devnull)\n            except OSError:  # Command not found\n                return {}\n        content = stdout.decode(sys.getfilesystemencoding()).splitlines()\n        return self._parse_lsb_release_content(content)",
        "begin_line": 1056,
        "end_line": 1072,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_lsb_release_content#1075",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_lsb_release_content(lines)",
        "snippet": "    def _parse_lsb_release_content(lines):\n        \"\"\"\n        Parse the output of the lsb_release command.\n\n        Parameters:\n\n        * lines: Iterable through the lines of the lsb_release output.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        for line in lines:\n            kv = line.strip('\\n').split(':', 1)\n            if len(kv) != 2:\n                # Ignore lines without colon.\n                continue\n            k, v = kv\n            props.update({k.replace(' ', '_').lower(): v.strip()})\n        return props",
        "begin_line": 1075,
        "end_line": 1096,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._uname_info#1099",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._uname_info(self)",
        "snippet": "    def _uname_info(self):\n        with open(os.devnull, 'w') as devnull:\n            try:\n                cmd = ('uname', '-rs')\n                stdout = _check_output(cmd, stderr=devnull)\n            except OSError:\n                return {}\n        content = stdout.decode(sys.getfilesystemencoding()).splitlines()\n        return self._parse_uname_content(content)",
        "begin_line": 1099,
        "end_line": 1107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_uname_content#1110",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_uname_content(lines)",
        "snippet": "    def _parse_uname_content(lines):\n        props = {}\n        match = re.search(r'^([^\\s]+)\\s+([\\d\\.]+)', lines[0].strip())\n        if match:\n            name, version = match.groups()\n\n            # This is to prevent the Linux kernel version from\n            # appearing as the 'best' version on otherwise\n            # identifiable distributions.\n            if name == 'Linux':\n                return {}\n            props['id'] = name.lower()\n            props['name'] = name\n            props['release'] = version\n        return props",
        "begin_line": 1110,
        "end_line": 1124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._distro_release_info#1127",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._distro_release_info(self)",
        "snippet": "    def _distro_release_info(self):\n        \"\"\"\n        Get the information items from the specified distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if self.distro_release_file:\n            # If it was specified, we use it and parse what we can, even if\n            # its file name or content does not match the expected pattern.\n            distro_info = self._parse_distro_release_file(\n                self.distro_release_file)\n            basename = os.path.basename(self.distro_release_file)\n            # The file name pattern for user-specified distro release files\n            # is somewhat more tolerant (compared to when searching for the\n            # file), because we want to use what was specified as best as\n            # possible.\n            match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n            if 'name' in distro_info \\\n               and 'cloudlinux' in distro_info['name'].lower():\n                distro_info['id'] = 'cloudlinux'\n            elif match:\n                distro_info['id'] = match.group(1)\n            return distro_info\n        else:\n            try:\n                basenames = os.listdir(_UNIXCONFDIR)\n                # We sort for repeatability in cases where there are multiple\n                # distro specific files; e.g. CentOS, Oracle, Enterprise all\n                # containing `redhat-release` on top of their own.\n                basenames.sort()\n            except OSError:\n                # This may occur when /etc is not readable but we can't be\n                # sure about the *-release files. Check common entries of\n                # /etc for information. If they turn out to not be there the\n                # error is handled in `_parse_distro_release_file()`.\n                basenames = ['SuSE-release',\n                             'arch-release',\n                             'base-release',\n                             'centos-release',\n                             'fedora-release',\n                             'gentoo-release',\n                             'mageia-release',\n                             'mandrake-release',\n                             'mandriva-release',\n                             'mandrivalinux-release',\n                             'manjaro-release',\n                             'oracle-release',\n                             'redhat-release',\n                             'sl-release',\n                             'slackware-version']\n            for basename in basenames:\n                if basename in _DISTRO_RELEASE_IGNORE_BASENAMES:\n                    continue\n                match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n                if match:\n                    filepath = os.path.join(_UNIXCONFDIR, basename)\n                    distro_info = self._parse_distro_release_file(filepath)\n                    if 'name' in distro_info:\n                        # The name is always present if the pattern matches\n                        self.distro_release_file = filepath\n                        distro_info['id'] = match.group(1)\n                        if 'cloudlinux' in distro_info['name'].lower():\n                            distro_info['id'] = 'cloudlinux'\n                        return distro_info\n            return {}",
        "begin_line": 1127,
        "end_line": 1192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_distro_release_file#1194",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_distro_release_file(self, filepath)",
        "snippet": "    def _parse_distro_release_file(self, filepath):\n        \"\"\"\n        Parse a distro release file.\n\n        Parameters:\n\n        * filepath: Path name of the distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        try:\n            with open(filepath) as fp:\n                # Only parse the first line. For instance, on SLES there\n                # are multiple lines. We don't want them...\n                return self._parse_distro_release_content(fp.readline())\n        except (OSError, IOError):\n            # Ignore not being able to read a specific, seemingly version\n            # related file.\n            # See https://github.com/nir0s/distro/issues/162\n            return {}",
        "begin_line": 1194,
        "end_line": 1214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_distro_release_content#1217",
        "src_path": "lib/ansible/module_utils/distro/_distro.py",
        "class_name": "lib.ansible.module_utils.distro._distro.LinuxDistribution",
        "signature": "lib.ansible.module_utils.distro._distro.LinuxDistribution._parse_distro_release_content(line)",
        "snippet": "    def _parse_distro_release_content(line):\n        \"\"\"\n        Parse a line from a distro release file.\n\n        Parameters:\n        * line: Line from the distro release file. Must be a unicode string\n                or a UTF-8 encoded byte string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if isinstance(line, bytes):\n            line = line.decode('utf-8')\n        matches = _DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(\n            line.strip()[::-1])\n        distro_info = {}\n        if matches:\n            # regexp ensures non-None\n            distro_info['name'] = matches.group(3)[::-1]\n            if matches.group(2):\n                distro_info['version_id'] = matches.group(2)[::-1]\n            if matches.group(1):\n                distro_info['codename'] = matches.group(1)[::-1]\n        elif line:\n            distro_info['name'] = line.strip()\n        return distro_info",
        "begin_line": 1217,
        "end_line": 1242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.__init__.get_collections_galaxy_meta_info#37",
        "src_path": "lib/ansible/galaxy/__init__.py",
        "class_name": "lib.ansible.galaxy.__init__",
        "signature": "lib.ansible.galaxy.__init__.get_collections_galaxy_meta_info()",
        "snippet": "def get_collections_galaxy_meta_info():\n    meta_path = os.path.join(os.path.dirname(__file__), 'data', 'collections_galaxy_meta.yml')\n    with open(to_bytes(meta_path, errors='surrogate_or_strict'), 'rb') as galaxy_obj:\n        return yaml.safe_load(galaxy_obj)",
        "begin_line": 37,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007993605115907274,
            "pseudo_dstar_susp": 0.0007968127490039841,
            "pseudo_tarantula_susp": 0.0008340283569641367,
            "pseudo_op2_susp": 0.0007968127490039841,
            "pseudo_barinel_susp": 0.0008340283569641367
        }
    },
    {
        "name": "lib.ansible.galaxy.__init__.Galaxy.__init__#46",
        "src_path": "lib/ansible/galaxy/__init__.py",
        "class_name": "lib.ansible.galaxy.__init__.Galaxy",
        "signature": "lib.ansible.galaxy.__init__.Galaxy.__init__(self)",
        "snippet": "    def __init__(self):\n        # TODO: eventually remove this as it contains a mismash of properties that aren't really global\n\n        # roles_path needs to be a list and will be by default\n        roles_path = context.CLIARGS.get('roles_path', C.DEFAULT_ROLES_PATH)\n        # cli option handling is responsible for splitting roles_path\n        self.roles_paths = roles_path\n\n        self.roles = {}\n\n        # load data path for resource usage\n        this_dir, this_filename = os.path.split(__file__)\n        type_path = context.CLIARGS.get('role_type', 'default')\n        if type_path == 'default':\n            type_path = os.path.join(type_path, context.CLIARGS.get('type'))\n\n        self.DATA_PATH = os.path.join(this_dir, 'data', type_path)",
        "begin_line": 46,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017211703958691911,
            "pseudo_dstar_susp": 0.002325581395348837,
            "pseudo_tarantula_susp": 0.0014104372355430183,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0014104372355430183
        }
    },
    {
        "name": "lib.ansible.galaxy.__init__.Galaxy.default_role_skeleton_path#65",
        "src_path": "lib/ansible/galaxy/__init__.py",
        "class_name": "lib.ansible.galaxy.__init__.Galaxy",
        "signature": "lib.ansible.galaxy.__init__.Galaxy.default_role_skeleton_path(self)",
        "snippet": "    def default_role_skeleton_path(self):\n        return self.DATA_PATH",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.vars.AnsibleJ2Vars.__init__#43",
        "src_path": "lib/ansible/template/vars.py",
        "class_name": "lib.ansible.template.vars.AnsibleJ2Vars",
        "signature": "lib.ansible.template.vars.AnsibleJ2Vars.__init__(self, templar, globals, locals=None, *extras)",
        "snippet": "    def __init__(self, templar, globals, locals=None, *extras):\n        '''\n        Initializes this object with a valid Templar() object, as\n        well as several dictionaries of variables representing\n        different scopes (in jinja2 terminology).\n        '''\n\n        self._templar = templar\n        self._globals = globals\n        self._extras = extras\n        self._locals = dict()\n        if isinstance(locals, dict):\n            for key, val in iteritems(locals):\n                if val is not missing:\n                    if key[:2] == 'l_':\n                        self._locals[key[2:]] = val\n                    elif key not in ('context', 'environment', 'template'):\n                        self._locals[key] = val",
        "begin_line": 43,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005868544600938967,
            "pseudo_dstar_susp": 0.0005868544600938967,
            "pseudo_tarantula_susp": 0.0005871990604815032,
            "pseudo_op2_susp": 0.0005868544600938967,
            "pseudo_barinel_susp": 0.0005871990604815032
        }
    },
    {
        "name": "lib.ansible.template.vars.AnsibleJ2Vars.__contains__#62",
        "src_path": "lib/ansible/template/vars.py",
        "class_name": "lib.ansible.template.vars.AnsibleJ2Vars",
        "signature": "lib.ansible.template.vars.AnsibleJ2Vars.__contains__(self, k)",
        "snippet": "    def __contains__(self, k):\n        if k in self._templar.available_variables:\n            return True\n        if k in self._locals:\n            return True\n        for i in self._extras:\n            if k in i:\n                return True\n        if k in self._globals:\n            return True\n        return False",
        "begin_line": 62,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006811989100817438,
            "pseudo_dstar_susp": 0.0006811989100817438,
            "pseudo_tarantula_susp": 0.0006821282401091405,
            "pseudo_op2_susp": 0.0006811989100817438,
            "pseudo_barinel_susp": 0.0006821282401091405
        }
    },
    {
        "name": "lib.ansible.template.vars.AnsibleJ2Vars.__iter__#74",
        "src_path": "lib/ansible/template/vars.py",
        "class_name": "lib.ansible.template.vars.AnsibleJ2Vars",
        "signature": "lib.ansible.template.vars.AnsibleJ2Vars.__iter__(self)",
        "snippet": "    def __iter__(self):\n        keys = set()\n        keys.update(self._templar.available_variables, self._locals, self._globals, *self._extras)\n        return iter(keys)",
        "begin_line": 74,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.template.vars.AnsibleJ2Vars.__getitem__#84",
        "src_path": "lib/ansible/template/vars.py",
        "class_name": "lib.ansible.template.vars.AnsibleJ2Vars",
        "signature": "lib.ansible.template.vars.AnsibleJ2Vars.__getitem__(self, varname)",
        "snippet": "    def __getitem__(self, varname):\n        if varname not in self._templar.available_variables:\n            if varname in self._locals:\n                return self._locals[varname]\n            for i in self._extras:\n                if varname in i:\n                    return i[varname]\n            if varname in self._globals:\n                return self._globals[varname]\n            else:\n                raise KeyError(\"undefined variable: %s\" % varname)\n\n        variable = self._templar.available_variables[varname]\n\n        # HostVars is special, return it as-is, as is the special variable\n        # 'vars', which contains the vars structure\n        from ansible.vars.hostvars import HostVars\n        if isinstance(variable, dict) and varname == \"vars\" or isinstance(variable, HostVars) or hasattr(variable, '__UNSAFE__'):\n            return variable\n        else:\n            value = None\n            try:\n                value = self._templar.template(variable)\n            except AnsibleUndefinedVariable:\n                raise\n            except Exception as e:\n                msg = getattr(e, 'message', None) or to_native(e)\n                raise AnsibleError(\"An unhandled exception occurred while templating '%s'. \"\n                                   \"Error was a %s, original message: %s\" % (to_native(variable), type(e), msg))\n\n            return value",
        "begin_line": 84,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006963788300835655,
            "pseudo_dstar_susp": 0.0006963788300835655,
            "pseudo_tarantula_susp": 0.000700770847932726,
            "pseudo_op2_susp": 0.0006963788300835655,
            "pseudo_barinel_susp": 0.000700770847932726
        }
    },
    {
        "name": "lib.ansible.template.vars.AnsibleJ2Vars.add_locals#116",
        "src_path": "lib/ansible/template/vars.py",
        "class_name": "lib.ansible.template.vars.AnsibleJ2Vars",
        "signature": "lib.ansible.template.vars.AnsibleJ2Vars.add_locals(self, locals)",
        "snippet": "    def add_locals(self, locals):\n        '''\n        If locals are provided, create a copy of self containing those\n        locals in addition to what is already in this variable proxy.\n        '''\n        if locals is None:\n            return self\n\n        # FIXME run this only on jinja2>=2.9?\n        # prior to version 2.9, locals contained all of the vars and not just the current\n        # local vars so this was not necessary for locals to propagate down to nested includes\n        new_locals = self._locals.copy()\n        new_locals.update(locals)\n\n        return AnsibleJ2Vars(self._templar, self._globals, locals=new_locals, *self._extras)",
        "begin_line": 116,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006675567423230974,
            "pseudo_dstar_susp": 0.0006675567423230974,
            "pseudo_tarantula_susp": 0.0006684491978609625,
            "pseudo_op2_susp": 0.0006675567423230974,
            "pseudo_barinel_susp": 0.0006684491978609625
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.is_netmask#16",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.is_netmask(val)",
        "snippet": "def is_netmask(val):\n    parts = str(val).split('.')\n    if not len(parts) == 4:\n        return False\n    for part in parts:\n        try:\n            if int(part) not in VALID_MASKS:\n                raise ValueError\n        except ValueError:\n            return False\n    return True",
        "begin_line": 16,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.is_masklen#29",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.is_masklen(val)",
        "snippet": "def is_masklen(val):\n    try:\n        return 0 <= int(val) <= 32\n    except ValueError:\n        return False",
        "begin_line": 29,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.to_netmask#36",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.to_netmask(val)",
        "snippet": "def to_netmask(val):\n    \"\"\" converts a masklen to a netmask \"\"\"\n    if not is_masklen(val):\n        raise ValueError('invalid value for masklen')\n\n    bits = 0\n    for i in range(32 - int(val), 32):\n        bits |= (1 << i)\n\n    return inet_ntoa(pack('>I', bits))",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.to_masklen#48",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.to_masklen(val)",
        "snippet": "def to_masklen(val):\n    \"\"\" converts a netmask to a masklen \"\"\"\n    if not is_netmask(val):\n        raise ValueError('invalid value for netmask: %s' % val)\n\n    bits = list()\n    for x in val.split('.'):\n        octet = bin(int(x)).count('1')\n        bits.append(octet)\n\n    return sum(bits)",
        "begin_line": 48,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.to_subnet#61",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.to_subnet(addr, mask, dotted_notation=False)",
        "snippet": "def to_subnet(addr, mask, dotted_notation=False):\n    \"\"\" coverts an addr / mask pair to a subnet in cidr notation \"\"\"\n    try:\n        if not is_masklen(mask):\n            raise ValueError\n        cidr = int(mask)\n        mask = to_netmask(mask)\n    except ValueError:\n        cidr = to_masklen(mask)\n\n    addr = addr.split('.')\n    mask = mask.split('.')\n\n    network = list()\n    for s_addr, s_mask in zip(addr, mask):\n        network.append(str(int(s_addr) & int(s_mask)))\n\n    if dotted_notation:\n        return '%s %s' % ('.'.join(network), to_netmask(cidr))\n    return '%s/%s' % ('.'.join(network), cidr)",
        "begin_line": 61,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.network.to_ipv6_network#113",
        "src_path": "lib/ansible/module_utils/common/network.py",
        "class_name": "lib.ansible.module_utils.common.network",
        "signature": "lib.ansible.module_utils.common.network.to_ipv6_network(addr)",
        "snippet": "def to_ipv6_network(addr):\n    \"\"\" IPv6 addresses are eight groupings. The first three groupings (48 bits) comprise the network address. \"\"\"\n\n    # Split by :: to identify omitted zeros\n    ipv6_prefix = addr.split('::')[0]\n\n    # Get the first three groups, or as many as are found + ::\n    found_groups = []\n    for group in ipv6_prefix.split(':'):\n        found_groups.append(group)\n        if len(found_groups) == 3:\n            break\n    if len(found_groups) < 3:\n        found_groups.append('::')\n\n    # Concatenate network address parts\n    network_addr = ''\n    for group in found_groups:\n        if group != '::':\n            network_addr += str(group)\n        network_addr += str(':')\n\n    # Ensure network address ends with ::\n    if not network_addr.endswith('::'):\n        network_addr += str(':')\n    return network_addr",
        "begin_line": 113,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.order_patterns#66",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager",
        "signature": "lib.ansible.inventory.manager.order_patterns(patterns)",
        "snippet": "def order_patterns(patterns):\n    ''' takes a list of patterns and reorders them by modifier to apply them consistently '''\n\n    # FIXME: this goes away if we apply patterns incrementally or by groups\n    pattern_regular = []\n    pattern_intersection = []\n    pattern_exclude = []\n    for p in patterns:\n        if not p:\n            continue\n\n        if p[0] == \"!\":\n            pattern_exclude.append(p)\n        elif p[0] == \"&\":\n            pattern_intersection.append(p)\n        else:\n            pattern_regular.append(p)\n\n    # if no regular pattern was given, hence only exclude and/or intersection\n    # make that magically work\n    if pattern_regular == []:\n        pattern_regular = ['all']\n\n    # when applying the host selectors, run those without the \"&\" or \"!\"\n    # first, then the &s, then the !s.\n    return pattern_regular + pattern_intersection + pattern_exclude",
        "begin_line": 66,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.split_host_pattern#94",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager",
        "signature": "lib.ansible.inventory.manager.split_host_pattern(pattern)",
        "snippet": "def split_host_pattern(pattern):\n    \"\"\"\n    Takes a string containing host patterns separated by commas (or a list\n    thereof) and returns a list of single patterns (which may not contain\n    commas). Whitespace is ignored.\n\n    Also accepts ':' as a separator for backwards compatibility, but it is\n    not recommended due to the conflict with IPv6 addresses and host ranges.\n\n    Example: 'a,b[1], c[2:3] , d' -> ['a', 'b[1]', 'c[2:3]', 'd']\n    \"\"\"\n\n    if isinstance(pattern, list):\n        results = (split_host_pattern(p) for p in pattern)\n        # flatten the results\n        return list(itertools.chain.from_iterable(results))\n    elif not isinstance(pattern, string_types):\n        pattern = to_text(pattern, errors='surrogate_or_strict')\n\n    # If it's got commas in it, we'll treat it as a straightforward\n    # comma-separated list of patterns.\n    if u',' in pattern:\n        patterns = pattern.split(u',')\n\n    # If it doesn't, it could still be a single pattern. This accounts for\n    # non-separator uses of colons: IPv6 addresses and [x:y] host ranges.\n    else:\n        try:\n            (base, port) = parse_address(pattern, allow_ranges=True)\n            patterns = [pattern]\n        except Exception:\n            # The only other case we accept is a ':'-separated list of patterns.\n            # This mishandles IPv6 addresses, and is retained only for backwards\n            # compatibility.\n            patterns = re.findall(\n                to_text(r'''(?:     # We want to match something comprising:\n                        [^\\s:\\[\\]]  # (anything other than whitespace or ':[]'\n                        |           # ...or...\n                        \\[[^\\]]*\\]  # a single complete bracketed expression)\n                    )+              # occurring once or more\n                '''), pattern, re.X\n            )\n\n    return [p.strip() for p in patterns if p.strip()]",
        "begin_line": 94,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.__init__#143",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.__init__(self, loader, sources=None)",
        "snippet": "    def __init__(self, loader, sources=None):\n\n        # base objects\n        self._loader = loader\n        self._inventory = InventoryData()\n\n        # a list of host(names) to contain current inquiries to\n        self._restriction = None\n        self._subset = None\n\n        # caches\n        self._hosts_patterns_cache = {}  # resolved full patterns\n        self._pattern_cache = {}  # resolved individual patterns\n\n        # the inventory dirs, files, script paths or lists of hosts\n        if sources is None:\n            self._sources = []\n        elif isinstance(sources, string_types):\n            self._sources = [sources]\n        else:\n            self._sources = sources\n\n        # get to work!\n        self.parse_sources(cache=True)",
        "begin_line": 143,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.groups#173",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.groups(self)",
        "snippet": "    def groups(self):\n        return self._inventory.groups",
        "begin_line": 173,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.hosts#177",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.hosts(self)",
        "snippet": "    def hosts(self):\n        return self._inventory.hosts",
        "begin_line": 177,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.get_host#193",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.get_host(self, hostname)",
        "snippet": "    def get_host(self, hostname):\n        return self._inventory.get_host(hostname)",
        "begin_line": 193,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._fetch_inventory_plugins#196",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._fetch_inventory_plugins(self)",
        "snippet": "    def _fetch_inventory_plugins(self):\n        ''' sets up loaded inventory plugins for usage '''\n\n        display.vvvv('setting up inventory plugins')\n\n        plugins = []\n        for name in C.INVENTORY_ENABLED:\n            plugin = inventory_loader.get(name)\n            if plugin:\n                plugins.append(plugin)\n            else:\n                display.warning('Failed to load inventory plugin, skipping %s' % name)\n\n        if not plugins:\n            raise AnsibleError(\"No inventory plugins available to generate inventory, make sure you have at least one whitelisted.\")\n\n        return plugins",
        "begin_line": 196,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.parse_sources#214",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.parse_sources(self, cache=False)",
        "snippet": "    def parse_sources(self, cache=False):\n        ''' iterate over inventory sources and parse each one to populate it'''\n\n        parsed = False\n        # allow for multiple inventory parsing\n        for source in self._sources:\n\n            if source:\n                if ',' not in source:\n                    source = unfrackpath(source, follow=False)\n                parse = self.parse_source(source, cache=cache)\n                if parse and not parsed:\n                    parsed = True\n\n        if parsed:\n            # do post processing\n            self._inventory.reconcile_inventory()\n        else:\n            if C.INVENTORY_UNPARSED_IS_FAILED:\n                raise AnsibleError(\"No inventory was parsed, please check your configuration and options.\")\n            else:\n                display.warning(\"No inventory was parsed, only implicit localhost is available\")\n\n        for group in self.groups.values():\n            group.vars = combine_vars(group.vars, get_vars_from_inventory_sources(self._loader, self._sources, [group], 'inventory'))\n        for host in self.hosts.values():\n            host.vars = combine_vars(host.vars, get_vars_from_inventory_sources(self._loader, self._sources, [host], 'inventory'))",
        "begin_line": 214,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.parse_source#242",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.parse_source(self, source, cache=False)",
        "snippet": "    def parse_source(self, source, cache=False):\n        ''' Generate or update inventory for the source provided '''\n\n        parsed = False\n        display.debug(u'Examining possible inventory source: %s' % source)\n\n        # use binary for path functions\n        b_source = to_bytes(source)\n\n        # process directories as a collection of inventories\n        if os.path.isdir(b_source):\n            display.debug(u'Searching for inventory files in directory: %s' % source)\n            for i in sorted(os.listdir(b_source)):\n\n                display.debug(u'Considering %s' % i)\n                # Skip hidden files and stuff we explicitly ignore\n                if IGNORED.search(i):\n                    continue\n\n                # recursively deal with directory entries\n                fullpath = to_text(os.path.join(b_source, i), errors='surrogate_or_strict')\n                parsed_this_one = self.parse_source(fullpath, cache=cache)\n                display.debug(u'parsed %s as %s' % (fullpath, parsed_this_one))\n                if not parsed:\n                    parsed = parsed_this_one\n        else:\n            # left with strings or files, let plugins figure it out\n\n            # set so new hosts can use for inventory_file/dir vars\n            self._inventory.current_source = source\n\n            # try source with each plugin\n            failures = []\n            for plugin in self._fetch_inventory_plugins():\n\n                plugin_name = to_text(getattr(plugin, '_load_name', getattr(plugin, '_original_path', '')))\n                display.debug(u'Attempting to use plugin %s (%s)' % (plugin_name, plugin._original_path))\n\n                # initialize and figure out if plugin wants to attempt parsing this file\n                try:\n                    plugin_wants = bool(plugin.verify_file(source))\n                except Exception:\n                    plugin_wants = False\n\n                if plugin_wants:\n                    try:\n                        # FIXME in case plugin fails 1/2 way we have partial inventory\n                        plugin.parse(self._inventory, self._loader, source, cache=cache)\n                        try:\n                            plugin.update_cache_if_changed()\n                        except AttributeError:\n                            # some plugins might not implement caching\n                            pass\n                        parsed = True\n                        display.vvv('Parsed %s inventory source with %s plugin' % (source, plugin_name))\n                        break\n                    except AnsibleParserError as e:\n                        display.debug('%s was not parsable by %s' % (source, plugin_name))\n                        tb = ''.join(traceback.format_tb(sys.exc_info()[2]))\n                        failures.append({'src': source, 'plugin': plugin_name, 'exc': e, 'tb': tb})\n                    except Exception as e:\n                        display.debug('%s failed while attempting to parse %s' % (plugin_name, source))\n                        tb = ''.join(traceback.format_tb(sys.exc_info()[2]))\n                        failures.append({'src': source, 'plugin': plugin_name, 'exc': AnsibleError(e), 'tb': tb})\n                else:\n                    display.vvv(\"%s declined parsing %s as it did not pass its verify_file() method\" % (plugin_name, source))\n            else:\n                if not parsed and failures:\n                    # only if no plugin processed files should we show errors.\n                    for fail in failures:\n                        display.warning(u'\\n* Failed to parse %s with %s plugin: %s' % (to_text(fail['src']), fail['plugin'], to_text(fail['exc'])))\n                        if 'tb' in fail:\n                            display.vvv(to_text(fail['tb']))\n                    if C.INVENTORY_ANY_UNPARSED_IS_FAILED:\n                        raise AnsibleError(u'Completely failed to parse inventory source %s' % (source))\n        if not parsed:\n            if source != '/etc/ansible/hosts' or os.path.exists(source):\n                # only warn if NOT using the default and if using it, only if the file is present\n                display.warning(\"Unable to parse %s as an inventory source\" % source)\n\n        # clear up, jic\n        self._inventory.current_source = None\n\n        return parsed",
        "begin_line": 242,
        "end_line": 325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._match_list#340",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._match_list(self, items, pattern_str)",
        "snippet": "    def _match_list(self, items, pattern_str):\n        # compile patterns\n        try:\n            if not pattern_str[0] == '~':\n                pattern = re.compile(fnmatch.translate(pattern_str))\n            else:\n                pattern = re.compile(pattern_str[1:])\n        except Exception:\n            raise AnsibleError('Invalid host list pattern: %s' % pattern_str)\n\n        # apply patterns\n        results = []\n        for item in items:\n            if pattern.match(item):\n                results.append(item)\n        return results",
        "begin_line": 340,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.get_hosts#357",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.get_hosts(self, pattern='all', ignore_limits=False, ignore_restrictions=False, order=None)",
        "snippet": "    def get_hosts(self, pattern=\"all\", ignore_limits=False, ignore_restrictions=False, order=None):\n        \"\"\"\n        Takes a pattern or list of patterns and returns a list of matching\n        inventory host names, taking into account any active restrictions\n        or applied subsets\n        \"\"\"\n\n        hosts = []\n\n        # Check if pattern already computed\n        if isinstance(pattern, list):\n            pattern_list = pattern[:]\n        else:\n            pattern_list = [pattern]\n\n        if pattern_list:\n            if not ignore_limits and self._subset:\n                pattern_list.extend(self._subset)\n\n            if not ignore_restrictions and self._restriction:\n                pattern_list.extend(self._restriction)\n\n            # This is only used as a hash key in the self._hosts_patterns_cache dict\n            # a tuple is faster than stringifying\n            pattern_hash = tuple(pattern_list)\n\n            if pattern_hash not in self._hosts_patterns_cache:\n\n                patterns = split_host_pattern(pattern)\n                hosts = self._evaluate_patterns(patterns)\n\n                # mainly useful for hostvars[host] access\n                if not ignore_limits and self._subset:\n                    # exclude hosts not in a subset, if defined\n                    subset_uuids = set(s._uuid for s in self._evaluate_patterns(self._subset))\n                    hosts = [h for h in hosts if h._uuid in subset_uuids]\n\n                if not ignore_restrictions and self._restriction:\n                    # exclude hosts mentioned in any restriction (ex: failed hosts)\n                    hosts = [h for h in hosts if h.name in self._restriction]\n\n                self._hosts_patterns_cache[pattern_hash] = deduplicate_list(hosts)\n\n            # sort hosts list if needed (should only happen when called from strategy)\n            if order in ['sorted', 'reverse_sorted']:\n                hosts = sorted(self._hosts_patterns_cache[pattern_hash][:], key=attrgetter('name'), reverse=(order == 'reverse_sorted'))\n            elif order == 'reverse_inventory':\n                hosts = self._hosts_patterns_cache[pattern_hash][::-1]\n            else:\n                hosts = self._hosts_patterns_cache[pattern_hash][:]\n                if order == 'shuffle':\n                    shuffle(hosts)\n                elif order not in [None, 'inventory']:\n                    raise AnsibleOptionsError(\"Invalid 'order' specified for inventory hosts: %s\" % order)\n\n        return hosts",
        "begin_line": 357,
        "end_line": 412,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._evaluate_patterns#414",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._evaluate_patterns(self, patterns)",
        "snippet": "    def _evaluate_patterns(self, patterns):\n        \"\"\"\n        Takes a list of patterns and returns a list of matching host names,\n        taking into account any negative and intersection patterns.\n        \"\"\"\n\n        patterns = order_patterns(patterns)\n        hosts = []\n\n        for p in patterns:\n            # avoid resolving a pattern that is a plain host\n            if p in self._inventory.hosts:\n                hosts.append(self._inventory.get_host(p))\n            else:\n                that = self._match_one_pattern(p)\n                if p[0] == \"!\":\n                    that = set(that)\n                    hosts = [h for h in hosts if h not in that]\n                elif p[0] == \"&\":\n                    that = set(that)\n                    hosts = [h for h in hosts if h in that]\n                else:\n                    existing_hosts = set(y.name for y in hosts)\n                    hosts.extend([h for h in that if h.name not in existing_hosts])\n        return hosts",
        "begin_line": 414,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._match_one_pattern#440",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._match_one_pattern(self, pattern)",
        "snippet": "    def _match_one_pattern(self, pattern):\n        \"\"\"\n        Takes a single pattern and returns a list of matching host names.\n        Ignores intersection (&) and exclusion (!) specifiers.\n\n        The pattern may be:\n\n            1. A regex starting with ~, e.g. '~[abc]*'\n            2. A shell glob pattern with ?/*/[chars]/[!chars], e.g. 'foo*'\n            3. An ordinary word that matches itself only, e.g. 'foo'\n\n        The pattern is matched using the following rules:\n\n            1. If it's 'all', it matches all hosts in all groups.\n            2. Otherwise, for each known group name:\n                (a) if it matches the group name, the results include all hosts\n                    in the group or any of its children.\n                (b) otherwise, if it matches any hosts in the group, the results\n                    include the matching hosts.\n\n        This means that 'foo*' may match one or more groups (thus including all\n        hosts therein) but also hosts in other groups.\n\n        The built-in groups 'all' and 'ungrouped' are special. No pattern can\n        match these group names (though 'all' behaves as though it matches, as\n        described above). The word 'ungrouped' can match a host of that name,\n        and patterns like 'ungr*' and 'al*' can match either hosts or groups\n        other than all and ungrouped.\n\n        If the pattern matches one or more group names according to these rules,\n        it may have an optional range suffix to select a subset of the results.\n        This is allowed only if the pattern is not a regex, i.e. '~foo[1]' does\n        not work (the [1] is interpreted as part of the regex), but 'foo*[1]'\n        would work if 'foo*' matched the name of one or more groups.\n\n        Duplicate matches are always eliminated from the results.\n        \"\"\"\n\n        if pattern[0] in (\"&\", \"!\"):\n            pattern = pattern[1:]\n\n        if pattern not in self._pattern_cache:\n            (expr, slice) = self._split_subscript(pattern)\n            hosts = self._enumerate_matches(expr)\n            try:\n                hosts = self._apply_subscript(hosts, slice)\n            except IndexError:\n                raise AnsibleError(\"No hosts matched the subscripted pattern '%s'\" % pattern)\n            self._pattern_cache[pattern] = hosts\n\n        return self._pattern_cache[pattern]",
        "begin_line": 440,
        "end_line": 490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._split_subscript#492",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._split_subscript(self, pattern)",
        "snippet": "    def _split_subscript(self, pattern):\n        \"\"\"\n        Takes a pattern, checks if it has a subscript, and returns the pattern\n        without the subscript and a (start,end) tuple representing the given\n        subscript (or None if there is no subscript).\n\n        Validates that the subscript is in the right syntax, but doesn't make\n        sure the actual indices make sense in context.\n        \"\"\"\n\n        # Do not parse regexes for enumeration info\n        if pattern[0] == '~':\n            return (pattern, None)\n\n        # We want a pattern followed by an integer or range subscript.\n        # (We can't be more restrictive about the expression because the\n        # fnmatch semantics permit [\\[:\\]] to occur.)\n\n        subscript = None\n        m = PATTERN_WITH_SUBSCRIPT.match(pattern)\n        if m:\n            (pattern, idx, start, sep, end) = m.groups()\n            if idx:\n                subscript = (int(idx), None)\n            else:\n                if not end:\n                    end = -1\n                subscript = (int(start), int(end))\n                if sep == '-':\n                    display.warning(\"Use [x:y] inclusive subscripts instead of [x-y] which has been removed\")\n\n        return (pattern, subscript)",
        "begin_line": 492,
        "end_line": 523,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._apply_subscript#525",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._apply_subscript(self, hosts, subscript)",
        "snippet": "    def _apply_subscript(self, hosts, subscript):\n        \"\"\"\n        Takes a list of hosts and a (start,end) tuple and returns the subset of\n        hosts based on the subscript (which may be None to return all hosts).\n        \"\"\"\n\n        if not hosts or not subscript:\n            return hosts\n\n        (start, end) = subscript\n\n        if end:\n            if end == -1:\n                end = len(hosts) - 1\n            return hosts[start:end + 1]\n        else:\n            return [hosts[start]]",
        "begin_line": 525,
        "end_line": 541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager._enumerate_matches#543",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager._enumerate_matches(self, pattern)",
        "snippet": "    def _enumerate_matches(self, pattern):\n        \"\"\"\n        Returns a list of host names matching the given pattern according to the\n        rules explained above in _match_one_pattern.\n        \"\"\"\n\n        results = []\n        # check if pattern matches group\n        matching_groups = self._match_list(self._inventory.groups, pattern)\n        if matching_groups:\n            for groupname in matching_groups:\n                results.extend(self._inventory.groups[groupname].get_hosts())\n\n        # check hosts if no groups matched or it is a regex/glob pattern\n        if not matching_groups or pattern[0] == '~' or any(special in pattern for special in ('.', '?', '*', '[')):\n            # pattern might match host\n            matching_hosts = self._match_list(self._inventory.hosts, pattern)\n            if matching_hosts:\n                for hostname in matching_hosts:\n                    results.append(self._inventory.hosts[hostname])\n\n        if not results and pattern in C.LOCALHOST:\n            # get_host autocreates implicit when needed\n            implicit = self._inventory.get_host(pattern)\n            if implicit:\n                results.append(implicit)\n\n        # Display warning if specified host pattern did not match any groups or hosts\n        if not results and not matching_groups and pattern != 'all':\n            msg = \"Could not match supplied host pattern, ignoring: %s\" % pattern\n            display.debug(msg)\n            if C.HOST_PATTERN_MISMATCH == 'warning':\n                display.warning(msg)\n            elif C.HOST_PATTERN_MISMATCH == 'error':\n                raise AnsibleError(msg)\n            # no need to write 'ignore' state\n\n        return results",
        "begin_line": 543,
        "end_line": 580,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.manager.InventoryManager.list_hosts#582",
        "src_path": "lib/ansible/inventory/manager.py",
        "class_name": "lib.ansible.inventory.manager.InventoryManager",
        "signature": "lib.ansible.inventory.manager.InventoryManager.list_hosts(self, pattern='all')",
        "snippet": "    def list_hosts(self, pattern=\"all\"):\n        \"\"\" return a list of hostnames for a pattern \"\"\"\n        # FIXME: cache?\n        result = self.get_hosts(pattern)\n\n        # allow implicit localhost if pattern matches and no other results\n        if len(result) == 0 and pattern in C.LOCALHOST:\n            result = [pattern]\n\n        return result",
        "begin_line": 582,
        "end_line": 591,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.plugins.get_vars_from_path#42",
        "src_path": "lib/ansible/vars/plugins.py",
        "class_name": "lib.ansible.vars.plugins",
        "signature": "lib.ansible.vars.plugins.get_vars_from_path(loader, path, entities, stage)",
        "snippet": "def get_vars_from_path(loader, path, entities, stage):\n\n    data = {}\n\n    vars_plugin_list = list(vars_loader.all())\n    for plugin_name in C.VARIABLE_PLUGINS_ENABLED:\n        if AnsibleCollectionRef.is_valid_fqcr(plugin_name):\n            vars_plugin = vars_loader.get(plugin_name)\n            if vars_plugin is None:\n                # Error if there's no play directory or the name is wrong?\n                continue\n            if vars_plugin not in vars_plugin_list:\n                vars_plugin_list.append(vars_plugin)\n\n    for plugin in vars_plugin_list:\n        if plugin._load_name not in C.VARIABLE_PLUGINS_ENABLED and getattr(plugin, 'REQUIRES_WHITELIST', False):\n            # 2.x plugins shipped with ansible should require whitelisting, older or non shipped should load automatically\n            continue\n\n        has_stage = hasattr(plugin, 'get_option') and plugin.has_option('stage')\n\n        # if a plugin-specific setting has not been provided, use the global setting\n        # older/non shipped plugins that don't support the plugin-specific setting should also use the global setting\n        use_global = (has_stage and plugin.get_option('stage') is None) or not has_stage\n\n        if use_global:\n            if C.RUN_VARS_PLUGINS == 'demand' and stage == 'inventory':\n                continue\n            elif C.RUN_VARS_PLUGINS == 'start' and stage == 'task':\n                continue\n        elif has_stage and plugin.get_option('stage') not in ('all', stage):\n            continue\n\n        data = combine_vars(data, get_plugin_vars(loader, plugin, path, entities))\n\n    return data",
        "begin_line": 42,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.plugins.get_vars_from_inventory_sources#80",
        "src_path": "lib/ansible/vars/plugins.py",
        "class_name": "lib.ansible.vars.plugins",
        "signature": "lib.ansible.vars.plugins.get_vars_from_inventory_sources(loader, sources, entities, stage)",
        "snippet": "def get_vars_from_inventory_sources(loader, sources, entities, stage):\n\n    data = {}\n    for path in sources:\n\n        if path is None:\n            continue\n        if ',' in path and not os.path.exists(path):  # skip host lists\n            continue\n        elif not os.path.isdir(to_bytes(path)):\n            # always pass the directory of the inventory source file\n            path = os.path.dirname(path)\n\n        data = combine_vars(data, get_vars_from_path(loader, path, entities, stage))\n\n    return data",
        "begin_line": 80,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.interpreter_discovery.InterpreterDiscoveryRequiredError.__init__#25",
        "src_path": "lib/ansible/executor/interpreter_discovery.py",
        "class_name": "lib.ansible.executor.interpreter_discovery.InterpreterDiscoveryRequiredError",
        "signature": "lib.ansible.executor.interpreter_discovery.InterpreterDiscoveryRequiredError.__init__(self, message, interpreter_name, discovery_mode)",
        "snippet": "    def __init__(self, message, interpreter_name, discovery_mode):\n        super(InterpreterDiscoveryRequiredError, self).__init__(message)\n        self.interpreter_name = interpreter_name\n        self.discovery_mode = discovery_mode",
        "begin_line": 25,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.interpreter_discovery.discover_interpreter#38",
        "src_path": "lib/ansible/executor/interpreter_discovery.py",
        "class_name": "lib.ansible.executor.interpreter_discovery",
        "signature": "lib.ansible.executor.interpreter_discovery.discover_interpreter(action, interpreter_name, discovery_mode, task_vars)",
        "snippet": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\n    # interpreter discovery is a 2-step process with the target. First, we use a simple shell-agnostic bootstrap to\n    # get the system type from uname, and find any random Python that can get us the info we need. For supported\n    # target OS types, we'll dispatch a Python script that calls plaform.dist() (for older platforms, where available)\n    # and brings back /etc/os-release (if present). The proper Python path is looked up in a table of known\n    # distros/versions with included Pythons; if nothing is found, depending on the discovery mode, either the\n    # default fallback of /usr/bin/python is used (if we know it's there), or discovery fails.\n\n    # FUTURE: add logical equivalence for \"python3\" in the case of py3-only modules?\n    if interpreter_name != 'python':\n        raise ValueError('Interpreter discovery not supported for {0}'.format(interpreter_name))\n\n    host = task_vars.get('inventory_hostname', 'unknown')\n    res = None\n    platform_type = 'unknown'\n    found_interpreters = [u'/usr/bin/python']  # fallback value\n    is_auto_legacy = discovery_mode.startswith('auto_legacy')\n    is_silent = discovery_mode.endswith('_silent')\n\n    try:\n        platform_python_map = C.config.get_config_value('INTERPRETER_PYTHON_DISTRO_MAP', variables=task_vars)\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\n\n        display.vvv(msg=u\"Attempting {0} interpreter discovery\".format(interpreter_name), host=host)\n\n        # not all command -v impls accept a list of commands, so we have to call it once per python\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\n        shell_bootstrap = \"echo PLATFORM; uname; echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\n\n        # FUTURE: in most cases we probably don't want to use become, but maybe sometimes we do?\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\n\n        raw_stdout = res.get('stdout', u'')\n\n        match = foundre.match(raw_stdout)\n\n        if not match:\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\n            raise ValueError('unexpected output from Python interpreter discovery')\n\n        platform_type = match.groups()[0].lower().strip()\n\n        found_interpreters = [interp.strip() for interp in match.groups()[1].splitlines() if interp.startswith('/')]\n\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\n\n        if not found_interpreters:\n            action._discovery_warnings.append(u'No python interpreters found for host {0} (tried {1})'.format(host, bootstrap_python_list))\n            # this is lame, but returning None or throwing an exception is uglier\n            return u'/usr/bin/python'\n\n        if platform_type != 'linux':\n            raise NotImplementedError('unsupported platform for extended discovery: {0}'.format(to_native(platform_type)))\n\n        platform_script = pkgutil.get_data('ansible.executor.discovery', 'python_target.py')\n\n        # FUTURE: respect pipelining setting instead of just if the connection supports it?\n        if action._connection.has_pipelining:\n            res = action._low_level_execute_command(found_interpreters[0], sudoable=False, in_data=platform_script)\n        else:\n            # FUTURE: implement on-disk case (via script action or ?)\n            raise NotImplementedError('pipelining support required for extended interpreter discovery')\n\n        platform_info = json.loads(res.get('stdout'))\n\n        distro, version = _get_linux_distro(platform_info)\n\n        if not distro or not version:\n            raise NotImplementedError('unable to get Linux distribution/version info')\n\n        version_map = platform_python_map.get(distro.lower().strip())\n        if not version_map:\n            raise NotImplementedError('unsupported Linux distribution: {0}'.format(distro))\n\n        platform_interpreter = to_text(_version_fuzzy_match(version, version_map), errors='surrogate_or_strict')\n\n        # provide a transition period for hosts that were using /usr/bin/python previously (but shouldn't have been)\n        if is_auto_legacy:\n            if platform_interpreter != u'/usr/bin/python' and u'/usr/bin/python' in found_interpreters:\n                # FIXME: support comments in sivel's deprecation scanner so we can get reminded on this\n                if not is_silent:\n                    action._discovery_deprecation_warnings.append(dict(\n                        msg=u\"Distribution {0} {1} on host {2} should use {3}, but is using \"\n                            u\"/usr/bin/python for backward compatibility with prior Ansible releases. \"\n                            u\"A future Ansible release will default to using the discovered platform \"\n                            u\"python for this host. See {4} for more information\"\n                            .format(distro, version, host, platform_interpreter,\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')),\n                        version='2.12'))\n                return u'/usr/bin/python'\n\n        if platform_interpreter not in found_interpreters:\n            if platform_interpreter not in bootstrap_python_list:\n                # sanity check to make sure we looked for it\n                if not is_silent:\n                    action._discovery_warnings \\\n                        .append(u\"Platform interpreter {0} on host {1} is missing from bootstrap list\"\n                                .format(platform_interpreter, host))\n\n            if not is_silent:\n                action._discovery_warnings \\\n                    .append(u\"Distribution {0} {1} on host {2} should use {3}, but is using {4}, since the \"\n                            u\"discovered platform python interpreter was not present. See {5} \"\n                            u\"for more information.\"\n                            .format(distro, version, host, platform_interpreter, found_interpreters[0],\n                                    get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\n            return found_interpreters[0]\n\n        return platform_interpreter\n    except NotImplementedError as ex:\n        display.vvv(msg=u'Python interpreter discovery fallback ({0})'.format(to_text(ex)), host=host)\n    except Exception as ex:\n        if not is_silent:\n            display.warning(msg=u'Unhandled error in Python interpreter discovery for host {0}: {1}'.format(host, to_text(ex)))\n            display.debug(msg=u'Interpreter discovery traceback:\\n{0}'.format(to_text(format_exc())), host=host)\n            if res and res.get('stderr'):\n                display.vvv(msg=u'Interpreter discovery remote stderr:\\n{0}'.format(to_text(res.get('stderr'))), host=host)\n\n    if not is_silent:\n        action._discovery_warnings \\\n            .append(u\"Platform {0} on host {1} is using the discovered Python interpreter at {2}, but future installation of \"\n                    u\"another Python interpreter could change this. See {3} \"\n                    u\"for more information.\"\n                    .format(platform_type, host, found_interpreters[0],\n                            get_versioned_doclink('reference_appendices/interpreter_discovery.html')))\n    return found_interpreters[0]",
        "begin_line": 38,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.interpreter_discovery._get_linux_distro#166",
        "src_path": "lib/ansible/executor/interpreter_discovery.py",
        "class_name": "lib.ansible.executor.interpreter_discovery",
        "signature": "lib.ansible.executor.interpreter_discovery._get_linux_distro(platform_info)",
        "snippet": "def _get_linux_distro(platform_info):\n    dist_result = platform_info.get('platform_dist_result', [])\n\n    if len(dist_result) == 3 and any(dist_result):\n        return dist_result[0], dist_result[1]\n\n    osrelease_content = platform_info.get('osrelease_content')\n\n    if not osrelease_content:\n        return u'', u''\n\n    osr = LinuxDistribution._parse_os_release_content(osrelease_content)\n\n    return osr.get('id', u''), osr.get('version_id', u'')",
        "begin_line": 166,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.interpreter_discovery._version_fuzzy_match#182",
        "src_path": "lib/ansible/executor/interpreter_discovery.py",
        "class_name": "lib.ansible.executor.interpreter_discovery",
        "signature": "lib.ansible.executor.interpreter_discovery._version_fuzzy_match(version, version_map)",
        "snippet": "def _version_fuzzy_match(version, version_map):\n    # try exact match first\n    res = version_map.get(version)\n    if res:\n        return res\n\n    sorted_looseversions = sorted([LooseVersion(v) for v in version_map.keys()])\n\n    find_looseversion = LooseVersion(version)\n\n    # slot match; return nearest previous version we're newer than\n    kpos = bisect.bisect(sorted_looseversions, find_looseversion)\n\n    if kpos == 0:\n        # older than everything in the list, return the oldest version\n        # TODO: warning-worthy?\n        return version_map.get(sorted_looseversions[0].vstring)\n\n    # TODO: is \"past the end of the list\" warning-worthy too (at least if it's not a major version match)?\n\n    # return the next-oldest entry that we're newer than...\n    return version_map.get(sorted_looseversions[kpos - 1].vstring)",
        "begin_line": 182,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.Conditional.__init__#51",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.Conditional",
        "signature": "lib.ansible.playbook.conditional.Conditional.__init__(self, loader=None)",
        "snippet": "    def __init__(self, loader=None):\n        # when used directly, this class needs a loader, but we want to\n        # make sure we don't trample on the existing one if this class\n        # is used as a mix-in with a playbook base class\n        if not hasattr(self, '_loader'):\n            if loader is None:\n                raise AnsibleError(\"a loader must be specified when using Conditional() directly\")\n            else:\n                self._loader = loader\n        super(Conditional, self).__init__()",
        "begin_line": 51,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.619047619047618e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.Conditional._validate_when#62",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.Conditional",
        "signature": "lib.ansible.playbook.conditional.Conditional._validate_when(self, attr, name, value)",
        "snippet": "    def _validate_when(self, attr, name, value):\n        if not isinstance(value, list):\n            setattr(self, name, [value])",
        "begin_line": 62,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.049700387733522e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.Conditional.extract_defined_undefined#66",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.Conditional",
        "signature": "lib.ansible.playbook.conditional.Conditional.extract_defined_undefined(self, conditional)",
        "snippet": "    def extract_defined_undefined(self, conditional):\n        results = []\n\n        cond = conditional\n        m = DEFINED_REGEX.search(cond)\n        while m:\n            results.append(m.groups())\n            cond = cond[m.end():]\n            m = DEFINED_REGEX.search(cond)\n\n        return results",
        "begin_line": 66,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.Conditional.evaluate_conditional#78",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.Conditional",
        "signature": "lib.ansible.playbook.conditional.Conditional.evaluate_conditional(self, templar, all_vars)",
        "snippet": "    def evaluate_conditional(self, templar, all_vars):\n        '''\n        Loops through the conditionals set on this object, returning\n        False if any of them evaluate as such.\n        '''\n\n        # since this is a mix-in, it may not have an underlying datastructure\n        # associated with it, so we pull it out now in case we need it for\n        # error reporting below\n        ds = None\n        if hasattr(self, '_ds'):\n            ds = getattr(self, '_ds')\n\n        try:\n            for conditional in self.when:\n                if not self._check_conditional(conditional, templar, all_vars):\n                    return False\n        except Exception as e:\n            raise AnsibleError(\n                \"The conditional check '%s' failed. The error was: %s\" % (to_native(conditional), to_native(e)), obj=ds\n            )\n\n        return True",
        "begin_line": 78,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.Conditional._check_conditional#102",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.Conditional",
        "signature": "lib.ansible.playbook.conditional.Conditional._check_conditional(self, conditional, templar, all_vars)",
        "snippet": "    def _check_conditional(self, conditional, templar, all_vars):\n        '''\n        This method does the low-level evaluation of each conditional\n        set on this object, using jinja2 to wrap the conditionals for\n        evaluation.\n        '''\n\n        original = conditional\n        if conditional is None or conditional == '':\n            return True\n\n        # this allows for direct boolean assignments to conditionals \"when: False\"\n        if isinstance(conditional, bool):\n            return conditional\n\n        if templar.is_template(conditional):\n            display.warning('conditional statements should not include jinja2 '\n                            'templating delimiters such as {{ }} or {%% %%}. '\n                            'Found: %s' % conditional)\n\n        bare_vars_warning = False\n        if C.CONDITIONAL_BARE_VARS:\n            if conditional in all_vars and VALID_VAR_REGEX.match(conditional):\n                conditional = all_vars[conditional]\n                bare_vars_warning = True\n\n        # make sure the templar is using the variables specified with this method\n        templar.available_variables = all_vars\n\n        try:\n            # if the conditional is \"unsafe\", disable lookups\n            disable_lookups = hasattr(conditional, '__UNSAFE__')\n            conditional = templar.template(conditional, disable_lookups=disable_lookups)\n            if bare_vars_warning and not isinstance(conditional, bool):\n                display.deprecated('evaluating %r as a bare variable, this behaviour will go away and you might need to add |bool'\n                                   ' to the expression in the future. Also see CONDITIONAL_BARE_VARS configuration toggle' % original, \"2.12\")\n            if not isinstance(conditional, text_type) or conditional == \"\":\n                return conditional\n\n            # update the lookups flag, as the string returned above may now be unsafe\n            # and we don't want future templating calls to do unsafe things\n            disable_lookups |= hasattr(conditional, '__UNSAFE__')\n\n            # First, we do some low-level jinja2 parsing involving the AST format of the\n            # statement to ensure we don't do anything unsafe (using the disable_lookup flag above)\n            class CleansingNodeVisitor(ast.NodeVisitor):\n                def generic_visit(self, node, inside_call=False, inside_yield=False):\n                    if isinstance(node, ast.Call):\n                        inside_call = True\n                    elif isinstance(node, ast.Yield):\n                        inside_yield = True\n                    elif isinstance(node, ast.Str):\n                        if disable_lookups:\n                            if inside_call and node.s.startswith(\"__\"):\n                                # calling things with a dunder is generally bad at this point...\n                                raise AnsibleError(\n                                    \"Invalid access found in the conditional: '%s'\" % conditional\n                                )\n                            elif inside_yield:\n                                # we're inside a yield, so recursively parse and traverse the AST\n                                # of the result to catch forbidden syntax from executing\n                                parsed = ast.parse(node.s, mode='exec')\n                                cnv = CleansingNodeVisitor()\n                                cnv.visit(parsed)\n                    # iterate over all child nodes\n                    for child_node in ast.iter_child_nodes(node):\n                        self.generic_visit(\n                            child_node,\n                            inside_call=inside_call,\n                            inside_yield=inside_yield\n                        )\n            try:\n                e = templar.environment.overlay()\n                e.filters.update(templar._get_filters())\n                e.tests.update(templar._get_tests())\n\n                res = e._parse(conditional, None, None)\n                res = generate(res, e, None, None)\n                parsed = ast.parse(res, mode='exec')\n\n                cnv = CleansingNodeVisitor()\n                cnv.visit(parsed)\n            except Exception as e:\n                raise AnsibleError(\"Invalid conditional detected: %s\" % to_native(e))\n\n            # and finally we generate and template the presented string and look at the resulting string\n            presented = \"{%% if %s %%} True {%% else %%} False {%% endif %%}\" % conditional\n            val = templar.template(presented, disable_lookups=disable_lookups).strip()\n            if val == \"True\":\n                return True\n            elif val == \"False\":\n                return False\n            else:\n                raise AnsibleError(\"unable to evaluate conditional: %s\" % original)\n        except (AnsibleUndefinedVariable, UndefinedError) as e:\n            # the templating failed, meaning most likely a variable was undefined. If we happened\n            # to be looking for an undefined variable, return True, otherwise fail\n            try:\n                # first we extract the variable name from the error message\n                var_name = re.compile(r\"'(hostvars\\[.+\\]|[\\w_]+)' is undefined\").search(str(e)).groups()[0]\n                # next we extract all defined/undefined tests from the conditional string\n                def_undef = self.extract_defined_undefined(conditional)\n                # then we loop through these, comparing the error variable name against\n                # each def/undef test we found above. If there is a match, we determine\n                # whether the logic/state mean the variable should exist or not and return\n                # the corresponding True/False\n                for (du_var, logic, state) in def_undef:\n                    # when we compare the var names, normalize quotes because something\n                    # like hostvars['foo'] may be tested against hostvars[\"foo\"]\n                    if var_name.replace(\"'\", '\"') == du_var.replace(\"'\", '\"'):\n                        # the should exist is a xor test between a negation in the logic portion\n                        # against the state (defined or undefined)\n                        should_exist = ('not' in logic) != (state == 'defined')\n                        if should_exist:\n                            return False\n                        else:\n                            return True\n                # as nothing above matched the failed var name, re-raise here to\n                # trigger the AnsibleUndefinedVariable exception again below\n                raise\n            except Exception:\n                raise AnsibleUndefinedVariable(\"error while evaluating conditional (%s): %s\" % (original, e))",
        "begin_line": 102,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.CleansingNodeVisitor._check_conditional#102",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.CleansingNodeVisitor",
        "signature": "lib.ansible.playbook.conditional.CleansingNodeVisitor._check_conditional(self, conditional, templar, all_vars)",
        "snippet": "    def _check_conditional(self, conditional, templar, all_vars):\n        '''\n        This method does the low-level evaluation of each conditional\n        set on this object, using jinja2 to wrap the conditionals for\n        evaluation.\n        '''\n\n        original = conditional\n        if conditional is None or conditional == '':\n            return True\n\n        # this allows for direct boolean assignments to conditionals \"when: False\"\n        if isinstance(conditional, bool):\n            return conditional\n\n        if templar.is_template(conditional):\n            display.warning('conditional statements should not include jinja2 '\n                            'templating delimiters such as {{ }} or {%% %%}. '\n                            'Found: %s' % conditional)\n\n        bare_vars_warning = False\n        if C.CONDITIONAL_BARE_VARS:\n            if conditional in all_vars and VALID_VAR_REGEX.match(conditional):\n                conditional = all_vars[conditional]\n                bare_vars_warning = True\n\n        # make sure the templar is using the variables specified with this method\n        templar.available_variables = all_vars\n\n        try:\n            # if the conditional is \"unsafe\", disable lookups\n            disable_lookups = hasattr(conditional, '__UNSAFE__')\n            conditional = templar.template(conditional, disable_lookups=disable_lookups)\n            if bare_vars_warning and not isinstance(conditional, bool):\n                display.deprecated('evaluating %r as a bare variable, this behaviour will go away and you might need to add |bool'\n                                   ' to the expression in the future. Also see CONDITIONAL_BARE_VARS configuration toggle' % original, \"2.12\")\n            if not isinstance(conditional, text_type) or conditional == \"\":\n                return conditional\n\n            # update the lookups flag, as the string returned above may now be unsafe\n            # and we don't want future templating calls to do unsafe things\n            disable_lookups |= hasattr(conditional, '__UNSAFE__')\n\n            # First, we do some low-level jinja2 parsing involving the AST format of the\n            # statement to ensure we don't do anything unsafe (using the disable_lookup flag above)\n            class CleansingNodeVisitor(ast.NodeVisitor):\n                def generic_visit(self, node, inside_call=False, inside_yield=False):\n                    if isinstance(node, ast.Call):\n                        inside_call = True\n                    elif isinstance(node, ast.Yield):\n                        inside_yield = True\n                    elif isinstance(node, ast.Str):\n                        if disable_lookups:\n                            if inside_call and node.s.startswith(\"__\"):\n                                # calling things with a dunder is generally bad at this point...\n                                raise AnsibleError(\n                                    \"Invalid access found in the conditional: '%s'\" % conditional\n                                )\n                            elif inside_yield:\n                                # we're inside a yield, so recursively parse and traverse the AST\n                                # of the result to catch forbidden syntax from executing\n                                parsed = ast.parse(node.s, mode='exec')\n                                cnv = CleansingNodeVisitor()\n                                cnv.visit(parsed)\n                    # iterate over all child nodes\n                    for child_node in ast.iter_child_nodes(node):\n                        self.generic_visit(\n                            child_node,\n                            inside_call=inside_call,\n                            inside_yield=inside_yield\n                        )\n            try:\n                e = templar.environment.overlay()\n                e.filters.update(templar._get_filters())\n                e.tests.update(templar._get_tests())\n\n                res = e._parse(conditional, None, None)\n                res = generate(res, e, None, None)\n                parsed = ast.parse(res, mode='exec')\n\n                cnv = CleansingNodeVisitor()\n                cnv.visit(parsed)\n            except Exception as e:\n                raise AnsibleError(\"Invalid conditional detected: %s\" % to_native(e))\n\n            # and finally we generate and template the presented string and look at the resulting string\n            presented = \"{%% if %s %%} True {%% else %%} False {%% endif %%}\" % conditional\n            val = templar.template(presented, disable_lookups=disable_lookups).strip()\n            if val == \"True\":\n                return True\n            elif val == \"False\":\n                return False\n            else:\n                raise AnsibleError(\"unable to evaluate conditional: %s\" % original)\n        except (AnsibleUndefinedVariable, UndefinedError) as e:\n            # the templating failed, meaning most likely a variable was undefined. If we happened\n            # to be looking for an undefined variable, return True, otherwise fail\n            try:\n                # first we extract the variable name from the error message\n                var_name = re.compile(r\"'(hostvars\\[.+\\]|[\\w_]+)' is undefined\").search(str(e)).groups()[0]\n                # next we extract all defined/undefined tests from the conditional string\n                def_undef = self.extract_defined_undefined(conditional)\n                # then we loop through these, comparing the error variable name against\n                # each def/undef test we found above. If there is a match, we determine\n                # whether the logic/state mean the variable should exist or not and return\n                # the corresponding True/False\n                for (du_var, logic, state) in def_undef:\n                    # when we compare the var names, normalize quotes because something\n                    # like hostvars['foo'] may be tested against hostvars[\"foo\"]\n                    if var_name.replace(\"'\", '\"') == du_var.replace(\"'\", '\"'):\n                        # the should exist is a xor test between a negation in the logic portion\n                        # against the state (defined or undefined)\n                        should_exist = ('not' in logic) != (state == 'defined')\n                        if should_exist:\n                            return False\n                        else:\n                            return True\n                # as nothing above matched the failed var name, re-raise here to\n                # trigger the AnsibleUndefinedVariable exception again below\n                raise\n            except Exception:\n                raise AnsibleUndefinedVariable(\"error while evaluating conditional (%s): %s\" % (original, e))",
        "begin_line": 102,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.2811999417504e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.conditional.CleansingNodeVisitor.generic_visit#148",
        "src_path": "lib/ansible/playbook/conditional.py",
        "class_name": "lib.ansible.playbook.conditional.CleansingNodeVisitor",
        "signature": "lib.ansible.playbook.conditional.CleansingNodeVisitor.generic_visit(self, node, inside_call=False, inside_yield=False)",
        "snippet": "                def generic_visit(self, node, inside_call=False, inside_yield=False):\n                    if isinstance(node, ast.Call):\n                        inside_call = True\n                    elif isinstance(node, ast.Yield):\n                        inside_yield = True\n                    elif isinstance(node, ast.Str):\n                        if disable_lookups:\n                            if inside_call and node.s.startswith(\"__\"):\n                                # calling things with a dunder is generally bad at this point...\n                                raise AnsibleError(\n                                    \"Invalid access found in the conditional: '%s'\" % conditional\n                                )\n                            elif inside_yield:\n                                # we're inside a yield, so recursively parse and traverse the AST\n                                # of the result to catch forbidden syntax from executing\n                                parsed = ast.parse(node.s, mode='exec')\n                                cnv = CleansingNodeVisitor()\n                                cnv.visit(parsed)\n                    # iterate over all child nodes\n                    for child_node in ast.iter_child_nodes(node):\n                        self.generic_visit(\n                            child_node,\n                            inside_call=inside_call,\n                            inside_yield=inside_yield\n                        )",
        "begin_line": 148,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.remove_omit#42",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor",
        "signature": "lib.ansible.executor.task_executor.remove_omit(task_args, omit_token)",
        "snippet": "def remove_omit(task_args, omit_token):\n    '''\n    Remove args with a value equal to the ``omit_token`` recursively\n    to align with now having suboptions in the argument_spec\n    '''\n\n    if not isinstance(task_args, dict):\n        return task_args\n\n    new_args = {}\n    for i in iteritems(task_args):\n        if i[1] == omit_token:\n            continue\n        elif isinstance(i[1], dict):\n            new_args[i[0]] = remove_omit(i[1], omit_token)\n        elif isinstance(i[1], list):\n            new_args[i[0]] = [remove_omit(v, omit_token) for v in i[1]]\n        else:\n            new_args[i[0]] = i[1]\n\n    return new_args",
        "begin_line": 42,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor.__init__#78",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor.__init__(self, host, task, job_vars, play_context, new_stdin, loader, shared_loader_obj, final_q)",
        "snippet": "    def __init__(self, host, task, job_vars, play_context, new_stdin, loader, shared_loader_obj, final_q):\n        self._host = host\n        self._task = task\n        self._job_vars = job_vars\n        self._play_context = play_context\n        self._new_stdin = new_stdin\n        self._loader = loader\n        self._shared_loader_obj = shared_loader_obj\n        self._connection = None\n        self._final_q = final_q\n        self._loop_eval_error = None\n\n        self._task.squash()",
        "begin_line": 78,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor.run#92",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor.run(self)",
        "snippet": "    def run(self):\n        '''\n        The main executor entrypoint, where we determine if the specified\n        task requires looping and either runs the task with self._run_loop()\n        or self._execute(). After that, the returned results are parsed and\n        returned as a dict.\n        '''\n\n        display.debug(\"in run() - task %s\" % self._task._uuid)\n\n        try:\n            try:\n                items = self._get_loop_items()\n            except AnsibleUndefinedVariable as e:\n                # save the error raised here for use later\n                items = None\n                self._loop_eval_error = e\n\n            if items is not None:\n                if len(items) > 0:\n                    item_results = self._run_loop(items)\n\n                    # create the overall result item\n                    res = dict(results=item_results)\n\n                    # loop through the item results, and set the global changed/failed result flags based on any item.\n                    for item in item_results:\n                        if 'changed' in item and item['changed'] and not res.get('changed'):\n                            res['changed'] = True\n                        if 'failed' in item and item['failed']:\n                            item_ignore = item.pop('_ansible_ignore_errors')\n                            if not res.get('failed'):\n                                res['failed'] = True\n                                res['msg'] = 'One or more items failed'\n                                self._task.ignore_errors = item_ignore\n                            elif self._task.ignore_errors and not item_ignore:\n                                self._task.ignore_errors = item_ignore\n\n                        # ensure to accumulate these\n                        for array in ['warnings', 'deprecations']:\n                            if array in item and item[array]:\n                                if array not in res:\n                                    res[array] = []\n                                if not isinstance(item[array], list):\n                                    item[array] = [item[array]]\n                                res[array] = res[array] + item[array]\n                                del item[array]\n\n                    if not res.get('Failed', False):\n                        res['msg'] = 'All items completed'\n                else:\n                    res = dict(changed=False, skipped=True, skipped_reason='No items in the list', results=[])\n            else:\n                display.debug(\"calling self._execute()\")\n                res = self._execute()\n                display.debug(\"_execute() done\")\n\n            # make sure changed is set in the result, if it's not present\n            if 'changed' not in res:\n                res['changed'] = False\n\n            def _clean_res(res, errors='surrogate_or_strict'):\n                if isinstance(res, binary_type):\n                    return to_unsafe_text(res, errors=errors)\n                elif isinstance(res, dict):\n                    for k in res:\n                        try:\n                            res[k] = _clean_res(res[k], errors=errors)\n                        except UnicodeError:\n                            if k == 'diff':\n                                # If this is a diff, substitute a replacement character if the value\n                                # is undecodable as utf8.  (Fix #21804)\n                                display.warning(\"We were unable to decode all characters in the module return data.\"\n                                                \" Replaced some in an effort to return as much as possible\")\n                                res[k] = _clean_res(res[k], errors='surrogate_then_replace')\n                            else:\n                                raise\n                elif isinstance(res, list):\n                    for idx, item in enumerate(res):\n                        res[idx] = _clean_res(item, errors=errors)\n                return res\n\n            display.debug(\"dumping result to json\")\n            res = _clean_res(res)\n            display.debug(\"done dumping result, returning\")\n            return res\n        except AnsibleError as e:\n            return dict(failed=True, msg=wrap_var(to_text(e, nonstring='simplerepr')), _ansible_no_log=self._play_context.no_log)\n        except Exception as e:\n            return dict(failed=True, msg='Unexpected failure during module execution.', exception=to_text(traceback.format_exc()),\n                        stdout='', _ansible_no_log=self._play_context.no_log)\n        finally:\n            try:\n                self._connection.close()\n            except AttributeError:\n                pass\n            except Exception as e:\n                display.debug(u\"error closing connection: %s\" % to_text(e))",
        "begin_line": 92,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._clean_res#153",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._clean_res(res, errors='surrogate_or_strict')",
        "snippet": "            def _clean_res(res, errors='surrogate_or_strict'):\n                if isinstance(res, binary_type):\n                    return to_unsafe_text(res, errors=errors)\n                elif isinstance(res, dict):\n                    for k in res:\n                        try:\n                            res[k] = _clean_res(res[k], errors=errors)\n                        except UnicodeError:\n                            if k == 'diff':\n                                # If this is a diff, substitute a replacement character if the value\n                                # is undecodable as utf8.  (Fix #21804)\n                                display.warning(\"We were unable to decode all characters in the module return data.\"\n                                                \" Replaced some in an effort to return as much as possible\")\n                                res[k] = _clean_res(res[k], errors='surrogate_then_replace')\n                            else:\n                                raise\n                elif isinstance(res, list):\n                    for idx, item in enumerate(res):\n                        res[idx] = _clean_res(item, errors=errors)\n                return res",
        "begin_line": 153,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._get_loop_items#191",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._get_loop_items(self)",
        "snippet": "    def _get_loop_items(self):\n        '''\n        Loads a lookup plugin to handle the with_* portion of a task (if specified),\n        and returns the items result.\n        '''\n\n        # save the play context variables to a temporary dictionary,\n        # so that we can modify the job vars without doing a full copy\n        # and later restore them to avoid modifying things too early\n        play_context_vars = dict()\n        self._play_context.update_vars(play_context_vars)\n\n        old_vars = dict()\n        for k in play_context_vars:\n            if k in self._job_vars:\n                old_vars[k] = self._job_vars[k]\n            self._job_vars[k] = play_context_vars[k]\n\n        # get search path for this task to pass to lookup plugins\n        self._job_vars['ansible_search_path'] = self._task.get_search_path()\n\n        # ensure basedir is always in (dwim already searches here but we need to display it)\n        if self._loader.get_basedir() not in self._job_vars['ansible_search_path']:\n            self._job_vars['ansible_search_path'].append(self._loader.get_basedir())\n\n        templar = Templar(loader=self._loader, shared_loader_obj=self._shared_loader_obj, variables=self._job_vars)\n        items = None\n        loop_cache = self._job_vars.get('_ansible_loop_cache')\n        if loop_cache is not None:\n            # _ansible_loop_cache may be set in `get_vars` when calculating `delegate_to`\n            # to avoid reprocessing the loop\n            items = loop_cache\n        elif self._task.loop_with:\n            if self._task.loop_with in self._shared_loader_obj.lookup_loader:\n                fail = True\n                if self._task.loop_with == 'first_found':\n                    # first_found loops are special. If the item is undefined then we want to fall through to the next value rather than failing.\n                    fail = False\n\n                loop_terms = listify_lookup_plugin_terms(terms=self._task.loop, templar=templar, loader=self._loader, fail_on_undefined=fail,\n                                                         convert_bare=False)\n                if not fail:\n                    loop_terms = [t for t in loop_terms if not templar.is_template(t)]\n\n                # get lookup\n                mylookup = self._shared_loader_obj.lookup_loader.get(self._task.loop_with, loader=self._loader, templar=templar)\n\n                # give lookup task 'context' for subdir (mostly needed for first_found)\n                for subdir in ['template', 'var', 'file']:  # TODO: move this to constants?\n                    if subdir in self._task.action:\n                        break\n                setattr(mylookup, '_subdir', subdir + 's')\n\n                # run lookup\n                items = wrap_var(mylookup.run(terms=loop_terms, variables=self._job_vars, wantlist=True))\n            else:\n                raise AnsibleError(\"Unexpected failure in finding the lookup named '%s' in the available lookup plugins\" % self._task.loop_with)\n\n        elif self._task.loop is not None:\n            items = templar.template(self._task.loop)\n            if not isinstance(items, list):\n                raise AnsibleError(\n                    \"Invalid data passed to 'loop', it requires a list, got this instead: %s.\"\n                    \" Hint: If you passed a list/dict of just one element,\"\n                    \" try adding wantlist=True to your lookup invocation or use q/query instead of lookup.\" % items\n                )\n\n        # now we restore any old job variables that may have been modified,\n        # and delete them if they were in the play context vars but not in\n        # the old variables dictionary\n        for k in play_context_vars:\n            if k in old_vars:\n                self._job_vars[k] = old_vars[k]\n            else:\n                del self._job_vars[k]\n\n        return items",
        "begin_line": 191,
        "end_line": 267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._run_loop#269",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._run_loop(self, items)",
        "snippet": "    def _run_loop(self, items):\n        '''\n        Runs the task with the loop items specified and collates the result\n        into an array named 'results' which is inserted into the final result\n        along with the item for which the loop ran.\n        '''\n\n        results = []\n\n        # make copies of the job vars and task so we can add the item to\n        # the variables and re-validate the task with the item variable\n        # task_vars = self._job_vars.copy()\n        task_vars = self._job_vars\n\n        loop_var = 'item'\n        index_var = None\n        label = None\n        loop_pause = 0\n        extended = False\n        templar = Templar(loader=self._loader, shared_loader_obj=self._shared_loader_obj, variables=self._job_vars)\n\n        # FIXME: move this to the object itself to allow post_validate to take care of templating (loop_control.post_validate)\n        if self._task.loop_control:\n            loop_var = templar.template(self._task.loop_control.loop_var)\n            index_var = templar.template(self._task.loop_control.index_var)\n            loop_pause = templar.template(self._task.loop_control.pause)\n            extended = templar.template(self._task.loop_control.extended)\n\n            # This may be 'None',so it is templated below after we ensure a value and an item is assigned\n            label = self._task.loop_control.label\n\n        # ensure we always have a label\n        if label is None:\n            label = '{{' + loop_var + '}}'\n\n        if loop_var in task_vars:\n            display.warning(u\"The loop variable '%s' is already in use. \"\n                            u\"You should set the `loop_var` value in the `loop_control` option for the task\"\n                            u\" to something else to avoid variable collisions and unexpected behavior.\" % loop_var)\n\n        ran_once = False\n        if self._task.loop_with:\n            # Only squash with 'with_:' not with the 'loop:', 'magic' squashing can be removed once with_ loops are\n            items = self._squash_items(items, loop_var, task_vars)\n\n        no_log = False\n        items_len = len(items)\n        for item_index, item in enumerate(items):\n            task_vars['ansible_loop_var'] = loop_var\n\n            task_vars[loop_var] = item\n            if index_var:\n                task_vars['ansible_index_var'] = index_var\n                task_vars[index_var] = item_index\n\n            if extended:\n                task_vars['ansible_loop'] = {\n                    'allitems': items,\n                    'index': item_index + 1,\n                    'index0': item_index,\n                    'first': item_index == 0,\n                    'last': item_index + 1 == items_len,\n                    'length': items_len,\n                    'revindex': items_len - item_index,\n                    'revindex0': items_len - item_index - 1,\n                }\n                try:\n                    task_vars['ansible_loop']['nextitem'] = items[item_index + 1]\n                except IndexError:\n                    pass\n                if item_index - 1 >= 0:\n                    task_vars['ansible_loop']['previtem'] = items[item_index - 1]\n\n            # Update template vars to reflect current loop iteration\n            templar.available_variables = task_vars\n\n            # pause between loop iterations\n            if loop_pause and ran_once:\n                try:\n                    time.sleep(float(loop_pause))\n                except ValueError as e:\n                    raise AnsibleError('Invalid pause value: %s, produced error: %s' % (loop_pause, to_native(e)))\n            else:\n                ran_once = True\n\n            try:\n                tmp_task = self._task.copy(exclude_parent=True, exclude_tasks=True)\n                tmp_task._parent = self._task._parent\n                tmp_play_context = self._play_context.copy()\n            except AnsibleParserError as e:\n                results.append(dict(failed=True, msg=to_text(e)))\n                continue\n\n            # now we swap the internal task and play context with their copies,\n            # execute, and swap them back so we can do the next iteration cleanly\n            (self._task, tmp_task) = (tmp_task, self._task)\n            (self._play_context, tmp_play_context) = (tmp_play_context, self._play_context)\n            res = self._execute(variables=task_vars)\n            task_fields = self._task.dump_attrs()\n            (self._task, tmp_task) = (tmp_task, self._task)\n            (self._play_context, tmp_play_context) = (tmp_play_context, self._play_context)\n\n            # update 'general no_log' based on specific no_log\n            no_log = no_log or tmp_task.no_log\n\n            # now update the result with the item info, and append the result\n            # to the list of results\n            res[loop_var] = item\n            res['ansible_loop_var'] = loop_var\n            if index_var:\n                res[index_var] = item_index\n                res['ansible_index_var'] = index_var\n            if extended:\n                res['ansible_loop'] = task_vars['ansible_loop']\n\n            res['_ansible_item_result'] = True\n            res['_ansible_ignore_errors'] = task_fields.get('ignore_errors')\n\n            # gets templated here unlike rest of loop_control fields, depends on loop_var above\n            try:\n                res['_ansible_item_label'] = templar.template(label, cache=False)\n            except AnsibleUndefinedVariable as e:\n                res.update({\n                    'failed': True,\n                    'msg': 'Failed to template loop_control.label: %s' % to_text(e)\n                })\n\n            self._final_q.put(\n                TaskResult(\n                    self._host.name,\n                    self._task._uuid,\n                    res,\n                    task_fields=task_fields,\n                ),\n                block=False,\n            )\n            results.append(res)\n            del task_vars[loop_var]\n\n            # clear 'connection related' plugin variables for next iteration\n            if self._connection:\n                clear_plugins = {\n                    'connection': self._connection._load_name,\n                    'shell': self._connection._shell._load_name\n                }\n                if self._connection.become:\n                    clear_plugins['become'] = self._connection.become._load_name\n\n                for plugin_type, plugin_name in iteritems(clear_plugins):\n                    for var in C.config.get_plugin_vars(plugin_type, plugin_name):\n                        if var in task_vars and var not in self._job_vars:\n                            del task_vars[var]\n\n        self._task.no_log = no_log\n\n        return results",
        "begin_line": 269,
        "end_line": 424,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._squash_items#426",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._squash_items(self, items, loop_var, variables)",
        "snippet": "    def _squash_items(self, items, loop_var, variables):\n        '''\n        Squash items down to a comma-separated list for certain modules which support it\n        (typically package management modules).\n        '''\n        name = None\n        try:\n            # _task.action could contain templatable strings (via action: and\n            # local_action:)  Template it before comparing.  If we don't end up\n            # optimizing it here, the templatable string might use template vars\n            # that aren't available until later (it could even use vars from the\n            # with_items loop) so don't make the templated string permanent yet.\n            templar = Templar(loader=self._loader, shared_loader_obj=self._shared_loader_obj, variables=variables)\n            task_action = self._task.action\n            if templar.is_template(task_action):\n                task_action = templar.template(task_action, fail_on_undefined=False)\n\n            if len(items) > 0 and task_action in self.SQUASH_ACTIONS:\n                if all(isinstance(o, string_types) for o in items):\n                    final_items = []\n\n                    found = None\n                    for allowed in ['name', 'pkg', 'package']:\n                        name = self._task.args.pop(allowed, None)\n                        if name is not None:\n                            found = allowed\n                            break\n\n                    # This gets the information to check whether the name field\n                    # contains a template that we can squash for\n                    template_no_item = template_with_item = None\n                    if name:\n                        if templar.is_template(name):\n                            variables[loop_var] = '\\0$'\n                            template_no_item = templar.template(name, variables, cache=False)\n                            variables[loop_var] = '\\0@'\n                            template_with_item = templar.template(name, variables, cache=False)\n                            del variables[loop_var]\n\n                        # Check if the user is doing some operation that doesn't take\n                        # name/pkg or the name/pkg field doesn't have any variables\n                        # and thus the items can't be squashed\n                        if template_no_item != template_with_item:\n                            if self._task.loop_with and self._task.loop_with not in ('items', 'list'):\n                                value_text = \"\\\"{{ query('%s', %r) }}\\\"\" % (self._task.loop_with, self._task.loop)\n                            else:\n                                value_text = '%r' % self._task.loop\n                            # Without knowing the data structure well, it's easiest to strip python2 unicode\n                            # literals after stringifying\n                            value_text = re.sub(r\"\\bu'\", \"'\", value_text)\n\n                            display.deprecated(\n                                'Invoking \"%s\" only once while using a loop via squash_actions is deprecated. '\n                                'Instead of using a loop to supply multiple items and specifying `%s: \"%s\"`, '\n                                'please use `%s: %s` and remove the loop' % (self._task.action, found, name, found, value_text),\n                                version='2.11'\n                            )\n                            for item in items:\n                                variables[loop_var] = item\n                                if self._task.evaluate_conditional(templar, variables):\n                                    new_item = templar.template(name, cache=False)\n                                    final_items.append(new_item)\n                            self._task.args['name'] = final_items\n                            # Wrap this in a list so that the calling function loop\n                            # executes exactly once\n                            return [final_items]\n                        else:\n                            # Restore the name parameter\n                            self._task.args['name'] = name\n                # elif:\n                    # Right now we only optimize single entries.  In the future we\n                    # could optimize more types:\n                    # * lists can be squashed together\n                    # * dicts could squash entries that match in all cases except the\n                    #   name or pkg field.\n        except Exception:\n            # Squashing is an optimization.  If it fails for any reason,\n            # simply use the unoptimized list of items.\n\n            # Restore the name parameter\n            if name is not None:\n                self._task.args['name'] = name\n        return items",
        "begin_line": 426,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._execute#510",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._execute(self, variables=None)",
        "snippet": "    def _execute(self, variables=None):\n        '''\n        The primary workhorse of the executor system, this runs the task\n        on the specified host (which may be the delegated_to host) and handles\n        the retry/until and block rescue/always execution\n        '''\n\n        if variables is None:\n            variables = self._job_vars\n\n        templar = Templar(loader=self._loader, shared_loader_obj=self._shared_loader_obj, variables=variables)\n\n        context_validation_error = None\n        try:\n            # apply the given task's information to the connection info,\n            # which may override some fields already set by the play or\n            # the options specified on the command line\n            self._play_context = self._play_context.set_task_and_variable_override(task=self._task, variables=variables, templar=templar)\n\n            # fields set from the play/task may be based on variables, so we have to\n            # do the same kind of post validation step on it here before we use it.\n            self._play_context.post_validate(templar=templar)\n\n            # now that the play context is finalized, if the remote_addr is not set\n            # default to using the host's address field as the remote address\n            if not self._play_context.remote_addr:\n                self._play_context.remote_addr = self._host.address\n\n            # We also add \"magic\" variables back into the variables dict to make sure\n            # a certain subset of variables exist.\n            self._play_context.update_vars(variables)\n\n            # FIXME: update connection/shell plugin options\n        except AnsibleError as e:\n            # save the error, which we'll raise later if we don't end up\n            # skipping this task during the conditional evaluation step\n            context_validation_error = e\n\n        # Evaluate the conditional (if any) for this task, which we do before running\n        # the final task post-validation. We do this before the post validation due to\n        # the fact that the conditional may specify that the task be skipped due to a\n        # variable not being present which would otherwise cause validation to fail\n        try:\n            if not self._task.evaluate_conditional(templar, variables):\n                display.debug(\"when evaluation is False, skipping this task\")\n                return dict(changed=False, skipped=True, skip_reason='Conditional result was False', _ansible_no_log=self._play_context.no_log)\n        except AnsibleError:\n            # loop error takes precedence\n            if self._loop_eval_error is not None:\n                raise self._loop_eval_error  # pylint: disable=raising-bad-type\n            raise\n\n        # Not skipping, if we had loop error raised earlier we need to raise it now to halt the execution of this task\n        if self._loop_eval_error is not None:\n            raise self._loop_eval_error  # pylint: disable=raising-bad-type\n\n        # if we ran into an error while setting up the PlayContext, raise it now\n        if context_validation_error is not None:\n            raise context_validation_error  # pylint: disable=raising-bad-type\n\n        # if this task is a TaskInclude, we just return now with a success code so the\n        # main thread can expand the task list for the given host\n        if self._task.action in ('include', 'include_tasks'):\n            include_args = self._task.args.copy()\n            include_file = include_args.pop('_raw_params', None)\n            if not include_file:\n                return dict(failed=True, msg=\"No include file was specified to the include\")\n\n            include_file = templar.template(include_file)\n            return dict(include=include_file, include_args=include_args)\n\n        # if this task is a IncludeRole, we just return now with a success code so the main thread can expand the task list for the given host\n        elif self._task.action == 'include_role':\n            include_args = self._task.args.copy()\n            return dict(include_args=include_args)\n\n        # Now we do final validation on the task, which sets all fields to their final values.\n        self._task.post_validate(templar=templar)\n        if '_variable_params' in self._task.args:\n            variable_params = self._task.args.pop('_variable_params')\n            if isinstance(variable_params, dict):\n                if C.INJECT_FACTS_AS_VARS:\n                    display.warning(\"Using a variable for a task's 'args' is unsafe in some situations \"\n                                    \"(see https://docs.ansible.com/ansible/devel/reference_appendices/faq.html#argsplat-unsafe)\")\n                variable_params.update(self._task.args)\n                self._task.args = variable_params\n\n        # get the connection and the handler for this execution\n        if (not self._connection or\n                not getattr(self._connection, 'connected', False) or\n                self._play_context.remote_addr != self._connection._play_context.remote_addr):\n            self._connection = self._get_connection(variables=variables, templar=templar)\n        else:\n            # if connection is reused, its _play_context is no longer valid and needs\n            # to be replaced with the one templated above, in case other data changed\n            self._connection._play_context = self._play_context\n\n        self._set_connection_options(variables, templar)\n\n        # get handler\n        self._handler = self._get_action_handler(connection=self._connection, templar=templar)\n\n        # Apply default params for action/module, if present\n        self._task.args = get_action_args_with_defaults(self._task.action, self._task.args, self._task.module_defaults, templar)\n\n        # And filter out any fields which were set to default(omit), and got the omit token value\n        omit_token = variables.get('omit')\n        if omit_token is not None:\n            self._task.args = remove_omit(self._task.args, omit_token)\n\n        # Read some values from the task, so that we can modify them if need be\n        if self._task.until:\n            retries = self._task.retries\n            if retries is None:\n                retries = 3\n            elif retries <= 0:\n                retries = 1\n            else:\n                retries += 1\n        else:\n            retries = 1\n\n        delay = self._task.delay\n        if delay < 0:\n            delay = 1\n\n        # make a copy of the job vars here, in case we need to update them\n        # with the registered variable value later on when testing conditions\n        vars_copy = variables.copy()\n\n        display.debug(\"starting attempt loop\")\n        result = None\n        for attempt in xrange(1, retries + 1):\n            display.debug(\"running the handler\")\n            try:\n                result = self._handler.run(task_vars=variables)\n            except AnsibleActionSkip as e:\n                return dict(skipped=True, msg=to_text(e))\n            except AnsibleActionFail as e:\n                return dict(failed=True, msg=to_text(e))\n            except AnsibleConnectionFailure as e:\n                return dict(unreachable=True, msg=to_text(e))\n            finally:\n                self._handler.cleanup()\n            display.debug(\"handler run complete\")\n\n            # preserve no log\n            result[\"_ansible_no_log\"] = self._play_context.no_log\n\n            # update the local copy of vars with the registered value, if specified,\n            # or any facts which may have been generated by the module execution\n            if self._task.register:\n                if not isidentifier(self._task.register):\n                    raise AnsibleError(\"Invalid variable name in 'register' specified: '%s'\" % self._task.register)\n\n                vars_copy[self._task.register] = result = wrap_var(result)\n\n            if self._task.async_val > 0:\n                if self._task.poll > 0 and not result.get('skipped') and not result.get('failed'):\n                    result = self._poll_async_result(result=result, templar=templar, task_vars=vars_copy)\n                    # FIXME callback 'v2_runner_on_async_poll' here\n\n                # ensure no log is preserved\n                result[\"_ansible_no_log\"] = self._play_context.no_log\n\n            # helper methods for use below in evaluating changed/failed_when\n            def _evaluate_changed_when_result(result):\n                if self._task.changed_when is not None and self._task.changed_when:\n                    cond = Conditional(loader=self._loader)\n                    cond.when = self._task.changed_when\n                    result['changed'] = cond.evaluate_conditional(templar, vars_copy)\n\n            def _evaluate_failed_when_result(result):\n                if self._task.failed_when:\n                    cond = Conditional(loader=self._loader)\n                    cond.when = self._task.failed_when\n                    failed_when_result = cond.evaluate_conditional(templar, vars_copy)\n                    result['failed_when_result'] = result['failed'] = failed_when_result\n                else:\n                    failed_when_result = False\n                return failed_when_result\n\n            if 'ansible_facts' in result:\n                if self._task.action in ('set_fact', 'include_vars'):\n                    vars_copy.update(result['ansible_facts'])\n                else:\n                    # TODO: cleaning of facts should eventually become part of taskresults instead of vars\n                    af = wrap_var(result['ansible_facts'])\n                    vars_copy.update(namespace_facts(af))\n                    if C.INJECT_FACTS_AS_VARS:\n                        vars_copy.update(clean_facts(af))\n\n            # set the failed property if it was missing.\n            if 'failed' not in result:\n                # rc is here for backwards compatibility and modules that use it instead of 'failed'\n                if 'rc' in result and result['rc'] not in [0, \"0\"]:\n                    result['failed'] = True\n                else:\n                    result['failed'] = False\n\n            # Make attempts and retries available early to allow their use in changed/failed_when\n            if self._task.until:\n                result['attempts'] = attempt\n\n            # set the changed property if it was missing.\n            if 'changed' not in result:\n                result['changed'] = False\n\n            # re-update the local copy of vars with the registered value, if specified,\n            # or any facts which may have been generated by the module execution\n            # This gives changed/failed_when access to additional recently modified\n            # attributes of result\n            if self._task.register:\n                vars_copy[self._task.register] = result = wrap_var(result)\n\n            # if we didn't skip this task, use the helpers to evaluate the changed/\n            # failed_when properties\n            if 'skipped' not in result:\n                _evaluate_changed_when_result(result)\n                _evaluate_failed_when_result(result)\n\n            if retries > 1:\n                cond = Conditional(loader=self._loader)\n                cond.when = self._task.until\n                if cond.evaluate_conditional(templar, vars_copy):\n                    break\n                else:\n                    # no conditional check, or it failed, so sleep for the specified time\n                    if attempt < retries:\n                        result['_ansible_retry'] = True\n                        result['retries'] = retries\n                        display.debug('Retrying task, attempt %d of %d' % (attempt, retries))\n                        self._final_q.put(TaskResult(self._host.name, self._task._uuid, result, task_fields=self._task.dump_attrs()), block=False)\n                        time.sleep(delay)\n                        self._handler = self._get_action_handler(connection=self._connection, templar=templar)\n        else:\n            if retries > 1:\n                # we ran out of attempts, so mark the result as failed\n                result['attempts'] = retries - 1\n                result['failed'] = True\n\n        # do the final update of the local variables here, for both registered\n        # values and any facts which may have been created\n        if self._task.register:\n            variables[self._task.register] = result = wrap_var(result)\n\n        if 'ansible_facts' in result:\n            if self._task.action in ('set_fact', 'include_vars'):\n                variables.update(result['ansible_facts'])\n            else:\n                # TODO: cleaning of facts should eventually become part of taskresults instead of vars\n                af = wrap_var(result['ansible_facts'])\n                variables.update(namespace_facts(af))\n                if C.INJECT_FACTS_AS_VARS:\n                    variables.update(clean_facts(af))\n\n        # save the notification target in the result, if it was specified, as\n        # this task may be running in a loop in which case the notification\n        # may be item-specific, ie. \"notify: service {{item}}\"\n        if self._task.notify is not None:\n            result['_ansible_notify'] = self._task.notify\n\n        # add the delegated vars to the result, so we can reference them\n        # on the results side without having to do any further templating\n        # FIXME: we only want a limited set of variables here, so this is currently\n        #        hardcoded but should be possibly fixed if we want more or if\n        #        there is another source of truth we can use\n        delegated_vars = variables.get('ansible_delegated_vars', dict()).get(self._task.delegate_to, dict()).copy()\n        if len(delegated_vars) > 0:\n            result[\"_ansible_delegated_vars\"] = {'ansible_delegated_host': self._task.delegate_to}\n            for k in ('ansible_host', ):\n                result[\"_ansible_delegated_vars\"][k] = delegated_vars.get(k)\n\n        # and return\n        display.debug(\"attempt loop complete, returning result\")\n        return result",
        "begin_line": 510,
        "end_line": 785,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._evaluate_changed_when_result#676",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._evaluate_changed_when_result(result)",
        "snippet": "            def _evaluate_changed_when_result(result):\n                if self._task.changed_when is not None and self._task.changed_when:\n                    cond = Conditional(loader=self._loader)\n                    cond.when = self._task.changed_when\n                    result['changed'] = cond.evaluate_conditional(templar, vars_copy)",
        "begin_line": 676,
        "end_line": 680,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._evaluate_failed_when_result#682",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._evaluate_failed_when_result(result)",
        "snippet": "            def _evaluate_failed_when_result(result):\n                if self._task.failed_when:\n                    cond = Conditional(loader=self._loader)\n                    cond.when = self._task.failed_when\n                    failed_when_result = cond.evaluate_conditional(templar, vars_copy)\n                    result['failed_when_result'] = result['failed'] = failed_when_result\n                else:\n                    failed_when_result = False\n                return failed_when_result",
        "begin_line": 682,
        "end_line": 690,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._poll_async_result#787",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._poll_async_result(self, result, templar, task_vars=None)",
        "snippet": "    def _poll_async_result(self, result, templar, task_vars=None):\n        '''\n        Polls for the specified JID to be complete\n        '''\n\n        if task_vars is None:\n            task_vars = self._job_vars\n\n        async_jid = result.get('ansible_job_id')\n        if async_jid is None:\n            return dict(failed=True, msg=\"No job id was returned by the async task\")\n\n        # Create a new pseudo-task to run the async_status module, and run\n        # that (with a sleep for \"poll\" seconds between each retry) until the\n        # async time limit is exceeded.\n\n        async_task = Task().load(dict(action='async_status jid=%s' % async_jid, environment=self._task.environment))\n\n        # FIXME: this is no longer the case, normal takes care of all, see if this can just be generalized\n        # Because this is an async task, the action handler is async. However,\n        # we need the 'normal' action handler for the status check, so get it\n        # now via the action_loader\n        async_handler = self._shared_loader_obj.action_loader.get(\n            'async_status',\n            task=async_task,\n            connection=self._connection,\n            play_context=self._play_context,\n            loader=self._loader,\n            templar=templar,\n            shared_loader_obj=self._shared_loader_obj,\n        )\n\n        time_left = self._task.async_val\n        while time_left > 0:\n            time.sleep(self._task.poll)\n\n            try:\n                async_result = async_handler.run(task_vars=task_vars)\n                # We do not bail out of the loop in cases where the failure\n                # is associated with a parsing error. The async_runner can\n                # have issues which result in a half-written/unparseable result\n                # file on disk, which manifests to the user as a timeout happening\n                # before it's time to timeout.\n                if (int(async_result.get('finished', 0)) == 1 or\n                        ('failed' in async_result and async_result.get('_ansible_parsed', False)) or\n                        'skipped' in async_result):\n                    break\n            except Exception as e:\n                # Connections can raise exceptions during polling (eg, network bounce, reboot); these should be non-fatal.\n                # On an exception, call the connection's reset method if it has one\n                # (eg, drop/recreate WinRM connection; some reused connections are in a broken state)\n                display.vvvv(\"Exception during async poll, retrying... (%s)\" % to_text(e))\n                display.debug(\"Async poll exception was:\\n%s\" % to_text(traceback.format_exc()))\n                try:\n                    async_handler._connection.reset()\n                except AttributeError:\n                    pass\n\n                # Little hack to raise the exception if we've exhausted the timeout period\n                time_left -= self._task.poll\n                if time_left <= 0:\n                    raise\n            else:\n                time_left -= self._task.poll\n\n        if int(async_result.get('finished', 0)) != 1:\n            if async_result.get('_ansible_parsed'):\n                return dict(failed=True, msg=\"async task did not complete within the requested time - %ss\" % self._task.async_val)\n            else:\n                return dict(failed=True, msg=\"async task produced unparseable results\", async_result=async_result)\n        else:\n            async_handler.cleanup(force=True)\n            return async_result",
        "begin_line": 787,
        "end_line": 859,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._set_plugin_options#953",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._set_plugin_options(self, plugin_type, variables, templar, task_keys)",
        "snippet": "    def _set_plugin_options(self, plugin_type, variables, templar, task_keys):\n        try:\n            plugin = getattr(self._connection, '_%s' % plugin_type)\n        except AttributeError:\n            # Some plugins are assigned to private attrs, ``become`` is not\n            plugin = getattr(self._connection, plugin_type)\n        option_vars = C.config.get_plugin_vars(plugin_type, plugin._load_name)\n        options = {}\n        for k in option_vars:\n            if k in variables:\n                options[k] = templar.template(variables[k])\n        # TODO move to task method?\n        plugin.set_options(task_keys=task_keys, var_options=options)",
        "begin_line": 953,
        "end_line": 965,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._set_connection_options#967",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._set_connection_options(self, variables, templar)",
        "snippet": "    def _set_connection_options(self, variables, templar):\n\n        # Keep the pre-delegate values for these keys\n        PRESERVE_ORIG = ('inventory_hostname',)\n\n        # create copy with delegation built in\n        final_vars = combine_vars(\n            variables,\n            variables.get('ansible_delegated_vars', {}).get(self._task.delegate_to, {})\n        )\n\n        # grab list of usable vars for this plugin\n        option_vars = C.config.get_plugin_vars('connection', self._connection._load_name)\n\n        # create dict of 'templated vars'\n        options = {'_extras': {}}\n        for k in option_vars:\n            if k in PRESERVE_ORIG:\n                options[k] = templar.template(variables[k])\n            elif k in final_vars:\n                options[k] = templar.template(final_vars[k])\n\n        # add extras if plugin supports them\n        if getattr(self._connection, 'allow_extras', False):\n            for k in final_vars:\n                if k.startswith('ansible_%s_' % self._connection._load_name) and k not in options:\n                    options['_extras'][k] = templar.template(final_vars[k])\n\n        task_keys = self._task.dump_attrs()\n\n        # set options with 'templated vars' specific to this plugin and dependant ones\n        self._connection.set_options(task_keys=task_keys, var_options=options)\n        self._set_plugin_options('shell', final_vars, templar, task_keys)\n\n        if self._connection.become is not None:\n            # FIXME: find alternate route to provide passwords,\n            # keep out of play objects to avoid accidental disclosure\n            task_keys['become_pass'] = self._play_context.become_pass\n            self._set_plugin_options('become', final_vars, templar, task_keys)\n\n            # FOR BACKWARDS COMPAT:\n            for option in ('become_user', 'become_flags', 'become_exe'):\n                try:\n                    setattr(self._play_context, option, self._connection.become.get_option(option))\n                except KeyError:\n                    pass  # some plugins don't support all base flags\n            self._play_context.prompt = self._connection.become.prompt",
        "begin_line": 967,
        "end_line": 1013,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_executor.TaskExecutor._get_action_handler#1015",
        "src_path": "lib/ansible/executor/task_executor.py",
        "class_name": "lib.ansible.executor.task_executor.TaskExecutor",
        "signature": "lib.ansible.executor.task_executor.TaskExecutor._get_action_handler(self, connection, templar)",
        "snippet": "    def _get_action_handler(self, connection, templar):\n        '''\n        Returns the correct action plugin to handle the requestion task action\n        '''\n\n        module_collection, separator, module_name = self._task.action.rpartition(\".\")\n        module_prefix = module_name.split('_')[0]\n        if module_collection:\n            # For network modules, which look for one action plugin per platform, look for the\n            # action plugin in the same collection as the module by prefixing the action plugin\n            # with the same collection.\n            network_action = \"{0}.{1}\".format(module_collection, module_prefix)\n        else:\n            network_action = module_prefix\n\n        collections = self._task.collections\n\n        # let action plugin override module, fallback to 'normal' action plugin otherwise\n        if self._shared_loader_obj.action_loader.has_plugin(self._task.action, collection_list=collections):\n            handler_name = self._task.action\n        elif all((module_prefix in C.NETWORK_GROUP_MODULES, self._shared_loader_obj.action_loader.has_plugin(network_action, collection_list=collections))):\n            handler_name = network_action\n        else:\n            # FUTURE: once we're comfortable with collections impl, preface this action with ansible.builtin so it can't be hijacked\n            handler_name = 'normal'\n            collections = None  # until then, we don't want the task's collection list to be consulted; use the builtin\n\n        handler = self._shared_loader_obj.action_loader.get(\n            handler_name,\n            task=self._task,\n            connection=connection,\n            play_context=self._play_context,\n            loader=self._loader,\n            templar=templar,\n            shared_loader_obj=self._shared_loader_obj,\n            collection_list=collections\n        )\n\n        if not handler:\n            raise AnsibleError(\"the handler '%s' was not found\" % handler_name)\n\n        return handler",
        "begin_line": 1015,
        "end_line": 1056,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.BaseFactCollector.__init__#65",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector.BaseFactCollector",
        "signature": "lib.ansible.module_utils.facts.collector.BaseFactCollector.__init__(self, collectors=None, namespace=None)",
        "snippet": "    def __init__(self, collectors=None, namespace=None):\n        '''Base class for things that collect facts.\n\n        'collectors' is an optional list of other FactCollectors for composing.'''\n        self.collectors = collectors or []\n\n        # self.namespace is a object with a 'transform' method that transforms\n        # the name to indicate the namespace (ie, adds a prefix or suffix).\n        self.namespace = namespace\n\n        self.fact_ids = set([self.name])\n        self.fact_ids.update(self._fact_ids)",
        "begin_line": 65,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.896551724137931e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.BaseFactCollector.platform_match#79",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector.BaseFactCollector",
        "signature": "lib.ansible.module_utils.facts.collector.BaseFactCollector.platform_match(cls, platform_info)",
        "snippet": "    def platform_match(cls, platform_info):\n        if platform_info.get('system', None) == cls._platform:\n            return cls\n        return None",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.2811999417504e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.BaseFactCollector._transform_dict_keys#89",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector.BaseFactCollector",
        "signature": "lib.ansible.module_utils.facts.collector.BaseFactCollector._transform_dict_keys(self, fact_dict)",
        "snippet": "    def _transform_dict_keys(self, fact_dict):\n        '''update a dicts keys to use new names as transformed by self._transform_name'''\n\n        for old_key in list(fact_dict.keys()):\n            new_key = self._transform_name(old_key)\n            # pop the item by old_key and replace it using new_key\n            fact_dict[new_key] = fact_dict.pop(old_key)\n        return fact_dict",
        "begin_line": 89,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.BaseFactCollector.collect_with_namespace#99",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector.BaseFactCollector",
        "signature": "lib.ansible.module_utils.facts.collector.BaseFactCollector.collect_with_namespace(self, module=None, collected_facts=None)",
        "snippet": "    def collect_with_namespace(self, module=None, collected_facts=None):\n        # collect, then transform the key names if needed\n        facts_dict = self.collect(module=module, collected_facts=collected_facts)\n        if self.namespace:\n            facts_dict = self._transform_dict_keys(facts_dict)\n        return facts_dict",
        "begin_line": 99,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.get_collector_names#120",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.get_collector_names(valid_subsets=None, minimal_gather_subset=None, gather_subset=None, aliases_map=None, platform_info=None)",
        "snippet": "def get_collector_names(valid_subsets=None,\n                        minimal_gather_subset=None,\n                        gather_subset=None,\n                        aliases_map=None,\n                        platform_info=None):\n    '''return a set of FactCollector names based on gather_subset spec.\n\n    gather_subset is a spec describing which facts to gather.\n    valid_subsets is a frozenset of potential matches for gather_subset ('all', 'network') etc\n    minimal_gather_subsets is a frozenset of matches to always use, even for gather_subset='!all'\n    '''\n\n    # Retrieve module parameters\n    gather_subset = gather_subset or ['all']\n\n    # the list of everything that 'all' expands to\n    valid_subsets = valid_subsets or frozenset()\n\n    # if provided, minimal_gather_subset is always added, even after all negations\n    minimal_gather_subset = minimal_gather_subset or frozenset()\n\n    aliases_map = aliases_map or defaultdict(set)\n\n    # Retrieve all facts elements\n    additional_subsets = set()\n    exclude_subsets = set()\n\n    # total always starts with the min set, then\n    # adds of the additions in gather_subset, then\n    # excludes all of the excludes, then add any explicitly\n    # requested subsets.\n    gather_subset_with_min = ['min']\n    gather_subset_with_min.extend(gather_subset)\n\n    # subsets we mention in gather_subset explicitly, except for 'all'/'min'\n    explicitly_added = set()\n\n    for subset in gather_subset_with_min:\n        subset_id = subset\n        if subset_id == 'min':\n            additional_subsets.update(minimal_gather_subset)\n            continue\n        if subset_id == 'all':\n            additional_subsets.update(valid_subsets)\n            continue\n        if subset_id.startswith('!'):\n            subset = subset[1:]\n            if subset == 'min':\n                exclude_subsets.update(minimal_gather_subset)\n                continue\n            if subset == 'all':\n                exclude_subsets.update(valid_subsets - minimal_gather_subset)\n                continue\n            exclude = True\n        else:\n            exclude = False\n\n        if exclude:\n            # include 'devices', 'dmi' etc for '!hardware'\n            exclude_subsets.update(aliases_map.get(subset, set()))\n            exclude_subsets.add(subset)\n        else:\n            # NOTE: this only considers adding an unknown gather subsetup an error. Asking to\n            #       exclude an unknown gather subset is ignored.\n            if subset_id not in valid_subsets:\n                raise TypeError(\"Bad subset '%s' given to Ansible. gather_subset options allowed: all, %s\" %\n                                (subset, \", \".join(sorted(valid_subsets))))\n\n            explicitly_added.add(subset)\n            additional_subsets.add(subset)\n\n    if not additional_subsets:\n        additional_subsets.update(valid_subsets)\n\n    additional_subsets.difference_update(exclude_subsets - explicitly_added)\n\n    return additional_subsets",
        "begin_line": 120,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.find_collectors_for_platform#199",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.find_collectors_for_platform(all_collector_classes, compat_platforms)",
        "snippet": "def find_collectors_for_platform(all_collector_classes, compat_platforms):\n    found_collectors = set()\n    found_collectors_names = set()\n\n    # start from specific platform, then try generic\n    for compat_platform in compat_platforms:\n        platform_match = None\n        for all_collector_class in all_collector_classes:\n\n            # ask the class if it is compatible with the platform info\n            platform_match = all_collector_class.platform_match(compat_platform)\n\n            if not platform_match:\n                continue\n\n            primary_name = all_collector_class.name\n\n            if primary_name not in found_collectors_names:\n                found_collectors.add(all_collector_class)\n                found_collectors_names.add(all_collector_class.name)\n\n    return found_collectors",
        "begin_line": 199,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.2811999417504e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.build_fact_id_to_collector_map#223",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.build_fact_id_to_collector_map(collectors_for_platform)",
        "snippet": "def build_fact_id_to_collector_map(collectors_for_platform):\n    fact_id_to_collector_map = defaultdict(list)\n    aliases_map = defaultdict(set)\n\n    for collector_class in collectors_for_platform:\n        primary_name = collector_class.name\n\n        fact_id_to_collector_map[primary_name].append(collector_class)\n\n        for fact_id in collector_class._fact_ids:\n            fact_id_to_collector_map[fact_id].append(collector_class)\n            aliases_map[primary_name].add(fact_id)\n\n    return fact_id_to_collector_map, aliases_map",
        "begin_line": 223,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.499625018749062e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.select_collector_classes#239",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.select_collector_classes(collector_names, all_fact_subsets)",
        "snippet": "def select_collector_classes(collector_names, all_fact_subsets):\n    seen_collector_classes = set()\n\n    selected_collector_classes = []\n\n    for collector_name in collector_names:\n        collector_classes = all_fact_subsets.get(collector_name, [])\n        for collector_class in collector_classes:\n            if collector_class not in seen_collector_classes:\n                selected_collector_classes.append(collector_class)\n                seen_collector_classes.add(collector_class)\n\n    return selected_collector_classes",
        "begin_line": 239,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.552870090634441e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector._get_requires_by_collector_name#254",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector._get_requires_by_collector_name(collector_name, all_fact_subsets)",
        "snippet": "def _get_requires_by_collector_name(collector_name, all_fact_subsets):\n    required_facts = set()\n\n    try:\n        collector_classes = all_fact_subsets[collector_name]\n    except KeyError:\n        raise CollectorNotFoundError('Fact collector \"%s\" not found' % collector_name)\n    for collector_class in collector_classes:\n        required_facts.update(collector_class.required_facts)\n    return required_facts",
        "begin_line": 254,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.find_unresolved_requires#266",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.find_unresolved_requires(collector_names, all_fact_subsets)",
        "snippet": "def find_unresolved_requires(collector_names, all_fact_subsets):\n    '''Find any collector names that have unresolved requires\n\n    Returns a list of collector names that correspond to collector\n    classes whose .requires_facts() are not in collector_names.\n    '''\n    unresolved = set()\n\n    for collector_name in collector_names:\n        required_facts = _get_requires_by_collector_name(collector_name, all_fact_subsets)\n        for required_fact in required_facts:\n            if required_fact not in collector_names:\n                unresolved.add(required_fact)\n\n    return unresolved",
        "begin_line": 266,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.resolve_requires#283",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.resolve_requires(unresolved_requires, all_fact_subsets)",
        "snippet": "def resolve_requires(unresolved_requires, all_fact_subsets):\n    new_names = set()\n    failed = []\n    for unresolved in unresolved_requires:\n        if unresolved in all_fact_subsets:\n            new_names.add(unresolved)\n        else:\n            failed.append(unresolved)\n\n    if failed:\n        raise UnresolvedFactDep('unresolved fact dep %s' % ','.join(failed))\n    return new_names",
        "begin_line": 283,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.build_dep_data#297",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.build_dep_data(collector_names, all_fact_subsets)",
        "snippet": "def build_dep_data(collector_names, all_fact_subsets):\n    dep_map = defaultdict(set)\n    for collector_name in collector_names:\n        collector_deps = set()\n        for collector in all_fact_subsets[collector_name]:\n            for dep in collector.required_facts:\n                collector_deps.add(dep)\n        dep_map[collector_name] = collector_deps\n    return dep_map",
        "begin_line": 297,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.258320257659592e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.tsort#308",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.tsort(dep_map)",
        "snippet": "def tsort(dep_map):\n    sorted_list = []\n\n    unsorted_map = dep_map.copy()\n\n    while unsorted_map:\n        acyclic = False\n        for node, edges in list(unsorted_map.items()):\n            for edge in edges:\n                if edge in unsorted_map:\n                    break\n            else:\n                acyclic = True\n                del unsorted_map[node]\n                sorted_list.append((node, edges))\n\n        if not acyclic:\n            raise CycleFoundInFactDeps('Unable to tsort deps, there was a cycle in the graph. sorted=%s' % sorted_list)\n\n    return sorted_list",
        "begin_line": 308,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector._solve_deps#330",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector._solve_deps(collector_names, all_fact_subsets)",
        "snippet": "def _solve_deps(collector_names, all_fact_subsets):\n    unresolved = collector_names.copy()\n    solutions = collector_names.copy()\n\n    while True:\n        unresolved = find_unresolved_requires(solutions, all_fact_subsets)\n        if unresolved == set():\n            break\n\n        new_names = resolve_requires(unresolved, all_fact_subsets)\n        solutions.update(new_names)\n\n    return solutions",
        "begin_line": 330,
        "end_line": 342,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.collector.collector_classes_from_gather_subset#345",
        "src_path": "lib/ansible/module_utils/facts/collector.py",
        "class_name": "lib.ansible.module_utils.facts.collector",
        "signature": "lib.ansible.module_utils.facts.collector.collector_classes_from_gather_subset(all_collector_classes=None, valid_subsets=None, minimal_gather_subset=None, gather_subset=None, gather_timeout=None, platform_info=None)",
        "snippet": "def collector_classes_from_gather_subset(all_collector_classes=None,\n                                         valid_subsets=None,\n                                         minimal_gather_subset=None,\n                                         gather_subset=None,\n                                         gather_timeout=None,\n                                         platform_info=None):\n    '''return a list of collector classes that match the args'''\n\n    # use gather_name etc to get the list of collectors\n\n    all_collector_classes = all_collector_classes or []\n\n    minimal_gather_subset = minimal_gather_subset or frozenset()\n\n    platform_info = platform_info or {'system': platform.system()}\n\n    gather_timeout = gather_timeout or timeout.DEFAULT_GATHER_TIMEOUT\n\n    # tweak the modules GATHER_TIMEOUT\n    timeout.GATHER_TIMEOUT = gather_timeout\n\n    valid_subsets = valid_subsets or frozenset()\n\n    # maps alias names like 'hardware' to the list of names that are part of hardware\n    # like 'devices' and 'dmi'\n    aliases_map = defaultdict(set)\n\n    compat_platforms = [platform_info, {'system': 'Generic'}]\n\n    collectors_for_platform = find_collectors_for_platform(all_collector_classes, compat_platforms)\n\n    # all_facts_subsets maps the subset name ('hardware') to the class that provides it.\n\n    # TODO: name collisions here? are there facts with the same name as a gather_subset (all, network, hardware, virtual, ohai, facter)\n    all_fact_subsets, aliases_map = build_fact_id_to_collector_map(collectors_for_platform)\n\n    all_valid_subsets = frozenset(all_fact_subsets.keys())\n\n    # expand any fact_id/collectorname/gather_subset term ('all', 'env', etc) to the list of names that represents\n    collector_names = get_collector_names(valid_subsets=all_valid_subsets,\n                                          minimal_gather_subset=minimal_gather_subset,\n                                          gather_subset=gather_subset,\n                                          aliases_map=aliases_map,\n                                          platform_info=platform_info)\n\n    complete_collector_names = _solve_deps(collector_names, all_fact_subsets)\n\n    dep_map = build_dep_data(complete_collector_names, all_fact_subsets)\n\n    ordered_deps = tsort(dep_map)\n    ordered_collector_names = [x[0] for x in ordered_deps]\n\n    selected_collector_classes = select_collector_classes(ordered_collector_names,\n                                                          all_fact_subsets)\n\n    return selected_collector_classes",
        "begin_line": 345,
        "end_line": 400,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin.__init__#34",
        "src_path": "lib/ansible/plugins/vars/__init__.py",
        "class_name": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin",
        "signature": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin.__init__(self)",
        "snippet": "    def __init__(self):\n        \"\"\" constructor \"\"\"\n        super(BaseVarsPlugin, self).__init__()\n        self._display = display",
        "begin_line": 34,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin.get_vars#39",
        "src_path": "lib/ansible/plugins/vars/__init__.py",
        "class_name": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin",
        "signature": "lib.ansible.plugins.vars.__init__.BaseVarsPlugin.get_vars(self, loader, path, entities)",
        "snippet": "    def get_vars(self, loader, path, entities):\n        \"\"\" Gets variables. \"\"\"\n        self._basedir = basedir(path)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_param#487",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_param(rule, param, flag, is_list)",
        "snippet": "def append_param(rule, param, flag, is_list):\n    if is_list:\n        for item in param:\n            append_param(rule, item, flag, False)\n    else:\n        if param is not None:\n            if param[0] == '!':\n                rule.extend(['!', flag, param[1:]])\n            else:\n                rule.extend([flag, param])",
        "begin_line": 487,
        "end_line": 496,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_tcp_flags#499",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_tcp_flags(rule, param, flag)",
        "snippet": "def append_tcp_flags(rule, param, flag):\n    if param:\n        if 'flags' in param and 'flags_set' in param:\n            rule.extend([flag, ','.join(param['flags']), ','.join(param['flags_set'])])",
        "begin_line": 499,
        "end_line": 502,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_match_flag#505",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_match_flag(rule, param, flag, negatable)",
        "snippet": "def append_match_flag(rule, param, flag, negatable):\n    if param == 'match':\n        rule.extend([flag])\n    elif negatable and param == 'negate':\n        rule.extend(['!', flag])",
        "begin_line": 505,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_csv#512",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_csv(rule, param, flag)",
        "snippet": "def append_csv(rule, param, flag):\n    if param:\n        rule.extend([flag, ','.join(param)])",
        "begin_line": 512,
        "end_line": 514,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_match#517",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_match(rule, param, match)",
        "snippet": "def append_match(rule, param, match):\n    if param:\n        rule.extend(['-m', match])",
        "begin_line": 517,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_jump#522",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_jump(rule, param, jump)",
        "snippet": "def append_jump(rule, param, jump):\n    if param:\n        rule.extend(['-j', jump])",
        "begin_line": 522,
        "end_line": 524,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_wait#527",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_wait(rule, param, flag)",
        "snippet": "def append_wait(rule, param, flag):\n    if param:\n        rule.extend([flag, param])",
        "begin_line": 527,
        "end_line": 529,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.construct_rule#532",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.construct_rule(params)",
        "snippet": "def construct_rule(params):\n    rule = []\n    append_wait(rule, params['wait'], '-w')\n    append_param(rule, params['protocol'], '-p', False)\n    append_param(rule, params['source'], '-s', False)\n    append_param(rule, params['destination'], '-d', False)\n    append_param(rule, params['match'], '-m', True)\n    append_tcp_flags(rule, params['tcp_flags'], '--tcp-flags')\n    append_param(rule, params['jump'], '-j', False)\n    if params.get('jump') and params['jump'].lower() == 'tee':\n        append_param(rule, params['gateway'], '--gateway', False)\n    append_param(rule, params['log_prefix'], '--log-prefix', False)\n    append_param(rule, params['log_level'], '--log-level', False)\n    append_param(rule, params['to_destination'], '--to-destination', False)\n    append_param(rule, params['to_source'], '--to-source', False)\n    append_param(rule, params['goto'], '-g', False)\n    append_param(rule, params['in_interface'], '-i', False)\n    append_param(rule, params['out_interface'], '-o', False)\n    append_param(rule, params['fragment'], '-f', False)\n    append_param(rule, params['set_counters'], '-c', False)\n    append_param(rule, params['source_port'], '--source-port', False)\n    append_param(rule, params['destination_port'], '--destination-port', False)\n    append_param(rule, params['to_ports'], '--to-ports', False)\n    append_param(rule, params['set_dscp_mark'], '--set-dscp', False)\n    append_param(\n        rule,\n        params['set_dscp_mark_class'],\n        '--set-dscp-class',\n        False)\n    append_match_flag(rule, params['syn'], '--syn', True)\n    append_match(rule, params['comment'], 'comment')\n    append_param(rule, params['comment'], '--comment', False)\n    if 'conntrack' in params['match']:\n        append_csv(rule, params['ctstate'], '--ctstate')\n    elif 'state' in params['match']:\n        append_csv(rule, params['ctstate'], '--state')\n    elif params['ctstate']:\n        append_match(rule, params['ctstate'], 'conntrack')\n        append_csv(rule, params['ctstate'], '--ctstate')\n    if 'iprange' in params['match']:\n        append_param(rule, params['src_range'], '--src-range', False)\n        append_param(rule, params['dst_range'], '--dst-range', False)\n    elif params['src_range'] or params['dst_range']:\n        append_match(rule, params['src_range'] or params['dst_range'], 'iprange')\n        append_param(rule, params['src_range'], '--src-range', False)\n        append_param(rule, params['dst_range'], '--dst-range', False)\n    append_match(rule, params['limit'] or params['limit_burst'], 'limit')\n    append_param(rule, params['limit'], '--limit', False)\n    append_param(rule, params['limit_burst'], '--limit-burst', False)\n    append_match(rule, params['uid_owner'], 'owner')\n    append_match_flag(rule, params['uid_owner'], '--uid-owner', True)\n    append_param(rule, params['uid_owner'], '--uid-owner', False)\n    append_match(rule, params['gid_owner'], 'owner')\n    append_match_flag(rule, params['gid_owner'], '--gid-owner', True)\n    append_param(rule, params['gid_owner'], '--gid-owner', False)\n    if params['jump'] is None:\n        append_jump(rule, params['reject_with'], 'REJECT')\n    append_param(rule, params['reject_with'], '--reject-with', False)\n    append_param(\n        rule,\n        params['icmp_type'],\n        ICMP_TYPE_OPTIONS[params['ip_version']],\n        False)\n    return rule",
        "begin_line": 532,
        "end_line": 595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.push_arguments#598",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.push_arguments(iptables_path, action, params, make_rule=True)",
        "snippet": "def push_arguments(iptables_path, action, params, make_rule=True):\n    cmd = [iptables_path]\n    cmd.extend(['-t', params['table']])\n    cmd.extend([action, params['chain']])\n    if action == '-I' and params['rule_num']:\n        cmd.extend([params['rule_num']])\n    if make_rule:\n        cmd.extend(construct_rule(params))\n    return cmd",
        "begin_line": 598,
        "end_line": 606,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.258320257659592e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.check_present#609",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.check_present(iptables_path, module, params)",
        "snippet": "def check_present(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-C', params)\n    rc, _, __ = module.run_command(cmd, check_rc=False)\n    return (rc == 0)",
        "begin_line": 609,
        "end_line": 612,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.append_rule#615",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.append_rule(iptables_path, module, params)",
        "snippet": "def append_rule(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-A', params)\n    module.run_command(cmd, check_rc=True)",
        "begin_line": 615,
        "end_line": 617,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.insert_rule#620",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.insert_rule(iptables_path, module, params)",
        "snippet": "def insert_rule(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-I', params)\n    module.run_command(cmd, check_rc=True)",
        "begin_line": 620,
        "end_line": 622,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.remove_rule#625",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.remove_rule(iptables_path, module, params)",
        "snippet": "def remove_rule(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-D', params)\n    module.run_command(cmd, check_rc=True)",
        "begin_line": 625,
        "end_line": 627,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.flush_table#630",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.flush_table(iptables_path, module, params)",
        "snippet": "def flush_table(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-F', params, make_rule=False)\n    module.run_command(cmd, check_rc=True)",
        "begin_line": 630,
        "end_line": 632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.set_chain_policy#635",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.set_chain_policy(iptables_path, module, params)",
        "snippet": "def set_chain_policy(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-P', params, make_rule=False)\n    cmd.append(params['policy'])\n    module.run_command(cmd, check_rc=True)",
        "begin_line": 635,
        "end_line": 638,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.get_chain_policy#641",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.get_chain_policy(iptables_path, module, params)",
        "snippet": "def get_chain_policy(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-L', params)\n    rc, out, _ = module.run_command(cmd, check_rc=True)\n    chain_header = out.split(\"\\n\")[0]\n    result = re.search(r'\\(policy ([A-Z]+)\\)', chain_header)\n    if result:\n        return result.group(1)\n    return None",
        "begin_line": 641,
        "end_line": 648,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.iptables.main#657",
        "src_path": "lib/ansible/modules/system/iptables.py",
        "class_name": "lib.ansible.modules.system.iptables",
        "signature": "lib.ansible.modules.system.iptables.main()",
        "snippet": "def main():\n    module = AnsibleModule(\n        supports_check_mode=True,\n        argument_spec=dict(\n            table=dict(type='str', default='filter', choices=['filter', 'nat', 'mangle', 'raw', 'security']),\n            state=dict(type='str', default='present', choices=['absent', 'present']),\n            action=dict(type='str', default='append', choices=['append', 'insert']),\n            ip_version=dict(type='str', default='ipv4', choices=['ipv4', 'ipv6']),\n            chain=dict(type='str'),\n            rule_num=dict(type='str'),\n            protocol=dict(type='str'),\n            wait=dict(type='str'),\n            source=dict(type='str'),\n            to_source=dict(type='str'),\n            destination=dict(type='str'),\n            to_destination=dict(type='str'),\n            match=dict(type='list', default=[]),\n            tcp_flags=dict(type='dict',\n                           options=dict(\n                                flags=dict(type='list'),\n                                flags_set=dict(type='list'))\n                           ),\n            jump=dict(type='str'),\n            gateway=dict(type='str'),\n            log_prefix=dict(type='str'),\n            log_level=dict(type='str',\n                           choices=['0', '1', '2', '3', '4', '5', '6', '7',\n                                    'emerg', 'alert', 'crit', 'error',\n                                    'warning', 'notice', 'info', 'debug'],\n                           default=None,\n                           ),\n            goto=dict(type='str'),\n            in_interface=dict(type='str'),\n            out_interface=dict(type='str'),\n            fragment=dict(type='str'),\n            set_counters=dict(type='str'),\n            source_port=dict(type='str'),\n            destination_port=dict(type='str'),\n            to_ports=dict(type='str'),\n            set_dscp_mark=dict(type='str'),\n            set_dscp_mark_class=dict(type='str'),\n            comment=dict(type='str'),\n            ctstate=dict(type='list', default=[]),\n            src_range=dict(type='str'),\n            dst_range=dict(type='str'),\n            limit=dict(type='str'),\n            limit_burst=dict(type='str'),\n            uid_owner=dict(type='str'),\n            gid_owner=dict(type='str'),\n            reject_with=dict(type='str'),\n            icmp_type=dict(type='str'),\n            syn=dict(type='str', default='ignore', choices=['ignore', 'match', 'negate']),\n            flush=dict(type='bool', default=False),\n            policy=dict(type='str', choices=['ACCEPT', 'DROP', 'QUEUE', 'RETURN']),\n        ),\n        mutually_exclusive=(\n            ['set_dscp_mark', 'set_dscp_mark_class'],\n            ['flush', 'policy'],\n        ),\n        required_if=[\n            ['jump', 'TEE', ['gateway']],\n            ['jump', 'tee', ['gateway']],\n        ]\n    )\n    args = dict(\n        changed=False,\n        failed=False,\n        ip_version=module.params['ip_version'],\n        table=module.params['table'],\n        chain=module.params['chain'],\n        flush=module.params['flush'],\n        rule=' '.join(construct_rule(module.params)),\n        state=module.params['state'],\n    )\n\n    ip_version = module.params['ip_version']\n    iptables_path = module.get_bin_path(BINS[ip_version], True)\n\n    # Check if chain option is required\n    if args['flush'] is False and args['chain'] is None:\n        module.fail_json(msg=\"Either chain or flush parameter must be specified.\")\n\n    if module.params.get('log_prefix', None) or module.params.get('log_level', None):\n        if module.params['jump'] is None:\n            module.params['jump'] = 'LOG'\n        elif module.params['jump'] != 'LOG':\n            module.fail_json(msg=\"Logging options can only be used with the LOG jump target.\")\n\n    # Check if wait option is supported\n    iptables_version = LooseVersion(get_iptables_version(iptables_path, module))\n\n    if iptables_version >= LooseVersion(IPTABLES_WAIT_SUPPORT_ADDED):\n        if iptables_version < LooseVersion(IPTABLES_WAIT_WITH_SECONDS_SUPPORT_ADDED):\n            module.params['wait'] = ''\n    else:\n        module.params['wait'] = None\n\n    # Flush the table\n    if args['flush'] is True:\n        args['changed'] = True\n        if not module.check_mode:\n            flush_table(iptables_path, module, module.params)\n\n    # Set the policy\n    elif module.params['policy']:\n        current_policy = get_chain_policy(iptables_path, module, module.params)\n        if not current_policy:\n            module.fail_json(msg='Can\\'t detect current policy')\n\n        changed = current_policy != module.params['policy']\n        args['changed'] = changed\n        if changed and not module.check_mode:\n            set_chain_policy(iptables_path, module, module.params)\n\n    else:\n        insert = (module.params['action'] == 'insert')\n        rule_is_present = check_present(iptables_path, module, module.params)\n        should_be_present = (args['state'] == 'present')\n\n        # Check if target is up to date\n        args['changed'] = (rule_is_present != should_be_present)\n        if args['changed'] is False:\n            # Target is already up to date\n            module.exit_json(**args)\n\n        # Check only; don't modify\n        if not module.check_mode:\n            if should_be_present:\n                if insert:\n                    insert_rule(iptables_path, module, module.params)\n                else:\n                    append_rule(iptables_path, module, module.params)\n            else:\n                remove_rule(iptables_path, module, module.params)\n\n    module.exit_json(**args)",
        "begin_line": 657,
        "end_line": 792,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.data.ConfigData.__init__#10",
        "src_path": "lib/ansible/config/data.py",
        "class_name": "lib.ansible.config.data.ConfigData",
        "signature": "lib.ansible.config.data.ConfigData.__init__(self)",
        "snippet": "    def __init__(self):\n        self._global_settings = {}\n        self._plugins = {}",
        "begin_line": 10,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.data.ConfigData.get_setting#14",
        "src_path": "lib/ansible/config/data.py",
        "class_name": "lib.ansible.config.data.ConfigData",
        "signature": "lib.ansible.config.data.ConfigData.get_setting(self, name, plugin=None)",
        "snippet": "    def get_setting(self, name, plugin=None):\n\n        setting = None\n        if plugin is None:\n            setting = self._global_settings.get(name)\n        elif plugin.type in self._plugins and plugin.name in self._plugins[plugin.type]:\n            setting = self._plugins[plugin.type][plugin.name].get(name)\n\n        return setting",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.data.ConfigData.get_settings#24",
        "src_path": "lib/ansible/config/data.py",
        "class_name": "lib.ansible.config.data.ConfigData",
        "signature": "lib.ansible.config.data.ConfigData.get_settings(self, plugin=None)",
        "snippet": "    def get_settings(self, plugin=None):\n\n        settings = []\n        if plugin is None:\n            settings = [self._global_settings[k] for k in self._global_settings]\n        elif plugin.type in self._plugins and plugin.name in self._plugins[plugin.type]:\n            settings = [self._plugins[plugin.type][plugin.name][k] for k in self._plugins[plugin.type][plugin.name]]\n\n        return settings",
        "begin_line": 24,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.data.ConfigData.update_setting#34",
        "src_path": "lib/ansible/config/data.py",
        "class_name": "lib.ansible.config.data.ConfigData",
        "signature": "lib.ansible.config.data.ConfigData.update_setting(self, setting, plugin=None)",
        "snippet": "    def update_setting(self, setting, plugin=None):\n\n        if plugin is None:\n            self._global_settings[setting.name] = setting\n        else:\n            if plugin.type not in self._plugins:\n                self._plugins[plugin.type] = {}\n            if plugin.name not in self._plugins[plugin.type]:\n                self._plugins[plugin.type][plugin.name] = {}\n            self._plugins[plugin.type][plugin.name][setting.name] = setting",
        "begin_line": 34,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.auto.InventoryModule.verify_file#34",
        "src_path": "lib/ansible/plugins/inventory/auto.py",
        "class_name": "lib.ansible.plugins.inventory.auto.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.auto.InventoryModule.verify_file(self, path)",
        "snippet": "    def verify_file(self, path):\n        if not path.endswith('.yml') and not path.endswith('.yaml'):\n            return False\n        return super(InventoryModule, self).verify_file(path)",
        "begin_line": 34,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.auto.InventoryModule.parse#39",
        "src_path": "lib/ansible/plugins/inventory/auto.py",
        "class_name": "lib.ansible.plugins.inventory.auto.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.auto.InventoryModule.parse(self, inventory, loader, path, cache=True)",
        "snippet": "    def parse(self, inventory, loader, path, cache=True):\n        config_data = loader.load_from_file(path, cache=False)\n\n        try:\n            plugin_name = config_data.get('plugin', None)\n        except AttributeError:\n            plugin_name = None\n\n        if not plugin_name:\n            raise AnsibleParserError(\"no root 'plugin' key found, '{0}' is not a valid YAML inventory plugin config file\".format(path))\n\n        plugin = inventory_loader.get(plugin_name)\n\n        if not plugin:\n            raise AnsibleParserError(\"inventory config '{0}' specifies unknown plugin '{1}'\".format(path, plugin_name))\n\n        if not plugin.verify_file(path):\n            raise AnsibleParserError(\"inventory config '{0}' could not be verified by plugin '{1}'\".format(path, plugin_name))\n\n        plugin.parse(inventory, loader, path, cache=cache)\n        try:\n            plugin.update_cache_if_changed()\n        except AttributeError:\n            pass",
        "begin_line": 39,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.parameters._return_datastructure_name#46",
        "src_path": "lib/ansible/module_utils/common/parameters.py",
        "class_name": "lib.ansible.module_utils.common.parameters",
        "signature": "lib.ansible.module_utils.common.parameters._return_datastructure_name(obj)",
        "snippet": "def _return_datastructure_name(obj):\n    \"\"\" Return native stringified values from datastructures.\n\n    For use with removing sensitive values pre-jsonification.\"\"\"\n    if isinstance(obj, (text_type, binary_type)):\n        if obj:\n            yield to_native(obj, errors='surrogate_or_strict')\n        return\n    elif isinstance(obj, Mapping):\n        for element in obj.items():\n            for subelement in _return_datastructure_name(element[1]):\n                yield subelement\n    elif is_iterable(obj):\n        for element in obj:\n            for subelement in _return_datastructure_name(element):\n                yield subelement\n    elif isinstance(obj, (bool, NoneType)):\n        # This must come before int because bools are also ints\n        return\n    elif isinstance(obj, tuple(list(integer_types) + [float])):\n        yield to_native(obj, nonstring='simplerepr')\n    else:\n        raise TypeError('Unknown parameter type: %s' % (type(obj)))",
        "begin_line": 46,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.parameters.list_no_log_values#71",
        "src_path": "lib/ansible/module_utils/common/parameters.py",
        "class_name": "lib.ansible.module_utils.common.parameters",
        "signature": "lib.ansible.module_utils.common.parameters.list_no_log_values(argument_spec, params)",
        "snippet": "def list_no_log_values(argument_spec, params):\n    \"\"\"Return set of no log values\n\n    :arg argument_spec: An argument spec dictionary from a module\n    :arg params: Dictionary of all module parameters\n\n    :returns: Set of strings that should be hidden from output::\n\n        {'secret_dict_value', 'secret_list_item_one', 'secret_list_item_two', 'secret_string'}\n    \"\"\"\n\n    no_log_values = set()\n    for arg_name, arg_opts in argument_spec.items():\n        if arg_opts.get('no_log', False):\n            # Find the value for the no_log'd param\n            no_log_object = params.get(arg_name, None)\n\n            if no_log_object:\n                try:\n                    no_log_values.update(_return_datastructure_name(no_log_object))\n                except TypeError as e:\n                    raise TypeError('Failed to convert \"%s\": %s' % (arg_name, to_native(e)))\n\n        # Get no_log values from suboptions\n        sub_argument_spec = arg_opts.get('options')\n        if sub_argument_spec is not None:\n            wanted_type = arg_opts.get('type')\n            sub_parameters = params.get(arg_name)\n\n            if sub_parameters is not None:\n                if wanted_type == 'dict' or (wanted_type == 'list' and arg_opts.get('elements', '') == 'dict'):\n                    # Sub parameters can be a dict or list of dicts. Ensure parameters are always a list.\n                    if not isinstance(sub_parameters, list):\n                        sub_parameters = [sub_parameters]\n\n                    for sub_param in sub_parameters:\n                        # Validate dict fields in case they came in as strings\n\n                        if isinstance(sub_param, string_types):\n                            sub_param = check_type_dict(sub_param)\n\n                        if not isinstance(sub_param, Mapping):\n                            raise TypeError(\"Value '{1}' in the sub parameter field '{0}' must by a {2}, \"\n                                            \"not '{1.__class__.__name__}'\".format(arg_name, sub_param, wanted_type))\n\n                        no_log_values.update(list_no_log_values(sub_argument_spec, sub_param))\n\n    return no_log_values",
        "begin_line": 71,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.parameters.list_deprecations#121",
        "src_path": "lib/ansible/module_utils/common/parameters.py",
        "class_name": "lib.ansible.module_utils.common.parameters",
        "signature": "lib.ansible.module_utils.common.parameters.list_deprecations(argument_spec, params, prefix='')",
        "snippet": "def list_deprecations(argument_spec, params, prefix=''):\n    \"\"\"Return a list of deprecations\n\n    :arg argument_spec: An argument spec dictionary from a module\n    :arg params: Dictionary of all module parameters\n\n    :returns: List of dictionaries containing a message and version in which\n        the deprecated parameter will be removed, or an empty list::\n\n            [{'msg': \"Param 'deptest' is deprecated. See the module docs for more information\", 'version': '2.9'}]\n    \"\"\"\n\n    deprecations = []\n    for arg_name, arg_opts in argument_spec.items():\n        if arg_name in params:\n            if prefix:\n                sub_prefix = '%s[\"%s\"]' % (prefix, arg_name)\n            else:\n                sub_prefix = arg_name\n            if arg_opts.get('removed_in_version') is not None:\n                deprecations.append({\n                    'msg': \"Param '%s' is deprecated. See the module docs for more information\" % sub_prefix,\n                    'version': arg_opts.get('removed_in_version')\n                })\n            # Check sub-argument spec\n            sub_argument_spec = arg_opts.get('options')\n            if sub_argument_spec is not None:\n                sub_arguments = params[arg_name]\n                if isinstance(sub_arguments, Mapping):\n                    sub_arguments = [sub_arguments]\n                if isinstance(sub_arguments, list):\n                    for sub_params in sub_arguments:\n                        if isinstance(sub_params, Mapping):\n                            deprecations.extend(list_deprecations(sub_argument_spec, sub_params, prefix=sub_prefix))\n\n    return deprecations",
        "begin_line": 121,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.parameters.handle_aliases#159",
        "src_path": "lib/ansible/module_utils/common/parameters.py",
        "class_name": "lib.ansible.module_utils.common.parameters",
        "signature": "lib.ansible.module_utils.common.parameters.handle_aliases(argument_spec, params, alias_warnings=None)",
        "snippet": "def handle_aliases(argument_spec, params, alias_warnings=None):\n    \"\"\"Return a two item tuple. The first is a dictionary of aliases, the second is\n    a list of legal inputs.\n\n    If a list is provided to the alias_warnings parameter, it will be filled with tuples\n    (option, alias) in every case where both an option and its alias are specified.\n    \"\"\"\n\n    legal_inputs = ['_ansible_%s' % k for k in PASS_VARS]\n    aliases_results = {}  # alias:canon\n\n    for (k, v) in argument_spec.items():\n        legal_inputs.append(k)\n        aliases = v.get('aliases', None)\n        default = v.get('default', None)\n        required = v.get('required', False)\n        if default is not None and required:\n            # not alias specific but this is a good place to check this\n            raise ValueError(\"internal error: required and default are mutually exclusive for %s\" % k)\n        if aliases is None:\n            continue\n        if not is_iterable(aliases) or isinstance(aliases, (binary_type, text_type)):\n            raise TypeError('internal error: aliases must be a list or tuple')\n        for alias in aliases:\n            legal_inputs.append(alias)\n            aliases_results[alias] = k\n            if alias in params:\n                if k in params and alias_warnings is not None:\n                    alias_warnings.append((k, alias))\n                params[k] = params[alias]\n\n    return aliases_results, legal_inputs",
        "begin_line": 159,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.py3compat._TextEnviron.__getitem__#47",
        "src_path": "lib/ansible/utils/py3compat.py",
        "class_name": "lib.ansible.utils.py3compat._TextEnviron",
        "signature": "lib.ansible.utils.py3compat._TextEnviron.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        value = self._raw_environ[key]\n        if PY3:\n            return value\n        # Cache keys off of the undecoded values to handle any environment variables which change\n        # during a run\n        if value not in self._value_cache:\n            self._value_cache[value] = to_text(value, encoding=self.encoding,\n                                               nonstring='passthru', errors='surrogate_or_strict')\n        return self._value_cache[value]",
        "begin_line": 47,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004347826086956522,
            "pseudo_dstar_susp": 0.002777777777777778,
            "pseudo_tarantula_susp": 0.0027247956403269754,
            "pseudo_op2_susp": 0.002777777777777778,
            "pseudo_barinel_susp": 0.0027247956403269754
        }
    },
    {
        "name": "lib.ansible.config.manager.ensure_type#55",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager",
        "signature": "lib.ansible.config.manager.ensure_type(value, value_type, origin=None)",
        "snippet": "def ensure_type(value, value_type, origin=None):\n    ''' return a configuration variable with casting\n    :arg value: The value to ensure correct typing of\n    :kwarg value_type: The type of the value.  This can be any of the following strings:\n        :boolean: sets the value to a True or False value\n        :bool: Same as 'boolean'\n        :integer: Sets the value to an integer or raises a ValueType error\n        :int: Same as 'integer'\n        :float: Sets the value to a float or raises a ValueType error\n        :list: Treats the value as a comma separated list.  Split the value\n            and return it as a python list.\n        :none: Sets the value to None\n        :path: Expands any environment variables and tilde's in the value.\n        :tmppath: Create a unique temporary directory inside of the directory\n            specified by value and return its path.\n        :temppath: Same as 'tmppath'\n        :tmp: Same as 'tmppath'\n        :pathlist: Treat the value as a typical PATH string.  (On POSIX, this\n            means colon separated strings.)  Split the value and then expand\n            each part for environment variables and tildes.\n        :pathspec: Treat the value as a PATH string. Expands any environment variables\n            tildes's in the value.\n        :str: Sets the value to string types.\n        :string: Same as 'str'\n    '''\n\n    errmsg = ''\n    basedir = None\n    if origin and os.path.isabs(origin) and os.path.exists(to_bytes(origin)):\n        basedir = origin\n\n    if value_type:\n        value_type = value_type.lower()\n\n    if value is not None:\n        if value_type in ('boolean', 'bool'):\n            value = boolean(value, strict=False)\n\n        elif value_type in ('integer', 'int'):\n            value = int(value)\n\n        elif value_type == 'float':\n            value = float(value)\n\n        elif value_type == 'list':\n            if isinstance(value, string_types):\n                value = [x.strip() for x in value.split(',')]\n            elif not isinstance(value, Sequence):\n                errmsg = 'list'\n\n        elif value_type == 'none':\n            if value == \"None\":\n                value = None\n\n            if value is not None:\n                errmsg = 'None'\n\n        elif value_type == 'path':\n            if isinstance(value, string_types):\n                value = resolve_path(value, basedir=basedir)\n            else:\n                errmsg = 'path'\n\n        elif value_type in ('tmp', 'temppath', 'tmppath'):\n            if isinstance(value, string_types):\n                value = resolve_path(value, basedir=basedir)\n                if not os.path.exists(value):\n                    makedirs_safe(value, 0o700)\n                prefix = 'ansible-local-%s' % os.getpid()\n                value = tempfile.mkdtemp(prefix=prefix, dir=value)\n                atexit.register(cleanup_tmp_file, value, warn=True)\n            else:\n                errmsg = 'temppath'\n\n        elif value_type == 'pathspec':\n            if isinstance(value, string_types):\n                value = value.split(os.pathsep)\n\n            if isinstance(value, Sequence):\n                value = [resolve_path(x, basedir=basedir) for x in value]\n            else:\n                errmsg = 'pathspec'\n\n        elif value_type == 'pathlist':\n            if isinstance(value, string_types):\n                value = [x.strip() for x in value.split(',')]\n\n            if isinstance(value, Sequence):\n                value = [resolve_path(x, basedir=basedir) for x in value]\n            else:\n                errmsg = 'pathlist'\n\n        elif value_type in ('str', 'string'):\n            if isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n                value = unquote(to_text(value, errors='surrogate_or_strict'))\n            else:\n                errmsg = 'string'\n\n        # defaults to string type\n        elif isinstance(value, (string_types, AnsibleVaultEncryptedUnicode)):\n            value = unquote(to_text(value, errors='surrogate_or_strict'))\n\n        if errmsg:\n            raise ValueError('Invalid type provided for \"%s\": %s' % (errmsg, to_native(value)))\n\n    return to_text(value, errors='surrogate_or_strict', nonstring='passthru')",
        "begin_line": 55,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020408163265306124,
            "pseudo_dstar_susp": 0.004149377593360996,
            "pseudo_tarantula_susp": 0.0015360983102918587,
            "pseudo_op2_susp": 0.004149377593360996,
            "pseudo_barinel_susp": 0.0015360983102918587
        }
    },
    {
        "name": "lib.ansible.config.manager.resolve_path#164",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager",
        "signature": "lib.ansible.config.manager.resolve_path(path, basedir=None)",
        "snippet": "def resolve_path(path, basedir=None):\n    ''' resolve relative or 'variable' paths '''\n    if '{{CWD}}' in path:  # allow users to force CWD using 'magic' {{CWD}}\n        path = path.replace('{{CWD}}', os.getcwd())\n\n    return unfrackpath(path, follow=False, basedir=basedir)",
        "begin_line": 164,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.get_config_type#173",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager",
        "signature": "lib.ansible.config.manager.get_config_type(cfile)",
        "snippet": "def get_config_type(cfile):\n\n    ftype = None\n    if cfile is not None:\n        ext = os.path.splitext(cfile)[-1]\n        if ext in ('.ini', '.cfg'):\n            ftype = 'ini'\n        elif ext in ('.yaml', '.yml'):\n            ftype = 'yaml'\n        else:\n            raise AnsibleOptionsError(\"Unsupported configuration file extension for %s: %s\" % (cfile, to_native(ext)))\n\n    return ftype",
        "begin_line": 173,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022522522522522522,
            "pseudo_dstar_susp": 0.004347826086956522,
            "pseudo_tarantula_susp": 0.001652892561983471,
            "pseudo_op2_susp": 0.004347826086956522,
            "pseudo_barinel_susp": 0.001652892561983471
        }
    },
    {
        "name": "lib.ansible.config.manager.get_ini_config_value#189",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager",
        "signature": "lib.ansible.config.manager.get_ini_config_value(p, entry)",
        "snippet": "def get_ini_config_value(p, entry):\n    ''' returns the value of last ini entry found '''\n    value = None\n    if p is not None:\n        try:\n            value = p.get(entry.get('section', 'defaults'), entry.get('key', ''), raw=True)\n        except Exception:  # FIXME: actually report issues here\n            pass\n    return value",
        "begin_line": 189,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.find_ini_config_file#200",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager",
        "signature": "lib.ansible.config.manager.find_ini_config_file(warnings=None)",
        "snippet": "def find_ini_config_file(warnings=None):\n    ''' Load INI Config File order(first found is used): ENV, CWD, HOME, /etc/ansible '''\n    # FIXME: eventually deprecate ini configs\n\n    if warnings is None:\n        # Note: In this case, warnings does nothing\n        warnings = set()\n\n    # A value that can never be a valid path so that we can tell if ANSIBLE_CONFIG was set later\n    # We can't use None because we could set path to None.\n    SENTINEL = object\n\n    potential_paths = []\n\n    # Environment setting\n    path_from_env = os.getenv(\"ANSIBLE_CONFIG\", SENTINEL)\n    if path_from_env is not SENTINEL:\n        path_from_env = unfrackpath(path_from_env, follow=False)\n        if os.path.isdir(to_bytes(path_from_env)):\n            path_from_env = os.path.join(path_from_env, \"ansible.cfg\")\n        potential_paths.append(path_from_env)\n\n    # Current working directory\n    warn_cmd_public = False\n    try:\n        cwd = os.getcwd()\n        perms = os.stat(cwd)\n        cwd_cfg = os.path.join(cwd, \"ansible.cfg\")\n        if perms.st_mode & stat.S_IWOTH:\n            # Working directory is world writable so we'll skip it.\n            # Still have to look for a file here, though, so that we know if we have to warn\n            if os.path.exists(cwd_cfg):\n                warn_cmd_public = True\n        else:\n            potential_paths.append(cwd_cfg)\n    except OSError:\n        # If we can't access cwd, we'll simply skip it as a possible config source\n        pass\n\n    # Per user location\n    potential_paths.append(unfrackpath(\"~/.ansible.cfg\", follow=False))\n\n    # System location\n    potential_paths.append(\"/etc/ansible/ansible.cfg\")\n\n    for path in potential_paths:\n        b_path = to_bytes(path)\n        if os.path.exists(b_path) and os.access(b_path, os.R_OK):\n            break\n    else:\n        path = None\n\n    # Emit a warning if all the following are true:\n    # * We did not use a config from ANSIBLE_CONFIG\n    # * There's an ansible.cfg in the current working directory that we skipped\n    if path_from_env != path and warn_cmd_public:\n        warnings.add(u\"Ansible is being run in a world writable directory (%s),\"\n                     u\" ignoring it as an ansible.cfg source.\"\n                     u\" For more information see\"\n                     u\" https://docs.ansible.com/ansible/devel/reference_appendices/config.html#cfg-in-world-writable-dir\"\n                     % to_text(cwd))\n\n    return path",
        "begin_line": 200,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005235602094240838,
            "pseudo_dstar_susp": 0.0013123359580052493,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0013123359580052493,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.__init__#270",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.__init__(self, conf_file=None, defs_file=None)",
        "snippet": "    def __init__(self, conf_file=None, defs_file=None):\n\n        self._base_defs = {}\n        self._plugins = {}\n        self._parsers = {}\n\n        self._config_file = conf_file\n        self.data = ConfigData()\n\n        self._base_defs = self._read_config_yaml_file(defs_file or ('%s/base.yml' % os.path.dirname(__file__)))\n\n        if self._config_file is None:\n            # set config using ini\n            self._config_file = find_ini_config_file(self.WARNINGS)\n\n        # consume configuration\n        if self._config_file:\n            # initialize parser and read config\n            self._parse_config_file()\n\n        # update constants\n        self.update_config_data()\n        try:\n            self.update_module_defaults_groups()\n        except Exception as e:\n            # Since this is a 2.7 preview feature, we want to have it fail as gracefully as possible when there are issues.\n            sys.stderr.write('Could not load module_defaults_groups: %s: %s\\n\\n' % (type(e).__name__, e))\n            self.module_defaults_groups = {}",
        "begin_line": 270,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager._read_config_yaml_file#299",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager._read_config_yaml_file(self, yml_file)",
        "snippet": "    def _read_config_yaml_file(self, yml_file):\n        # TODO: handle relative paths as relative to the directory containing the current playbook instead of CWD\n        # Currently this is only used with absolute paths to the `ansible/config` directory\n        yml_file = to_bytes(yml_file)\n        if os.path.exists(yml_file):\n            with open(yml_file, 'rb') as config_def:\n                return yaml_load(config_def, Loader=SafeLoader) or {}\n        raise AnsibleError(\n            \"Missing base YAML definition file (bad install?): %s\" % to_native(yml_file))",
        "begin_line": 299,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager._parse_config_file#309",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager._parse_config_file(self, cfile=None)",
        "snippet": "    def _parse_config_file(self, cfile=None):\n        ''' return flat configuration settings from file(s) '''\n        # TODO: take list of files with merge/nomerge\n\n        if cfile is None:\n            cfile = self._config_file\n\n        ftype = get_config_type(cfile)\n        if cfile is not None:\n            if ftype == 'ini':\n                self._parsers[cfile] = configparser.ConfigParser()\n                with open(to_bytes(cfile), 'rb') as f:\n                    try:\n                        cfg_text = to_text(f.read(), errors='surrogate_or_strict')\n                    except UnicodeError as e:\n                        raise AnsibleOptionsError(\"Error reading config file(%s) because the config file was not utf8 encoded: %s\" % (cfile, to_native(e)))\n                try:\n                    if PY3:\n                        self._parsers[cfile].read_string(cfg_text)\n                    else:\n                        cfg_file = io.StringIO(cfg_text)\n                        self._parsers[cfile].readfp(cfg_file)\n                except configparser.Error as e:\n                    raise AnsibleOptionsError(\"Error reading config file (%s): %s\" % (cfile, to_native(e)))\n            # FIXME: this should eventually handle yaml config files\n            # elif ftype == 'yaml':\n            #     with open(cfile, 'rb') as config_stream:\n            #         self._parsers[cfile] = yaml.safe_load(config_stream)\n            else:\n                raise AnsibleOptionsError(\"Unsupported configuration file type: %s\" % to_native(ftype))",
        "begin_line": 309,
        "end_line": 338,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026455026455026454,
            "pseudo_dstar_susp": 0.005050505050505051,
            "pseudo_tarantula_susp": 0.0019267822736030828,
            "pseudo_op2_susp": 0.005050505050505051,
            "pseudo_barinel_susp": 0.0019267822736030828
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_plugin_options#344",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_plugin_options(self, plugin_type, name, keys=None, variables=None, direct=None)",
        "snippet": "    def get_plugin_options(self, plugin_type, name, keys=None, variables=None, direct=None):\n\n        options = {}\n        defs = self.get_configuration_definitions(plugin_type, name)\n        for option in defs:\n            options[option] = self.get_config_value(option, plugin_type=plugin_type, plugin_name=name, keys=keys, variables=variables, direct=direct)\n\n        return options",
        "begin_line": 344,
        "end_line": 351,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_plugin_vars#353",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_plugin_vars(self, plugin_type, name)",
        "snippet": "    def get_plugin_vars(self, plugin_type, name):\n\n        pvars = []\n        for pdef in self.get_configuration_definitions(plugin_type, name).values():\n            if 'vars' in pdef and pdef['vars']:\n                for var_entry in pdef['vars']:\n                    pvars.append(var_entry['name'])\n        return pvars",
        "begin_line": 353,
        "end_line": 360,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_configuration_definition#362",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_configuration_definition(self, name, plugin_type=None, plugin_name=None)",
        "snippet": "    def get_configuration_definition(self, name, plugin_type=None, plugin_name=None):\n\n        ret = {}\n        if plugin_type is None:\n            ret = self._base_defs.get(name, None)\n        elif plugin_name is None:\n            ret = self._plugins.get(plugin_type, {}).get(name, None)\n        else:\n            ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n\n        return ret",
        "begin_line": 362,
        "end_line": 372,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0045662100456621,
            "pseudo_dstar_susp": 0.02631578947368421,
            "pseudo_tarantula_susp": 0.002066115702479339,
            "pseudo_op2_susp": 0.02631578947368421,
            "pseudo_barinel_susp": 0.002066115702479339
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_configuration_definitions#374",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_configuration_definitions(self, plugin_type=None, name=None)",
        "snippet": "    def get_configuration_definitions(self, plugin_type=None, name=None):\n        ''' just list the possible settings, either base or for specific plugins or plugin '''\n\n        ret = {}\n        if plugin_type is None:\n            ret = self._base_defs\n        elif name is None:\n            ret = self._plugins.get(plugin_type, {})\n        else:\n            ret = self._plugins.get(plugin_type, {}).get(name, {})\n\n        return ret",
        "begin_line": 374,
        "end_line": 385,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003355704697986577,
            "pseudo_dstar_susp": 0.0051813471502590676,
            "pseudo_tarantula_susp": 0.0021551724137931034,
            "pseudo_op2_susp": 0.0051813471502590676,
            "pseudo_barinel_susp": 0.002150537634408602
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager._loop_entries#387",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager._loop_entries(self, container, entry_list)",
        "snippet": "    def _loop_entries(self, container, entry_list):\n        ''' repeat code for value entry assignment '''\n\n        value = None\n        origin = None\n        for entry in entry_list:\n            name = entry.get('name')\n            try:\n                temp_value = container.get(name, None)\n            except UnicodeEncodeError:\n                self.WARNINGS.add(u'value for config entry {0} contains invalid characters, ignoring...'.format(to_text(name)))\n                continue\n            if temp_value is not None:  # only set if entry is defined in container\n                # inline vault variables should be converted to a text string\n                if isinstance(temp_value, AnsibleVaultEncryptedUnicode):\n                    temp_value = to_text(temp_value, errors='surrogate_or_strict')\n\n                value = temp_value\n                origin = name\n\n                # deal with deprecation of setting source, if used\n                if 'deprecated' in entry:\n                    self.DEPRECATED.append((entry['name'], entry['deprecated']))\n\n        return value, origin",
        "begin_line": 387,
        "end_line": 411,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0026666666666666666,
            "pseudo_tarantula_susp": 0.0022123893805309734,
            "pseudo_op2_susp": 0.0026666666666666666,
            "pseudo_barinel_susp": 0.0022123893805309734
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_config_value#413",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_config_value(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None)",
        "snippet": "    def get_config_value(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None):\n        ''' wrapper '''\n\n        try:\n            value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n                                                            keys=keys, variables=variables, direct=direct)\n        except AnsibleError:\n            raise\n        except Exception as e:\n            raise AnsibleError(\"Unhandled exception when retrieving %s:\\n%s\" % (config, to_native(e)), orig_exc=e)\n        return value",
        "begin_line": 413,
        "end_line": 423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026041666666666665,
            "pseudo_dstar_susp": 0.004901960784313725,
            "pseudo_tarantula_susp": 0.0019047619047619048,
            "pseudo_op2_susp": 0.004901960784313725,
            "pseudo_barinel_susp": 0.0019047619047619048
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.get_config_value_and_origin#425",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.get_config_value_and_origin(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None)",
        "snippet": "    def get_config_value_and_origin(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None):\n        ''' Given a config key figure out the actual value and report on the origin of the settings '''\n        if cfile is None:\n            # use default config\n            cfile = self._config_file\n\n        # Note: sources that are lists listed in low to high precedence (last one wins)\n        value = None\n        origin = None\n\n        defs = self.get_configuration_definitions(plugin_type, plugin_name)\n        if config in defs:\n\n            # direct setting via plugin arguments, can set to None so we bypass rest of processing/defaults\n            direct_aliases = []\n            if direct:\n                direct_aliases = [direct[alias] for alias in defs[config].get('aliases', []) if alias in direct]\n            if direct and config in direct:\n                value = direct[config]\n                origin = 'Direct'\n            elif direct and direct_aliases:\n                value = direct_aliases[0]\n                origin = 'Direct'\n\n            else:\n                # Use 'variable overrides' if present, highest precedence, but only present when querying running play\n                if variables and defs[config].get('vars'):\n                    value, origin = self._loop_entries(variables, defs[config]['vars'])\n                    origin = 'var: %s' % origin\n\n                # use playbook keywords if you have em\n                if value is None and keys and config in keys:\n                    value, origin = keys[config], 'keyword'\n                    origin = 'keyword: %s' % origin\n\n                # env vars are next precedence\n                if value is None and defs[config].get('env'):\n                    value, origin = self._loop_entries(py3compat.environ, defs[config]['env'])\n                    origin = 'env: %s' % origin\n\n                # try config file entries next, if we have one\n                if self._parsers.get(cfile, None) is None:\n                    self._parse_config_file(cfile)\n\n                if value is None and cfile is not None:\n                    ftype = get_config_type(cfile)\n                    if ftype and defs[config].get(ftype):\n                        if ftype == 'ini':\n                            # load from ini config\n                            try:  # FIXME: generalize _loop_entries to allow for files also, most of this code is dupe\n                                for ini_entry in defs[config]['ini']:\n                                    temp_value = get_ini_config_value(self._parsers[cfile], ini_entry)\n                                    if temp_value is not None:\n                                        value = temp_value\n                                        origin = cfile\n                                        if 'deprecated' in ini_entry:\n                                            self.DEPRECATED.append(('[%s]%s' % (ini_entry['section'], ini_entry['key']), ini_entry['deprecated']))\n                            except Exception as e:\n                                sys.stderr.write(\"Error while loading ini config %s: %s\" % (cfile, to_native(e)))\n                        elif ftype == 'yaml':\n                            # FIXME: implement, also , break down key from defs (. notation???)\n                            origin = cfile\n\n                # set default if we got here w/o a value\n                if value is None:\n                    if defs[config].get('required', False):\n                        if not plugin_type or config not in INTERNAL_DEFS.get(plugin_type, {}):\n                            raise AnsibleError(\"No setting was provided for required configuration %s\" %\n                                               to_native(_get_entry(plugin_type, plugin_name, config)))\n                    else:\n                        value = defs[config].get('default')\n                        origin = 'default'\n                        # skip typing as this is a templated default that will be resolved later in constants, which has needed vars\n                        if plugin_type is None and isinstance(value, string_types) and (value.startswith('{{') and value.endswith('}}')):\n                            return value, origin\n\n            # ensure correct type, can raise exceptions on mismatched types\n            try:\n                value = ensure_type(value, defs[config].get('type'), origin=origin)\n            except ValueError as e:\n                if origin.startswith('env:') and value == '':\n                    # this is empty env var for non string so we can set to default\n                    origin = 'default'\n                    value = ensure_type(defs[config].get('default'), defs[config].get('type'), origin=origin)\n                else:\n                    raise AnsibleOptionsError('Invalid type for configuration option %s: %s' %\n                                              (to_native(_get_entry(plugin_type, plugin_name, config)), to_native(e)))\n\n            # deal with deprecation of the setting\n            if 'deprecated' in defs[config] and origin != 'default':\n                self.DEPRECATED.append((config, defs[config].get('deprecated')))\n        else:\n            raise AnsibleError('Requested entry (%s) was not defined in configuration.' % to_native(_get_entry(plugin_type, plugin_name, config)))\n\n        return value, origin",
        "begin_line": 425,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004347826086956522,
            "pseudo_dstar_susp": 0.005128205128205128,
            "pseudo_tarantula_susp": 0.0027247956403269754,
            "pseudo_op2_susp": 0.005128205128205128,
            "pseudo_barinel_susp": 0.0027247956403269754
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.initialize_plugin_configuration_definitions#521",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.initialize_plugin_configuration_definitions(self, plugin_type, name, defs)",
        "snippet": "    def initialize_plugin_configuration_definitions(self, plugin_type, name, defs):\n\n        if plugin_type not in self._plugins:\n            self._plugins[plugin_type] = {}\n\n        self._plugins[plugin_type][name] = defs",
        "begin_line": 521,
        "end_line": 526,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.update_module_defaults_groups#528",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.update_module_defaults_groups(self)",
        "snippet": "    def update_module_defaults_groups(self):\n        defaults_config = self._read_config_yaml_file(\n            '%s/module_defaults.yml' % os.path.join(os.path.dirname(__file__))\n        )\n        if defaults_config.get('version') not in ('1', '1.0', 1, 1.0):\n            raise AnsibleError('module_defaults.yml has an invalid version \"%s\" for configuration. Could be a bad install.' % defaults_config.get('version'))\n        self.module_defaults_groups = defaults_config.get('groupings', {})",
        "begin_line": 528,
        "end_line": 534,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.config.manager.ConfigManager.update_config_data#536",
        "src_path": "lib/ansible/config/manager.py",
        "class_name": "lib.ansible.config.manager.ConfigManager",
        "signature": "lib.ansible.config.manager.ConfigManager.update_config_data(self, defs=None, configfile=None)",
        "snippet": "    def update_config_data(self, defs=None, configfile=None):\n        ''' really: update constants '''\n\n        if defs is None:\n            defs = self._base_defs\n\n        if configfile is None:\n            configfile = self._config_file\n\n        if not isinstance(defs, dict):\n            raise AnsibleOptionsError(\"Invalid configuration definition type: %s for %s\" % (type(defs), defs))\n\n        # update the constant for config file\n        self.data.update_setting(Setting('CONFIG_FILE', configfile, '', 'string'))\n\n        origin = None\n        # env and config defs can have several entries, ordered in list from lowest to highest precedence\n        for config in defs:\n            if not isinstance(defs[config], dict):\n                raise AnsibleOptionsError(\"Invalid configuration definition '%s': type is %s\" % (to_native(config), type(defs[config])))\n\n            # get value and origin\n            try:\n                value, origin = self.get_config_value_and_origin(config, configfile)\n            except Exception as e:\n                # Printing the problem here because, in the current code:\n                # (1) we can't reach the error handler for AnsibleError before we\n                #     hit a different error due to lack of working config.\n                # (2) We don't have access to display yet because display depends on config\n                #     being properly loaded.\n                #\n                # If we start getting double errors printed from this section of code, then the\n                # above problem #1 has been fixed.  Revamp this to be more like the try: except\n                # in get_config_value() at that time.\n                sys.stderr.write(\"Unhandled error:\\n %s\\n\\n\" % traceback.format_exc())\n                raise AnsibleError(\"Invalid settings supplied for %s: %s\\n\" % (config, to_native(e)), orig_exc=e)\n\n            # set the constant\n            self.data.update_setting(Setting(config, value, origin, defs[config].get('type', 'string')))",
        "begin_line": 536,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.__init__#60",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.__init__(self, namespace, name, b_path, api, versions, requirement, force, parent=None, metadata=None, files=None, skip=False, allow_pre_releases=False)",
        "snippet": "    def __init__(self, namespace, name, b_path, api, versions, requirement, force, parent=None, metadata=None,\n                 files=None, skip=False, allow_pre_releases=False):\n        \"\"\"\n        Represents a collection requirement, the versions that are available to be installed as well as any\n        dependencies the collection has.\n\n        :param namespace: The collection namespace.\n        :param name: The collection name.\n        :param b_path: Byte str of the path to the collection tarball if it has already been downloaded.\n        :param api: The GalaxyAPI to use if the collection is from Galaxy.\n        :param versions: A list of versions of the collection that are available.\n        :param requirement: The version requirement string used to verify the list of versions fit the requirements.\n        :param force: Whether the force flag applied to the collection.\n        :param parent: The name of the parent the collection is a dependency of.\n        :param metadata: The galaxy.api.CollectionVersionMetadata that has already been retrieved from the Galaxy\n            server.\n        :param files: The files that exist inside the collection. This is based on the FILES.json file inside the\n            collection artifact.\n        :param skip: Whether to skip installing the collection. Should be set if the collection is already installed\n            and force is not set.\n        :param allow_pre_releases: Whether to skip pre-release versions of collections.\n        \"\"\"\n        self.namespace = namespace\n        self.name = name\n        self.b_path = b_path\n        self.api = api\n        self._versions = set(versions)\n        self.force = force\n        self.skip = skip\n        self.required_by = []\n        self.allow_pre_releases = allow_pre_releases\n\n        self._metadata = metadata\n        self._files = files\n\n        self.add_requirement(parent, requirement)",
        "begin_line": 60,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007097232079489,
            "pseudo_dstar_susp": 0.0007097232079489,
            "pseudo_tarantula_susp": 0.0007163323782234957,
            "pseudo_op2_susp": 0.0007097232079489,
            "pseudo_barinel_susp": 0.0007163323782234957
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.__str__#97",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.__str__(self)",
        "snippet": "    def __str__(self):\n        return to_native(\"%s.%s\" % (self.namespace, self.name))",
        "begin_line": 97,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008928571428571428,
            "pseudo_dstar_susp": 0.0008857395925597874,
            "pseudo_tarantula_susp": 0.000945179584120983,
            "pseudo_op2_susp": 0.0008857395925597874,
            "pseudo_barinel_susp": 0.000945179584120983
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.metadata#104",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.metadata(self)",
        "snippet": "    def metadata(self):\n        self._get_metadata()\n        return self._metadata",
        "begin_line": 104,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.versions#109",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.versions(self)",
        "snippet": "    def versions(self):\n        if self.allow_pre_releases:\n            return self._versions\n        return set(v for v in self._versions if v == '*' or not SemanticVersion(v).is_prerelease)",
        "begin_line": 109,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008110300081103001,
            "pseudo_dstar_susp": 0.001226993865030675,
            "pseudo_tarantula_susp": 0.0007283321194464676,
            "pseudo_op2_susp": 0.001226993865030675,
            "pseudo_barinel_susp": 0.0007283321194464676
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.versions#115",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.versions(self, value)",
        "snippet": "    def versions(self, value):\n        self._versions = set(value)",
        "begin_line": 115,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007194244604316547,
            "pseudo_dstar_susp": 0.0007194244604316547,
            "pseudo_tarantula_susp": 0.0007267441860465116,
            "pseudo_op2_susp": 0.0007194244604316547,
            "pseudo_barinel_susp": 0.0007267441860465116
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.latest_version#123",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.latest_version(self)",
        "snippet": "    def latest_version(self):\n        try:\n            return max([v for v in self.versions if v != '*'], key=SemanticVersion)\n        except ValueError:  # ValueError: max() arg is an empty sequence\n            return '*'",
        "begin_line": 123,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008952551477170994,
            "pseudo_dstar_susp": 0.0012345679012345679,
            "pseudo_tarantula_susp": 0.0007530120481927711,
            "pseudo_op2_susp": 0.0012345679012345679,
            "pseudo_barinel_susp": 0.0007530120481927711
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.dependencies#130",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.dependencies(self)",
        "snippet": "    def dependencies(self):\n        if not self._metadata:\n            if len(self.versions) > 1:\n                return {}\n            self._get_metadata()\n\n        dependencies = self._metadata.dependencies\n\n        if dependencies is None:\n            return {}\n\n        return dependencies",
        "begin_line": 130,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.add_requirement#143",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.add_requirement(self, parent, requirement)",
        "snippet": "    def add_requirement(self, parent, requirement):\n        self.required_by.append((parent, requirement))\n        new_versions = set(v for v in self.versions if self._meets_requirements(v, requirement, parent))\n        if len(new_versions) == 0:\n            if self.skip:\n                force_flag = '--force-with-deps' if parent else '--force'\n                version = self.latest_version if self.latest_version != '*' else 'unknown'\n                msg = \"Cannot meet requirement %s:%s as it is already installed at version '%s'. Use %s to overwrite\" \\\n                      % (to_text(self), requirement, version, force_flag)\n                raise AnsibleError(msg)\n            elif parent is None:\n                msg = \"Cannot meet requirement %s for dependency %s\" % (requirement, to_text(self))\n            else:\n                msg = \"Cannot meet dependency requirement '%s:%s' for collection %s\" \\\n                      % (to_text(self), requirement, parent)\n\n            collection_source = to_text(self.b_path, nonstring='passthru') or self.api.api_server\n            req_by = \"\\n\".join(\n                \"\\t%s - '%s:%s'\" % (to_text(p) if p else 'base', to_text(self), r)\n                for p, r in self.required_by\n            )\n\n            versions = \", \".join(sorted(self.versions, key=SemanticVersion))\n            if not self.versions and self.pre_releases:\n                pre_release_msg = (\n                    '\\nThis collection only contains pre-releases. Utilize `--pre` to install pre-releases, or '\n                    'explicitly provide the pre-release version.'\n                )\n            else:\n                pre_release_msg = ''\n\n            raise AnsibleError(\n                \"%s from source '%s'. Available versions before last requirement added: %s\\nRequirements from:\\n%s%s\"\n                % (msg, collection_source, versions, req_by, pre_release_msg)\n            )\n\n        self.versions = new_versions",
        "begin_line": 143,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007880220646178094,
            "pseudo_dstar_susp": 0.0012254901960784314,
            "pseudo_tarantula_susp": 0.0007267441860465116,
            "pseudo_op2_susp": 0.0012254901960784314,
            "pseudo_barinel_susp": 0.0007267441860465116
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.download#181",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.download(self, b_path)",
        "snippet": "    def download(self, b_path):\n        download_url = self._metadata.download_url\n        artifact_hash = self._metadata.artifact_sha256\n        headers = {}\n        self.api._add_auth_token(headers, download_url, required=False)\n\n        b_collection_path = _download_file(download_url, b_path, artifact_hash, self.api.validate_certs,\n                                           headers=headers)\n\n        return to_text(b_collection_path, errors='surrogate_or_strict')",
        "begin_line": 181,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.install#192",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.install(self, path, b_temp_path)",
        "snippet": "    def install(self, path, b_temp_path):\n        if self.skip:\n            display.display(\"Skipping '%s' as it is already installed\" % to_text(self))\n            return\n\n        # Install if it is not\n        collection_path = os.path.join(path, self.namespace, self.name)\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n        display.display(\"Installing '%s:%s' to '%s'\" % (to_text(self), self.latest_version, collection_path))\n\n        if self.b_path is None:\n            self.b_path = self.download(b_temp_path)\n\n        if os.path.exists(b_collection_path):\n            shutil.rmtree(b_collection_path)\n        os.makedirs(b_collection_path)\n\n        try:\n            with tarfile.open(self.b_path, mode='r') as collection_tar:\n                files_member_obj = collection_tar.getmember('FILES.json')\n                with _tarfile_extract(collection_tar, files_member_obj) as files_obj:\n                    files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n\n                _extract_tar_file(collection_tar, 'MANIFEST.json', b_collection_path, b_temp_path)\n                _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n\n                for file_info in files['files']:\n                    file_name = file_info['name']\n                    if file_name == '.':\n                        continue\n\n                    if file_info['ftype'] == 'file':\n                        _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                          expected_hash=file_info['chksum_sha256'])\n                    else:\n                        os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n        except Exception:\n            # Ensure we don't leave the dir behind in case of a failure.\n            shutil.rmtree(b_collection_path)\n\n            b_namespace_path = os.path.dirname(b_collection_path)\n            if not os.listdir(b_namespace_path):\n                os.rmdir(b_namespace_path)\n\n            raise",
        "begin_line": 192,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.set_latest_version#238",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.set_latest_version(self)",
        "snippet": "    def set_latest_version(self):\n        self.versions = set([self.latest_version])\n        self._get_metadata()",
        "begin_line": 238,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.verify#242",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.verify(self, remote_collection, path, b_temp_tar_path)",
        "snippet": "    def verify(self, remote_collection, path, b_temp_tar_path):\n        if not self.skip:\n            display.display(\"'%s' has not been installed, nothing to verify\" % (to_text(self)))\n            return\n\n        collection_path = os.path.join(path, self.namespace, self.name)\n        b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n\n        display.vvv(\"Verifying '%s:%s'.\" % (to_text(self), self.latest_version))\n        display.vvv(\"Installed collection found at '%s'\" % collection_path)\n        display.vvv(\"Remote collection found at '%s'\" % remote_collection.metadata.download_url)\n\n        # Compare installed version versus requirement version\n        if self.latest_version != remote_collection.latest_version:\n            err = \"%s has the version '%s' but is being compared to '%s'\" % (to_text(self), self.latest_version, remote_collection.latest_version)\n            display.display(err)\n            return\n\n        modified_content = []\n\n        # Verify the manifest hash matches before verifying the file manifest\n        expected_hash = _get_tar_file_hash(b_temp_tar_path, 'MANIFEST.json')\n        self._verify_file_hash(b_collection_path, 'MANIFEST.json', expected_hash, modified_content)\n        manifest = _get_json_from_tar_file(b_temp_tar_path, 'MANIFEST.json')\n\n        # Use the manifest to verify the file manifest checksum\n        file_manifest_data = manifest['file_manifest_file']\n        file_manifest_filename = file_manifest_data['name']\n        expected_hash = file_manifest_data['chksum_%s' % file_manifest_data['chksum_type']]\n\n        # Verify the file manifest before using it to verify individual files\n        self._verify_file_hash(b_collection_path, file_manifest_filename, expected_hash, modified_content)\n        file_manifest = _get_json_from_tar_file(b_temp_tar_path, file_manifest_filename)\n\n        # Use the file manifest to verify individual file checksums\n        for manifest_data in file_manifest['files']:\n            if manifest_data['ftype'] == 'file':\n                expected_hash = manifest_data['chksum_%s' % manifest_data['chksum_type']]\n                self._verify_file_hash(b_collection_path, manifest_data['name'], expected_hash, modified_content)\n\n        if modified_content:\n            display.display(\"Collection %s contains modified content in the following files:\" % to_text(self))\n            display.display(to_text(self))\n            display.vvv(to_text(self.b_path))\n            for content_change in modified_content:\n                display.display('    %s' % content_change.filename)\n                display.vvv(\"    Expected: %s\\n    Found: %s\" % (content_change.expected, content_change.installed))\n        else:\n            display.vvv(\"Successfully verified that checksums for '%s:%s' match the remote collection\" % (to_text(self), self.latest_version))",
        "begin_line": 242,
        "end_line": 290,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement._verify_file_hash#292",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement._verify_file_hash(self, b_path, filename, expected_hash, error_queue)",
        "snippet": "    def _verify_file_hash(self, b_path, filename, expected_hash, error_queue):\n        b_file_path = to_bytes(os.path.join(to_text(b_path), filename), errors='surrogate_or_strict')\n\n        if not os.path.isfile(b_file_path):\n            actual_hash = None\n        else:\n            with open(b_file_path, mode='rb') as file_object:\n                actual_hash = _consume_file(file_object)\n\n        if expected_hash != actual_hash:\n            error_queue.append(ModifiedContent(filename=filename, expected=expected_hash, installed=actual_hash))",
        "begin_line": 292,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement._get_metadata#304",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement._get_metadata(self)",
        "snippet": "    def _get_metadata(self):\n        if self._metadata:\n            return\n        self._metadata = self.api.get_collection_version_metadata(self.namespace, self.name, self.latest_version)",
        "begin_line": 304,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement._meets_requirements#309",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement._meets_requirements(self, version, requirements, parent)",
        "snippet": "    def _meets_requirements(self, version, requirements, parent):\n        \"\"\"\n        Supports version identifiers can be '==', '!=', '>', '>=', '<', '<=', '*'. Each requirement is delimited by ','\n        \"\"\"\n        op_map = {\n            '!=': operator.ne,\n            '==': operator.eq,\n            '=': operator.eq,\n            '>=': operator.ge,\n            '>': operator.gt,\n            '<=': operator.le,\n            '<': operator.lt,\n        }\n\n        for req in list(requirements.split(',')):\n            op_pos = 2 if len(req) > 1 and req[1] == '=' else 1\n            op = op_map.get(req[:op_pos])\n\n            requirement = req[op_pos:]\n            if not op:\n                requirement = req\n                op = operator.eq\n\n            # In the case we are checking a new requirement on a base requirement (parent != None) we can't accept\n            # version as '*' (unknown version) unless the requirement is also '*'.\n            if parent and version == '*' and requirement != '*':\n                display.warning(\"Failed to validate the collection requirement '%s:%s' for %s when the existing \"\n                                \"install does not have a version set, the collection may not work.\"\n                                % (to_text(self), req, parent))\n                continue\n            elif requirement == '*' or version == '*':\n                continue\n\n            if not op(SemanticVersion(version), SemanticVersion.from_loose_version(LooseVersion(requirement))):\n                break\n        else:\n            return True\n\n        # The loop was broken early, it does not meet all the requirements\n        return False",
        "begin_line": 309,
        "end_line": 348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007380073800738007,
            "pseudo_dstar_susp": 0.0007374631268436578,
            "pseudo_tarantula_susp": 0.0007507507507507507,
            "pseudo_op2_susp": 0.0007374631268436578,
            "pseudo_barinel_susp": 0.0007507507507507507
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.from_tar#351",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.from_tar(b_path, force, parent=None)",
        "snippet": "    def from_tar(b_path, force, parent=None):\n        if not tarfile.is_tarfile(b_path):\n            raise AnsibleError(\"Collection artifact at '%s' is not a valid tar file.\" % to_native(b_path))\n\n        info = {}\n        with tarfile.open(b_path, mode='r') as collection_tar:\n            for b_member_name, property_name in CollectionRequirement._FILE_MAPPING:\n                n_member_name = to_native(b_member_name)\n                try:\n                    member = collection_tar.getmember(n_member_name)\n                except KeyError:\n                    raise AnsibleError(\"Collection at '%s' does not contain the required file %s.\"\n                                       % (to_native(b_path), n_member_name))\n\n                with _tarfile_extract(collection_tar, member) as member_obj:\n                    try:\n                        info[property_name] = json.loads(to_text(member_obj.read(), errors='surrogate_or_strict'))\n                    except ValueError:\n                        raise AnsibleError(\"Collection tar file member %s does not contain a valid json string.\"\n                                           % n_member_name)\n\n        meta = info['manifest_file']['collection_info']\n        files = info['files_file']['files']\n\n        namespace = meta['namespace']\n        name = meta['name']\n        version = meta['version']\n        meta = CollectionVersionMetadata(namespace, name, version, None, None, meta['dependencies'])\n\n        if SemanticVersion(version).is_prerelease:\n            allow_pre_release = True\n        else:\n            allow_pre_release = False\n\n        return CollectionRequirement(namespace, name, b_path, None, [version], version, force, parent=parent,\n                                     metadata=meta, files=files, allow_pre_releases=allow_pre_release)",
        "begin_line": 351,
        "end_line": 386,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011086474501108647,
            "pseudo_dstar_susp": 0.0010427528675703858,
            "pseudo_tarantula_susp": 0.002512562814070352,
            "pseudo_op2_susp": 0.0010427528675703858,
            "pseudo_barinel_susp": 0.002512562814070352
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.from_path#389",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.from_path(b_path, force, parent=None)",
        "snippet": "    def from_path(b_path, force, parent=None):\n        info = {}\n        for b_file_name, property_name in CollectionRequirement._FILE_MAPPING:\n            b_file_path = os.path.join(b_path, b_file_name)\n            if not os.path.exists(b_file_path):\n                continue\n\n            with open(b_file_path, 'rb') as file_obj:\n                try:\n                    info[property_name] = json.loads(to_text(file_obj.read(), errors='surrogate_or_strict'))\n                except ValueError:\n                    raise AnsibleError(\"Collection file at '%s' does not contain a valid json string.\"\n                                       % to_native(b_file_path))\n\n        allow_pre_release = False\n        if 'manifest_file' in info:\n            manifest = info['manifest_file']['collection_info']\n            namespace = manifest['namespace']\n            name = manifest['name']\n            version = to_text(manifest['version'], errors='surrogate_or_strict')\n\n            try:\n                _v = SemanticVersion()\n                _v.parse(version)\n                if _v.is_prerelease:\n                    allow_pre_release = True\n            except ValueError:\n                display.warning(\"Collection at '%s' does not have a valid version set, falling back to '*'. Found \"\n                                \"version: '%s'\" % (to_text(b_path), version))\n                version = '*'\n\n            dependencies = manifest['dependencies']\n        else:\n            display.warning(\"Collection at '%s' does not have a MANIFEST.json file, cannot detect version.\"\n                            % to_text(b_path))\n            parent_dir, name = os.path.split(to_text(b_path, errors='surrogate_or_strict'))\n            namespace = os.path.split(parent_dir)[1]\n\n            version = '*'\n            dependencies = {}\n\n        meta = CollectionVersionMetadata(namespace, name, version, None, None, dependencies)\n\n        files = info.get('files_file', {}).get('files', {})\n\n        return CollectionRequirement(namespace, name, b_path, None, [version], version, force, parent=parent,\n                                     metadata=meta, files=files, skip=True, allow_pre_releases=allow_pre_release)",
        "begin_line": 389,
        "end_line": 435,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.CollectionRequirement.from_name#438",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection.CollectionRequirement",
        "signature": "lib.ansible.galaxy.collection.CollectionRequirement.from_name(collection, apis, requirement, force, parent=None, allow_pre_release=False)",
        "snippet": "    def from_name(collection, apis, requirement, force, parent=None, allow_pre_release=False):\n        namespace, name = collection.split('.', 1)\n        galaxy_meta = None\n\n        for api in apis:\n            try:\n                if not (requirement == '*' or requirement.startswith('<') or requirement.startswith('>') or\n                        requirement.startswith('!=')):\n                    # Exact requirement\n                    allow_pre_release = True\n\n                    if requirement.startswith('='):\n                        requirement = requirement.lstrip('=')\n\n                    resp = api.get_collection_version_metadata(namespace, name, requirement)\n\n                    galaxy_meta = resp\n                    versions = [resp.version]\n                else:\n                    versions = api.get_collection_versions(namespace, name)\n            except GalaxyError as err:\n                if err.http_code == 404:\n                    display.vvv(\"Collection '%s' is not available from server %s %s\"\n                                % (collection, api.name, api.api_server))\n                    continue\n                raise\n\n            display.vvv(\"Collection '%s' obtained from server %s %s\" % (collection, api.name, api.api_server))\n            break\n        else:\n            raise AnsibleError(\"Failed to find collection %s:%s\" % (collection, requirement))\n\n        req = CollectionRequirement(namespace, name, None, api, versions, requirement, force, parent=parent,\n                                    metadata=galaxy_meta, allow_pre_releases=allow_pre_release)\n        return req",
        "begin_line": 438,
        "end_line": 472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.build_collection#475",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.build_collection(collection_path, output_path, force)",
        "snippet": "def build_collection(collection_path, output_path, force):\n    \"\"\"\n    Creates the Ansible collection artifact in a .tar.gz file.\n\n    :param collection_path: The path to the collection to build. This should be the directory that contains the\n        galaxy.yml file.\n    :param output_path: The path to create the collection build artifact. This should be a directory.\n    :param force: Whether to overwrite an existing collection build artifact or fail.\n    :return: The path to the collection build artifact.\n    \"\"\"\n    b_collection_path = to_bytes(collection_path, errors='surrogate_or_strict')\n    b_galaxy_path = os.path.join(b_collection_path, b'galaxy.yml')\n    if not os.path.exists(b_galaxy_path):\n        raise AnsibleError(\"The collection galaxy.yml path '%s' does not exist.\" % to_native(b_galaxy_path))\n\n    collection_meta = _get_galaxy_yml(b_galaxy_path)\n    file_manifest = _build_files_manifest(b_collection_path, collection_meta['namespace'], collection_meta['name'],\n                                          collection_meta['build_ignore'])\n    collection_manifest = _build_manifest(**collection_meta)\n\n    collection_output = os.path.join(output_path, \"%s-%s-%s.tar.gz\" % (collection_meta['namespace'],\n                                                                       collection_meta['name'],\n                                                                       collection_meta['version']))\n\n    b_collection_output = to_bytes(collection_output, errors='surrogate_or_strict')\n    if os.path.exists(b_collection_output):\n        if os.path.isdir(b_collection_output):\n            raise AnsibleError(\"The output collection artifact '%s' already exists, \"\n                               \"but is a directory - aborting\" % to_native(collection_output))\n        elif not force:\n            raise AnsibleError(\"The file '%s' already exists. You can use --force to re-create \"\n                               \"the collection artifact.\" % to_native(collection_output))\n\n    _build_collection_tar(b_collection_path, b_collection_output, collection_manifest, file_manifest)",
        "begin_line": 475,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009433962264150943,
            "pseudo_dstar_susp": 0.0009319664492078285,
            "pseudo_tarantula_susp": 0.0013227513227513227,
            "pseudo_op2_susp": 0.0009319664492078285,
            "pseudo_barinel_susp": 0.0013227513227513227
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.publish_collection#548",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.publish_collection(collection_path, api, wait, timeout)",
        "snippet": "def publish_collection(collection_path, api, wait, timeout):\n    \"\"\"\n    Publish an Ansible collection tarball into an Ansible Galaxy server.\n\n    :param collection_path: The path to the collection tarball to publish.\n    :param api: A GalaxyAPI to publish the collection to.\n    :param wait: Whether to wait until the import process is complete.\n    :param timeout: The time in seconds to wait for the import process to finish, 0 is indefinite.\n    \"\"\"\n    import_uri = api.publish_collection(collection_path)\n\n    if wait:\n        # Galaxy returns a url fragment which differs between v2 and v3.  The second to last entry is\n        # always the task_id, though.\n        # v2: {\"task\": \"https://galaxy-dev.ansible.com/api/v2/collection-imports/35573/\"}\n        # v3: {\"task\": \"/api/automation-hub/v3/imports/collections/838d1308-a8f4-402c-95cb-7823f3806cd8/\"}\n        task_id = None\n        for path_segment in reversed(import_uri.split('/')):\n            if path_segment:\n                task_id = path_segment\n                break\n\n        if not task_id:\n            raise AnsibleError(\"Publishing the collection did not return valid task info. Cannot wait for task status. Returned task info: '%s'\" % import_uri)\n\n        display.display(\"Collection has been published to the Galaxy server %s %s\" % (api.name, api.api_server))\n        with _display_progress():\n            api.wait_import_task(task_id, timeout)\n        display.display(\"Collection has been successfully published and imported to the Galaxy server %s %s\"\n                        % (api.name, api.api_server))\n    else:\n        display.display(\"Collection has been pushed to the Galaxy server %s %s, not waiting until import has \"\n                        \"completed due to --no-wait being set. Import task results can be found at %s\"\n                        % (api.name, api.api_server, import_uri))",
        "begin_line": 548,
        "end_line": 581,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.install_collections#584",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.install_collections(collections, output_path, apis, validate_certs, ignore_errors, no_deps, force, force_deps, allow_pre_release=False)",
        "snippet": "def install_collections(collections, output_path, apis, validate_certs, ignore_errors, no_deps, force, force_deps,\n                        allow_pre_release=False):\n    \"\"\"\n    Install Ansible collections to the path specified.\n\n    :param collections: The collections to install, should be a list of tuples with (name, requirement, Galaxy server).\n    :param output_path: The path to install the collections to.\n    :param apis: A list of GalaxyAPIs to query when searching for a collection.\n    :param validate_certs: Whether to validate the certificates if downloading a tarball.\n    :param ignore_errors: Whether to ignore any errors when installing the collection.\n    :param no_deps: Ignore any collection dependencies and only install the base requirements.\n    :param force: Re-install a collection if it has already been installed.\n    :param force_deps: Re-install a collection as well as its dependencies if they have already been installed.\n    \"\"\"\n    existing_collections = find_existing_collections(output_path)\n\n    with _tempdir() as b_temp_path:\n        display.display(\"Process install dependency map\")\n        with _display_progress():\n            dependency_map = _build_dependency_map(collections, existing_collections, b_temp_path, apis,\n                                                   validate_certs, force, force_deps, no_deps,\n                                                   allow_pre_release=allow_pre_release)\n\n        display.display(\"Starting collection install process\")\n        with _display_progress():\n            for collection in dependency_map.values():\n                try:\n                    collection.install(output_path, b_temp_path)\n                except AnsibleError as err:\n                    if ignore_errors:\n                        display.warning(\"Failed to install collection %s but skipping due to --ignore-errors being set. \"\n                                        \"Error: %s\" % (to_text(collection), to_text(err)))\n                    else:\n                        raise",
        "begin_line": 584,
        "end_line": 617,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.validate_collection_name#620",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.validate_collection_name(name)",
        "snippet": "def validate_collection_name(name):\n    \"\"\"\n    Validates the collection name as an input from the user or a requirements file fit the requirements.\n\n    :param name: The input name with optional range specifier split by ':'.\n    :return: The input value, required for argparse validation.\n    \"\"\"\n    collection, dummy, dummy = name.partition(':')\n    if AnsibleCollectionRef.is_valid_collection_name(collection):\n        return name\n\n    raise AnsibleError(\"Invalid collection name '%s', \"\n                       \"name must be in the format <namespace>.<collection>. \\n\"\n                       \"Please make sure namespace and collection name contains \"\n                       \"characters from [a-zA-Z0-9_] only.\" % name)",
        "begin_line": 620,
        "end_line": 634,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007451564828614009,
            "pseudo_dstar_susp": 0.0007446016381236039,
            "pseudo_tarantula_susp": 0.0007722007722007722,
            "pseudo_op2_susp": 0.0007446016381236039,
            "pseudo_barinel_susp": 0.0007722007722007722
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.validate_collection_path#637",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.validate_collection_path(collection_path)",
        "snippet": "def validate_collection_path(collection_path):\n    \"\"\" Ensure a given path ends with 'ansible_collections'\n\n    :param collection_path: The path that should end in 'ansible_collections'\n    :return: collection_path ending in 'ansible_collections' if it does not already.\n    \"\"\"\n\n    if os.path.split(collection_path)[1] != 'ansible_collections':\n        return os.path.join(collection_path, 'ansible_collections')\n\n    return collection_path",
        "begin_line": 637,
        "end_line": 647,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00211864406779661,
            "pseudo_dstar_susp": 0.0014005602240896359,
            "pseudo_tarantula_susp": 0.0035842293906810036,
            "pseudo_op2_susp": 0.0014005602240896359,
            "pseudo_barinel_susp": 0.0035842293906810036
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.verify_collections#650",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.verify_collections(collections, search_paths, apis, validate_certs, ignore_errors, allow_pre_release=False)",
        "snippet": "def verify_collections(collections, search_paths, apis, validate_certs, ignore_errors, allow_pre_release=False):\n\n    with _display_progress():\n        with _tempdir() as b_temp_path:\n            for collection in collections:\n                try:\n\n                    local_collection = None\n                    b_collection = to_bytes(collection[0], errors='surrogate_or_strict')\n\n                    if os.path.isfile(b_collection) or urlparse(collection[0]).scheme.lower() in ['http', 'https'] or len(collection[0].split('.')) != 2:\n                        raise AnsibleError(message=\"'%s' is not a valid collection name. The format namespace.name is expected.\" % collection[0])\n\n                    collection_name = collection[0]\n                    namespace, name = collection_name.split('.')\n                    collection_version = collection[1]\n\n                    # Verify local collection exists before downloading it from a galaxy server\n                    for search_path in search_paths:\n                        b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')\n                        if os.path.isdir(b_search_path):\n                            local_collection = CollectionRequirement.from_path(b_search_path, False)\n                            break\n                    if local_collection is None:\n                        raise AnsibleError(message='Collection %s is not installed in any of the collection paths.' % collection_name)\n\n                    # Download collection on a galaxy server for comparison\n                    try:\n                        remote_collection = CollectionRequirement.from_name(collection_name, apis, collection_version, False, parent=None,\n                                                                            allow_pre_release=allow_pre_release)\n                    except AnsibleError as e:\n                        if e.message == 'Failed to find collection %s:%s' % (collection[0], collection[1]):\n                            raise AnsibleError('Failed to find remote collection %s:%s on any of the galaxy servers' % (collection[0], collection[1]))\n                        raise\n\n                    download_url = remote_collection.metadata.download_url\n                    headers = {}\n                    remote_collection.api._add_auth_token(headers, download_url, required=False)\n                    b_temp_tar_path = _download_file(download_url, b_temp_path, None, validate_certs, headers=headers)\n\n                    local_collection.verify(remote_collection, search_path, b_temp_tar_path)\n\n                except AnsibleError as err:\n                    if ignore_errors:\n                        display.warning(\"Failed to verify collection %s but skipping due to --ignore-errors being set. \"\n                                        \"Error: %s\" % (collection[0], to_text(err)))\n                    else:\n                        raise",
        "begin_line": 650,
        "end_line": 697,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._tempdir#701",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._tempdir()",
        "snippet": "def _tempdir():\n    b_temp_path = tempfile.mkdtemp(dir=to_bytes(C.DEFAULT_LOCAL_TMP, errors='surrogate_or_strict'))\n    yield b_temp_path\n    shutil.rmtree(b_temp_path)",
        "begin_line": 701,
        "end_line": 704,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008833922261484099,
            "pseudo_dstar_susp": 0.0008764241893076249,
            "pseudo_tarantula_susp": 0.0009337068160597573,
            "pseudo_op2_susp": 0.0008764241893076249,
            "pseudo_barinel_susp": 0.0009337068160597573
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._tarfile_extract#708",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._tarfile_extract(tar, member)",
        "snippet": "def _tarfile_extract(tar, member):\n    tar_obj = tar.extractfile(member)\n    yield tar_obj\n    tar_obj.close()",
        "begin_line": 708,
        "end_line": 711,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001002004008016032,
            "pseudo_dstar_susp": 0.000980392156862745,
            "pseudo_tarantula_susp": 0.0015220700152207,
            "pseudo_op2_susp": 0.000980392156862745,
            "pseudo_barinel_susp": 0.0015220700152207
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._display_progress#715",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._display_progress()",
        "snippet": "def _display_progress():\n    config_display = C.GALAXY_DISPLAY_PROGRESS\n    display_wheel = sys.stdout.isatty() if config_display is None else config_display\n\n    if not display_wheel:\n        yield\n        return\n\n    def progress(display_queue, actual_display):\n        actual_display.debug(\"Starting display_progress display thread\")\n        t = threading.current_thread()\n\n        while True:\n            for c in \"|/-\\\\\":\n                actual_display.display(c + \"\\b\", newline=False)\n                time.sleep(0.1)\n\n                # Display a message from the main thread\n                while True:\n                    try:\n                        method, args, kwargs = display_queue.get(block=False, timeout=0.1)\n                    except queue.Empty:\n                        break\n                    else:\n                        func = getattr(actual_display, method)\n                        func(*args, **kwargs)\n\n                if getattr(t, \"finish\", False):\n                    actual_display.debug(\"Received end signal for display_progress display thread\")\n                    return\n\n    class DisplayThread(object):\n\n        def __init__(self, display_queue):\n            self.display_queue = display_queue\n\n        def __getattr__(self, attr):\n            def call_display(*args, **kwargs):\n                self.display_queue.put((attr, args, kwargs))\n\n            return call_display\n\n    # Temporary override the global display class with our own which add the calls to a queue for the thread to call.\n    global display\n    old_display = display\n    try:\n        display_queue = queue.Queue()\n        display = DisplayThread(display_queue)\n        t = threading.Thread(target=progress, args=(display_queue, old_display))\n        t.daemon = True\n        t.start()\n\n        try:\n            yield\n        finally:\n            t.finish = True\n            t.join()\n    except Exception:\n        # The exception is re-raised so we can sure the thread is finished and not using the display anymore\n        raise\n    finally:\n        display = old_display",
        "begin_line": 715,
        "end_line": 776,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._get_galaxy_yml#779",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._get_galaxy_yml(b_galaxy_yml_path)",
        "snippet": "def _get_galaxy_yml(b_galaxy_yml_path):\n    meta_info = get_collections_galaxy_meta_info()\n\n    mandatory_keys = set()\n    string_keys = set()\n    list_keys = set()\n    dict_keys = set()\n\n    for info in meta_info:\n        if info.get('required', False):\n            mandatory_keys.add(info['key'])\n\n        key_list_type = {\n            'str': string_keys,\n            'list': list_keys,\n            'dict': dict_keys,\n        }[info.get('type', 'str')]\n        key_list_type.add(info['key'])\n\n    all_keys = frozenset(list(mandatory_keys) + list(string_keys) + list(list_keys) + list(dict_keys))\n\n    try:\n        with open(b_galaxy_yml_path, 'rb') as g_yaml:\n            galaxy_yml = yaml.safe_load(g_yaml)\n    except YAMLError as err:\n        raise AnsibleError(\"Failed to parse the galaxy.yml at '%s' with the following error:\\n%s\"\n                           % (to_native(b_galaxy_yml_path), to_native(err)))\n\n    set_keys = set(galaxy_yml.keys())\n    missing_keys = mandatory_keys.difference(set_keys)\n    if missing_keys:\n        raise AnsibleError(\"The collection galaxy.yml at '%s' is missing the following mandatory keys: %s\"\n                           % (to_native(b_galaxy_yml_path), \", \".join(sorted(missing_keys))))\n\n    extra_keys = set_keys.difference(all_keys)\n    if len(extra_keys) > 0:\n        display.warning(\"Found unknown keys in collection galaxy.yml at '%s': %s\"\n                        % (to_text(b_galaxy_yml_path), \", \".join(extra_keys)))\n\n    # Add the defaults if they have not been set\n    for optional_string in string_keys:\n        if optional_string not in galaxy_yml:\n            galaxy_yml[optional_string] = None\n\n    for optional_list in list_keys:\n        list_val = galaxy_yml.get(optional_list, None)\n\n        if list_val is None:\n            galaxy_yml[optional_list] = []\n        elif not isinstance(list_val, list):\n            galaxy_yml[optional_list] = [list_val]\n\n    for optional_dict in dict_keys:\n        if optional_dict not in galaxy_yml:\n            galaxy_yml[optional_dict] = {}\n\n    # license is a builtin var in Python, to avoid confusion we just rename it to license_ids\n    galaxy_yml['license_ids'] = galaxy_yml['license']\n    del galaxy_yml['license']\n\n    return galaxy_yml",
        "begin_line": 779,
        "end_line": 839,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008952551477170994,
            "pseudo_dstar_susp": 0.0008873114463176575,
            "pseudo_tarantula_susp": 0.0009514747859181732,
            "pseudo_op2_susp": 0.0008873114463176575,
            "pseudo_barinel_susp": 0.0009514747859181732
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._build_files_manifest#842",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._build_files_manifest(b_collection_path, namespace, name, ignore_patterns)",
        "snippet": "def _build_files_manifest(b_collection_path, namespace, name, ignore_patterns):\n    # We always ignore .pyc and .retry files as well as some well known version control directories. The ignore\n    # patterns can be extended by the build_ignore key in galaxy.yml\n    b_ignore_patterns = [\n        b'galaxy.yml',\n        b'.git',\n        b'*.pyc',\n        b'*.retry',\n        b'tests/output',  # Ignore ansible-test result output directory.\n        to_bytes('{0}-{1}-*.tar.gz'.format(namespace, name)),  # Ignores previously built artifacts in the root dir.\n    ]\n    b_ignore_patterns += [to_bytes(p) for p in ignore_patterns]\n    b_ignore_dirs = frozenset([b'CVS', b'.bzr', b'.hg', b'.git', b'.svn', b'__pycache__', b'.tox'])\n\n    entry_template = {\n        'name': None,\n        'ftype': None,\n        'chksum_type': None,\n        'chksum_sha256': None,\n        'format': MANIFEST_FORMAT\n    }\n    manifest = {\n        'files': [\n            {\n                'name': '.',\n                'ftype': 'dir',\n                'chksum_type': None,\n                'chksum_sha256': None,\n                'format': MANIFEST_FORMAT,\n            },\n        ],\n        'format': MANIFEST_FORMAT,\n    }\n\n    def _walk(b_path, b_top_level_dir):\n        for b_item in os.listdir(b_path):\n            b_abs_path = os.path.join(b_path, b_item)\n            b_rel_base_dir = b'' if b_path == b_top_level_dir else b_path[len(b_top_level_dir) + 1:]\n            b_rel_path = os.path.join(b_rel_base_dir, b_item)\n            rel_path = to_text(b_rel_path, errors='surrogate_or_strict')\n\n            if os.path.isdir(b_abs_path):\n                if any(b_item == b_path for b_path in b_ignore_dirs) or \\\n                        any(fnmatch.fnmatch(b_rel_path, b_pattern) for b_pattern in b_ignore_patterns):\n                    display.vvv(\"Skipping '%s' for collection build\" % to_text(b_abs_path))\n                    continue\n\n                if os.path.islink(b_abs_path):\n                    b_link_target = os.path.realpath(b_abs_path)\n\n                    if not b_link_target.startswith(b_top_level_dir):\n                        display.warning(\"Skipping '%s' as it is a symbolic link to a directory outside the collection\"\n                                        % to_text(b_abs_path))\n                        continue\n\n                manifest_entry = entry_template.copy()\n                manifest_entry['name'] = rel_path\n                manifest_entry['ftype'] = 'dir'\n\n                manifest['files'].append(manifest_entry)\n\n                _walk(b_abs_path, b_top_level_dir)\n            else:\n                if any(fnmatch.fnmatch(b_rel_path, b_pattern) for b_pattern in b_ignore_patterns):\n                    display.vvv(\"Skipping '%s' for collection build\" % to_text(b_abs_path))\n                    continue\n\n                manifest_entry = entry_template.copy()\n                manifest_entry['name'] = rel_path\n                manifest_entry['ftype'] = 'file'\n                manifest_entry['chksum_type'] = 'sha256'\n                manifest_entry['chksum_sha256'] = secure_hash(b_abs_path, hash_func=sha256)\n\n                manifest['files'].append(manifest_entry)\n\n    _walk(b_collection_path, b_collection_path)\n\n    return manifest",
        "begin_line": 842,
        "end_line": 919,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001002004008016032,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.00089126559714795,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.00089126559714795
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._walk#876",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._walk(b_path, b_top_level_dir)",
        "snippet": "    def _walk(b_path, b_top_level_dir):\n        for b_item in os.listdir(b_path):\n            b_abs_path = os.path.join(b_path, b_item)\n            b_rel_base_dir = b'' if b_path == b_top_level_dir else b_path[len(b_top_level_dir) + 1:]\n            b_rel_path = os.path.join(b_rel_base_dir, b_item)\n            rel_path = to_text(b_rel_path, errors='surrogate_or_strict')\n\n            if os.path.isdir(b_abs_path):\n                if any(b_item == b_path for b_path in b_ignore_dirs) or \\\n                        any(fnmatch.fnmatch(b_rel_path, b_pattern) for b_pattern in b_ignore_patterns):\n                    display.vvv(\"Skipping '%s' for collection build\" % to_text(b_abs_path))\n                    continue\n\n                if os.path.islink(b_abs_path):\n                    b_link_target = os.path.realpath(b_abs_path)\n\n                    if not b_link_target.startswith(b_top_level_dir):\n                        display.warning(\"Skipping '%s' as it is a symbolic link to a directory outside the collection\"\n                                        % to_text(b_abs_path))\n                        continue\n\n                manifest_entry = entry_template.copy()\n                manifest_entry['name'] = rel_path\n                manifest_entry['ftype'] = 'dir'\n\n                manifest['files'].append(manifest_entry)\n\n                _walk(b_abs_path, b_top_level_dir)\n            else:\n                if any(fnmatch.fnmatch(b_rel_path, b_pattern) for b_pattern in b_ignore_patterns):\n                    display.vvv(\"Skipping '%s' for collection build\" % to_text(b_abs_path))\n                    continue\n\n                manifest_entry = entry_template.copy()\n                manifest_entry['name'] = rel_path\n                manifest_entry['ftype'] = 'file'\n                manifest_entry['chksum_type'] = 'sha256'\n                manifest_entry['chksum_sha256'] = secure_hash(b_abs_path, hash_func=sha256)\n\n                manifest['files'].append(manifest_entry)",
        "begin_line": 876,
        "end_line": 915,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001002004008016032,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.00089126559714795,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.00089126559714795
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._build_manifest#922",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._build_manifest(namespace, name, version, authors, readme, tags, description, license_ids, license_file, dependencies, repository, documentation, homepage, issues, **kwargs)",
        "snippet": "def _build_manifest(namespace, name, version, authors, readme, tags, description, license_ids, license_file,\n                    dependencies, repository, documentation, homepage, issues, **kwargs):\n\n    manifest = {\n        'collection_info': {\n            'namespace': namespace,\n            'name': name,\n            'version': version,\n            'authors': authors,\n            'readme': readme,\n            'tags': tags,\n            'description': description,\n            'license': license_ids,\n            'license_file': license_file if license_file else None,  # Handle galaxy.yml having an empty string (None)\n            'dependencies': dependencies,\n            'repository': repository,\n            'documentation': documentation,\n            'homepage': homepage,\n            'issues': issues,\n        },\n        'file_manifest_file': {\n            'name': 'FILES.json',\n            'ftype': 'file',\n            'chksum_type': 'sha256',\n            'chksum_sha256': None,  # Filled out in _build_collection_tar\n            'format': MANIFEST_FORMAT\n        },\n        'format': MANIFEST_FORMAT,\n    }\n\n    return manifest",
        "begin_line": 922,
        "end_line": 952,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009132420091324201,
            "pseudo_dstar_susp": 0.0009049773755656109,
            "pseudo_tarantula_susp": 0.000984251968503937,
            "pseudo_op2_susp": 0.0009049773755656109,
            "pseudo_barinel_susp": 0.000984251968503937
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._build_collection_tar#955",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._build_collection_tar(b_collection_path, b_tar_path, collection_manifest, file_manifest)",
        "snippet": "def _build_collection_tar(b_collection_path, b_tar_path, collection_manifest, file_manifest):\n    files_manifest_json = to_bytes(json.dumps(file_manifest, indent=True), errors='surrogate_or_strict')\n    collection_manifest['file_manifest_file']['chksum_sha256'] = secure_hash_s(files_manifest_json, hash_func=sha256)\n    collection_manifest_json = to_bytes(json.dumps(collection_manifest, indent=True), errors='surrogate_or_strict')\n\n    with _tempdir() as b_temp_path:\n        b_tar_filepath = os.path.join(b_temp_path, os.path.basename(b_tar_path))\n\n        with tarfile.open(b_tar_filepath, mode='w:gz') as tar_file:\n            # Add the MANIFEST.json and FILES.json file to the archive\n            for name, b in [('MANIFEST.json', collection_manifest_json), ('FILES.json', files_manifest_json)]:\n                b_io = BytesIO(b)\n                tar_info = tarfile.TarInfo(name)\n                tar_info.size = len(b)\n                tar_info.mtime = time.time()\n                tar_info.mode = 0o0644\n                tar_file.addfile(tarinfo=tar_info, fileobj=b_io)\n\n            for file_info in file_manifest['files']:\n                if file_info['name'] == '.':\n                    continue\n\n                # arcname expects a native string, cannot be bytes\n                filename = to_native(file_info['name'], errors='surrogate_or_strict')\n                b_src_path = os.path.join(b_collection_path, to_bytes(filename, errors='surrogate_or_strict'))\n\n                def reset_stat(tarinfo):\n                    existing_is_exec = tarinfo.mode & stat.S_IXUSR\n                    tarinfo.mode = 0o0755 if existing_is_exec or tarinfo.isdir() else 0o0644\n                    tarinfo.uid = tarinfo.gid = 0\n                    tarinfo.uname = tarinfo.gname = ''\n                    return tarinfo\n\n                tar_file.add(os.path.realpath(b_src_path), arcname=filename, recursive=False, filter=reset_stat)\n\n        shutil.copy(b_tar_filepath, b_tar_path)\n        collection_name = \"%s.%s\" % (collection_manifest['collection_info']['namespace'],\n                                     collection_manifest['collection_info']['name'])\n        display.display('Created collection for %s at %s' % (collection_name, to_text(b_tar_path)))",
        "begin_line": 955,
        "end_line": 993,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009433962264150943,
            "pseudo_dstar_susp": 0.0009319664492078285,
            "pseudo_tarantula_susp": 0.0013227513227513227,
            "pseudo_op2_susp": 0.0009319664492078285,
            "pseudo_barinel_susp": 0.0013227513227513227
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.reset_stat#981",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.reset_stat(tarinfo)",
        "snippet": "                def reset_stat(tarinfo):\n                    existing_is_exec = tarinfo.mode & stat.S_IXUSR\n                    tarinfo.mode = 0o0755 if existing_is_exec or tarinfo.isdir() else 0o0644\n                    tarinfo.uid = tarinfo.gid = 0\n                    tarinfo.uname = tarinfo.gname = ''\n                    return tarinfo",
        "begin_line": 981,
        "end_line": 986,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010427528675703858,
            "pseudo_dstar_susp": 0.001256281407035176,
            "pseudo_tarantula_susp": 0.0013227513227513227,
            "pseudo_op2_susp": 0.001256281407035176,
            "pseudo_barinel_susp": 0.0013227513227513227
        }
    },
    {
        "name": "lib.ansible.galaxy.collection.find_existing_collections#996",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection.find_existing_collections(path)",
        "snippet": "def find_existing_collections(path):\n    collections = []\n\n    b_path = to_bytes(path, errors='surrogate_or_strict')\n    for b_namespace in os.listdir(b_path):\n        b_namespace_path = os.path.join(b_path, b_namespace)\n        if os.path.isfile(b_namespace_path):\n            continue\n\n        for b_collection in os.listdir(b_namespace_path):\n            b_collection_path = os.path.join(b_namespace_path, b_collection)\n            if os.path.isdir(b_collection_path):\n                req = CollectionRequirement.from_path(b_collection_path, False)\n                display.vvv(\"Found installed collection %s:%s at '%s'\" % (to_text(req), req.latest_version,\n                                                                          to_text(b_collection_path)))\n                collections.append(req)\n\n    return collections",
        "begin_line": 996,
        "end_line": 1013,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._build_dependency_map#1016",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._build_dependency_map(collections, existing_collections, b_temp_path, apis, validate_certs, force, force_deps, no_deps, allow_pre_release=False)",
        "snippet": "def _build_dependency_map(collections, existing_collections, b_temp_path, apis, validate_certs, force, force_deps,\n                          no_deps, allow_pre_release=False):\n    dependency_map = {}\n\n    # First build the dependency map on the actual requirements\n    for name, version, source in collections:\n        _get_collection_info(dependency_map, existing_collections, name, version, source, b_temp_path, apis,\n                             validate_certs, (force or force_deps), allow_pre_release=allow_pre_release)\n\n    checked_parents = set([to_text(c) for c in dependency_map.values() if c.skip])\n    while len(dependency_map) != len(checked_parents):\n        while not no_deps:  # Only parse dependencies if no_deps was not set\n            parents_to_check = set(dependency_map.keys()).difference(checked_parents)\n\n            deps_exhausted = True\n            for parent in parents_to_check:\n                parent_info = dependency_map[parent]\n\n                if parent_info.dependencies:\n                    deps_exhausted = False\n                    for dep_name, dep_requirement in parent_info.dependencies.items():\n                        _get_collection_info(dependency_map, existing_collections, dep_name, dep_requirement,\n                                             parent_info.api, b_temp_path, apis, validate_certs, force_deps,\n                                             parent=parent, allow_pre_release=allow_pre_release)\n\n                    checked_parents.add(parent)\n\n            # No extra dependencies were resolved, exit loop\n            if deps_exhausted:\n                break\n\n        # Now we have resolved the deps to our best extent, now select the latest version for collections with\n        # multiple versions found and go from there\n        deps_not_checked = set(dependency_map.keys()).difference(checked_parents)\n        for collection in deps_not_checked:\n            dependency_map[collection].set_latest_version()\n            if no_deps or len(dependency_map[collection].dependencies) == 0:\n                checked_parents.add(collection)\n\n    return dependency_map",
        "begin_line": 1016,
        "end_line": 1055,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._get_collection_info#1058",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis, validate_certs, force, parent=None, allow_pre_release=False)",
        "snippet": "def _get_collection_info(dep_map, existing_collections, collection, requirement, source, b_temp_path, apis,\n                         validate_certs, force, parent=None, allow_pre_release=False):\n    dep_msg = \"\"\n    if parent:\n        dep_msg = \" - as dependency of %s\" % parent\n    display.vvv(\"Processing requirement collection '%s'%s\" % (to_text(collection), dep_msg))\n\n    b_tar_path = None\n    if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):\n        display.vvvv(\"Collection requirement '%s' is a tar artifact\" % to_text(collection))\n        b_tar_path = to_bytes(collection, errors='surrogate_or_strict')\n    elif urlparse(collection).scheme.lower() in ['http', 'https']:\n        display.vvvv(\"Collection requirement '%s' is a URL to a tar artifact\" % collection)\n        try:\n            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)\n        except urllib_error.URLError as err:\n            raise AnsibleError(\"Failed to download collection tar from '%s': %s\"\n                               % (to_native(collection), to_native(err)))\n\n    if b_tar_path:\n        req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)\n\n        collection_name = to_text(req)\n        if collection_name in dep_map:\n            collection_info = dep_map[collection_name]\n            collection_info.add_requirement(None, req.latest_version)\n        else:\n            collection_info = req\n    else:\n        validate_collection_name(collection)\n\n        display.vvvv(\"Collection requirement '%s' is the name of a collection\" % collection)\n        if collection in dep_map:\n            collection_info = dep_map[collection]\n            collection_info.add_requirement(parent, requirement)\n        else:\n            apis = [source] if source else apis\n            collection_info = CollectionRequirement.from_name(collection, apis, requirement, force, parent=parent,\n                                                              allow_pre_release=allow_pre_release)\n\n    existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]\n    if existing and not collection_info.force:\n        # Test that the installed collection fits the requirement\n        existing[0].add_requirement(parent, requirement)\n        collection_info = existing[0]\n\n    dep_map[to_text(collection_info)] = collection_info",
        "begin_line": 1058,
        "end_line": 1104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._download_file#1107",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._download_file(url, b_path, expected_hash, validate_certs, headers=None)",
        "snippet": "def _download_file(url, b_path, expected_hash, validate_certs, headers=None):\n    urlsplit = os.path.splitext(to_text(url.rsplit('/', 1)[1]))\n    b_file_name = to_bytes(urlsplit[0], errors='surrogate_or_strict')\n    b_file_ext = to_bytes(urlsplit[1], errors='surrogate_or_strict')\n    b_file_path = tempfile.NamedTemporaryFile(dir=b_path, prefix=b_file_name, suffix=b_file_ext, delete=False).name\n\n    display.vvv(\"Downloading %s to %s\" % (url, to_text(b_path)))\n    # Galaxy redirs downloads to S3 which reject the request if an Authorization header is attached so don't redir that\n    resp = open_url(to_native(url, errors='surrogate_or_strict'), validate_certs=validate_certs, headers=headers,\n                    unredirected_headers=['Authorization'], http_agent=user_agent())\n\n    with open(b_file_path, 'wb') as download_file:\n        actual_hash = _consume_file(resp, download_file)\n\n    if expected_hash:\n        display.vvvv(\"Validating downloaded file hash %s with expected hash %s\" % (actual_hash, expected_hash))\n        if expected_hash != actual_hash:\n            raise AnsibleError(\"Mismatch artifact hash with downloaded file\")\n\n    return b_file_path",
        "begin_line": 1107,
        "end_line": 1126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._extract_tar_file#1129",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None)",
        "snippet": "def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n    with _get_tar_file_member(tar, filename) as tar_obj:\n        with tempfile.NamedTemporaryFile(dir=b_temp_path, delete=False) as tmpfile_obj:\n            actual_hash = _consume_file(tar_obj, tmpfile_obj)\n\n        if expected_hash and actual_hash != expected_hash:\n            raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                               % (to_native(filename, errors='surrogate_or_strict'), to_native(tar.name)))\n\n        b_dest_filepath = os.path.abspath(os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict')))\n        b_parent_dir = os.path.dirname(b_dest_filepath)\n        if b_parent_dir != b_dest and not b_parent_dir.startswith(b_dest + to_bytes(os.path.sep)):\n            raise AnsibleError(\"Cannot extract tar entry '%s' as it will be placed outside the collection directory\"\n                               % to_native(filename, errors='surrogate_or_strict'))\n\n        if not os.path.exists(b_parent_dir):\n            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n            # makes sure we create the parent directory even if it wasn't set in the metadata.\n            os.makedirs(b_parent_dir, mode=0o0755)\n\n        shutil.move(to_bytes(tmpfile_obj.name, errors='surrogate_or_strict'), b_dest_filepath)\n\n        # Default to rw-r--r-- and only add execute if the tar file has execute.\n        tar_member = tar.getmember(to_native(filename, errors='surrogate_or_strict'))\n        new_mode = 0o644\n        if stat.S_IMODE(tar_member.mode) & stat.S_IXUSR:\n            new_mode |= 0o0111\n\n        os.chmod(b_dest_filepath, new_mode)",
        "begin_line": 1129,
        "end_line": 1157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011560693641618498,
            "pseudo_dstar_susp": 0.0010741138560687433,
            "pseudo_tarantula_susp": 0.0029154518950437317,
            "pseudo_op2_susp": 0.0010741138560687433,
            "pseudo_barinel_susp": 0.0029154518950437317
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._get_tar_file_member#1160",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._get_tar_file_member(tar, filename)",
        "snippet": "def _get_tar_file_member(tar, filename):\n    n_filename = to_native(filename, errors='surrogate_or_strict')\n    try:\n        member = tar.getmember(n_filename)\n    except KeyError:\n        raise AnsibleError(\"Collection tar at '%s' does not contain the expected file '%s'.\" % (\n            to_native(tar.name),\n            n_filename))\n\n    return _tarfile_extract(tar, member)",
        "begin_line": 1160,
        "end_line": 1169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010204081632653062,
            "pseudo_dstar_susp": 0.0009861932938856016,
            "pseudo_tarantula_susp": 0.0016233766233766235,
            "pseudo_op2_susp": 0.0009861932938856016,
            "pseudo_barinel_susp": 0.0016233766233766235
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._get_json_from_tar_file#1172",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._get_json_from_tar_file(b_path, filename)",
        "snippet": "def _get_json_from_tar_file(b_path, filename):\n    file_contents = ''\n\n    with tarfile.open(b_path, mode='r') as collection_tar:\n        with _get_tar_file_member(collection_tar, filename) as tar_obj:\n            bufsize = 65536\n            data = tar_obj.read(bufsize)\n            while data:\n                file_contents += to_text(data)\n                data = tar_obj.read(bufsize)\n\n    return json.loads(file_contents)",
        "begin_line": 1172,
        "end_line": 1183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._get_tar_file_hash#1186",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._get_tar_file_hash(b_path, filename)",
        "snippet": "def _get_tar_file_hash(b_path, filename):\n    with tarfile.open(b_path, mode='r') as collection_tar:\n        with _get_tar_file_member(collection_tar, filename) as tar_obj:\n            return _consume_file(tar_obj)",
        "begin_line": 1186,
        "end_line": 1189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.collection._consume_file#1192",
        "src_path": "lib/ansible/galaxy/collection.py",
        "class_name": "lib.ansible.galaxy.collection",
        "signature": "lib.ansible.galaxy.collection._consume_file(read_from, write_to=None)",
        "snippet": "def _consume_file(read_from, write_to=None):\n    bufsize = 65536\n    sha256_digest = sha256()\n    data = read_from.read(bufsize)\n    while data:\n        if write_to is not None:\n            write_to.write(data)\n            write_to.flush()\n        sha256_digest.update(data)\n        data = read_from.read(bufsize)\n\n    return sha256_digest.hexdigest()",
        "begin_line": 1192,
        "end_line": 1203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010204081632653062,
            "pseudo_dstar_susp": 0.0009861932938856016,
            "pseudo_tarantula_susp": 0.0016233766233766235,
            "pseudo_op2_susp": 0.0009861932938856016,
            "pseudo_barinel_susp": 0.0016233766233766235
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser.__init__#105",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser.__init__(self, task_ds=None, collection_list=None)",
        "snippet": "    def __init__(self, task_ds=None, collection_list=None):\n        task_ds = {} if task_ds is None else task_ds\n\n        if not isinstance(task_ds, dict):\n            raise AnsibleAssertionError(\"the type of 'task_ds' should be a dict, but is a %s\" % type(task_ds))\n        self._task_ds = task_ds\n        self._collection_list = collection_list\n        # delayed local imports to prevent circular import\n        from ansible.playbook.task import Task\n        from ansible.playbook.handler import Handler\n        # store the valid Task/Handler attrs for quick access\n        self._task_attrs = set(Task._valid_attrs.keys())\n        self._task_attrs.update(set(Handler._valid_attrs.keys()))\n        # HACK: why are these not FieldAttributes on task with a post-validate to check usage?\n        self._task_attrs.update(['local_action', 'static'])\n        self._task_attrs = frozenset(self._task_attrs)",
        "begin_line": 105,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.01655907942745e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser._split_module_string#122",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser._split_module_string(self, module_string)",
        "snippet": "    def _split_module_string(self, module_string):\n        '''\n        when module names are expressed like:\n        action: copy src=a dest=b\n        the first part of the string is the name of the module\n        and the rest are strings pertaining to the arguments.\n        '''\n\n        tokens = split_args(module_string)\n        if len(tokens) > 1:\n            return (tokens[0], \" \".join(tokens[1:]))\n        else:\n            return (tokens[0], \"\")",
        "begin_line": 122,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_parameters#136",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_parameters(self, thing, action=None, additional_args=None)",
        "snippet": "    def _normalize_parameters(self, thing, action=None, additional_args=None):\n        '''\n        arguments can be fuzzy.  Deal with all the forms.\n        '''\n\n        additional_args = {} if additional_args is None else additional_args\n\n        # final args are the ones we'll eventually return, so first update\n        # them with any additional args specified, which have lower priority\n        # than those which may be parsed/normalized next\n        final_args = dict()\n        if additional_args:\n            if isinstance(additional_args, string_types):\n                templar = Templar(loader=None)\n                if templar.is_template(additional_args):\n                    final_args['_variable_params'] = additional_args\n                else:\n                    raise AnsibleParserError(\"Complex args containing variables cannot use bare variables (without Jinja2 delimiters), \"\n                                             \"and must use the full variable style ('{{var_name}}')\")\n            elif isinstance(additional_args, dict):\n                final_args.update(additional_args)\n            else:\n                raise AnsibleParserError('Complex args must be a dictionary or variable string (\"{{var}}\").')\n\n        # how we normalize depends if we figured out what the module name is\n        # yet.  If we have already figured it out, it's a 'new style' invocation.\n        # otherwise, it's not\n\n        if action is not None:\n            args = self._normalize_new_style_args(thing, action)\n        else:\n            (action, args) = self._normalize_old_style_args(thing)\n\n            # this can occasionally happen, simplify\n            if args and 'args' in args:\n                tmp_args = args.pop('args')\n                if isinstance(tmp_args, string_types):\n                    tmp_args = parse_kv(tmp_args)\n                args.update(tmp_args)\n\n        # only internal variables can start with an underscore, so\n        # we don't allow users to set them directly in arguments\n        if args and action not in FREEFORM_ACTIONS:\n            for arg in args:\n                arg = to_text(arg)\n                if arg.startswith('_ansible_'):\n                    raise AnsibleError(\"invalid parameter specified for action '%s': '%s'\" % (action, arg))\n\n        # finally, update the args we're going to return with the ones\n        # which were normalized above\n        if args:\n            final_args.update(args)\n\n        return (action, final_args)",
        "begin_line": 136,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_new_style_args#191",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_new_style_args(self, thing, action)",
        "snippet": "    def _normalize_new_style_args(self, thing, action):\n        '''\n        deals with fuzziness in new style module invocations\n        accepting key=value pairs and dictionaries, and returns\n        a dictionary of arguments\n\n        possible example inputs:\n            'echo hi', 'shell'\n            {'region': 'xyz'}, 'ec2'\n        standardized outputs like:\n            { _raw_params: 'echo hi', _uses_shell: True }\n        '''\n\n        if isinstance(thing, dict):\n            # form is like: { xyz: { x: 2, y: 3 } }\n            args = thing\n        elif isinstance(thing, string_types):\n            # form is like: copy: src=a dest=b\n            check_raw = action in FREEFORM_ACTIONS\n            args = parse_kv(thing, check_raw=check_raw)\n        elif thing is None:\n            # this can happen with modules which take no params, like ping:\n            args = None\n        else:\n            raise AnsibleParserError(\"unexpected parameter type in action: %s\" % type(thing), obj=self._task_ds)\n        return args",
        "begin_line": 191,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_old_style_args#218",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser._normalize_old_style_args(self, thing)",
        "snippet": "    def _normalize_old_style_args(self, thing):\n        '''\n        deals with fuzziness in old-style (action/local_action) module invocations\n        returns tuple of (module_name, dictionary_args)\n\n        possible example inputs:\n           { 'shell' : 'echo hi' }\n           'shell echo hi'\n           {'module': 'ec2', 'x': 1 }\n        standardized outputs like:\n           ('ec2', { 'x': 1} )\n        '''\n\n        action = None\n        args = None\n\n        if isinstance(thing, dict):\n            # form is like:  action: { module: 'copy', src: 'a', dest: 'b' }\n            thing = thing.copy()\n            if 'module' in thing:\n                action, module_args = self._split_module_string(thing['module'])\n                args = thing.copy()\n                check_raw = action in FREEFORM_ACTIONS\n                args.update(parse_kv(module_args, check_raw=check_raw))\n                del args['module']\n\n        elif isinstance(thing, string_types):\n            # form is like:  action: copy src=a dest=b\n            (action, args) = self._split_module_string(thing)\n            check_raw = action in FREEFORM_ACTIONS\n            args = parse_kv(args, check_raw=check_raw)\n\n        else:\n            # need a dict or a string, so giving up\n            raise AnsibleParserError(\"unexpected parameter type in action: %s\" % type(thing), obj=self._task_ds)\n\n        return (action, args)",
        "begin_line": 218,
        "end_line": 254,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.mod_args.ModuleArgsParser.parse#256",
        "src_path": "lib/ansible/parsing/mod_args.py",
        "class_name": "lib.ansible.parsing.mod_args.ModuleArgsParser",
        "signature": "lib.ansible.parsing.mod_args.ModuleArgsParser.parse(self, skip_action_validation=False)",
        "snippet": "    def parse(self, skip_action_validation=False):\n        '''\n        Given a task in one of the supported forms, parses and returns\n        returns the action, arguments, and delegate_to values for the\n        task, dealing with all sorts of levels of fuzziness.\n        '''\n\n        thing = None\n\n        action = None\n        delegate_to = self._task_ds.get('delegate_to', Sentinel)\n        args = dict()\n\n        # This is the standard YAML form for command-type modules. We grab\n        # the args and pass them in as additional arguments, which can/will\n        # be overwritten via dict updates from the other arg sources below\n        additional_args = self._task_ds.get('args', dict())\n\n        # We can have one of action, local_action, or module specified\n        # action\n        if 'action' in self._task_ds:\n            # an old school 'action' statement\n            thing = self._task_ds['action']\n            action, args = self._normalize_parameters(thing, action=action, additional_args=additional_args)\n\n        # local_action\n        if 'local_action' in self._task_ds:\n            # local_action is similar but also implies a delegate_to\n            if action is not None:\n                raise AnsibleParserError(\"action and local_action are mutually exclusive\", obj=self._task_ds)\n            thing = self._task_ds.get('local_action', '')\n            delegate_to = 'localhost'\n            action, args = self._normalize_parameters(thing, action=action, additional_args=additional_args)\n\n        # module: <stuff> is the more new-style invocation\n\n        # filter out task attributes so we're only querying unrecognized keys as actions/modules\n        non_task_ds = dict((k, v) for k, v in iteritems(self._task_ds) if (k not in self._task_attrs) and (not k.startswith('with_')))\n\n        # walk the filtered input dictionary to see if we recognize a module name\n        for item, value in iteritems(non_task_ds):\n            if item in BUILTIN_TASKS or skip_action_validation or action_loader.has_plugin(item, collection_list=self._collection_list) or \\\n                    module_loader.has_plugin(item, collection_list=self._collection_list):\n                # finding more than one module name is a problem\n                if action is not None:\n                    raise AnsibleParserError(\"conflicting action statements: %s, %s\" % (action, item), obj=self._task_ds)\n                action = item\n                thing = value\n                action, args = self._normalize_parameters(thing, action=action, additional_args=additional_args)\n\n        # if we didn't see any module in the task at all, it's not a task really\n        if action is None:\n            if non_task_ds:  # there was one non-task action, but we couldn't find it\n                bad_action = list(non_task_ds.keys())[0]\n                raise AnsibleParserError(\"couldn't resolve module/action '{0}'. This often indicates a \"\n                                         \"misspelling, missing collection, or incorrect module path.\".format(bad_action),\n                                         obj=self._task_ds)\n            else:\n                raise AnsibleParserError(\"no module/action detected in task.\",\n                                         obj=self._task_ds)\n        elif args.get('_raw_params', '') != '' and action not in RAW_PARAM_MODULES:\n            templar = Templar(loader=None)\n            raw_params = args.pop('_raw_params')\n            if templar.is_template(raw_params):\n                args['_variable_params'] = raw_params\n            else:\n                raise AnsibleParserError(\"this task '%s' has extra params, which is only allowed in the following modules: %s\" % (action,\n                                                                                                                                  \", \".join(RAW_PARAM_MODULES)),\n                                         obj=self._task_ds)\n\n        return (action, args, delegate_to)",
        "begin_line": 256,
        "end_line": 326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.packaging.language.pip._is_package_name#300",
        "src_path": "lib/ansible/modules/packaging/language/pip.py",
        "class_name": "lib.ansible.modules.packaging.language.pip",
        "signature": "lib.ansible.modules.packaging.language.pip._is_package_name(name)",
        "snippet": "def _is_package_name(name):\n    \"\"\"Test whether the name is a package name or a version specifier.\"\"\"\n    return not name.lstrip().startswith(tuple(op_dict.keys()))",
        "begin_line": 300,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.packaging.language.pip._recover_package_name#305",
        "src_path": "lib/ansible/modules/packaging/language/pip.py",
        "class_name": "lib.ansible.modules.packaging.language.pip",
        "signature": "lib.ansible.modules.packaging.language.pip._recover_package_name(names)",
        "snippet": "def _recover_package_name(names):\n    \"\"\"Recover package names as list from user's raw input.\n\n    :input: a mixed and invalid list of names or version specifiers\n    :return: a list of valid package name\n\n    eg.\n    input: ['django>1.11.1', '<1.11.3', 'ipaddress', 'simpleproject>1.1.0', '<2.0.0']\n    return: ['django>1.11.1,<1.11.3', 'ipaddress', 'simpleproject>1.1.0,<2.0.0']\n\n    input: ['django>1.11.1,<1.11.3,ipaddress', 'simpleproject>1.1.0,<2.0.0']\n    return: ['django>1.11.1,<1.11.3', 'ipaddress', 'simpleproject>1.1.0,<2.0.0']\n    \"\"\"\n    # rebuild input name to a flat list so we can tolerate any combination of input\n    tmp = []\n    for one_line in names:\n        tmp.extend(one_line.split(\",\"))\n    names = tmp\n\n    # reconstruct the names\n    name_parts = []\n    package_names = []\n    in_brackets = False\n    for name in names:\n        if _is_package_name(name) and not in_brackets:\n            if name_parts:\n                package_names.append(\",\".join(name_parts))\n            name_parts = []\n        if \"[\" in name:\n            in_brackets = True\n        if in_brackets and \"]\" in name:\n            in_brackets = False\n        name_parts.append(name)\n    package_names.append(\",\".join(name_parts))\n    return package_names",
        "begin_line": 305,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._strip_comments#391",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._strip_comments(source)",
        "snippet": "def _strip_comments(source):\n    # Strip comments and blank lines from the wrapper\n    buf = []\n    for line in source.splitlines():\n        l = line.strip()\n        if not l or l.startswith(u'#'):\n            continue\n        buf.append(line)\n    return u'\\n'.join(buf)",
        "begin_line": 391,
        "end_line": 399,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.ModuleDepFinder.__init__#439",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common.ModuleDepFinder",
        "signature": "lib.ansible.executor.module_common.ModuleDepFinder.__init__(self, module_fqn, *args, **kwargs)",
        "snippet": "    def __init__(self, module_fqn, *args, **kwargs):\n        \"\"\"\n        Walk the ast tree for the python module.\n        :arg module_fqn: The fully qualified name to reach this module in dotted notation.\n            example: ansible.module_utils.basic\n\n        Save submodule[.submoduleN][.identifier] into self.submodules\n        when they are from ansible.module_utils or ansible_collections packages\n\n        self.submodules will end up with tuples like:\n          - ('ansible', 'module_utils', 'basic',)\n          - ('ansible', 'module_utils', 'urls', 'fetch_url')\n          - ('ansible', 'module_utils', 'database', 'postgres')\n          - ('ansible', 'module_utils', 'database', 'postgres', 'quote')\n          - ('ansible', 'module_utils', 'database', 'postgres', 'quote')\n          - ('ansible_collections', 'my_ns', 'my_col', 'plugins', 'module_utils', 'foo')\n\n        It's up to calling code to determine whether the final element of the\n        tuple are module names or something else (function, class, or variable names)\n        .. seealso:: :python3:class:`ast.NodeVisitor`\n        \"\"\"\n        super(ModuleDepFinder, self).__init__(*args, **kwargs)\n        self.submodules = set()\n        self.module_fqn = module_fqn",
        "begin_line": 439,
        "end_line": 462,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.ModuleDepFinder.visit_Import#464",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common.ModuleDepFinder",
        "signature": "lib.ansible.executor.module_common.ModuleDepFinder.visit_Import(self, node)",
        "snippet": "    def visit_Import(self, node):\n        \"\"\"\n        Handle import ansible.module_utils.MODLIB[.MODLIBn] [as asname]\n\n        We save these as interesting submodules when the imported library is in ansible.module_utils\n        or ansible.collections\n        \"\"\"\n        for alias in node.names:\n            if (alias.name.startswith('ansible.module_utils.') or\n                    alias.name.startswith('ansible_collections.')):\n                py_mod = tuple(alias.name.split('.'))\n                self.submodules.add(py_mod)\n        self.generic_visit(node)",
        "begin_line": 464,
        "end_line": 476,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.ModuleDepFinder.visit_ImportFrom#478",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common.ModuleDepFinder",
        "signature": "lib.ansible.executor.module_common.ModuleDepFinder.visit_ImportFrom(self, node)",
        "snippet": "    def visit_ImportFrom(self, node):\n        \"\"\"\n        Handle from ansible.module_utils.MODLIB import [.MODLIBn] [as asname]\n\n        Also has to handle relative imports\n\n        We save these as interesting submodules when the imported library is in ansible.module_utils\n        or ansible.collections\n        \"\"\"\n\n        # FIXME: These should all get skipped:\n        # from ansible.executor import module_common\n        # from ...executor import module_common\n        # from ... import executor (Currently it gives a non-helpful error)\n        if node.level > 0:\n            if self.module_fqn:\n                parts = tuple(self.module_fqn.split('.'))\n                if node.module:\n                    # relative import: from .module import x\n                    node_module = '.'.join(parts[:-node.level] + (node.module,))\n                else:\n                    # relative import: from . import x\n                    node_module = '.'.join(parts[:-node.level])\n            else:\n                # fall back to an absolute import\n                node_module = node.module\n        else:\n            # absolute import: from module import x\n            node_module = node.module\n\n        # Specialcase: six is a special case because of its\n        # import logic\n        py_mod = None\n        if node.names[0].name == '_six':\n            self.submodules.add(('_six',))\n        elif node_module.startswith('ansible.module_utils'):\n            # from ansible.module_utils.MODULE1[.MODULEn] import IDENTIFIER [as asname]\n            # from ansible.module_utils.MODULE1[.MODULEn] import MODULEn+1 [as asname]\n            # from ansible.module_utils.MODULE1[.MODULEn] import MODULEn+1 [,IDENTIFIER] [as asname]\n            # from ansible.module_utils import MODULE1 [,MODULEn] [as asname]\n            py_mod = tuple(node_module.split('.'))\n\n        elif node_module.startswith('ansible_collections.'):\n            if node_module.endswith('plugins.module_utils') or '.plugins.module_utils.' in node_module:\n                # from ansible_collections.ns.coll.plugins.module_utils import MODULE [as aname] [,MODULE2] [as aname]\n                # from ansible_collections.ns.coll.plugins.module_utils.MODULE import IDENTIFIER [as aname]\n                # FIXME: Unhandled cornercase (needs to be ignored):\n                # from ansible_collections.ns.coll.plugins.[!module_utils].[FOO].plugins.module_utils import IDENTIFIER\n                py_mod = tuple(node_module.split('.'))\n            else:\n                # Not from module_utils so ignore.  for instance:\n                # from ansible_collections.ns.coll.plugins.lookup import IDENTIFIER\n                pass\n\n        if py_mod:\n            for alias in node.names:\n                self.submodules.add(py_mod + (alias.name,))\n\n        self.generic_visit(node)",
        "begin_line": 478,
        "end_line": 536,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._slurp#539",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._slurp(path)",
        "snippet": "def _slurp(path):\n    if not os.path.exists(path):\n        raise AnsibleError(\"imported module support code does not exist at %s\" % os.path.abspath(path))\n    with open(path, 'rb') as fd:\n        data = fd.read()\n    return data",
        "begin_line": 539,
        "end_line": 544,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._get_shebang#547",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._get_shebang(interpreter, task_vars, templar, args=tuple())",
        "snippet": "def _get_shebang(interpreter, task_vars, templar, args=tuple()):\n    \"\"\"\n    Note not stellar API:\n       Returns None instead of always returning a shebang line.  Doing it this\n       way allows the caller to decide to use the shebang it read from the\n       file rather than trust that we reformatted what they already have\n       correctly.\n    \"\"\"\n    interpreter_name = os.path.basename(interpreter).strip()\n\n    # FUTURE: add logical equivalence for python3 in the case of py3-only modules\n\n    # check for first-class interpreter config\n    interpreter_config_key = \"INTERPRETER_%s\" % interpreter_name.upper()\n\n    if C.config.get_configuration_definitions().get(interpreter_config_key):\n        # a config def exists for this interpreter type; consult config for the value\n        interpreter_out = C.config.get_config_value(interpreter_config_key, variables=task_vars)\n        discovered_interpreter_config = u'discovered_interpreter_%s' % interpreter_name\n\n        interpreter_out = templar.template(interpreter_out.strip())\n\n        facts_from_task_vars = task_vars.get('ansible_facts', {})\n\n        # handle interpreter discovery if requested\n        if interpreter_out in ['auto', 'auto_legacy', 'auto_silent', 'auto_legacy_silent']:\n            if discovered_interpreter_config not in facts_from_task_vars:\n                # interpreter discovery is desired, but has not been run for this host\n                raise InterpreterDiscoveryRequiredError(\"interpreter discovery needed\",\n                                                        interpreter_name=interpreter_name,\n                                                        discovery_mode=interpreter_out)\n            else:\n                interpreter_out = facts_from_task_vars[discovered_interpreter_config]\n    else:\n        # a config def does not exist for this interpreter type; consult vars for a possible direct override\n        interpreter_config = u'ansible_%s_interpreter' % interpreter_name\n\n        if interpreter_config not in task_vars:\n            return None, interpreter\n\n        interpreter_out = templar.template(task_vars[interpreter_config].strip())\n\n    shebang = u'#!' + interpreter_out\n\n    if args:\n        shebang = shebang + u' ' + u' '.join(args)\n\n    return shebang, interpreter_out",
        "begin_line": 547,
        "end_line": 594,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.ModuleInfo.__init__#598",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common.ModuleInfo",
        "signature": "lib.ansible.executor.module_common.ModuleInfo.__init__(self, name, paths)",
        "snippet": "    def __init__(self, name, paths):\n        self.py_src = False\n        self.pkg_dir = False\n        path = None\n\n        if imp is None:\n            self._info = info = importlib.machinery.PathFinder.find_spec(name, paths)\n            if info is not None:\n                self.py_src = os.path.splitext(info.origin)[1] in importlib.machinery.SOURCE_SUFFIXES\n                self.pkg_dir = info.origin.endswith('/__init__.py')\n                path = info.origin\n            else:\n                raise ImportError(\"No module named '%s'\" % name)\n        else:\n            self._info = info = imp.find_module(name, paths)\n            self.py_src = info[2][2] == imp.PY_SOURCE\n            self.pkg_dir = info[2][2] == imp.PKG_DIRECTORY\n            if self.pkg_dir:\n                path = os.path.join(info[1], '__init__.py')\n            else:\n                path = info[1]\n\n        self.path = path",
        "begin_line": 598,
        "end_line": 620,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.ModuleInfo.get_source#622",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common.ModuleInfo",
        "signature": "lib.ansible.executor.module_common.ModuleInfo.get_source(self)",
        "snippet": "    def get_source(self):\n        if imp and self.py_src:\n            try:\n                return self._info[0].read()\n            finally:\n                self._info[0].close()\n        return _slurp(self.path)",
        "begin_line": 622,
        "end_line": 628,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.recursive_finder#664",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common.recursive_finder(name, module_fqn, data, py_module_names, py_module_cache, zf)",
        "snippet": "def recursive_finder(name, module_fqn, data, py_module_names, py_module_cache, zf):\n    \"\"\"\n    Using ModuleDepFinder, make sure we have all of the module_utils files that\n    the module and its module_utils files needs.\n    :arg name: Name of the python module we're examining\n    :arg module_fqn: Fully qualified name of the python module we're scanning\n    :arg py_module_names: set of the fully qualified module names represented as a tuple of their\n        FQN with __init__ appended if the module is also a python package).  Presence of a FQN in\n        this set means that we've already examined it for module_util deps.\n    :arg py_module_cache: map python module names (represented as a tuple of their FQN with __init__\n        appended if the module is also a python package) to a tuple of the code in the module and\n        the pathname the module would have inside of a Python toplevel (like site-packages)\n    :arg zf: An open :python:class:`zipfile.ZipFile` object that holds the Ansible module payload\n        which we're assembling\n    \"\"\"\n    # Parse the module and find the imports of ansible.module_utils\n    try:\n        tree = ast.parse(data)\n    except (SyntaxError, IndentationError) as e:\n        raise AnsibleError(\"Unable to import %s due to %s\" % (name, e.msg))\n\n    finder = ModuleDepFinder(module_fqn)\n    finder.visit(tree)\n\n    #\n    # Determine what imports that we've found are modules (vs class, function.\n    # variable names) for packages\n    #\n    module_utils_paths = [p for p in module_utils_loader._get_paths(subdirs=False) if os.path.isdir(p)]\n    # FIXME: Do we still need this?  It feels like module-utils_loader should include\n    # _MODULE_UTILS_PATH\n    module_utils_paths.append(_MODULE_UTILS_PATH)\n\n    normalized_modules = set()\n    # Loop through the imports that we've found to normalize them\n    # Exclude paths that match with paths we've already processed\n    # (Have to exclude them a second time once the paths are processed)\n\n    for py_module_name in finder.submodules.difference(py_module_names):\n        module_info = None\n\n        if py_module_name[0:3] == ('ansible', 'module_utils', 'six'):\n            # Special case the python six library because it messes with the\n            # import process in an incompatible way\n            module_info = ModuleInfo('six', module_utils_paths)\n            py_module_name = ('ansible', 'module_utils', 'six')\n            idx = 0\n        elif py_module_name[0:3] == ('ansible', 'module_utils', '_six'):\n            # Special case the python six library because it messes with the\n            # import process in an incompatible way\n            module_info = ModuleInfo('_six', [os.path.join(p, 'six') for p in module_utils_paths])\n            py_module_name = ('ansible', 'module_utils', 'six', '_six')\n            idx = 0\n        elif py_module_name[0] == 'ansible_collections':\n            # FIXME (nitz): replicate module name resolution like below for granular imports\n            for idx in (1, 2):\n                if len(py_module_name) < idx:\n                    break\n                try:\n                    # this is a collection-hosted MU; look it up with pkgutil.get_data()\n                    module_info = CollectionModuleInfo(py_module_name[-idx],\n                                                       [os.path.join(*py_module_name[:-idx])])\n                    break\n                except ImportError:\n                    continue\n        elif py_module_name[0:2] == ('ansible', 'module_utils'):\n            # Need to remove ansible.module_utils because PluginLoader may find different paths\n            # for us to look in\n            relative_module_utils_dir = py_module_name[2:]\n            # Check whether either the last or the second to last identifier is\n            # a module name\n            for idx in (1, 2):\n                if len(relative_module_utils_dir) < idx:\n                    break\n                try:\n                    module_info = ModuleInfo(py_module_name[-idx],\n                                             [os.path.join(p, *relative_module_utils_dir[:-idx]) for p in module_utils_paths])\n                    break\n                except ImportError:\n                    continue\n        else:\n            # If we get here, it's because of a bug in ModuleDepFinder.  If we get a reproducer we\n            # should then fix ModuleDepFinder\n            display.warning('ModuleDepFinder improperly found a non-module_utils import %s'\n                            % [py_module_name])\n            continue\n\n        # Could not find the module.  Construct a helpful error message.\n        if module_info is None:\n            msg = ['Could not find imported module support code for %s.  Looked for' % (name,)]\n            if idx == 2:\n                msg.append('either %s.py or %s.py' % (py_module_name[-1], py_module_name[-2]))\n            else:\n                msg.append(py_module_name[-1])\n            raise AnsibleError(' '.join(msg))\n\n        if isinstance(module_info, CollectionModuleInfo):\n            if idx == 2:\n                # We've determined that the last portion was an identifier and\n                # thus, not part of the module name\n                py_module_name = py_module_name[:-1]\n\n            # HACK: maybe surface collection dirs in here and use existing find_module code?\n            normalized_name = py_module_name\n            normalized_data = module_info.get_source()\n            normalized_path = os.path.join(*py_module_name)\n            py_module_cache[normalized_name] = (normalized_data, normalized_path)\n            normalized_modules.add(normalized_name)\n\n            # HACK: walk back up the package hierarchy to pick up package inits; this won't do the right thing\n            # for actual packages yet...\n            accumulated_pkg_name = []\n            for pkg in py_module_name[:-1]:\n                accumulated_pkg_name.append(pkg)  # we're accumulating this across iterations\n                normalized_name = tuple(accumulated_pkg_name[:] + ['__init__'])  # extra machinations to get a hashable type (list is not)\n                if normalized_name not in py_module_cache:\n                    normalized_path = os.path.join(*accumulated_pkg_name)\n                    # HACK: possibly preserve some of the actual package file contents; problematic for extend_paths and others though?\n                    normalized_data = ''\n                    py_module_cache[normalized_name] = (normalized_data, normalized_path)\n                    normalized_modules.add(normalized_name)\n\n        else:\n            # Found a byte compiled file rather than source.  We cannot send byte\n            # compiled over the wire as the python version might be different.\n            # imp.find_module seems to prefer to return source packages so we just\n            # error out if imp.find_module returns byte compiled files (This is\n            # fragile as it depends on undocumented imp.find_module behaviour)\n            if not module_info.pkg_dir and not module_info.py_src:\n                msg = ['Could not find python source for imported module support code for %s.  Looked for' % name]\n                if idx == 2:\n                    msg.append('either %s.py or %s.py' % (py_module_name[-1], py_module_name[-2]))\n                else:\n                    msg.append(py_module_name[-1])\n                raise AnsibleError(' '.join(msg))\n\n            if idx == 2:\n                # We've determined that the last portion was an identifier and\n                # thus, not part of the module name\n                py_module_name = py_module_name[:-1]\n\n            # If not already processed then we've got work to do\n            # If not in the cache, then read the file into the cache\n            # We already have a file handle for the module open so it makes\n            # sense to read it now\n            if py_module_name not in py_module_cache:\n                if module_info.pkg_dir:\n                    # Read the __init__.py instead of the module file as this is\n                    # a python package\n                    normalized_name = py_module_name + ('__init__',)\n                    if normalized_name not in py_module_names:\n                        normalized_data = module_info.get_source()\n                        py_module_cache[normalized_name] = (normalized_data, module_info.path)\n                        normalized_modules.add(normalized_name)\n                else:\n                    normalized_name = py_module_name\n                    if normalized_name not in py_module_names:\n                        normalized_data = module_info.get_source()\n                        py_module_cache[normalized_name] = (normalized_data, module_info.path)\n                        normalized_modules.add(normalized_name)\n\n                #\n                # Make sure that all the packages that this module is a part of\n                # are also added\n                #\n                for i in range(1, len(py_module_name)):\n                    py_pkg_name = py_module_name[:-i] + ('__init__',)\n                    if py_pkg_name not in py_module_names:\n                        # Need to remove ansible.module_utils because PluginLoader may find\n                        # different paths for us to look in\n                        relative_module_utils = py_pkg_name[2:]\n                        pkg_dir_info = ModuleInfo(relative_module_utils[-1],\n                                                  [os.path.join(p, *relative_module_utils[:-1]) for p in module_utils_paths])\n                        normalized_modules.add(py_pkg_name)\n                        py_module_cache[py_pkg_name] = (pkg_dir_info.get_source(), pkg_dir_info.path)\n\n    # FIXME: Currently the AnsiBallZ wrapper monkeypatches module args into a global\n    # variable in basic.py.  If a module doesn't import basic.py, then the AnsiBallZ wrapper will\n    # traceback when it tries to monkypatch.  So, for now, we have to unconditionally include\n    # basic.py.\n    #\n    # In the future we need to change the wrapper to monkeypatch the args into a global variable in\n    # their own, separate python module.  That way we won't require basic.py.  Modules which don't\n    # want basic.py can import that instead.  AnsibleModule will need to change to import the vars\n    # from the separate python module and mirror the args into its global variable for backwards\n    # compatibility.\n    if ('ansible', 'module_utils', 'basic',) not in py_module_names:\n        pkg_dir_info = ModuleInfo('basic', module_utils_paths)\n        normalized_modules.add(('ansible', 'module_utils', 'basic',))\n        py_module_cache[('ansible', 'module_utils', 'basic',)] = (pkg_dir_info.get_source(), pkg_dir_info.path)\n    # End of AnsiballZ hack\n\n    #\n    # iterate through all of the ansible.module_utils* imports that we haven't\n    # already checked for new imports\n    #\n\n    # set of modules that we haven't added to the zipfile\n    unprocessed_py_module_names = normalized_modules.difference(py_module_names)\n\n    for py_module_name in unprocessed_py_module_names:\n\n        py_module_path = os.path.join(*py_module_name)\n        py_module_file_name = '%s.py' % py_module_path\n\n        zf.writestr(py_module_file_name, py_module_cache[py_module_name][0])\n        display.vvvvv(\"Using module_utils file %s\" % py_module_cache[py_module_name][1])\n\n    # Add the names of the files we're scheduling to examine in the loop to\n    # py_module_names so that we don't re-examine them in the next pass\n    # through recursive_finder()\n    py_module_names.update(unprocessed_py_module_names)\n\n    for py_module_file in unprocessed_py_module_names:\n        next_fqn = '.'.join(py_module_file)\n        recursive_finder(py_module_file[-1], next_fqn, py_module_cache[py_module_file][0],\n                         py_module_names, py_module_cache, zf)\n        # Save memory; the file won't have to be read again for this ansible module.\n        del py_module_cache[py_module_file]",
        "begin_line": 664,
        "end_line": 882,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._is_binary#885",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._is_binary(b_module_data)",
        "snippet": "def _is_binary(b_module_data):\n    textchars = bytearray(set([7, 8, 9, 10, 12, 13, 27]) | set(range(0x20, 0x100)) - set([0x7f]))\n    start = b_module_data[:1024]\n    return bool(start.translate(None, textchars))",
        "begin_line": 885,
        "end_line": 888,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._get_ansible_module_fqn#891",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._get_ansible_module_fqn(module_path)",
        "snippet": "def _get_ansible_module_fqn(module_path):\n    \"\"\"\n    Get the fully qualified name for an ansible module based on its pathname\n\n    remote_module_fqn is the fully qualified name.  Like ansible.modules.system.ping\n    Or ansible_collections.Namespace.Collection_name.plugins.modules.ping\n    .. warning:: This function is for ansible modules only.  It won't work for other things\n        (non-module plugins, etc)\n    \"\"\"\n    remote_module_fqn = None\n\n    # Is this a core module?\n    match = CORE_LIBRARY_PATH_RE.search(module_path)\n    if not match:\n        # Is this a module in a collection?\n        match = COLLECTION_PATH_RE.search(module_path)\n\n    # We can tell the FQN for core modules and collection modules\n    if match:\n        path = match.group('path')\n        if '.' in path:\n            # FQNs must be valid as python identifiers.  This sanity check has failed.\n            # we could check other things as well\n            raise ValueError('Module name (or path) was not a valid python identifier')\n\n        remote_module_fqn = '.'.join(path.split('/'))\n    else:\n        # Currently we do not handle modules in roles so we can end up here for that reason\n        raise ValueError(\"Unable to determine module's fully qualified name\")\n\n    return remote_module_fqn",
        "begin_line": 891,
        "end_line": 921,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._add_module_to_zip#924",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._add_module_to_zip(zf, remote_module_fqn, b_module_data)",
        "snippet": "def _add_module_to_zip(zf, remote_module_fqn, b_module_data):\n    \"\"\"Add a module from ansible or from an ansible collection into the module zip\"\"\"\n    module_path_parts = remote_module_fqn.split('.')\n\n    # Write the module\n    module_path = '/'.join(module_path_parts) + '.py'\n    zf.writestr(module_path, b_module_data)\n\n    # Write the __init__.py's necessary to get there\n    if module_path_parts[0] == 'ansible':\n        # The ansible namespace is setup as part of the module_utils setup...\n        start = 2\n        existing_paths = frozenset()\n    else:\n        # ... but ansible_collections and other toplevels are not\n        start = 1\n        existing_paths = frozenset(zf.namelist())\n\n    for idx in range(start, len(module_path_parts)):\n        package_path = '/'.join(module_path_parts[:idx]) + '/__init__.py'\n        # If a collections module uses module_utils from a collection then most packages will have already been added by recursive_finder.\n        if package_path in existing_paths:\n            continue\n        # Note: We don't want to include more than one ansible module in a payload at this time\n        # so no need to fill the __init__.py with namespace code\n        zf.writestr(package_path, b'')",
        "begin_line": 924,
        "end_line": 949,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common._find_module_utils#952",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common._find_module_utils(module_name, b_module_data, module_path, module_args, task_vars, templar, module_compression, async_timeout, become, become_method, become_user, become_password, become_flags, environment)",
        "snippet": "def _find_module_utils(module_name, b_module_data, module_path, module_args, task_vars, templar, module_compression, async_timeout, become,\n                       become_method, become_user, become_password, become_flags, environment):\n    \"\"\"\n    Given the source of the module, convert it to a Jinja2 template to insert\n    module code and return whether it's a new or old style module.\n    \"\"\"\n    module_substyle = module_style = 'old'\n\n    # module_style is something important to calling code (ActionBase).  It\n    # determines how arguments are formatted (json vs k=v) and whether\n    # a separate arguments file needs to be sent over the wire.\n    # module_substyle is extra information that's useful internally.  It tells\n    # us what we have to look to substitute in the module files and whether\n    # we're using module replacer or ansiballz to format the module itself.\n    if _is_binary(b_module_data):\n        module_substyle = module_style = 'binary'\n    elif REPLACER in b_module_data:\n        # Do REPLACER before from ansible.module_utils because we need make sure\n        # we substitute \"from ansible.module_utils basic\" for REPLACER\n        module_style = 'new'\n        module_substyle = 'python'\n        b_module_data = b_module_data.replace(REPLACER, b'from ansible.module_utils.basic import *')\n    elif NEW_STYLE_PYTHON_MODULE_RE.search(b_module_data):\n        module_style = 'new'\n        module_substyle = 'python'\n    elif REPLACER_WINDOWS in b_module_data:\n        module_style = 'new'\n        module_substyle = 'powershell'\n        b_module_data = b_module_data.replace(REPLACER_WINDOWS, b'#Requires -Module Ansible.ModuleUtils.Legacy')\n    elif re.search(b'#Requires -Module', b_module_data, re.IGNORECASE) \\\n            or re.search(b'#Requires -Version', b_module_data, re.IGNORECASE)\\\n            or re.search(b'#AnsibleRequires -OSVersion', b_module_data, re.IGNORECASE) \\\n            or re.search(b'#AnsibleRequires -Powershell', b_module_data, re.IGNORECASE) \\\n            or re.search(b'#AnsibleRequires -CSharpUtil', b_module_data, re.IGNORECASE):\n        module_style = 'new'\n        module_substyle = 'powershell'\n    elif REPLACER_JSONARGS in b_module_data:\n        module_style = 'new'\n        module_substyle = 'jsonargs'\n    elif b'WANT_JSON' in b_module_data:\n        module_substyle = module_style = 'non_native_want_json'\n\n    shebang = None\n    # Neither old-style, non_native_want_json nor binary modules should be modified\n    # except for the shebang line (Done by modify_module)\n    if module_style in ('old', 'non_native_want_json', 'binary'):\n        return b_module_data, module_style, shebang\n\n    output = BytesIO()\n    py_module_names = set()\n\n    try:\n        remote_module_fqn = _get_ansible_module_fqn(module_path)\n    except ValueError:\n        # Modules in roles currently are not found by the fqn heuristic so we\n        # fallback to this.  This means that relative imports inside a module from\n        # a role may fail.  Absolute imports should be used for future-proofness.\n        # People should start writing collections instead of modules in roles so we\n        # may never fix this\n        display.debug('ANSIBALLZ: Could not determine module FQN')\n        remote_module_fqn = 'ansible.modules.%s' % module_name\n\n    if module_substyle == 'python':\n        params = dict(ANSIBLE_MODULE_ARGS=module_args,)\n        try:\n            python_repred_params = repr(json.dumps(params))\n        except TypeError as e:\n            raise AnsibleError(\"Unable to pass options to module, they must be JSON serializable: %s\" % to_native(e))\n\n        try:\n            compression_method = getattr(zipfile, module_compression)\n        except AttributeError:\n            display.warning(u'Bad module compression string specified: %s.  Using ZIP_STORED (no compression)' % module_compression)\n            compression_method = zipfile.ZIP_STORED\n\n        lookup_path = os.path.join(C.DEFAULT_LOCAL_TMP, 'ansiballz_cache')\n        cached_module_filename = os.path.join(lookup_path, \"%s-%s\" % (module_name, module_compression))\n\n        zipdata = None\n        # Optimization -- don't lock if the module has already been cached\n        if os.path.exists(cached_module_filename):\n            display.debug('ANSIBALLZ: using cached module: %s' % cached_module_filename)\n            with open(cached_module_filename, 'rb') as module_data:\n                zipdata = module_data.read()\n        else:\n            if module_name in action_write_locks.action_write_locks:\n                display.debug('ANSIBALLZ: Using lock for %s' % module_name)\n                lock = action_write_locks.action_write_locks[module_name]\n            else:\n                # If the action plugin directly invokes the module (instead of\n                # going through a strategy) then we don't have a cross-process\n                # Lock specifically for this module.  Use the \"unexpected\n                # module\" lock instead\n                display.debug('ANSIBALLZ: Using generic lock for %s' % module_name)\n                lock = action_write_locks.action_write_locks[None]\n\n            display.debug('ANSIBALLZ: Acquiring lock')\n            with lock:\n                display.debug('ANSIBALLZ: Lock acquired: %s' % id(lock))\n                # Check that no other process has created this while we were\n                # waiting for the lock\n                if not os.path.exists(cached_module_filename):\n                    display.debug('ANSIBALLZ: Creating module')\n                    # Create the module zip data\n                    zipoutput = BytesIO()\n                    zf = zipfile.ZipFile(zipoutput, mode='w', compression=compression_method)\n\n                    # py_module_cache maps python module names to a tuple of the code in the module\n                    # and the pathname to the module.  See the recursive_finder() documentation for\n                    # more info.\n                    # Here we pre-load it with modules which we create without bothering to\n                    # read from actual files (In some cases, these need to differ from what ansible\n                    # ships because they're namespace packages in the module)\n                    py_module_cache = {\n                        ('ansible', '__init__',): (\n                            b'from pkgutil import extend_path\\n'\n                            b'__path__=extend_path(__path__,__name__)\\n'\n                            b'__version__=\"' + to_bytes(__version__) +\n                            b'\"\\n__author__=\"' + to_bytes(__author__) + b'\"\\n',\n                            'ansible/__init__.py'),\n                        ('ansible', 'module_utils', '__init__',): (\n                            b'from pkgutil import extend_path\\n'\n                            b'__path__=extend_path(__path__,__name__)\\n',\n                            'ansible/module_utils/__init__.py')}\n\n                    for (py_module_name, (file_data, filename)) in py_module_cache.items():\n                        zf.writestr(filename, file_data)\n                        # py_module_names keeps track of which modules we've already scanned for\n                        # module_util dependencies\n                        py_module_names.add(py_module_name)\n\n                    # Returning the ast tree is a temporary hack.  We need to know if the module has\n                    # a main() function or not as we are deprecating new-style modules without\n                    # main().  Because parsing the ast is expensive, return it from recursive_finder\n                    # instead of reparsing.  Once the deprecation is over and we remove that code,\n                    # also remove returning of the ast tree.\n                    recursive_finder(module_name, remote_module_fqn, b_module_data, py_module_names,\n                                     py_module_cache, zf)\n\n                    display.debug('ANSIBALLZ: Writing module into payload')\n                    _add_module_to_zip(zf, remote_module_fqn, b_module_data)\n\n                    zf.close()\n                    zipdata = base64.b64encode(zipoutput.getvalue())\n\n                    # Write the assembled module to a temp file (write to temp\n                    # so that no one looking for the file reads a partially\n                    # written file)\n                    if not os.path.exists(lookup_path):\n                        # Note -- if we have a global function to setup, that would\n                        # be a better place to run this\n                        os.makedirs(lookup_path)\n                    display.debug('ANSIBALLZ: Writing module')\n                    with open(cached_module_filename + '-part', 'wb') as f:\n                        f.write(zipdata)\n\n                    # Rename the file into its final position in the cache so\n                    # future users of this module can read it off the\n                    # filesystem instead of constructing from scratch.\n                    display.debug('ANSIBALLZ: Renaming module')\n                    os.rename(cached_module_filename + '-part', cached_module_filename)\n                    display.debug('ANSIBALLZ: Done creating module')\n\n            if zipdata is None:\n                display.debug('ANSIBALLZ: Reading module after lock')\n                # Another process wrote the file while we were waiting for\n                # the write lock.  Go ahead and read the data from disk\n                # instead of re-creating it.\n                try:\n                    with open(cached_module_filename, 'rb') as f:\n                        zipdata = f.read()\n                except IOError:\n                    raise AnsibleError('A different worker process failed to create module file. '\n                                       'Look at traceback for that process for debugging information.')\n        zipdata = to_text(zipdata, errors='surrogate_or_strict')\n\n        shebang, interpreter = _get_shebang(u'/usr/bin/python', task_vars, templar)\n        if shebang is None:\n            shebang = u'#!/usr/bin/python'\n\n        # FUTURE: the module cache entry should be invalidated if we got this value from a host-dependent source\n        rlimit_nofile = C.config.get_config_value('PYTHON_MODULE_RLIMIT_NOFILE', variables=task_vars)\n\n        if not isinstance(rlimit_nofile, int):\n            rlimit_nofile = int(templar.template(rlimit_nofile))\n\n        if rlimit_nofile:\n            rlimit = ANSIBALLZ_RLIMIT_TEMPLATE % dict(\n                rlimit_nofile=rlimit_nofile,\n            )\n        else:\n            rlimit = ''\n\n        coverage_config = os.environ.get('_ANSIBLE_COVERAGE_CONFIG')\n\n        if coverage_config:\n            coverage_output = os.environ['_ANSIBLE_COVERAGE_OUTPUT']\n\n            if coverage_output:\n                # Enable code coverage analysis of the module.\n                # This feature is for internal testing and may change without notice.\n                coverage = ANSIBALLZ_COVERAGE_TEMPLATE % dict(\n                    coverage_config=coverage_config,\n                    coverage_output=coverage_output,\n                )\n            else:\n                # Verify coverage is available without importing it.\n                # This will detect when a module would fail with coverage enabled with minimal overhead.\n                coverage = ANSIBALLZ_COVERAGE_CHECK_TEMPLATE\n        else:\n            coverage = ''\n\n        now = datetime.datetime.utcnow()\n        output.write(to_bytes(ACTIVE_ANSIBALLZ_TEMPLATE % dict(\n            zipdata=zipdata,\n            ansible_module=module_name,\n            module_fqn=remote_module_fqn,\n            params=python_repred_params,\n            shebang=shebang,\n            coding=ENCODING_STRING,\n            year=now.year,\n            month=now.month,\n            day=now.day,\n            hour=now.hour,\n            minute=now.minute,\n            second=now.second,\n            coverage=coverage,\n            rlimit=rlimit,\n        )))\n        b_module_data = output.getvalue()\n\n    elif module_substyle == 'powershell':\n        # Powershell/winrm don't actually make use of shebang so we can\n        # safely set this here.  If we let the fallback code handle this\n        # it can fail in the presence of the UTF8 BOM commonly added by\n        # Windows text editors\n        shebang = u'#!powershell'\n        # create the common exec wrapper payload and set that as the module_data\n        # bytes\n        b_module_data = ps_manifest._create_powershell_wrapper(\n            b_module_data, module_path, module_args, environment,\n            async_timeout, become, become_method, become_user, become_password,\n            become_flags, module_substyle, task_vars, remote_module_fqn\n        )\n\n    elif module_substyle == 'jsonargs':\n        module_args_json = to_bytes(json.dumps(module_args))\n\n        # these strings could be included in a third-party module but\n        # officially they were included in the 'basic' snippet for new-style\n        # python modules (which has been replaced with something else in\n        # ansiballz) If we remove them from jsonargs-style module replacer\n        # then we can remove them everywhere.\n        python_repred_args = to_bytes(repr(module_args_json))\n        b_module_data = b_module_data.replace(REPLACER_VERSION, to_bytes(repr(__version__)))\n        b_module_data = b_module_data.replace(REPLACER_COMPLEX, python_repred_args)\n        b_module_data = b_module_data.replace(REPLACER_SELINUX, to_bytes(','.join(C.DEFAULT_SELINUX_SPECIAL_FS)))\n\n        # The main event -- substitute the JSON args string into the module\n        b_module_data = b_module_data.replace(REPLACER_JSONARGS, module_args_json)\n\n        facility = b'syslog.' + to_bytes(task_vars.get('ansible_syslog_facility', C.DEFAULT_SYSLOG_FACILITY), errors='surrogate_or_strict')\n        b_module_data = b_module_data.replace(b'syslog.LOG_USER', facility)\n\n    return (b_module_data, module_style, shebang)",
        "begin_line": 952,
        "end_line": 1216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.modify_module#1219",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common.modify_module(module_name, module_path, module_args, templar, task_vars=None, module_compression='ZIP_STORED', async_timeout=0, become=False, become_method=None, become_user=None, become_password=None, become_flags=None, environment=None)",
        "snippet": "def modify_module(module_name, module_path, module_args, templar, task_vars=None, module_compression='ZIP_STORED', async_timeout=0, become=False,\n                  become_method=None, become_user=None, become_password=None, become_flags=None, environment=None):\n    \"\"\"\n    Used to insert chunks of code into modules before transfer rather than\n    doing regular python imports.  This allows for more efficient transfer in\n    a non-bootstrapping scenario by not moving extra files over the wire and\n    also takes care of embedding arguments in the transferred modules.\n\n    This version is done in such a way that local imports can still be\n    used in the module code, so IDEs don't have to be aware of what is going on.\n\n    Example:\n\n    from ansible.module_utils.basic import *\n\n       ... will result in the insertion of basic.py into the module\n       from the module_utils/ directory in the source tree.\n\n    For powershell, this code effectively no-ops, as the exec wrapper requires access to a number of\n    properties not available here.\n\n    \"\"\"\n    task_vars = {} if task_vars is None else task_vars\n    environment = {} if environment is None else environment\n\n    with open(module_path, 'rb') as f:\n\n        # read in the module source\n        b_module_data = f.read()\n\n    (b_module_data, module_style, shebang) = _find_module_utils(module_name, b_module_data, module_path, module_args, task_vars, templar, module_compression,\n                                                                async_timeout=async_timeout, become=become, become_method=become_method,\n                                                                become_user=become_user, become_password=become_password, become_flags=become_flags,\n                                                                environment=environment)\n\n    if module_style == 'binary':\n        return (b_module_data, module_style, to_text(shebang, nonstring='passthru'))\n    elif shebang is None:\n        b_lines = b_module_data.split(b\"\\n\", 1)\n        if b_lines[0].startswith(b\"#!\"):\n            b_shebang = b_lines[0].strip()\n            # shlex.split on python-2.6 needs bytes.  On python-3.x it needs text\n            args = shlex.split(to_native(b_shebang[2:], errors='surrogate_or_strict'))\n\n            # _get_shebang() takes text strings\n            args = [to_text(a, errors='surrogate_or_strict') for a in args]\n            interpreter = args[0]\n            b_new_shebang = to_bytes(_get_shebang(interpreter, task_vars, templar, args[1:])[0],\n                                     errors='surrogate_or_strict', nonstring='passthru')\n\n            if b_new_shebang:\n                b_lines[0] = b_shebang = b_new_shebang\n\n            if os.path.basename(interpreter).startswith(u'python'):\n                b_lines.insert(1, b_ENCODING_STRING)\n\n            shebang = to_text(b_shebang, nonstring='passthru', errors='surrogate_or_strict')\n        else:\n            # No shebang, assume a binary module?\n            pass\n\n        b_module_data = b\"\\n\".join(b_lines)\n\n    return (b_module_data, module_style, shebang)",
        "begin_line": 1219,
        "end_line": 1282,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.module_common.get_action_args_with_defaults#1285",
        "src_path": "lib/ansible/executor/module_common.py",
        "class_name": "lib.ansible.executor.module_common",
        "signature": "lib.ansible.executor.module_common.get_action_args_with_defaults(action, args, defaults, templar)",
        "snippet": "def get_action_args_with_defaults(action, args, defaults, templar):\n\n    tmp_args = {}\n    module_defaults = {}\n\n    # Merge latest defaults into dict, since they are a list of dicts\n    if isinstance(defaults, list):\n        for default in defaults:\n            module_defaults.update(default)\n\n    # if I actually have defaults, template and merge\n    if module_defaults:\n        module_defaults = templar.template(module_defaults)\n\n        # deal with configured group defaults first\n        if action in C.config.module_defaults_groups:\n            for group in C.config.module_defaults_groups.get(action, []):\n                tmp_args.update((module_defaults.get('group/{0}'.format(group)) or {}).copy())\n\n        # handle specific action defaults\n        if action in module_defaults:\n            tmp_args.update(module_defaults[action].copy())\n\n    # direct args override all\n    tmp_args.update(args)\n\n    return tmp_args",
        "begin_line": 1285,
        "end_line": 1311,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.parsing.convert_bool.boolean#13",
        "src_path": "lib/ansible/module_utils/parsing/convert_bool.py",
        "class_name": "lib.ansible.module_utils.parsing.convert_bool",
        "signature": "lib.ansible.module_utils.parsing.convert_bool.boolean(value, strict=True)",
        "snippet": "def boolean(value, strict=True):\n    if isinstance(value, bool):\n        return value\n\n    normalized_value = value\n    if isinstance(value, (text_type, binary_type)):\n        normalized_value = to_text(value, errors='surrogate_or_strict').lower().strip()\n\n    if normalized_value in BOOLEANS_TRUE:\n        return True\n    elif normalized_value in BOOLEANS_FALSE or not strict:\n        return False\n\n    raise TypeError(\"The value '%s' is not a valid boolean.  Valid booleans include: %s\" % (to_text(value), ', '.join(repr(i) for i in BOOLEANS)))",
        "begin_line": 13,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.script.InventoryModule.__init__#64",
        "src_path": "lib/ansible/plugins/inventory/script.py",
        "class_name": "lib.ansible.plugins.inventory.script.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.script.InventoryModule.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(InventoryModule, self).__init__()\n\n        self._hosts = set()",
        "begin_line": 64,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.script.InventoryModule.verify_file#70",
        "src_path": "lib/ansible/plugins/inventory/script.py",
        "class_name": "lib.ansible.plugins.inventory.script.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.script.InventoryModule.verify_file(self, path)",
        "snippet": "    def verify_file(self, path):\n        ''' Verify if file is usable by this plugin, base does minimal accessibility check '''\n\n        valid = super(InventoryModule, self).verify_file(path)\n\n        if valid:\n            # not only accessible, file must be executable and/or have shebang\n            shebang_present = False\n            try:\n                with open(path, 'rb') as inv_file:\n                    initial_chars = inv_file.read(2)\n                    if initial_chars.startswith(b'#!'):\n                        shebang_present = True\n            except Exception:\n                pass\n\n            if not os.access(path, os.X_OK) and not shebang_present:\n                valid = False\n\n        return valid",
        "begin_line": 70,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.script.InventoryModule.parse#91",
        "src_path": "lib/ansible/plugins/inventory/script.py",
        "class_name": "lib.ansible.plugins.inventory.script.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.script.InventoryModule.parse(self, inventory, loader, path, cache=None)",
        "snippet": "    def parse(self, inventory, loader, path, cache=None):\n\n        super(InventoryModule, self).parse(inventory, loader, path)\n        self.set_options()\n\n        if self.get_option('cache') is not None:\n            display.deprecated(\n                msg=\"The 'cache' option is deprecated for the script inventory plugin. \"\n                \"External scripts implement their own caching and this option has never been used\",\n                version=\"2.12\"\n            )\n\n        # Support inventory scripts that are not prefixed with some\n        # path information but happen to be in the current working\n        # directory when '.' is not in PATH.\n        cmd = [path, \"--list\"]\n\n        try:\n            try:\n                sp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            except OSError as e:\n                raise AnsibleParserError(\"problem running %s (%s)\" % (' '.join(cmd), to_native(e)))\n            (stdout, stderr) = sp.communicate()\n\n            path = to_native(path)\n            err = to_native(stderr or \"\")\n\n            if err and not err.endswith('\\n'):\n                err += '\\n'\n\n            if sp.returncode != 0:\n                raise AnsibleError(\"Inventory script (%s) had an execution error: %s \" % (path, err))\n\n            # make sure script output is unicode so that json loader will output unicode strings itself\n            try:\n                data = to_text(stdout, errors=\"strict\")\n            except Exception as e:\n                raise AnsibleError(\"Inventory {0} contained characters that cannot be interpreted as UTF-8: {1}\".format(path, to_native(e)))\n\n            try:\n                processed = self.loader.load(data, json_only=True)\n            except Exception as e:\n                raise AnsibleError(\"failed to parse executable inventory script results from {0}: {1}\\n{2}\".format(path, to_native(e), err))\n\n            # if no other errors happened and you want to force displaying stderr, do so now\n            if stderr and self.get_option('always_show_stderr'):\n                self.display.error(msg=to_text(err))\n\n            if not isinstance(processed, Mapping):\n                raise AnsibleError(\"failed to parse executable inventory script results from {0}: needs to be a json dict\\n{1}\".format(path, err))\n\n            group = None\n            data_from_meta = None\n\n            # A \"_meta\" subelement may contain a variable \"hostvars\" which contains a hash for each host\n            # if this \"hostvars\" exists at all then do not call --host for each # host.\n            # This is for efficiency and scripts should still return data\n            # if called with --host for backwards compat with 1.2 and earlier.\n            for (group, gdata) in processed.items():\n                if group == '_meta':\n                    if 'hostvars' in gdata:\n                        data_from_meta = gdata['hostvars']\n                else:\n                    self._parse_group(group, gdata)\n\n            for host in self._hosts:\n                got = {}\n                if data_from_meta is None:\n                    got = self.get_host_variables(path, host)\n                else:\n                    try:\n                        got = data_from_meta.get(host, {})\n                    except AttributeError as e:\n                        raise AnsibleError(\"Improperly formatted host information for %s: %s\" % (host, to_native(e)), orig_exc=e)\n\n                self._populate_host_vars([host], got)\n\n        except Exception as e:\n            raise AnsibleParserError(to_native(e))",
        "begin_line": 91,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.script.InventoryModule._parse_group#171",
        "src_path": "lib/ansible/plugins/inventory/script.py",
        "class_name": "lib.ansible.plugins.inventory.script.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.script.InventoryModule._parse_group(self, group, data)",
        "snippet": "    def _parse_group(self, group, data):\n\n        group = self.inventory.add_group(group)\n\n        if not isinstance(data, dict):\n            data = {'hosts': data}\n        # is not those subkeys, then simplified syntax, host with vars\n        elif not any(k in data for k in ('hosts', 'vars', 'children')):\n            data = {'hosts': [group], 'vars': data}\n\n        if 'hosts' in data:\n            if not isinstance(data['hosts'], list):\n                raise AnsibleError(\"You defined a group '%s' with bad data for the host list:\\n %s\" % (group, data))\n\n            for hostname in data['hosts']:\n                self._hosts.add(hostname)\n                self.inventory.add_host(hostname, group)\n\n        if 'vars' in data:\n            if not isinstance(data['vars'], dict):\n                raise AnsibleError(\"You defined a group '%s' with bad data for variables:\\n %s\" % (group, data))\n\n            for k, v in iteritems(data['vars']):\n                self.inventory.set_variable(group, k, v)\n\n        if group != '_meta' and isinstance(data, dict) and 'children' in data:\n            for child_name in data['children']:\n                child_name = self.inventory.add_group(child_name)\n                self.inventory.add_child(group, child_name)",
        "begin_line": 171,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.script.InventoryModule.get_host_variables#201",
        "src_path": "lib/ansible/plugins/inventory/script.py",
        "class_name": "lib.ansible.plugins.inventory.script.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.script.InventoryModule.get_host_variables(self, path, host)",
        "snippet": "    def get_host_variables(self, path, host):\n        \"\"\" Runs <script> --host <hostname>, to determine additional host variables \"\"\"\n\n        cmd = [path, \"--host\", host]\n        try:\n            sp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        except OSError as e:\n            raise AnsibleError(\"problem running %s (%s)\" % (' '.join(cmd), e))\n        (out, err) = sp.communicate()\n        if out.strip() == '':\n            return {}\n        try:\n            return json_dict_bytes_to_unicode(self.loader.load(out, file_name=path))\n        except ValueError:\n            raise AnsibleError(\"could not parse post variable response: %s, %s\" % (cmd, out))",
        "begin_line": 201,
        "end_line": 215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.populate#36",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.populate(self, collected_facts=None)",
        "snippet": "    def populate(self, collected_facts=None):\n        network_facts = {}\n        ifconfig_path = self.module.get_bin_path('ifconfig')\n\n        if ifconfig_path is None:\n            return network_facts\n\n        route_path = self.module.get_bin_path('route')\n\n        if route_path is None:\n            return network_facts\n\n        default_ipv4, default_ipv6 = self.get_default_interfaces(route_path)\n        interfaces, ips = self.get_interfaces_info(ifconfig_path)\n        interfaces = self.detect_type_media(interfaces)\n\n        self.merge_default_interface(default_ipv4, interfaces, 'ipv4')\n        self.merge_default_interface(default_ipv6, interfaces, 'ipv6')\n        network_facts['interfaces'] = sorted(list(interfaces.keys()))\n\n        for iface in interfaces:\n            network_facts[iface] = interfaces[iface]\n\n        network_facts['default_ipv4'] = default_ipv4\n        network_facts['default_ipv6'] = default_ipv6\n        network_facts['all_ipv4_addresses'] = ips['all_ipv4_addresses']\n        network_facts['all_ipv6_addresses'] = ips['all_ipv6_addresses']\n\n        return network_facts",
        "begin_line": 36,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.detect_type_media#66",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.detect_type_media(self, interfaces)",
        "snippet": "    def detect_type_media(self, interfaces):\n        for iface in interfaces:\n            if 'media' in interfaces[iface]:\n                if 'ether' in interfaces[iface]['media'].lower():\n                    interfaces[iface]['type'] = 'ether'\n        return interfaces",
        "begin_line": 66,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_default_interfaces#73",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_default_interfaces(self, route_path)",
        "snippet": "    def get_default_interfaces(self, route_path):\n\n        # Use the commands:\n        #     route -n get default\n        #     route -n get -inet6 default\n        # to find out the default outgoing interface, address, and gateway\n\n        command = dict(v4=[route_path, '-n', 'get', 'default'],\n                       v6=[route_path, '-n', 'get', '-inet6', 'default'])\n\n        interface = dict(v4={}, v6={})\n\n        for v in 'v4', 'v6':\n\n            if v == 'v6' and not socket.has_ipv6:\n                continue\n            rc, out, err = self.module.run_command(command[v])\n            if not out:\n                # v6 routing may result in\n                #   RTNETLINK answers: Invalid argument\n                continue\n            for line in out.splitlines():\n                words = line.strip().split(': ')\n                # Collect output from route command\n                if len(words) > 1:\n                    if words[0] == 'interface':\n                        interface[v]['interface'] = words[1]\n                    if words[0] == 'gateway':\n                        interface[v]['gateway'] = words[1]\n                    # help pick the right interface address on OpenBSD\n                    if words[0] == 'if address':\n                        interface[v]['address'] = words[1]\n                    # help pick the right interface address on NetBSD\n                    if words[0] == 'local addr':\n                        interface[v]['address'] = words[1]\n\n        return interface['v4'], interface['v6']",
        "begin_line": 73,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_interfaces_info#111",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_interfaces_info(self, ifconfig_path, ifconfig_options='-a')",
        "snippet": "    def get_interfaces_info(self, ifconfig_path, ifconfig_options='-a'):\n        interfaces = {}\n        current_if = {}\n        ips = dict(\n            all_ipv4_addresses=[],\n            all_ipv6_addresses=[],\n        )\n        # FreeBSD, DragonflyBSD, NetBSD, OpenBSD and macOS all implicitly add '-a'\n        # when running the command 'ifconfig'.\n        # Solaris must explicitly run the command 'ifconfig -a'.\n        rc, out, err = self.module.run_command([ifconfig_path, ifconfig_options])\n\n        for line in out.splitlines():\n\n            if line:\n                words = line.split()\n\n                if words[0] == 'pass':\n                    continue\n                elif re.match(r'^\\S', line) and len(words) > 3:\n                    current_if = self.parse_interface_line(words)\n                    interfaces[current_if['device']] = current_if\n                elif words[0].startswith('options='):\n                    self.parse_options_line(words, current_if, ips)\n                elif words[0] == 'nd6':\n                    self.parse_nd6_line(words, current_if, ips)\n                elif words[0] == 'ether':\n                    self.parse_ether_line(words, current_if, ips)\n                elif words[0] == 'media:':\n                    self.parse_media_line(words, current_if, ips)\n                elif words[0] == 'status:':\n                    self.parse_status_line(words, current_if, ips)\n                elif words[0] == 'lladdr':\n                    self.parse_lladdr_line(words, current_if, ips)\n                elif words[0] == 'inet':\n                    self.parse_inet_line(words, current_if, ips)\n                elif words[0] == 'inet6':\n                    self.parse_inet6_line(words, current_if, ips)\n                elif words[0] == 'tunnel':\n                    self.parse_tunnel_line(words, current_if, ips)\n                else:\n                    self.parse_unknown_line(words, current_if, ips)\n\n        return interfaces, ips",
        "begin_line": 111,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_interface_line#156",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_interface_line(self, words)",
        "snippet": "    def parse_interface_line(self, words):\n        device = words[0][0:-1]\n        current_if = {'device': device, 'ipv4': [], 'ipv6': [], 'type': 'unknown'}\n        current_if['flags'] = self.get_options(words[1])\n        if 'LOOPBACK' in current_if['flags']:\n            current_if['type'] = 'loopback'\n        current_if['macaddress'] = 'unknown'    # will be overwritten later\n\n        if len(words) >= 5:  # Newer FreeBSD versions\n            current_if['metric'] = words[3]\n            current_if['mtu'] = words[5]\n        else:\n            current_if['mtu'] = words[3]\n\n        return current_if",
        "begin_line": 156,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_media_line#184",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_media_line(self, words, current_if, ips)",
        "snippet": "    def parse_media_line(self, words, current_if, ips):\n        # not sure if this is useful - we also drop information\n        current_if['media'] = words[1]\n        if len(words) > 2:\n            current_if['media_select'] = words[2]\n        if len(words) > 3:\n            current_if['media_type'] = words[3][1:]\n        if len(words) > 4:\n            current_if['media_options'] = self.get_options(words[4])",
        "begin_line": 184,
        "end_line": 192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_status_line#194",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_status_line(self, words, current_if, ips)",
        "snippet": "    def parse_status_line(self, words, current_if, ips):\n        current_if['status'] = words[1]",
        "begin_line": 194,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_inet_line#200",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_inet_line(self, words, current_if, ips)",
        "snippet": "    def parse_inet_line(self, words, current_if, ips):\n        # netbsd show aliases like this\n        #  lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 33184\n        #         inet 127.0.0.1 netmask 0xff000000\n        #         inet alias 127.1.1.1 netmask 0xff000000\n        if words[1] == 'alias':\n            del words[1]\n\n        address = {'address': words[1]}\n        # cidr style ip address (eg, 127.0.0.1/24) in inet line\n        # used in netbsd ifconfig -e output after 7.1\n        if '/' in address['address']:\n            ip_address, cidr_mask = address['address'].split('/')\n\n            address['address'] = ip_address\n\n            netmask_length = int(cidr_mask)\n            netmask_bin = (1 << 32) - (1 << 32 >> int(netmask_length))\n            address['netmask'] = socket.inet_ntoa(struct.pack('!L', netmask_bin))\n\n            if len(words) > 5:\n                address['broadcast'] = words[3]\n\n        else:\n            # deal with hex netmask\n            if re.match('([0-9a-f]){8}', words[3]) and len(words[3]) == 8:\n                words[3] = '0x' + words[3]\n            if words[3].startswith('0x'):\n                address['netmask'] = socket.inet_ntoa(struct.pack('!L', int(words[3], base=16)))\n            else:\n                # otherwise assume this is a dotted quad\n                address['netmask'] = words[3]\n        # calculate the network\n        address_bin = struct.unpack('!L', socket.inet_aton(address['address']))[0]\n        netmask_bin = struct.unpack('!L', socket.inet_aton(address['netmask']))[0]\n        address['network'] = socket.inet_ntoa(struct.pack('!L', address_bin & netmask_bin))\n        if 'broadcast' not in address:\n            # broadcast may be given or we need to calculate\n            if len(words) > 5:\n                address['broadcast'] = words[5]\n            else:\n                address['broadcast'] = socket.inet_ntoa(struct.pack('!L', address_bin | (~netmask_bin & 0xffffffff)))\n\n        # add to our list of addresses\n        if not words[1].startswith('127.'):\n            ips['all_ipv4_addresses'].append(address['address'])\n        current_if['ipv4'].append(address)",
        "begin_line": 200,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_inet6_line#248",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_inet6_line(self, words, current_if, ips)",
        "snippet": "    def parse_inet6_line(self, words, current_if, ips):\n        address = {'address': words[1]}\n\n        # using cidr style addresses, ala NetBSD ifconfig post 7.1\n        if '/' in address['address']:\n            ip_address, cidr_mask = address['address'].split('/')\n\n            address['address'] = ip_address\n            address['prefix'] = cidr_mask\n\n            if len(words) > 5:\n                address['scope'] = words[5]\n        else:\n            if (len(words) >= 4) and (words[2] == 'prefixlen'):\n                address['prefix'] = words[3]\n            if (len(words) >= 6) and (words[4] == 'scopeid'):\n                address['scope'] = words[5]\n\n        localhost6 = ['::1', '::1/128', 'fe80::1%lo0']\n        if address['address'] not in localhost6:\n            ips['all_ipv6_addresses'].append(address['address'])\n        current_if['ipv6'].append(address)",
        "begin_line": 248,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_unknown_line#274",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.parse_unknown_line(self, words, current_if, ips)",
        "snippet": "    def parse_unknown_line(self, words, current_if, ips):\n        # we are going to ignore unknown lines here - this may be\n        # a bad idea - but you can override it in your subclass\n        pass",
        "begin_line": 274,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_options#281",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.get_options(self, option_string)",
        "snippet": "    def get_options(self, option_string):\n        start = option_string.find('<') + 1\n        end = option_string.rfind('>')\n        if (start > 0) and (end > 0) and (end > start + 1):\n            option_csv = option_string[start:end]\n            return option_csv.split(',')\n        else:\n            return []",
        "begin_line": 281,
        "end_line": 288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.merge_default_interface#290",
        "src_path": "lib/ansible/module_utils/facts/network/generic_bsd.py",
        "class_name": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork",
        "signature": "lib.ansible.module_utils.facts.network.generic_bsd.GenericBsdIfconfigNetwork.merge_default_interface(self, defaults, interfaces, ip_type)",
        "snippet": "    def merge_default_interface(self, defaults, interfaces, ip_type):\n        if 'interface' not in defaults:\n            return\n        if not defaults['interface'] in interfaces:\n            return\n        ifinfo = interfaces[defaults['interface']]\n        # copy all the interface values across except addresses\n        for item in ifinfo:\n            if item != 'ipv4' and item != 'ipv6':\n                defaults[item] = ifinfo[item]\n\n        ipinfo = []\n        if 'address' in defaults:\n            ipinfo = [x for x in ifinfo[ip_type] if x['address'] == defaults['address']]\n\n        if len(ipinfo) == 0:\n            ipinfo = ifinfo[ip_type]\n\n        if len(ipinfo) > 0:\n            for item in ipinfo[0]:\n                defaults[item] = ipinfo[0][item]",
        "begin_line": 290,
        "end_line": 310,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.KeycloakToken.__init__#54",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.KeycloakToken",
        "signature": "lib.ansible.galaxy.token.KeycloakToken.__init__(self, access_token=None, auth_url=None, validate_certs=True)",
        "snippet": "    def __init__(self, access_token=None, auth_url=None, validate_certs=True):\n        self.access_token = access_token\n        self.auth_url = auth_url\n        self._token = None\n        self.validate_certs = validate_certs",
        "begin_line": 54,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.KeycloakToken.headers#92",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.KeycloakToken",
        "signature": "lib.ansible.galaxy.token.KeycloakToken.headers(self)",
        "snippet": "    def headers(self):\n        headers = {}\n        headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n        return headers",
        "begin_line": 92,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0055248618784530384,
            "pseudo_dstar_susp": 0.0024630541871921183,
            "pseudo_tarantula_susp": 0.004219409282700422,
            "pseudo_op2_susp": 0.0024630541871921183,
            "pseudo_barinel_susp": 0.004219409282700422
        }
    },
    {
        "name": "lib.ansible.galaxy.token.GalaxyToken.__init__#103",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.GalaxyToken",
        "signature": "lib.ansible.galaxy.token.GalaxyToken.__init__(self, token=None)",
        "snippet": "    def __init__(self, token=None):\n        self.b_file = to_bytes(C.GALAXY_TOKEN_PATH, errors='surrogate_or_strict')\n        # Done so the config file is only opened when set/get/save is called\n        self._config = None\n        self._token = token",
        "begin_line": 103,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011261261261261261,
            "pseudo_dstar_susp": 0.0014771048744460858,
            "pseudo_tarantula_susp": 0.0009225092250922509,
            "pseudo_op2_susp": 0.0014771048744460858,
            "pseudo_barinel_susp": 0.0009225092250922509
        }
    },
    {
        "name": "lib.ansible.galaxy.token.GalaxyToken.config#110",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.GalaxyToken",
        "signature": "lib.ansible.galaxy.token.GalaxyToken.config(self)",
        "snippet": "    def config(self):\n        if self._config is None:\n            self._config = self._read()\n\n        # Prioritise the token passed into the constructor\n        if self._token:\n            self._config['token'] = None if self._token is NoTokenSentinel else self._token\n\n        return self._config",
        "begin_line": 110,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.GalaxyToken._read#120",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.GalaxyToken",
        "signature": "lib.ansible.galaxy.token.GalaxyToken._read(self)",
        "snippet": "    def _read(self):\n        action = 'Opened'\n        if not os.path.isfile(self.b_file):\n            # token file not found, create and chomd u+rw\n            open(self.b_file, 'w').close()\n            os.chmod(self.b_file, S_IRUSR | S_IWUSR)  # owner has +rw\n            action = 'Created'\n\n        with open(self.b_file, 'r') as f:\n            config = yaml.safe_load(f)\n\n        display.vvv('%s %s' % (action, to_text(self.b_file)))\n\n        return config or {}",
        "begin_line": 120,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.GalaxyToken.get#139",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.GalaxyToken",
        "signature": "lib.ansible.galaxy.token.GalaxyToken.get(self)",
        "snippet": "    def get(self):\n        return self.config.get('token', None)",
        "begin_line": 139,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.028259473346178e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.GalaxyToken.headers#146",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.GalaxyToken",
        "signature": "lib.ansible.galaxy.token.GalaxyToken.headers(self)",
        "snippet": "    def headers(self):\n        headers = {}\n        token = self.get()\n        if token:\n            headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n        return headers",
        "begin_line": 146,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0035460992907801418,
            "pseudo_dstar_susp": 0.0024330900243309003,
            "pseudo_tarantula_susp": 0.003134796238244514,
            "pseudo_op2_susp": 0.0024330900243309003,
            "pseudo_barinel_susp": 0.003134796238244514
        }
    },
    {
        "name": "lib.ansible.galaxy.token.BasicAuthToken.__init__#157",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.BasicAuthToken",
        "signature": "lib.ansible.galaxy.token.BasicAuthToken.__init__(self, username, password=None)",
        "snippet": "    def __init__(self, username, password=None):\n        self.username = username\n        self.password = password\n        self._token = None",
        "begin_line": 157,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.BasicAuthToken._encode_token#163",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.BasicAuthToken",
        "signature": "lib.ansible.galaxy.token.BasicAuthToken._encode_token(username, password)",
        "snippet": "    def _encode_token(username, password):\n        token = \"%s:%s\" % (to_text(username, errors='surrogate_or_strict'),\n                           to_text(password, errors='surrogate_or_strict', nonstring='passthru') or '')\n        b64_val = base64.b64encode(to_bytes(token, encoding='utf-8', errors='surrogate_or_strict'))\n        return to_text(b64_val)",
        "begin_line": 163,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.BasicAuthToken.get#169",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.BasicAuthToken",
        "signature": "lib.ansible.galaxy.token.BasicAuthToken.get(self)",
        "snippet": "    def get(self):\n        if self._token:\n            return self._token\n\n        self._token = self._encode_token(self.username, self.password)\n\n        return self._token",
        "begin_line": 169,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.galaxy.token.BasicAuthToken.headers#177",
        "src_path": "lib/ansible/galaxy/token.py",
        "class_name": "lib.ansible.galaxy.token.BasicAuthToken",
        "signature": "lib.ansible.galaxy.token.BasicAuthToken.headers(self)",
        "snippet": "    def headers(self):\n        headers = {}\n        headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n        return headers",
        "begin_line": 177,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.display#152",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.display(self, msg, color=None, stderr=False, screen_only=False, log_only=False, newline=True)",
        "snippet": "    def display(self, msg, color=None, stderr=False, screen_only=False, log_only=False, newline=True):\n        \"\"\" Display a message to the user\n\n        Note: msg *must* be a unicode string to prevent UnicodeError tracebacks.\n        \"\"\"\n\n        nocolor = msg\n\n        if not log_only:\n\n            has_newline = msg.endswith(u'\\n')\n            if has_newline:\n                msg2 = msg[:-1]\n            else:\n                msg2 = msg\n\n            if color:\n                msg2 = stringc(msg2, color)\n\n            if has_newline or newline:\n                msg2 = msg2 + u'\\n'\n\n            msg2 = to_bytes(msg2, encoding=self._output_encoding(stderr=stderr))\n            if sys.version_info >= (3,):\n                # Convert back to text string on python3\n                # We first convert to a byte string so that we get rid of\n                # characters that are invalid in the user's locale\n                msg2 = to_text(msg2, self._output_encoding(stderr=stderr), errors='replace')\n\n            # Note: After Display() class is refactored need to update the log capture\n            # code in 'bin/ansible-connection' (and other relevant places).\n            if not stderr:\n                fileobj = sys.stdout\n            else:\n                fileobj = sys.stderr\n\n            fileobj.write(msg2)\n\n            try:\n                fileobj.flush()\n            except IOError as e:\n                # Ignore EPIPE in case fileobj has been prematurely closed, eg.\n                # when piping to \"head -n1\"\n                if e.errno != errno.EPIPE:\n                    raise\n\n        if logger and not screen_only:\n            # We first convert to a byte string so that we get rid of\n            # color and characters that are invalid in the user's locale\n            msg2 = to_bytes(nocolor.lstrip(u'\\n'))\n\n            if sys.version_info >= (3,):\n                # Convert back to text string on python3\n                msg2 = to_text(msg2, self._output_encoding(stderr=stderr))\n\n            lvl = logging.INFO\n            if color:\n                # set logger level based on color (not great)\n                try:\n                    lvl = color_to_log_level[color]\n                except KeyError:\n                    # this should not happen, but JIC\n                    raise AnsibleAssertionError('Invalid color supplied to display: %s' % color)\n            # actually log\n            logger.log(lvl, msg2)",
        "begin_line": 152,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000651890482398957,
            "pseudo_dstar_susp": 0.000651890482398957,
            "pseudo_tarantula_susp": 0.0006527415143603133,
            "pseudo_op2_susp": 0.000651890482398957,
            "pseudo_barinel_susp": 0.0006527415143603133
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.v#218",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.v(self, msg, host=None)",
        "snippet": "    def v(self, msg, host=None):\n        return self.verbose(msg, host=host, caplevel=0)",
        "begin_line": 218,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002531645569620253,
            "pseudo_dstar_susp": 0.006756756756756757,
            "pseudo_tarantula_susp": 0.0016051364365971107,
            "pseudo_op2_susp": 0.006756756756756757,
            "pseudo_barinel_susp": 0.0016051364365971107
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.vv#221",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.vv(self, msg, host=None)",
        "snippet": "    def vv(self, msg, host=None):\n        return self.verbose(msg, host=host, caplevel=1)",
        "begin_line": 221,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021598272138228943,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.0015128593040847202,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.0015128593040847202
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.vvv#224",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.vvv(self, msg, host=None)",
        "snippet": "    def vvv(self, msg, host=None):\n        return self.verbose(msg, host=host, caplevel=2)",
        "begin_line": 224,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008741258741258741,
            "pseudo_dstar_susp": 0.0012300123001230013,
            "pseudo_tarantula_susp": 0.0007385524372230429,
            "pseudo_op2_susp": 0.0012300123001230013,
            "pseudo_barinel_susp": 0.0007385524372230429
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.vvvv#227",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.vvvv(self, msg, host=None)",
        "snippet": "    def vvvv(self, msg, host=None):\n        return self.verbose(msg, host=host, caplevel=3)",
        "begin_line": 227,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003246753246753247,
            "pseudo_dstar_susp": 0.0070921985815602835,
            "pseudo_tarantula_susp": 0.001658374792703151,
            "pseudo_op2_susp": 0.0070921985815602835,
            "pseudo_barinel_susp": 0.001658374792703151
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.vvvvv#230",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.vvvvv(self, msg, host=None)",
        "snippet": "    def vvvvv(self, msg, host=None):\n        return self.verbose(msg, host=host, caplevel=4)",
        "begin_line": 230,
        "end_line": 231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.033338022225348e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.debug#236",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.debug(self, msg, host=None)",
        "snippet": "    def debug(self, msg, host=None):\n        if C.DEFAULT_DEBUG:\n            if host is None:\n                self.display(\"%6d %0.5f: %s\" % (os.getpid(), time.time(), msg), color=C.COLOR_DEBUG)\n            else:\n                self.display(\"%6d %0.5f [%s]: %s\" % (os.getpid(), time.time(), host, msg), color=C.COLOR_DEBUG)",
        "begin_line": 236,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0032258064516129032,
            "pseudo_dstar_susp": 0.05,
            "pseudo_tarantula_susp": 0.0013531799729364006,
            "pseudo_op2_susp": 0.05,
            "pseudo_barinel_susp": 0.0013531799729364006
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.verbose#243",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.verbose(self, msg, host=None, caplevel=2)",
        "snippet": "    def verbose(self, msg, host=None, caplevel=2):\n\n        to_stderr = C.VERBOSE_TO_STDERR\n        if self.verbosity > caplevel:\n            if host is None:\n                self.display(msg, color=C.COLOR_VERBOSE, stderr=to_stderr)\n            else:\n                self.display(\"<%s> %s\" % (host, msg), color=C.COLOR_VERBOSE, stderr=to_stderr)",
        "begin_line": 243,
        "end_line": 250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005405405405405406,
            "pseudo_dstar_susp": 0.125,
            "pseudo_tarantula_susp": 0.0027397260273972603,
            "pseudo_op2_susp": 0.125,
            "pseudo_barinel_susp": 0.0027397260273972603
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.deprecated#252",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.deprecated(self, msg, version=None, removed=False)",
        "snippet": "    def deprecated(self, msg, version=None, removed=False):\n        ''' used to print out a deprecation message.'''\n\n        if not removed and not C.DEPRECATION_WARNINGS:\n            return\n\n        if not removed:\n            if version:\n                new_msg = \"[DEPRECATION WARNING]: %s. This feature will be removed in version %s.\" % (msg, version)\n            else:\n                new_msg = \"[DEPRECATION WARNING]: %s. This feature will be removed in a future release.\" % (msg)\n            new_msg = new_msg + \" Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg.\\n\\n\"\n        else:\n            raise AnsibleError(\"[DEPRECATED]: %s.\\nPlease update your playbooks.\" % msg)\n\n        wrapped = textwrap.wrap(new_msg, self.columns, drop_whitespace=False)\n        new_msg = \"\\n\".join(wrapped) + \"\\n\"\n\n        if new_msg not in self._deprecations:\n            self.display(new_msg.strip(), color=C.COLOR_DEPRECATE, stderr=True)\n            self._deprecations[new_msg] = 1",
        "begin_line": 252,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006770480704129993,
            "pseudo_dstar_susp": 0.0006770480704129993,
            "pseudo_tarantula_susp": 0.0006779661016949153,
            "pseudo_op2_susp": 0.0006770480704129993,
            "pseudo_barinel_susp": 0.0006779661016949153
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.warning#274",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.warning(self, msg, formatted=False)",
        "snippet": "    def warning(self, msg, formatted=False):\n\n        if not formatted:\n            new_msg = \"[WARNING]: %s\" % msg\n            wrapped = textwrap.wrap(new_msg, self.columns)\n            new_msg = \"\\n\".join(wrapped) + \"\\n\"\n        else:\n            new_msg = \"\\n[WARNING]: \\n%s\" % msg\n\n        if new_msg not in self._warns:\n            self.display(new_msg, color=C.COLOR_WARN, stderr=True)\n            self._warns[new_msg] = 1",
        "begin_line": 274,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017953321364452424,
            "pseudo_dstar_susp": 0.004048582995951417,
            "pseudo_tarantula_susp": 0.000984251968503937,
            "pseudo_op2_susp": 0.004048582995951417,
            "pseudo_barinel_susp": 0.000984251968503937
        }
    },
    {
        "name": "lib.ansible.utils.display.Display.error#326",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display.error(self, msg, wrap_text=True)",
        "snippet": "    def error(self, msg, wrap_text=True):\n        if wrap_text:\n            new_msg = u\"\\n[ERROR]: %s\" % msg\n            wrapped = textwrap.wrap(new_msg, self.columns)\n            new_msg = u\"\\n\".join(wrapped) + u\"\\n\"\n        else:\n            new_msg = u\"ERROR! %s\" % msg\n        if new_msg not in self._errors:\n            self.display(new_msg, color=C.COLOR_ERROR, stderr=True)\n            self._errors[new_msg] = 1",
        "begin_line": 326,
        "end_line": 335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.display.Display._output_encoding#394",
        "src_path": "lib/ansible/utils/display.py",
        "class_name": "lib.ansible.utils.display.Display",
        "signature": "lib.ansible.utils.display.Display._output_encoding(stderr=False)",
        "snippet": "    def _output_encoding(stderr=False):\n        encoding = locale.getpreferredencoding()\n        # https://bugs.python.org/issue6202\n        # Python2 hardcodes an obsolete value on Mac.  Use MacOSX defaults\n        # instead.\n        if encoding in ('mac-roman',):\n            encoding = 'utf-8'\n        return encoding",
        "begin_line": 394,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005652911249293386,
            "pseudo_dstar_susp": 0.0005652911249293386,
            "pseudo_tarantula_susp": 0.0005652911249293386,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0005652911249293386
        }
    },
    {
        "name": "lib.ansible.utils.color.parsecolor#71",
        "src_path": "lib/ansible/utils/color.py",
        "class_name": "lib.ansible.utils.color",
        "signature": "lib.ansible.utils.color.parsecolor(color)",
        "snippet": "def parsecolor(color):\n    \"\"\"SGR parameter string for the specified color name.\"\"\"\n    matches = re.match(r\"color(?P<color>[0-9]+)\"\n                       r\"|(?P<rgb>rgb(?P<red>[0-5])(?P<green>[0-5])(?P<blue>[0-5]))\"\n                       r\"|gray(?P<gray>[0-9]+)\", color)\n    if not matches:\n        return codeCodes[color]\n    if matches.group('color'):\n        return u'38;5;%d' % int(matches.group('color'))\n    if matches.group('rgb'):\n        return u'38;5;%d' % (16 + 36 * int(matches.group('red')) +\n                             6 * int(matches.group('green')) +\n                             int(matches.group('blue')))\n    if matches.group('gray'):\n        return u'38;5;%d' % (232 + int(matches.group('gray')))",
        "begin_line": 71,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.955070246209486e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.color.stringc#88",
        "src_path": "lib/ansible/utils/color.py",
        "class_name": "lib.ansible.utils.color",
        "signature": "lib.ansible.utils.color.stringc(text, color, wrap_nonvisible_chars=False)",
        "snippet": "def stringc(text, color, wrap_nonvisible_chars=False):\n    \"\"\"String in color.\"\"\"\n\n    if ANSIBLE_COLOR:\n        color_code = parsecolor(color)\n        fmt = u\"\\033[%sm%s\\033[0m\"\n        if wrap_nonvisible_chars:\n            # This option is provided for use in cases when the\n            # formatting of a command line prompt is needed, such as\n            # `ansible-console`. As said in `readline` sources:\n            # readline/display.c:321\n            # /* Current implementation:\n            #         \\001 (^A) start non-visible characters\n            #         \\002 (^B) end non-visible characters\n            #    all characters except \\001 and \\002 (following a \\001) are copied to\n            #    the returned string; all characters except those between \\001 and\n            #    \\002 are assumed to be `visible'. */\n            fmt = u\"\\001\\033[%sm\\002%s\\001\\033[0m\\002\"\n        return u\"\\n\".join([fmt % (color_code, t) for t in text.split(u'\\n')])\n    else:\n        return text",
        "begin_line": 88,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.955070246209486e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.PrependListAction.__init__#52",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers.PrependListAction",
        "signature": "lib.ansible.cli.arguments.option_helpers.PrependListAction.__init__(self, option_strings, dest, nargs=None, const=None, default=None, type=None, choices=None, required=False, help=None, metavar=None)",
        "snippet": "    def __init__(self, option_strings, dest, nargs=None, const=None, default=None, type=None,\n                 choices=None, required=False, help=None, metavar=None):\n        if nargs == 0:\n            raise ValueError('nargs for append actions must be > 0; if arg '\n                             'strings are not supplying the value to append, '\n                             'the append const action may be more appropriate')\n        if const is not None and nargs != argparse.OPTIONAL:\n            raise ValueError('nargs must be %r to supply const' % argparse.OPTIONAL)\n        super(PrependListAction, self).__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=nargs,\n            const=const,\n            default=default,\n            type=type,\n            choices=choices,\n            required=required,\n            help=help,\n            metavar=metavar\n        )",
        "begin_line": 52,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0045662100456621,
            "pseudo_dstar_susp": 0.02631578947368421,
            "pseudo_tarantula_susp": 0.002066115702479339,
            "pseudo_op2_susp": 0.02631578947368421,
            "pseudo_barinel_susp": 0.002066115702479339
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.PrependListAction.__call__#73",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers.PrependListAction",
        "signature": "lib.ansible.cli.arguments.option_helpers.PrependListAction.__call__(self, parser, namespace, values, option_string=None)",
        "snippet": "    def __call__(self, parser, namespace, values, option_string=None):\n        items = copy.copy(ensure_value(namespace, self.dest, []))\n        items[0:0] = values\n        setattr(namespace, self.dest, items)",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.ensure_value#79",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.ensure_value(namespace, name, value)",
        "snippet": "def ensure_value(namespace, name, value):\n    if getattr(namespace, name, None) is None:\n        setattr(namespace, name, value)\n    return getattr(namespace, name)",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.unfrack_path#88",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.unfrack_path(pathsep=False)",
        "snippet": "def unfrack_path(pathsep=False):\n    \"\"\"Turn an Option's data into a single path in Ansible locations\"\"\"\n    def inner(value):\n        if pathsep:\n            return [unfrackpath(x) for x in value.split(os.pathsep) if x]\n\n        if value == '-':\n            return value\n\n        return unfrackpath(value)\n    return inner",
        "begin_line": 88,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0037313432835820895,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.0018115942028985507,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.0018115942028985507
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.inner#90",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.inner(value)",
        "snippet": "    def inner(value):\n        if pathsep:\n            return [unfrackpath(x) for x in value.split(os.pathsep) if x]\n\n        if value == '-':\n            return value\n\n        return unfrackpath(value)",
        "begin_line": 90,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0034965034965034965,
            "pseudo_dstar_susp": 0.011904761904761904,
            "pseudo_tarantula_susp": 0.0017730496453900709,
            "pseudo_op2_susp": 0.011904761904761904,
            "pseudo_barinel_susp": 0.0017730496453900709
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers._git_repo_info#101",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers._git_repo_info(repo_path)",
        "snippet": "def _git_repo_info(repo_path):\n    \"\"\" returns a string containing git branch, commit id and commit date \"\"\"\n    result = None\n    if os.path.exists(repo_path):\n        # Check if the .git is a file. If it is a file, it means that we are in a submodule structure.\n        if os.path.isfile(repo_path):\n            try:\n                gitdir = yaml.safe_load(open(repo_path)).get('gitdir')\n                # There is a possibility the .git file to have an absolute path.\n                if os.path.isabs(gitdir):\n                    repo_path = gitdir\n                else:\n                    repo_path = os.path.join(repo_path[:-4], gitdir)\n            except (IOError, AttributeError):\n                return ''\n        with open(os.path.join(repo_path, \"HEAD\")) as f:\n            line = f.readline().rstrip(\"\\n\")\n            if line.startswith(\"ref:\"):\n                branch_path = os.path.join(repo_path, line[5:])\n            else:\n                branch_path = None\n        if branch_path and os.path.exists(branch_path):\n            branch = '/'.join(line.split('/')[2:])\n            with open(branch_path) as f:\n                commit = f.readline()[:10]\n        else:\n            # detached HEAD\n            commit = line[:10]\n            branch = 'detached HEAD'\n            branch_path = os.path.join(repo_path, \"HEAD\")\n\n        date = time.localtime(os.stat(branch_path).st_mtime)\n        if time.daylight == 0:\n            offset = time.timezone\n        else:\n            offset = time.altzone\n        result = \"({0} {1}) last updated {2} (GMT {3:+04d})\".format(branch, commit, time.strftime(\"%Y/%m/%d %H:%M:%S\", date), int(offset / -36))\n    else:\n        result = ''\n    return result",
        "begin_line": 101,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024330900243309003,
            "pseudo_dstar_susp": 0.006134969325153374,
            "pseudo_tarantula_susp": 0.001567398119122257,
            "pseudo_op2_susp": 0.006134969325153374,
            "pseudo_barinel_susp": 0.001567398119122257
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers._gitinfo#143",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers._gitinfo()",
        "snippet": "def _gitinfo():\n    basedir = os.path.join(os.path.dirname(__file__), '..', '..', '..')\n    repo_path = os.path.join(basedir, '.git')\n    result = _git_repo_info(repo_path)\n    submodules = os.path.join(basedir, '.gitmodules')\n\n    if not os.path.exists(submodules):\n        return result\n\n    with open(submodules) as f:\n        for line in f:\n            tokens = line.strip().split(' ')\n            if tokens[0] == 'path':\n                submodule_path = tokens[2]\n                submodule_info = _git_repo_info(os.path.join(basedir, submodule_path, '.git'))\n                if not submodule_info:\n                    submodule_info = ' not found - use git submodule update --init ' + submodule_path\n                result += \"\\n  {0}: {1}\".format(submodule_path, submodule_info)\n    return result",
        "begin_line": 143,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024330900243309003,
            "pseudo_dstar_susp": 0.006134969325153374,
            "pseudo_tarantula_susp": 0.001567398119122257,
            "pseudo_op2_susp": 0.006134969325153374,
            "pseudo_barinel_susp": 0.001567398119122257
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.version#164",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.version(prog=None)",
        "snippet": "def version(prog=None):\n    \"\"\" return ansible version \"\"\"\n    if prog:\n        result = \" \".join((prog, __version__))\n    else:\n        result = __version__\n\n    gitinfo = _gitinfo()\n    if gitinfo:\n        result = result + \" {0}\".format(gitinfo)\n    result += \"\\n  config file = %s\" % C.CONFIG_FILE\n    if C.DEFAULT_MODULE_PATH is None:\n        cpath = \"Default w/o overrides\"\n    else:\n        cpath = C.DEFAULT_MODULE_PATH\n    result = result + \"\\n  configured module search path = %s\" % cpath\n    result = result + \"\\n  ansible python module location = %s\" % ':'.join(ansible.__path__)\n    result = result + \"\\n  executable location = %s\" % sys.argv[0]\n    result = result + \"\\n  python version = %s\" % ''.join(sys.version.splitlines())\n    return result",
        "begin_line": 164,
        "end_line": 183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002531645569620253,
            "pseudo_dstar_susp": 0.006756756756756757,
            "pseudo_tarantula_susp": 0.0016051364365971107,
            "pseudo_op2_susp": 0.006756756756756757,
            "pseudo_barinel_susp": 0.0016051364365971107
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.create_base_parser#190",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.create_base_parser(prog, usage='', desc=None, epilog=None)",
        "snippet": "def create_base_parser(prog, usage=\"\", desc=None, epilog=None):\n    \"\"\"\n    Create an options parser for all ansible scripts\n    \"\"\"\n    # base opts\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        formatter_class=SortingHelpFormatter,\n        epilog=epilog,\n        description=desc,\n        conflict_handler='resolve',\n    )\n    version_help = \"show program's version number, config file location, configured module search path,\" \\\n                   \" module location, executable location and exit\"\n\n    parser.add_argument('--version', action=AnsibleVersion, nargs=0, help=version_help)\n    add_verbosity_options(parser)\n    return parser",
        "begin_line": 190,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0037313432835820895,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.0018115942028985507,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.0018115942028985507
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_verbosity_options#210",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_verbosity_options(parser)",
        "snippet": "def add_verbosity_options(parser):\n    \"\"\"Add options for verbosity\"\"\"\n    parser.add_argument('-v', '--verbose', dest='verbosity', default=C.DEFAULT_VERBOSITY, action=\"count\",\n                        help=\"verbose mode (-vvv for more, -vvvv to enable connection debugging)\")",
        "begin_line": 210,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0037313432835820895,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.0018115942028985507,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.0018115942028985507
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_async_options#216",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_async_options(parser)",
        "snippet": "def add_async_options(parser):\n    \"\"\"Add options for commands which can launch async tasks\"\"\"\n    parser.add_argument('-P', '--poll', default=C.DEFAULT_POLL_INTERVAL, type=int, dest='poll_interval',\n                        help=\"set the poll interval if using -B (default=%s)\" % C.DEFAULT_POLL_INTERVAL)\n    parser.add_argument('-B', '--background', dest='seconds', type=int, default=0,\n                        help='run asynchronously, failing after X seconds (default=N/A)')",
        "begin_line": 216,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.025,
            "pseudo_dstar_susp": 0.003663003663003663,
            "pseudo_tarantula_susp": 0.007633587786259542,
            "pseudo_op2_susp": 0.003663003663003663,
            "pseudo_barinel_susp": 0.007633587786259542
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_basedir_options#224",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_basedir_options(parser)",
        "snippet": "def add_basedir_options(parser):\n    \"\"\"Add options for commands which can set a playbook basedir\"\"\"\n    parser.add_argument('--playbook-dir', default=C.config.get_config_value('PLAYBOOK_DIR'), dest='basedir', action='store',\n                        help=\"Since this tool does not use playbooks, use this as a substitute playbook directory.\"\n                             \"This sets the relative path for many features including roles/ group_vars/ etc.\")",
        "begin_line": 224,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013513513513513514,
            "pseudo_dstar_susp": 0.0033444816053511705,
            "pseudo_tarantula_susp": 0.006289308176100629,
            "pseudo_op2_susp": 0.0033444816053511705,
            "pseudo_barinel_susp": 0.006289308176100629
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_check_options#231",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_check_options(parser)",
        "snippet": "def add_check_options(parser):\n    \"\"\"Add options for commands which can run with diagnostic information of tasks\"\"\"\n    parser.add_argument(\"-C\", \"--check\", default=False, dest='check', action='store_true',\n                        help=\"don't make any changes; instead, try to predict some of the changes that may occur\")\n    parser.add_argument('--syntax-check', dest='syntax', action='store_true',\n                        help=\"perform a syntax check on the playbook, but do not execute it\")\n    parser.add_argument(\"-D\", \"--diff\", default=C.DIFF_ALWAYS, dest='diff', action='store_true',\n                        help=\"when changing (small) files and templates, show the differences in those\"\n                             \" files; works great with --check\")",
        "begin_line": 231,
        "end_line": 239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_connect_options#242",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_connect_options(parser)",
        "snippet": "def add_connect_options(parser):\n    \"\"\"Add options for commands which need to connection to other hosts\"\"\"\n    connect_group = parser.add_argument_group(\"Connection Options\", \"control as whom and how to connect to hosts\")\n\n    connect_group.add_argument('-k', '--ask-pass', default=C.DEFAULT_ASK_PASS, dest='ask_pass', action='store_true',\n                               help='ask for connection password')\n    connect_group.add_argument('--private-key', '--key-file', default=C.DEFAULT_PRIVATE_KEY_FILE, dest='private_key_file',\n                               help='use this file to authenticate the connection', type=unfrack_path())\n    connect_group.add_argument('-u', '--user', default=C.DEFAULT_REMOTE_USER, dest='remote_user',\n                               help='connect as this user (default=%s)' % C.DEFAULT_REMOTE_USER)\n    connect_group.add_argument('-c', '--connection', dest='connection', default=C.DEFAULT_TRANSPORT,\n                               help=\"connection type to use (default=%s)\" % C.DEFAULT_TRANSPORT)\n    connect_group.add_argument('-T', '--timeout', default=C.DEFAULT_TIMEOUT, type=int, dest='timeout',\n                               help=\"override the connection timeout in seconds (default=%s)\" % C.DEFAULT_TIMEOUT)\n    connect_group.add_argument('--ssh-common-args', default='', dest='ssh_common_args',\n                               help=\"specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand)\")\n    connect_group.add_argument('--sftp-extra-args', default='', dest='sftp_extra_args',\n                               help=\"specify extra arguments to pass to sftp only (e.g. -f, -l)\")\n    connect_group.add_argument('--scp-extra-args', default='', dest='scp_extra_args',\n                               help=\"specify extra arguments to pass to scp only (e.g. -l)\")\n    connect_group.add_argument('--ssh-extra-args', default='', dest='ssh_extra_args',\n                               help=\"specify extra arguments to pass to ssh only (e.g. -R)\")\n\n    parser.add_argument_group(connect_group)",
        "begin_line": 242,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_fork_options#268",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_fork_options(parser)",
        "snippet": "def add_fork_options(parser):\n    \"\"\"Add options for commands that can fork worker processes\"\"\"\n    parser.add_argument('-f', '--forks', dest='forks', default=C.DEFAULT_FORKS, type=int,\n                        help=\"specify number of parallel processes to use (default=%s)\" % C.DEFAULT_FORKS)",
        "begin_line": 268,
        "end_line": 271,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_inventory_options#274",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_inventory_options(parser)",
        "snippet": "def add_inventory_options(parser):\n    \"\"\"Add options for commands that utilize inventory\"\"\"\n    parser.add_argument('-i', '--inventory', '--inventory-file', dest='inventory', action=\"append\",\n                        help=\"specify inventory host path or comma separated host list. --inventory-file is deprecated\")\n    parser.add_argument('--list-hosts', dest='listhosts', action='store_true',\n                        help='outputs a list of matching hosts; does not execute anything else')\n    parser.add_argument('-l', '--limit', default=C.DEFAULT_SUBSET, dest='subset',\n                        help='further limit selected hosts to an additional pattern')",
        "begin_line": 274,
        "end_line": 281,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_meta_options#284",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_meta_options(parser)",
        "snippet": "def add_meta_options(parser):\n    \"\"\"Add options for commands which can launch meta tasks from the command line\"\"\"\n    parser.add_argument('--force-handlers', default=C.DEFAULT_FORCE_HANDLERS, dest='force_handlers', action='store_true',\n                        help=\"run handlers even if a task fails\")\n    parser.add_argument('--flush-cache', dest='flush_cache', action='store_true',\n                        help=\"clear the fact cache for every host in inventory\")",
        "begin_line": 284,
        "end_line": 289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_module_options#292",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_module_options(parser)",
        "snippet": "def add_module_options(parser):\n    \"\"\"Add options for commands that load modules\"\"\"\n    module_path = C.config.get_configuration_definition('DEFAULT_MODULE_PATH').get('default', '')\n    parser.add_argument('-M', '--module-path', dest='module_path', default=None,\n                        help=\"prepend colon-separated path(s) to module library (default=%s)\" % module_path,\n                        type=unfrack_path(pathsep=True), action=PrependListAction)",
        "begin_line": 292,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_output_options#300",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_output_options(parser)",
        "snippet": "def add_output_options(parser):\n    \"\"\"Add options for commands which can change their output\"\"\"\n    parser.add_argument('-o', '--one-line', dest='one_line', action='store_true',\n                        help='condense output')\n    parser.add_argument('-t', '--tree', dest='tree', default=None,\n                        help='log output to this directory')",
        "begin_line": 300,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.025,
            "pseudo_dstar_susp": 0.003663003663003663,
            "pseudo_tarantula_susp": 0.007633587786259542,
            "pseudo_op2_susp": 0.003663003663003663,
            "pseudo_barinel_susp": 0.007633587786259542
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_runas_options#308",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_runas_options(parser)",
        "snippet": "def add_runas_options(parser):\n    \"\"\"\n    Add options for commands which can run tasks as another user\n\n    Note that this includes the options from add_runas_prompt_options().  Only one of these\n    functions should be used.\n    \"\"\"\n    runas_group = parser.add_argument_group(\"Privilege Escalation Options\", \"control how and which user you become as on target hosts\")\n\n    # consolidated privilege escalation (become)\n    runas_group.add_argument(\"-b\", \"--become\", default=C.DEFAULT_BECOME, action=\"store_true\", dest='become',\n                             help=\"run operations with become (does not imply password prompting)\")\n    runas_group.add_argument('--become-method', dest='become_method', default=C.DEFAULT_BECOME_METHOD,\n                             help=\"privilege escalation method to use (default=%(default)s), use \"\n                                  \"`ansible-doc -t become -l` to list valid choices.\")\n    runas_group.add_argument('--become-user', default=None, dest='become_user', type=str,\n                             help='run operations as this user (default=%s)' % C.DEFAULT_BECOME_USER)\n\n    add_runas_prompt_options(parser, runas_group=runas_group)",
        "begin_line": 308,
        "end_line": 326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_runas_prompt_options#329",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_runas_prompt_options(parser, runas_group=None)",
        "snippet": "def add_runas_prompt_options(parser, runas_group=None):\n    \"\"\"\n    Add options for commands which need to prompt for privilege escalation credentials\n\n    Note that add_runas_options() includes these options already.  Only one of the two functions\n    should be used.\n    \"\"\"\n    if runas_group is None:\n        runas_group = parser.add_argument_group(\"Privilege Escalation Options\",\n                                                \"control how and which user you become as on target hosts\")\n\n    runas_group.add_argument('-K', '--ask-become-pass', dest='become_ask_pass', action='store_true',\n                             default=C.DEFAULT_BECOME_ASK_PASS,\n                             help='ask for privilege escalation password')\n\n    parser.add_argument_group(runas_group)",
        "begin_line": 329,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.005291005291005291,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.005291005291005291
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_runtask_options#347",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_runtask_options(parser)",
        "snippet": "def add_runtask_options(parser):\n    \"\"\"Add options for commands that run a task\"\"\"\n    parser.add_argument('-e', '--extra-vars', dest=\"extra_vars\", action=\"append\",\n                        help=\"set additional variables as key=value or YAML/JSON, if filename prepend with @\", default=[])",
        "begin_line": 347,
        "end_line": 350,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01639344262295082,
            "pseudo_dstar_susp": 0.0034482758620689655,
            "pseudo_tarantula_susp": 0.006711409395973154,
            "pseudo_op2_susp": 0.0034482758620689655,
            "pseudo_barinel_susp": 0.006711409395973154
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_subset_options#353",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_subset_options(parser)",
        "snippet": "def add_subset_options(parser):\n    \"\"\"Add options for commands which can run a subset of tasks\"\"\"\n    parser.add_argument('-t', '--tags', dest='tags', default=C.TAGS_RUN, action='append',\n                        help=\"only run plays and tasks tagged with these values\")\n    parser.add_argument('--skip-tags', dest='skip_tags', default=C.TAGS_SKIP, action='append',\n                        help=\"only run plays and tasks whose tags do not match these values\")",
        "begin_line": 353,
        "end_line": 358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.arguments.option_helpers.add_vault_options#361",
        "src_path": "lib/ansible/cli/arguments/option_helpers.py",
        "class_name": "lib.ansible.cli.arguments.option_helpers",
        "signature": "lib.ansible.cli.arguments.option_helpers.add_vault_options(parser)",
        "snippet": "def add_vault_options(parser):\n    \"\"\"Add options for loading vault files\"\"\"\n    parser.add_argument('--vault-id', default=[], dest='vault_ids', action='append', type=str,\n                        help='the vault identity to use')\n    base_group = parser.add_mutually_exclusive_group()\n    base_group.add_argument('--ask-vault-password', '--ask-vault-pass', default=C.DEFAULT_ASK_VAULT_PASS, dest='ask_vault_pass', action='store_true',\n                            help='ask for vault password')\n    base_group.add_argument('--vault-password-file', '--vault-pass-file', default=[], dest='vault_password_files',\n                            help=\"vault password file\", type=unfrack_path(), action='append')",
        "begin_line": 361,
        "end_line": 369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0041841004184100415,
            "pseudo_dstar_susp": 0.00273224043715847,
            "pseudo_tarantula_susp": 0.002680965147453083,
            "pseudo_op2_susp": 0.00273224043715847,
            "pseudo_barinel_susp": 0.002680965147453083
        }
    },
    {
        "name": "lib.ansible.playbook.play_context.PlayContext.__init__#138",
        "src_path": "lib/ansible/playbook/play_context.py",
        "class_name": "lib.ansible.playbook.play_context.PlayContext",
        "signature": "lib.ansible.playbook.play_context.PlayContext.__init__(self, play=None, passwords=None, connection_lockfd=None)",
        "snippet": "    def __init__(self, play=None, passwords=None, connection_lockfd=None):\n        # Note: play is really not optional.  The only time it could be omitted is when we create\n        # a PlayContext just so we can invoke its deserialize method to load it from a serialized\n        # data source.\n\n        super(PlayContext, self).__init__()\n\n        if passwords is None:\n            passwords = {}\n\n        self.password = passwords.get('conn_pass', '')\n        self.become_pass = passwords.get('become_pass', '')\n\n        self._become_plugin = None\n\n        self.prompt = ''\n        self.success_key = ''\n\n        # a file descriptor to be used during locking operations\n        self.connection_lockfd = connection_lockfd\n\n        # set options before play to allow play to override them\n        if context.CLIARGS:\n            self.set_attributes_from_cli()\n\n        if play:\n            self.set_attributes_from_play(play)",
        "begin_line": 138,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play_context.PlayContext.set_attributes_from_play#177",
        "src_path": "lib/ansible/playbook/play_context.py",
        "class_name": "lib.ansible.playbook.play_context.PlayContext",
        "signature": "lib.ansible.playbook.play_context.PlayContext.set_attributes_from_play(self, play)",
        "snippet": "    def set_attributes_from_play(self, play):\n        self.force_handlers = play.force_handlers",
        "begin_line": 177,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play_context.PlayContext.set_attributes_from_cli#180",
        "src_path": "lib/ansible/playbook/play_context.py",
        "class_name": "lib.ansible.playbook.play_context.PlayContext",
        "signature": "lib.ansible.playbook.play_context.PlayContext.set_attributes_from_cli(self)",
        "snippet": "    def set_attributes_from_cli(self):\n        '''\n        Configures this connection information instance with data from\n        options specified by the user on the command line. These have a\n        lower precedence than those set on the play or host.\n        '''\n        if context.CLIARGS.get('timeout', False):\n            self.timeout = int(context.CLIARGS['timeout'])\n\n        # From the command line.  These should probably be used directly by plugins instead\n        # For now, they are likely to be moved to FieldAttribute defaults\n        self.private_key_file = context.CLIARGS.get('private_key_file')  # Else default\n        self.verbosity = context.CLIARGS.get('verbosity')  # Else default\n        self.ssh_common_args = context.CLIARGS.get('ssh_common_args')  # Else default\n        self.ssh_extra_args = context.CLIARGS.get('ssh_extra_args')  # Else default\n        self.sftp_extra_args = context.CLIARGS.get('sftp_extra_args')  # Else default\n        self.scp_extra_args = context.CLIARGS.get('scp_extra_args')  # Else default\n\n        # Not every cli that uses PlayContext has these command line args so have a default\n        self.start_at_task = context.CLIARGS.get('start_at_task', None)  # Else default",
        "begin_line": 180,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.playbook_executor.PlaybookExecutor.__init__#47",
        "src_path": "lib/ansible/executor/playbook_executor.py",
        "class_name": "lib.ansible.executor.playbook_executor.PlaybookExecutor",
        "signature": "lib.ansible.executor.playbook_executor.PlaybookExecutor.__init__(self, playbooks, inventory, variable_manager, loader, passwords)",
        "snippet": "    def __init__(self, playbooks, inventory, variable_manager, loader, passwords):\n        self._playbooks = playbooks\n        self._inventory = inventory\n        self._variable_manager = variable_manager\n        self._loader = loader\n        self.passwords = passwords\n        self._unreachable_hosts = dict()\n\n        if context.CLIARGS.get('listhosts') or context.CLIARGS.get('listtasks') or \\\n                context.CLIARGS.get('listtags') or context.CLIARGS.get('syntax'):\n            self._tqm = None\n        else:\n            self._tqm = TaskQueueManager(\n                inventory=inventory,\n                variable_manager=variable_manager,\n                loader=loader,\n                passwords=self.passwords,\n                forks=context.CLIARGS.get('forks'),\n            )\n\n        # Note: We run this here to cache whether the default ansible ssh\n        # executable supports control persist.  Sometime in the future we may\n        # need to enhance this to check that ansible_ssh_executable specified\n        # in inventory is also cached.  We can't do this caching at the point\n        # where it is used (in task_executor) because that is post-fork and\n        # therefore would be discarded after every task.\n        check_for_controlpersist(C.ANSIBLE_SSH_EXECUTABLE)",
        "begin_line": 47,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.playbook_executor.PlaybookExecutor._get_serialized_batches#250",
        "src_path": "lib/ansible/executor/playbook_executor.py",
        "class_name": "lib.ansible.executor.playbook_executor.PlaybookExecutor",
        "signature": "lib.ansible.executor.playbook_executor.PlaybookExecutor._get_serialized_batches(self, play)",
        "snippet": "    def _get_serialized_batches(self, play):\n        '''\n        Returns a list of hosts, subdivided into batches based on\n        the serial size specified in the play.\n        '''\n\n        # make sure we have a unique list of hosts\n        all_hosts = self._inventory.get_hosts(play.hosts, order=play.order)\n        all_hosts_len = len(all_hosts)\n\n        # the serial value can be listed as a scalar or a list of\n        # scalars, so we make sure it's a list here\n        serial_batch_list = play.serial\n        if len(serial_batch_list) == 0:\n            serial_batch_list = [-1]\n\n        cur_item = 0\n        serialized_batches = []\n\n        while len(all_hosts) > 0:\n            # get the serial value from current item in the list\n            serial = pct_to_int(serial_batch_list[cur_item], all_hosts_len)\n\n            # if the serial count was not specified or is invalid, default to\n            # a list of all hosts, otherwise grab a chunk of the hosts equal\n            # to the current serial item size\n            if serial <= 0:\n                serialized_batches.append(all_hosts)\n                break\n            else:\n                play_hosts = []\n                for x in range(serial):\n                    if len(all_hosts) > 0:\n                        play_hosts.append(all_hosts.pop(0))\n\n                serialized_batches.append(play_hosts)\n\n            # increment the current batch list item number, and if we've hit\n            # the end keep using the last element until we've consumed all of\n            # the hosts in the inventory\n            cur_item += 1\n            if cur_item > len(serial_batch_list) - 1:\n                cur_item = len(serial_batch_list) - 1\n\n        return serialized_batches",
        "begin_line": 250,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.__init__#32",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.__init__(self, host, task, return_data, task_fields=None)",
        "snippet": "    def __init__(self, host, task, return_data, task_fields=None):\n        self._host = host\n        self._task = task\n\n        if isinstance(return_data, dict):\n            self._result = return_data.copy()\n        else:\n            self._result = DataLoader().load(return_data)\n\n        if task_fields is None:\n            self._task_fields = dict()\n        else:\n            self._task_fields = task_fields",
        "begin_line": 32,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.is_changed#50",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.is_changed(self)",
        "snippet": "    def is_changed(self):\n        return self._check_key('changed')",
        "begin_line": 50,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.is_skipped#53",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.is_skipped(self)",
        "snippet": "    def is_skipped(self):\n        # loop results\n        if 'results' in self._result:\n            results = self._result['results']\n            # Loop tasks are only considered skipped if all items were skipped.\n            # some squashed results (eg, yum) are not dicts and can't be skipped individually\n            if results and all(isinstance(res, dict) and res.get('skipped', False) for res in results):\n                return True\n\n        # regular tasks and squashed non-dict results\n        return self._result.get('skipped', False)",
        "begin_line": 53,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.is_failed#65",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.is_failed(self)",
        "snippet": "    def is_failed(self):\n        if 'failed_when_result' in self._result or \\\n           'results' in self._result and True in [True for x in self._result['results'] if 'failed_when_result' in x]:\n            return self._check_key('failed_when_result')\n        else:\n            return self._check_key('failed')",
        "begin_line": 65,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.is_unreachable#72",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.is_unreachable(self)",
        "snippet": "    def is_unreachable(self):\n        return self._check_key('unreachable')",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.needs_debugger#75",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.needs_debugger(self, globally_enabled=False)",
        "snippet": "    def needs_debugger(self, globally_enabled=False):\n        _debugger = self._task_fields.get('debugger')\n        _ignore_errors = C.TASK_DEBUGGER_IGNORE_ERRORS and self._task_fields.get('ignore_errors')\n\n        ret = False\n        if globally_enabled and ((self.is_failed() and not _ignore_errors) or self.is_unreachable()):\n            ret = True\n\n        if _debugger in ('always',):\n            ret = True\n        elif _debugger in ('never',):\n            ret = False\n        elif _debugger in ('on_failed',) and self.is_failed() and not _ignore_errors:\n            ret = True\n        elif _debugger in ('on_unreachable',) and self.is_unreachable():\n            ret = True\n        elif _debugger in('on_skipped',) and self.is_skipped():\n            ret = True\n\n        return ret",
        "begin_line": 75,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult._check_key#96",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult._check_key(self, key)",
        "snippet": "    def _check_key(self, key):\n        '''get a specific key from the result or its items'''\n\n        if isinstance(self._result, dict) and key in self._result:\n            return self._result.get(key, False)\n        else:\n            flag = False\n            for res in self._result.get('results', []):\n                if isinstance(res, dict):\n                    flag |= res.get(key, False)\n            return flag",
        "begin_line": 96,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_result.TaskResult.clean_copy#108",
        "src_path": "lib/ansible/executor/task_result.py",
        "class_name": "lib.ansible.executor.task_result.TaskResult",
        "signature": "lib.ansible.executor.task_result.TaskResult.clean_copy(self)",
        "snippet": "    def clean_copy(self):\n\n        ''' returns 'clean' taskresult object '''\n\n        # FIXME: clean task_fields, _task and _host copies\n        result = TaskResult(self._host, self._task, {}, self._task_fields)\n\n        # statuses are already reflected on the event type\n        if result._task and result._task.action in ['debug']:\n            # debug is verbose by default to display vars, no need to add invocation\n            ignore = _IGNORE + ('invocation',)\n        else:\n            ignore = _IGNORE\n\n        subset = {}\n        # preserve subset for later\n        for sub in _SUB_PRESERVE:\n            if sub in self._result:\n                subset[sub] = {}\n                for key in _SUB_PRESERVE[sub]:\n                    if key in self._result[sub]:\n                        subset[sub][key] = self._result[sub][key]\n\n        if isinstance(self._task.no_log, bool) and self._task.no_log or self._result.get('_ansible_no_log', False):\n            x = {\"censored\": \"the output has been hidden due to the fact that 'no_log: true' was specified for this result\"}\n\n            # preserve full\n            for preserve in _PRESERVE:\n                if preserve in self._result:\n                    x[preserve] = self._result[preserve]\n\n            result._result = x\n        elif self._result:\n            result._result = module_response_deepcopy(self._result)\n\n            # actualy remove\n            for remove_key in ignore:\n                if remove_key in result._result:\n                    del result._result[remove_key]\n\n            # remove almost ALL internal keys, keep ones relevant to callback\n            strip_internal_keys(result._result, exceptions=CLEAN_EXCEPTIONS)\n\n        # keep subset\n        result._result.update(subset)\n\n        return result",
        "begin_line": 108,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.namespace.FactNamespace.__init__#33",
        "src_path": "lib/ansible/module_utils/facts/namespace.py",
        "class_name": "lib.ansible.module_utils.facts.namespace.FactNamespace",
        "signature": "lib.ansible.module_utils.facts.namespace.FactNamespace.__init__(self, namespace_name)",
        "snippet": "    def __init__(self, namespace_name):\n        self.namespace_name = namespace_name",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.namespace.PrefixFactNamespace.__init__#45",
        "src_path": "lib/ansible/module_utils/facts/namespace.py",
        "class_name": "lib.ansible.module_utils.facts.namespace.PrefixFactNamespace",
        "signature": "lib.ansible.module_utils.facts.namespace.PrefixFactNamespace.__init__(self, namespace_name, prefix=None)",
        "snippet": "    def __init__(self, namespace_name, prefix=None):\n        super(PrefixFactNamespace, self).__init__(namespace_name)\n        self.prefix = prefix",
        "begin_line": 45,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.host_list.InventoryModule.verify_file#39",
        "src_path": "lib/ansible/plugins/inventory/host_list.py",
        "class_name": "lib.ansible.plugins.inventory.host_list.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.host_list.InventoryModule.verify_file(self, host_list)",
        "snippet": "    def verify_file(self, host_list):\n\n        valid = False\n        b_path = to_bytes(host_list, errors='surrogate_or_strict')\n        if not os.path.exists(b_path) and ',' in host_list:\n            valid = True\n        return valid",
        "begin_line": 39,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.host_list.InventoryModule.parse#47",
        "src_path": "lib/ansible/plugins/inventory/host_list.py",
        "class_name": "lib.ansible.plugins.inventory.host_list.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.host_list.InventoryModule.parse(self, inventory, loader, host_list, cache=True)",
        "snippet": "    def parse(self, inventory, loader, host_list, cache=True):\n        ''' parses the inventory file '''\n\n        super(InventoryModule, self).parse(inventory, loader, host_list)\n\n        try:\n            for h in host_list.split(','):\n                h = h.strip()\n                if h:\n                    try:\n                        (host, port) = parse_address(h, allow_ranges=False)\n                    except AnsibleError as e:\n                        self.display.vvv(\"Unable to parse address from hostname, leaving unchanged: %s\" % to_text(e))\n                        host = h\n                        port = None\n\n                    if host not in self.inventory.hosts:\n                        self.inventory.add_host(host, group='ungrouped', port=port)\n        except Exception as e:\n            raise AnsibleParserError(\"Invalid data from string, could not parse: %s\" % to_native(e))",
        "begin_line": 47,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.become.sudo.BecomeModule.build_become_command#84",
        "src_path": "lib/ansible/plugins/become/sudo.py",
        "class_name": "lib.ansible.plugins.become.sudo.BecomeModule",
        "signature": "lib.ansible.plugins.become.sudo.BecomeModule.build_become_command(self, cmd, shell)",
        "snippet": "    def build_become_command(self, cmd, shell):\n        super(BecomeModule, self).build_become_command(cmd, shell)\n\n        if not cmd:\n            return cmd\n\n        becomecmd = self.get_option('become_exe') or self.name\n\n        flags = self.get_option('become_flags') or ''\n        prompt = ''\n        if self.get_option('become_pass'):\n            self.prompt = '[sudo via ansible, key=%s] password:' % self._id\n            if flags:  # this could be simplified, but kept as is for now for backwards string matching\n                flags = flags.replace('-n', '')\n            prompt = '-p \"%s\"' % (self.prompt)\n\n        user = self.get_option('become_user') or ''\n        if user:\n            user = '-u %s' % (user)\n\n        return ' '.join([becomecmd, flags, prompt, user, self._build_success_command(cmd, shell)])",
        "begin_line": 84,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.file.LookupModule.run#60",
        "src_path": "lib/ansible/plugins/lookup/file.py",
        "class_name": "lib.ansible.plugins.lookup.file.LookupModule",
        "signature": "lib.ansible.plugins.lookup.file.LookupModule.run(self, terms, variables=None, **kwargs)",
        "snippet": "    def run(self, terms, variables=None, **kwargs):\n\n        ret = []\n\n        for term in terms:\n            display.debug(\"File lookup term: %s\" % term)\n\n            # Find the file in the expected search path\n            lookupfile = self.find_file_in_search_path(variables, 'files', term)\n            display.vvvv(u\"File lookup using %s as file\" % lookupfile)\n            try:\n                if lookupfile:\n                    b_contents, show_data = self._loader._get_file_contents(lookupfile)\n                    contents = to_text(b_contents, errors='surrogate_or_strict')\n                    if kwargs.get('lstrip', False):\n                        contents = contents.lstrip()\n                    if kwargs.get('rstrip', True):\n                        contents = contents.rstrip()\n                    ret.append(contents)\n                else:\n                    raise AnsibleParserError()\n            except AnsibleParserError:\n                raise AnsibleError(\"could not locate file in lookup: %s\" % term)\n\n        return ret",
        "begin_line": 60,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.__init__#86",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.__init__(self)",
        "snippet": "    def __init__(self):\n        super(Play, self).__init__()\n\n        self._included_conditional = None\n        self._included_path = None\n        self._removed_hosts = []\n        self.ROLE_CACHE = {}\n\n        self.only_tags = set(context.CLIARGS.get('tags', [])) or frozenset(('all',))\n        self.skip_tags = set(context.CLIARGS.get('skip_tags', []))",
        "begin_line": 86,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.757951900698215e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.__repr__#97",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return self.get_name()",
        "begin_line": 97,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.get_name#100",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.get_name(self)",
        "snippet": "    def get_name(self):\n        ''' return the name of the Play '''\n        return self.name",
        "begin_line": 100,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.load#105",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.load(data, variable_manager=None, loader=None, vars=None)",
        "snippet": "    def load(data, variable_manager=None, loader=None, vars=None):\n        if ('name' not in data or data['name'] is None) and 'hosts' in data:\n            if data['hosts'] is None or all(host is None for host in data['hosts']):\n                raise AnsibleParserError(\"Hosts list cannot be empty - please check your playbook\")\n            if isinstance(data['hosts'], list):\n                data['name'] = ','.join(data['hosts'])\n            else:\n                data['name'] = data['hosts']\n        p = Play()\n        if vars:\n            p.vars = vars.copy()\n        return p.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 105,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.preprocess_data#118",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        '''\n        Adjusts play datastructure to cleanup old/legacy items\n        '''\n\n        if not isinstance(ds, dict):\n            raise AnsibleAssertionError('while preprocessing data (%s), ds should be a dict but was a %s' % (ds, type(ds)))\n\n        # The use of 'user' in the Play datastructure was deprecated to\n        # line up with the same change for Tasks, due to the fact that\n        # 'user' conflicted with the user module.\n        if 'user' in ds:\n            # this should never happen, but error out with a helpful message\n            # to the user if it does...\n            if 'remote_user' in ds:\n                raise AnsibleParserError(\"both 'user' and 'remote_user' are set for %s. \"\n                                         \"The use of 'user' is deprecated, and should be removed\" % self.get_name(), obj=ds)\n\n            ds['remote_user'] = ds['user']\n            del ds['user']\n\n        return super(Play, self).preprocess_data(ds)",
        "begin_line": 118,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._load_tasks#141",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._load_tasks(self, attr, ds)",
        "snippet": "    def _load_tasks(self, attr, ds):\n        '''\n        Loads a list of blocks from a list which may be mixed tasks/blocks.\n        Bare tasks outside of a block are given an implicit block.\n        '''\n        try:\n            return load_list_of_blocks(ds=ds, play=self, variable_manager=self._variable_manager, loader=self._loader)\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading tasks: %s\" % to_native(e), obj=self._ds, orig_exc=e)",
        "begin_line": 141,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._load_pre_tasks#151",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._load_pre_tasks(self, attr, ds)",
        "snippet": "    def _load_pre_tasks(self, attr, ds):\n        '''\n        Loads a list of blocks from a list which may be mixed tasks/blocks.\n        Bare tasks outside of a block are given an implicit block.\n        '''\n        try:\n            return load_list_of_blocks(ds=ds, play=self, variable_manager=self._variable_manager, loader=self._loader)\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading pre_tasks\", obj=self._ds, orig_exc=e)",
        "begin_line": 151,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._load_post_tasks#161",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._load_post_tasks(self, attr, ds)",
        "snippet": "    def _load_post_tasks(self, attr, ds):\n        '''\n        Loads a list of blocks from a list which may be mixed tasks/blocks.\n        Bare tasks outside of a block are given an implicit block.\n        '''\n        try:\n            return load_list_of_blocks(ds=ds, play=self, variable_manager=self._variable_manager, loader=self._loader)\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading post_tasks\", obj=self._ds, orig_exc=e)",
        "begin_line": 161,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._load_handlers#171",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._load_handlers(self, attr, ds)",
        "snippet": "    def _load_handlers(self, attr, ds):\n        '''\n        Loads a list of blocks from a list which may be mixed handlers/blocks.\n        Bare handlers outside of a block are given an implicit block.\n        '''\n        try:\n            return self._extend_value(\n                self.handlers,\n                load_list_of_blocks(ds=ds, play=self, use_handlers=True, variable_manager=self._variable_manager, loader=self._loader),\n                prepend=True\n            )\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed block was encountered while loading handlers\", obj=self._ds, orig_exc=e)",
        "begin_line": 171,
        "end_line": 183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._load_roles#185",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._load_roles(self, attr, ds)",
        "snippet": "    def _load_roles(self, attr, ds):\n        '''\n        Loads and returns a list of RoleInclude objects from the datastructure\n        list of role definitions and creates the Role from those objects\n        '''\n\n        if ds is None:\n            ds = []\n\n        try:\n            role_includes = load_list_of_roles(ds, play=self, variable_manager=self._variable_manager,\n                                               loader=self._loader, collection_search_list=self.collections)\n        except AssertionError as e:\n            raise AnsibleParserError(\"A malformed role declaration was encountered.\", obj=self._ds, orig_exc=e)\n\n        roles = []\n        for ri in role_includes:\n            roles.append(Role.load(ri, play=self))\n\n        self.roles[:0] = roles\n\n        return self.roles",
        "begin_line": 185,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play._compile_roles#221",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play._compile_roles(self)",
        "snippet": "    def _compile_roles(self):\n        '''\n        Handles the role compilation step, returning a flat list of tasks\n        with the lowest level dependencies first. For example, if a role R\n        has a dependency D1, which also has a dependency D2, the tasks from\n        D2 are merged first, followed by D1, and lastly by the tasks from\n        the parent role R last. This is done for all roles in the Play.\n        '''\n\n        block_list = []\n\n        if len(self.roles) > 0:\n            for r in self.roles:\n                # Don't insert tasks from ``import/include_role``, preventing\n                # duplicate execution at the wrong time\n                if r.from_include:\n                    continue\n                block_list.extend(r.compile(play=self))\n\n        return block_list",
        "begin_line": 221,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.compile#258",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.compile(self)",
        "snippet": "    def compile(self):\n        '''\n        Compiles and returns the task list for this play, compiled from the\n        roles (which are themselves compiled recursively) and/or the list of\n        tasks specified in the play.\n        '''\n\n        # create a block containing a single flush handlers meta\n        # task, so we can be sure to run handlers at certain points\n        # of the playbook execution\n        flush_block = Block.load(\n            data={'meta': 'flush_handlers'},\n            play=self,\n            variable_manager=self._variable_manager,\n            loader=self._loader\n        )\n\n        block_list = []\n\n        block_list.extend(self.pre_tasks)\n        block_list.append(flush_block)\n        block_list.extend(self._compile_roles())\n        block_list.extend(self.tasks)\n        block_list.append(flush_block)\n        block_list.extend(self.post_tasks)\n        block_list.append(flush_block)\n\n        return block_list",
        "begin_line": 258,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.get_vars#287",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.get_vars(self)",
        "snippet": "    def get_vars(self):\n        return self.vars.copy()",
        "begin_line": 287,
        "end_line": 288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.get_vars_files#290",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.get_vars_files(self)",
        "snippet": "    def get_vars_files(self):\n        if self.vars_files is None:\n            return []\n        elif not isinstance(self.vars_files, list):\n            return [self.vars_files]\n        return self.vars_files",
        "begin_line": 290,
        "end_line": 295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.play.Play.get_roles#300",
        "src_path": "lib/ansible/playbook/play.py",
        "class_name": "lib.ansible.playbook.play.Play",
        "signature": "lib.ansible.playbook.play.Play.get_roles(self)",
        "snippet": "    def get_roles(self):\n        return self.roles[:]",
        "begin_line": 300,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.__init__#54",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.__init__(self, block=None, role=None, task_include=None)",
        "snippet": "    def __init__(self, block=None, role=None, task_include=None):\n        super(TaskInclude, self).__init__(block=block, role=role, task_include=task_include)\n        self.statically_loaded = False",
        "begin_line": 54,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.load#59",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None)",
        "snippet": "    def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n        ti = TaskInclude(block=block, role=role, task_include=task_include)\n        task = ti.load_data(data, variable_manager=variable_manager, loader=loader)\n\n        # Validate options\n        my_arg_names = frozenset(task.args.keys())\n\n        # validate bad args, otherwise we silently ignore\n        bad_opts = my_arg_names.difference(TaskInclude.VALID_ARGS)\n        if bad_opts and task.action in ('include_tasks', 'import_tasks'):\n            raise AnsibleParserError('Invalid options for %s: %s' % (task.action, ','.join(list(bad_opts))), obj=data)\n\n        if not task.args.get('_raw_params'):\n            task.args['_raw_params'] = task.args.pop('file', None)\n            if not task.args['_raw_params']:\n                raise AnsibleParserError('No file specified for %s' % task.action)\n\n        apply_attrs = task.args.get('apply', {})\n        if apply_attrs and task.action != 'include_tasks':\n            raise AnsibleParserError('Invalid options for %s: apply' % task.action, obj=data)\n        elif not isinstance(apply_attrs, dict):\n            raise AnsibleParserError('Expected a dict for apply but got %s instead' % type(apply_attrs), obj=data)\n\n        return task",
        "begin_line": 59,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.preprocess_data#84",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        ds = super(TaskInclude, self).preprocess_data(ds)\n\n        diff = set(ds.keys()).difference(self.VALID_INCLUDE_KEYWORDS)\n        for k in diff:\n            # This check doesn't handle ``include`` as we have no idea at this point if it is static or not\n            if ds[k] is not Sentinel and ds['action'] in ('include_tasks', 'include_role'):\n                if C.INVALID_TASK_ATTRIBUTE_FAILED:\n                    raise AnsibleParserError(\"'%s' is not a valid attribute for a %s\" % (k, self.__class__.__name__), obj=ds)\n                else:\n                    display.warning(\"Ignoring invalid attribute: %s\" % k)\n\n        return ds",
        "begin_line": 84,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.719027402547279e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.copy#98",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.copy(self, exclude_parent=False, exclude_tasks=False)",
        "snippet": "    def copy(self, exclude_parent=False, exclude_tasks=False):\n        new_me = super(TaskInclude, self).copy(exclude_parent=exclude_parent, exclude_tasks=exclude_tasks)\n        new_me.statically_loaded = self.statically_loaded\n        return new_me",
        "begin_line": 98,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.get_vars#103",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.get_vars(self)",
        "snippet": "    def get_vars(self):\n        '''\n        We override the parent Task() classes get_vars here because\n        we need to include the args of the include into the vars as\n        they are params to the included tasks. But ONLY for 'include'\n        '''\n        if self.action != 'include':\n            all_vars = super(TaskInclude, self).get_vars()\n        else:\n            all_vars = dict()\n            if self._parent:\n                all_vars.update(self._parent.get_vars())\n\n            all_vars.update(self.vars)\n            all_vars.update(self.args)\n\n            if 'tags' in all_vars:\n                del all_vars['tags']\n            if 'when' in all_vars:\n                del all_vars['when']\n\n        return all_vars",
        "begin_line": 103,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task_include.TaskInclude.build_parent_block#126",
        "src_path": "lib/ansible/playbook/task_include.py",
        "class_name": "lib.ansible.playbook.task_include.TaskInclude",
        "signature": "lib.ansible.playbook.task_include.TaskInclude.build_parent_block(self)",
        "snippet": "    def build_parent_block(self):\n        '''\n        This method is used to create the parent block for the included tasks\n        when ``apply`` is specified\n        '''\n        apply_attrs = self.args.pop('apply', {})\n        if apply_attrs:\n            apply_attrs['block'] = []\n            p_block = Block.load(\n                apply_attrs,\n                play=self._parent._play,\n                task_include=self,\n                role=self._role,\n                variable_manager=self._variable_manager,\n                loader=self._loader,\n            )\n        else:\n            p_block = self\n\n        return p_block",
        "begin_line": 126,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.AnsibleTomlEncoder.__init__#121",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml.AnsibleTomlEncoder",
        "signature": "lib.ansible.plugins.inventory.toml.AnsibleTomlEncoder.__init__(self, *args, **kwargs)",
        "snippet": "        def __init__(self, *args, **kwargs):\n            super(AnsibleTomlEncoder, self).__init__(*args, **kwargs)\n            # Map our custom YAML object types to dump_funcs from ``toml``\n            self.dump_funcs.update({\n                AnsibleSequence: self.dump_funcs.get(list),\n                AnsibleUnicode: self.dump_funcs.get(str),\n            })",
        "begin_line": 121,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.convert_yaml_objects_to_native#134",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml",
        "signature": "lib.ansible.plugins.inventory.toml.convert_yaml_objects_to_native(obj)",
        "snippet": "def convert_yaml_objects_to_native(obj):\n    \"\"\"Older versions of the ``toml`` python library, don't have a pluggable\n    way to tell the encoder about custom types, so we need to ensure objects\n    that we pass are native types.\n\n    Only used on ``toml<0.10.0`` where ``toml.TomlEncoder`` is missing.\n\n    This function recurses an object and ensures we cast any of the types from\n    ``ansible.parsing.yaml.objects`` into their native types, effectively cleansing\n    the data before we hand it over to ``toml``\n\n    This function doesn't directly check for the types from ``ansible.parsing.yaml.objects``\n    but instead checks for the types those objects inherit from, to offer more flexibility.\n    \"\"\"\n    if isinstance(obj, dict):\n        return dict((k, convert_yaml_objects_to_native(v)) for k, v in obj.items())\n    elif isinstance(obj, list):\n        return [convert_yaml_objects_to_native(v) for v in obj]\n    elif isinstance(obj, text_type):\n        return text_type(obj)\n    else:\n        return obj",
        "begin_line": 134,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.InventoryModule._parse_group#161",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.toml.InventoryModule._parse_group(self, group, group_data)",
        "snippet": "    def _parse_group(self, group, group_data):\n        if not isinstance(group_data, (MutableMapping, type(None))):\n            self.display.warning(\"Skipping '%s' as this is not a valid group definition\" % group)\n            return\n\n        group = self.inventory.add_group(group)\n        if group_data is None:\n            return\n\n        for key, data in group_data.items():\n            if key == 'vars':\n                if not isinstance(data, MutableMapping):\n                    raise AnsibleParserError(\n                        'Invalid \"vars\" entry for \"%s\" group, requires a dict, found \"%s\" instead.' %\n                        (group, type(data))\n                    )\n                for var, value in data.items():\n                    self.inventory.set_variable(group, var, value)\n\n            elif key == 'children':\n                if not isinstance(data, MutableSequence):\n                    raise AnsibleParserError(\n                        'Invalid \"children\" entry for \"%s\" group, requires a list, found \"%s\" instead.' %\n                        (group, type(data))\n                    )\n                for subgroup in data:\n                    self._parse_group(subgroup, {})\n                    self.inventory.add_child(group, subgroup)\n\n            elif key == 'hosts':\n                if not isinstance(data, MutableMapping):\n                    raise AnsibleParserError(\n                        'Invalid \"hosts\" entry for \"%s\" group, requires a dict, found \"%s\" instead.' %\n                        (group, type(data))\n                    )\n                for host_pattern, value in data.items():\n                    hosts, port = self._expand_hostpattern(host_pattern)\n                    self._populate_host_vars(hosts, value, group, port)\n            else:\n                self.display.warning(\n                    'Skipping unexpected key \"%s\" in group \"%s\", only \"vars\", \"children\" and \"hosts\" are valid' %\n                    (key, group)\n                )",
        "begin_line": 161,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.InventoryModule._load_file#205",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.toml.InventoryModule._load_file(self, file_name)",
        "snippet": "    def _load_file(self, file_name):\n        if not file_name or not isinstance(file_name, string_types):\n            raise AnsibleParserError(\"Invalid filename: '%s'\" % to_native(file_name))\n\n        b_file_name = to_bytes(self.loader.path_dwim(file_name))\n        if not self.loader.path_exists(b_file_name):\n            raise AnsibleFileNotFound(\"Unable to retrieve file contents\", file_name=file_name)\n\n        try:\n            (b_data, private) = self.loader._get_file_contents(file_name)\n            return toml.loads(to_text(b_data, errors='surrogate_or_strict'))\n        except toml.TomlDecodeError as e:\n            raise AnsibleParserError(\n                'TOML file (%s) is invalid: %s' % (file_name, to_native(e)),\n                orig_exc=e\n            )\n        except (IOError, OSError) as e:\n            raise AnsibleParserError(\n                \"An error occurred while trying to read the file '%s': %s\" % (file_name, to_native(e)),\n                orig_exc=e\n            )\n        except Exception as e:\n            raise AnsibleParserError(\n                \"An unexpected error occurred while parsing the file '%s': %s\" % (file_name, to_native(e)),\n                orig_exc=e\n            )",
        "begin_line": 205,
        "end_line": 230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.InventoryModule.parse#232",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.toml.InventoryModule.parse(self, inventory, loader, path, cache=True)",
        "snippet": "    def parse(self, inventory, loader, path, cache=True):\n        ''' parses the inventory file '''\n        if not HAS_TOML:\n            raise AnsibleParserError(\n                'The TOML inventory plugin requires the python \"toml\" library'\n            )\n\n        display.warning(WARNING_MSG)\n\n        super(InventoryModule, self).parse(inventory, loader, path)\n        self.set_options()\n\n        try:\n            data = self._load_file(path)\n        except Exception as e:\n            raise AnsibleParserError(e)\n\n        if not data:\n            raise AnsibleParserError('Parsed empty TOML file')\n        elif data.get('plugin'):\n            raise AnsibleParserError('Plugin configuration TOML file, not TOML inventory')\n\n        for group_name in data:\n            self._parse_group(group_name, data[group_name])",
        "begin_line": 232,
        "end_line": 255,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.toml.InventoryModule.verify_file#257",
        "src_path": "lib/ansible/plugins/inventory/toml.py",
        "class_name": "lib.ansible.plugins.inventory.toml.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.toml.InventoryModule.verify_file(self, path)",
        "snippet": "    def verify_file(self, path):\n        if super(InventoryModule, self).verify_file(path):\n            file_name, ext = os.path.splitext(path)\n            if ext == '.toml':\n                return True\n        return False",
        "begin_line": 257,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.__init__.LookupBase.__init__#35",
        "src_path": "lib/ansible/plugins/lookup/__init__.py",
        "class_name": "lib.ansible.plugins.lookup.__init__.LookupBase",
        "signature": "lib.ansible.plugins.lookup.__init__.LookupBase.__init__(self, loader=None, templar=None, **kwargs)",
        "snippet": "    def __init__(self, loader=None, templar=None, **kwargs):\n\n        super(LookupBase, self).__init__()\n\n        self._loader = loader\n        self._templar = templar\n\n        # Backwards compat: self._display isn't really needed, just import the global display and use that.\n        self._display = display",
        "begin_line": 35,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.671064743786438e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.__init__.LookupBase._flatten#52",
        "src_path": "lib/ansible/plugins/lookup/__init__.py",
        "class_name": "lib.ansible.plugins.lookup.__init__.LookupBase",
        "signature": "lib.ansible.plugins.lookup.__init__.LookupBase._flatten(terms)",
        "snippet": "    def _flatten(terms):\n        ret = []\n        for term in terms:\n            if isinstance(term, (list, tuple)):\n                ret.extend(term)\n            else:\n                ret.append(term)\n        return ret",
        "begin_line": 52,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common._utils.get_all_subclasses#14",
        "src_path": "lib/ansible/module_utils/common/_utils.py",
        "class_name": "lib.ansible.module_utils.common._utils",
        "signature": "lib.ansible.module_utils.common._utils.get_all_subclasses(cls)",
        "snippet": "def get_all_subclasses(cls):\n    '''\n    Recursively search and find all subclasses of a given class\n\n    :arg cls: A python class\n    :rtype: set\n    :returns: The set of python classes which are the subclasses of `cls`.\n\n    In python, you can use a class's :py:meth:`__subclasses__` method to determine what subclasses\n    of a class exist.  However, `__subclasses__` only goes one level deep.  This function searches\n    each child class's `__subclasses__` method to find all of the descendent classes.  It then\n    returns an iterable of the descendent classes.\n    '''\n    # Retrieve direct subclasses\n    subclasses = set(cls.__subclasses__())\n    to_visit = list(subclasses)\n    # Then visit all subclasses\n    while to_visit:\n        for sc in to_visit:\n            # The current class is now visited, so remove it from list\n            to_visit.remove(sc)\n            # Appending all subclasses to visit and keep a reference of available class\n            for ssc in sc.__subclasses__():\n                if ssc not in subclasses:\n                    to_visit.append(ssc)\n                    subclasses.add(ssc)\n    return subclasses",
        "begin_line": 14,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.local.Connection.__init__#43",
        "src_path": "lib/ansible/plugins/connection/local.py",
        "class_name": "lib.ansible.plugins.connection.local.Connection",
        "signature": "lib.ansible.plugins.connection.local.Connection.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n\n        super(Connection, self).__init__(*args, **kwargs)\n        self.cwd = None",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.__init__#91",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.__init__(self, block=None, role=None, task_include=None)",
        "snippet": "    def __init__(self, block=None, role=None, task_include=None):\n        ''' constructors a task, without the Task.load classmethod, it will be pretty blank '''\n\n        self._role = role\n        self._parent = None\n\n        if task_include:\n            self._parent = task_include\n        else:\n            self._parent = block\n\n        super(Task, self).__init__()",
        "begin_line": 91,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_path#104",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_path(self)",
        "snippet": "    def get_path(self):\n        ''' return the absolute path of the task with its line number '''\n\n        path = \"\"\n        if hasattr(self, '_ds') and hasattr(self._ds, '_data_source') and hasattr(self._ds, '_line_number'):\n            path = \"%s:%s\" % (self._ds._data_source, self._ds._line_number)\n        elif hasattr(self._parent._play, '_ds') and hasattr(self._parent._play._ds, '_data_source') and hasattr(self._parent._play._ds, '_line_number'):\n            path = \"%s:%s\" % (self._parent._play._ds._data_source, self._parent._play._ds._line_number)\n        return path",
        "begin_line": 104,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_name#114",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_name(self, include_role_fqcn=True)",
        "snippet": "    def get_name(self, include_role_fqcn=True):\n        ''' return the name of the task '''\n\n        if self._role:\n            role_name = self._role.get_name(include_role_fqcn=include_role_fqcn)\n\n        if self._role and self.name and role_name not in self.name:\n            return \"%s : %s\" % (role_name, self.name)\n        elif self.name:\n            return self.name\n        else:\n            if self._role:\n                return \"%s : %s\" % (role_name, self.action)\n            else:\n                return \"%s\" % (self.action,)",
        "begin_line": 114,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.load#145",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None)",
        "snippet": "    def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n        t = Task(block=block, role=role, task_include=task_include)\n        return t.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 145,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.18752246100769e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.__repr__#149",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.__repr__(self)",
        "snippet": "    def __repr__(self):\n        ''' returns a human readable representation of the task '''\n        if self.get_name() == 'meta':\n            return \"TASK: meta (%s)\" % self.args['_raw_params']\n        else:\n            return \"TASK: %s\" % self.get_name()",
        "begin_line": 149,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.preprocess_data#168",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.preprocess_data(self, ds)",
        "snippet": "    def preprocess_data(self, ds):\n        '''\n        tasks are especially complex arguments so need pre-processing.\n        keep it short.\n        '''\n\n        if not isinstance(ds, dict):\n            raise AnsibleAssertionError('ds (%s) should be a dict but was a %s' % (ds, type(ds)))\n\n        # the new, cleaned datastructure, which will have legacy\n        # items reduced to a standard structure suitable for the\n        # attributes of the task class\n        new_ds = AnsibleMapping()\n        if isinstance(ds, AnsibleBaseYAMLObject):\n            new_ds.ansible_pos = ds.ansible_pos\n\n        # since this affects the task action parsing, we have to resolve in preprocess instead of in typical validator\n        default_collection = AnsibleCollectionLoader().default_collection\n\n        collections_list = ds.get('collections')\n        if collections_list is None:\n            # use the parent value if our ds doesn't define it\n            collections_list = self.collections\n        else:\n            # Validate this untemplated field early on to guarantee we are dealing with a list.\n            # This is also done in CollectionSearch._load_collections() but this runs before that call.\n            collections_list = self.get_validated_value('collections', self._collections, collections_list, None)\n\n        if default_collection and not self._role:  # FIXME: and not a collections role\n            if collections_list:\n                if default_collection not in collections_list:\n                    collections_list.insert(0, default_collection)\n            else:\n                collections_list = [default_collection]\n\n        if collections_list and 'ansible.builtin' not in collections_list and 'ansible.legacy' not in collections_list:\n            collections_list.append('ansible.legacy')\n\n        if collections_list:\n            ds['collections'] = collections_list\n\n        # use the args parsing class to determine the action, args,\n        # and the delegate_to value from the various possible forms\n        # supported as legacy\n        args_parser = ModuleArgsParser(task_ds=ds, collection_list=collections_list)\n        try:\n            (action, args, delegate_to) = args_parser.parse()\n        except AnsibleParserError as e:\n            # if the raises exception was created with obj=ds args, then it includes the detail\n            # so we dont need to add it so we can just re raise.\n            if e._obj:\n                raise\n            # But if it wasn't, we can add the yaml object now to get more detail\n            raise AnsibleParserError(to_native(e), obj=ds, orig_exc=e)\n\n        # the command/shell/script modules used to support the `cmd` arg,\n        # which corresponds to what we now call _raw_params, so move that\n        # value over to _raw_params (assuming it is empty)\n        if action in ('command', 'shell', 'script'):\n            if 'cmd' in args:\n                if args.get('_raw_params', '') != '':\n                    raise AnsibleError(\"The 'cmd' argument cannot be used when other raw parameters are specified.\"\n                                       \" Please put everything in one or the other place.\", obj=ds)\n                args['_raw_params'] = args.pop('cmd')\n\n        new_ds['action'] = action\n        new_ds['args'] = args\n        new_ds['delegate_to'] = delegate_to\n\n        # we handle any 'vars' specified in the ds here, as we may\n        # be adding things to them below (special handling for includes).\n        # When that deprecated feature is removed, this can be too.\n        if 'vars' in ds:\n            # _load_vars is defined in Base, and is used to load a dictionary\n            # or list of dictionaries in a standard way\n            new_ds['vars'] = self._load_vars(None, ds.get('vars'))\n        else:\n            new_ds['vars'] = dict()\n\n        for (k, v) in iteritems(ds):\n            if k in ('action', 'local_action', 'args', 'delegate_to') or k == action or k == 'shell':\n                # we don't want to re-assign these values, which were determined by the ModuleArgsParser() above\n                continue\n            elif k.startswith('with_') and k.replace(\"with_\", \"\") in lookup_loader:\n                # transform into loop property\n                self._preprocess_with_loop(ds, new_ds, k, v)\n            else:\n                # pre-2.0 syntax allowed variables for include statements at the top level of the task,\n                # so we move those into the 'vars' dictionary here, and show a deprecation message\n                # as we will remove this at some point in the future.\n                if action in ('include',) and k not in self._valid_attrs and k not in self.DEPRECATED_ATTRIBUTES:\n                    display.deprecated(\"Specifying include variables at the top-level of the task is deprecated.\"\n                                       \" Please see:\\nhttps://docs.ansible.com/ansible/playbooks_roles.html#task-include-files-and-encouraging-reuse\\n\\n\"\n                                       \" for currently supported syntax regarding included files and variables\", version=\"2.12\")\n                    new_ds['vars'][k] = v\n                elif C.INVALID_TASK_ATTRIBUTE_FAILED or k in self._valid_attrs:\n                    new_ds[k] = v\n                else:\n                    display.warning(\"Ignoring invalid attribute: %s\" % k)\n\n        return super(Task, self).preprocess_data(new_ds)",
        "begin_line": 168,
        "end_line": 268,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task._validate_attributes#280",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task._validate_attributes(self, ds)",
        "snippet": "    def _validate_attributes(self, ds):\n        try:\n            super(Task, self)._validate_attributes(ds)\n        except AnsibleParserError as e:\n            e.message += '\\nThis error can be suppressed as a warning using the \"invalid_task_attribute_failed\" configuration'\n            raise e",
        "begin_line": 280,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.113387395077535e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_vars#370",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_vars(self)",
        "snippet": "    def get_vars(self):\n        all_vars = dict()\n        if self._parent:\n            all_vars.update(self._parent.get_vars())\n\n        all_vars.update(self.vars)\n\n        if 'tags' in all_vars:\n            del all_vars['tags']\n        if 'when' in all_vars:\n            del all_vars['when']\n\n        return all_vars",
        "begin_line": 370,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_include_params#384",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_include_params(self)",
        "snippet": "    def get_include_params(self):\n        all_vars = dict()\n        if self._parent:\n            all_vars.update(self._parent.get_include_params())\n        if self.action in ('include', 'include_tasks', 'include_role'):\n            all_vars.update(self.vars)\n        return all_vars",
        "begin_line": 384,
        "end_line": 390,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.copy#392",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.copy(self, exclude_parent=False, exclude_tasks=False)",
        "snippet": "    def copy(self, exclude_parent=False, exclude_tasks=False):\n        new_me = super(Task, self).copy()\n\n        new_me._parent = None\n        if self._parent and not exclude_parent:\n            new_me._parent = self._parent.copy(exclude_tasks=exclude_tasks)\n\n        new_me._role = None\n        if self._role:\n            new_me._role = self._role\n\n        return new_me",
        "begin_line": 392,
        "end_line": 403,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.set_loader#446",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.set_loader(self, loader)",
        "snippet": "    def set_loader(self, loader):\n        '''\n        Sets the loader on this object and recursively on parent, child objects.\n        This is used primarily after the Task has been serialized/deserialized, which\n        does not preserve the loader.\n        '''\n\n        self._loader = loader\n\n        if self._parent:\n            self._parent.set_loader(loader)",
        "begin_line": 446,
        "end_line": 456,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task._get_parent_attribute#458",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task._get_parent_attribute(self, attr, extend=False, prepend=False)",
        "snippet": "    def _get_parent_attribute(self, attr, extend=False, prepend=False):\n        '''\n        Generic logic to get the attribute or parent attribute for a task value.\n        '''\n\n        extend = self._valid_attrs[attr].extend\n        prepend = self._valid_attrs[attr].prepend\n        try:\n            value = self._attributes[attr]\n            # If parent is static, we can grab attrs from the parent\n            # otherwise, defer to the grandparent\n            if getattr(self._parent, 'statically_loaded', True):\n                _parent = self._parent\n            else:\n                _parent = self._parent._parent\n\n            if _parent and (value is Sentinel or extend):\n                if getattr(_parent, 'statically_loaded', True):\n                    # vars are always inheritable, other attributes might not be for the parent but still should be for other ancestors\n                    if attr != 'vars' and hasattr(_parent, '_get_parent_attribute'):\n                        parent_value = _parent._get_parent_attribute(attr)\n                    else:\n                        parent_value = _parent._attributes.get(attr, Sentinel)\n\n                    if extend:\n                        value = self._extend_value(value, parent_value, prepend)\n                    else:\n                        value = parent_value\n        except KeyError:\n            pass\n\n        return value",
        "begin_line": 458,
        "end_line": 489,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_dep_chain#491",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_dep_chain(self)",
        "snippet": "    def get_dep_chain(self):\n        if self._parent:\n            return self._parent.get_dep_chain()\n        else:\n            return None",
        "begin_line": 491,
        "end_line": 495,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.get_search_path#497",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.get_search_path(self)",
        "snippet": "    def get_search_path(self):\n        '''\n        Return the list of paths you should search for files, in order.\n        This follows role/playbook dependency chain.\n        '''\n        path_stack = []\n\n        dep_chain = self.get_dep_chain()\n        # inside role: add the dependency chain from current to dependent\n        if dep_chain:\n            path_stack.extend(reversed([x._role_path for x in dep_chain]))\n\n        # add path of task itself, unless it is already in the list\n        task_dir = os.path.dirname(self.get_path())\n        if task_dir not in path_stack:\n            path_stack.append(task_dir)\n\n        return path_stack",
        "begin_line": 497,
        "end_line": 514,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.task.Task.all_parents_static#516",
        "src_path": "lib/ansible/playbook/task.py",
        "class_name": "lib.ansible.playbook.task.Task",
        "signature": "lib.ansible.playbook.task.Task.all_parents_static(self)",
        "snippet": "    def all_parents_static(self):\n        if self._parent:\n            return self._parent.all_parents_static()\n        return True",
        "begin_line": 516,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.utils.jsonify.jsonify#25",
        "src_path": "lib/ansible/parsing/utils/jsonify.py",
        "class_name": "lib.ansible.parsing.utils.jsonify",
        "signature": "lib.ansible.parsing.utils.jsonify.jsonify(result, format=False)",
        "snippet": "def jsonify(result, format=False):\n    ''' format JSON output (uncompressed or uncompressed) '''\n\n    if result is None:\n        return \"{}\"\n\n    indent = None\n    if format:\n        indent = 4\n\n    try:\n        return json.dumps(result, sort_keys=True, indent=indent, ensure_ascii=False)\n    except UnicodeDecodeError:\n        return json.dumps(result, sort_keys=True, indent=indent)",
        "begin_line": 25,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.ssh_functions.check_for_controlpersist#31",
        "src_path": "lib/ansible/utils/ssh_functions.py",
        "class_name": "lib.ansible.utils.ssh_functions",
        "signature": "lib.ansible.utils.ssh_functions.check_for_controlpersist(ssh_executable)",
        "snippet": "def check_for_controlpersist(ssh_executable):\n    try:\n        # If we've already checked this executable\n        return _HAS_CONTROLPERSIST[ssh_executable]\n    except KeyError:\n        pass\n\n    b_ssh_exec = to_bytes(ssh_executable, errors='surrogate_or_strict')\n    has_cp = True\n    try:\n        cmd = subprocess.Popen([b_ssh_exec, '-o', 'ControlPersist'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        (out, err) = cmd.communicate()\n        if b\"Bad configuration option\" in err or b\"Usage:\" in err:\n            has_cp = False\n    except OSError:\n        has_cp = False\n\n    _HAS_CONTROLPERSIST[ssh_executable] = has_cp\n    return has_cp",
        "begin_line": 31,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__._import_module#89",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__",
        "signature": "lib.ansible.module_utils.six.__init__._import_module(name)",
        "snippet": "def _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]",
        "begin_line": 89,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__._LazyDescr.__get__#100",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__._LazyDescr",
        "signature": "lib.ansible.module_utils.six.__init__._LazyDescr.__get__(self, obj, tp)",
        "snippet": "    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result",
        "begin_line": 100,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__.MovedAttribute._resolve#168",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__.MovedAttribute",
        "signature": "lib.ansible.module_utils.six.__init__.MovedAttribute._resolve(self)",
        "snippet": "    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)",
        "begin_line": 168,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__._SixMetaPathImporter.find_module#193",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__._SixMetaPathImporter",
        "signature": "lib.ansible.module_utils.six.__init__._SixMetaPathImporter.find_module(self, fullname, path=None)",
        "snippet": "    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None",
        "begin_line": 193,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010570824524312897,
            "pseudo_dstar_susp": 0.0010141987829614604,
            "pseudo_tarantula_susp": 0.0021551724137931034,
            "pseudo_op2_susp": 0.0010141987829614604,
            "pseudo_barinel_susp": 0.002150537634408602
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__.itervalues#594",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__",
        "signature": "lib.ansible.module_utils.six.__init__.itervalues(d, **kw)",
        "snippet": "    def itervalues(d, **kw):\n        return iter(d.values(**kw))",
        "begin_line": 594,
        "end_line": 595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__.iteritems#597",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__",
        "signature": "lib.ansible.module_utils.six.__init__.iteritems(d, **kw)",
        "snippet": "    def iteritems(d, **kw):\n        return iter(d.items(**kw))",
        "begin_line": 597,
        "end_line": 598,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005321979776476849,
            "pseudo_dstar_susp": 0.0005321979776476849,
            "pseudo_tarantula_susp": 0.0005321979776476849,
            "pseudo_op2_susp": 0.0005321979776476849,
            "pseudo_barinel_susp": 0.0005321979776476849
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__.b#636",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__",
        "signature": "lib.ansible.module_utils.six.__init__.b(s)",
        "snippet": "    def b(s):\n        return s.encode(\"latin-1\")",
        "begin_line": 636,
        "end_line": 637,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.six.__init__.reraise#698",
        "src_path": "lib/ansible/module_utils/six/__init__.py",
        "class_name": "lib.ansible.module_utils.six.__init__",
        "signature": "lib.ansible.module_utils.six.__init__.reraise(tp, value, tb=None)",
        "snippet": "    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n                raise value.with_traceback(tb)\n            raise value\n        finally:\n            value = None\n            tb = None",
        "begin_line": 698,
        "end_line": 707,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.adhoc.AdHocCLI.init_parser#28",
        "src_path": "lib/ansible/cli/adhoc.py",
        "class_name": "lib.ansible.cli.adhoc.AdHocCLI",
        "signature": "lib.ansible.cli.adhoc.AdHocCLI.init_parser(self)",
        "snippet": "    def init_parser(self):\n        ''' create an options parser for bin/ansible '''\n        super(AdHocCLI, self).init_parser(usage='%prog <host-pattern> [options]',\n                                          desc=\"Define and run a single task 'playbook' against\"\n                                          \" a set of hosts\",\n                                          epilog=\"Some modules do not make sense in Ad-Hoc (include,\"\n                                          \" meta, etc)\")\n\n        opt_help.add_runas_options(self.parser)\n        opt_help.add_inventory_options(self.parser)\n        opt_help.add_async_options(self.parser)\n        opt_help.add_output_options(self.parser)\n        opt_help.add_connect_options(self.parser)\n        opt_help.add_check_options(self.parser)\n        opt_help.add_runtask_options(self.parser)\n        opt_help.add_vault_options(self.parser)\n        opt_help.add_fork_options(self.parser)\n        opt_help.add_module_options(self.parser)\n        opt_help.add_basedir_options(self.parser)\n\n        # options unique to ansible ad-hoc\n        self.parser.add_argument('-a', '--args', dest='module_args',\n                                 help=\"module arguments\", default=C.DEFAULT_MODULE_ARGS)\n        self.parser.add_argument('-m', '--module-name', dest='module_name',\n                                 help=\"module name to execute (default=%s)\" % C.DEFAULT_MODULE_NAME,\n                                 default=C.DEFAULT_MODULE_NAME)\n        self.parser.add_argument('args', metavar='pattern', help='host pattern')",
        "begin_line": 28,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.025,
            "pseudo_dstar_susp": 0.003663003663003663,
            "pseudo_tarantula_susp": 0.007633587786259542,
            "pseudo_op2_susp": 0.003663003663003663,
            "pseudo_barinel_susp": 0.007633587786259542
        }
    },
    {
        "name": "lib.ansible.cli.adhoc.AdHocCLI.post_process_args#56",
        "src_path": "lib/ansible/cli/adhoc.py",
        "class_name": "lib.ansible.cli.adhoc.AdHocCLI",
        "signature": "lib.ansible.cli.adhoc.AdHocCLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        '''Post process and validate options for bin/ansible '''\n\n        options = super(AdHocCLI, self).post_process_args(options)\n\n        display.verbosity = options.verbosity\n        self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n\n        return options",
        "begin_line": 56,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.05,
            "pseudo_dstar_susp": 0.003952569169960474,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.cli.adhoc.AdHocCLI._play_ds#66",
        "src_path": "lib/ansible/cli/adhoc.py",
        "class_name": "lib.ansible.cli.adhoc.AdHocCLI",
        "signature": "lib.ansible.cli.adhoc.AdHocCLI._play_ds(self, pattern, async_val, poll)",
        "snippet": "    def _play_ds(self, pattern, async_val, poll):\n        check_raw = context.CLIARGS['module_name'] in C.MODULE_REQUIRE_ARGS\n\n        mytask = {'action': {'module': context.CLIARGS['module_name'], 'args': parse_kv(context.CLIARGS['module_args'], check_raw=check_raw)}}\n\n        # avoid adding to tasks that don't support it, unless set, then give user an error\n        if context.CLIARGS['module_name'] not in ('include_role', 'include_tasks') and any(frozenset((async_val, poll))):\n            mytask['async_val'] = async_val\n            mytask['poll'] = poll\n\n        return dict(\n            name=\"Ansible Ad-Hoc\",\n            hosts=pattern,\n            gather_facts='no',\n            tasks=[mytask])",
        "begin_line": 66,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005235602094240838,
            "pseudo_dstar_susp": 0.0013123359580052493,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0013123359580052493,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.cli.adhoc.AdHocCLI.run#82",
        "src_path": "lib/ansible/cli/adhoc.py",
        "class_name": "lib.ansible.cli.adhoc.AdHocCLI",
        "signature": "lib.ansible.cli.adhoc.AdHocCLI.run(self)",
        "snippet": "    def run(self):\n        ''' create and execute the single task playbook '''\n\n        super(AdHocCLI, self).run()\n\n        # only thing left should be host pattern\n        pattern = to_text(context.CLIARGS['args'], errors='surrogate_or_strict')\n\n        sshpass = None\n        becomepass = None\n\n        (sshpass, becomepass) = self.ask_passwords()\n        passwords = {'conn_pass': sshpass, 'become_pass': becomepass}\n\n        # get basic objects\n        loader, inventory, variable_manager = self._play_prereqs()\n\n        try:\n            hosts = self.get_host_list(inventory, context.CLIARGS['subset'], pattern)\n        except AnsibleError:\n            if context.CLIARGS['subset']:\n                raise\n            else:\n                hosts = []\n                display.warning(\"No hosts matched, nothing to do\")\n\n        if context.CLIARGS['listhosts']:\n            display.display('  hosts (%d):' % len(hosts))\n            for host in hosts:\n                display.display('    %s' % host)\n            return 0\n\n        if context.CLIARGS['module_name'] in C.MODULE_REQUIRE_ARGS and not context.CLIARGS['module_args']:\n            err = \"No argument passed to %s module\" % context.CLIARGS['module_name']\n            if pattern.endswith(\".yml\"):\n                err = err + ' (did you mean to run ansible-playbook?)'\n            raise AnsibleOptionsError(err)\n\n        # Avoid modules that don't work with ad-hoc\n        if context.CLIARGS['module_name'] in ('import_playbook',):\n            raise AnsibleOptionsError(\"'%s' is not a valid action for ad-hoc commands\"\n                                      % context.CLIARGS['module_name'])\n\n        play_ds = self._play_ds(pattern, context.CLIARGS['seconds'], context.CLIARGS['poll_interval'])\n        play = Play().load(play_ds, variable_manager=variable_manager, loader=loader)\n\n        # used in start callback\n        playbook = Playbook(loader)\n        playbook._entries.append(play)\n        playbook._file_name = '__adhoc_playbook__'\n\n        if self.callback:\n            cb = self.callback\n        elif context.CLIARGS['one_line']:\n            cb = 'oneline'\n        # Respect custom 'stdout_callback' only with enabled 'bin_ansible_callbacks'\n        elif C.DEFAULT_LOAD_CALLBACK_PLUGINS and C.DEFAULT_STDOUT_CALLBACK != 'default':\n            cb = C.DEFAULT_STDOUT_CALLBACK\n        else:\n            cb = 'minimal'\n\n        run_tree = False\n        if context.CLIARGS['tree']:\n            C.DEFAULT_CALLBACK_WHITELIST.append('tree')\n            C.TREE_DIR = context.CLIARGS['tree']\n            run_tree = True\n\n        # now create a task queue manager to execute the play\n        self._tqm = None\n        try:\n            self._tqm = TaskQueueManager(\n                inventory=inventory,\n                variable_manager=variable_manager,\n                loader=loader,\n                passwords=passwords,\n                stdout_callback=cb,\n                run_additional_callbacks=C.DEFAULT_LOAD_CALLBACK_PLUGINS,\n                run_tree=run_tree,\n                forks=context.CLIARGS['forks'],\n            )\n\n            self._tqm.load_callbacks()\n            self._tqm.send_callback('v2_playbook_on_start', playbook)\n\n            result = self._tqm.run(play)\n\n            self._tqm.send_callback('v2_playbook_on_stats', self._tqm._stats)\n        finally:\n            if self._tqm:\n                self._tqm.cleanup()\n            if loader:\n                loader.cleanup_all_tmp_files()\n\n        return result",
        "begin_line": 82,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011904761904761904,
            "pseudo_dstar_susp": 0.0014534883720930232,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0014534883720930232,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase.__init__#50",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase.__init__(self, task, connection, play_context, loader, templar, shared_loader_obj)",
        "snippet": "    def __init__(self, task, connection, play_context, loader, templar, shared_loader_obj):\n        self._task = task\n        self._connection = connection\n        self._play_context = play_context\n        self._loader = loader\n        self._templar = templar\n        self._shared_loader_obj = shared_loader_obj\n        self._cleanup_remote_tmp = False\n\n        self._supports_check_mode = True\n        self._supports_async = False\n\n        # interpreter discovery state\n        self._discovered_interpreter_key = None\n        self._discovered_interpreter = False\n        self._discovery_deprecation_warnings = []\n        self._discovery_warnings = []\n\n        # Backwards compat: self._display isn't really needed, just import the global display and use that.\n        self._display = display\n\n        self._used_interpreter = None",
        "begin_line": 50,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.784524365561264e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase.run#74",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase.run(self, tmp=None, task_vars=None)",
        "snippet": "    def run(self, tmp=None, task_vars=None):\n        \"\"\" Action Plugins should implement this method to perform their\n        tasks.  Everything else in this base class is a helper method for the\n        action plugin to do that.\n\n        :kwarg tmp: Deprecated parameter.  This is no longer used.  An action plugin that calls\n            another one and wants to use the same remote tmp for both should set\n            self._connection._shell.tmpdir rather than this parameter.\n        :kwarg task_vars: The variables (host vars, group vars, config vars,\n            etc) associated with this task.\n        :returns: dictionary of results from the module\n\n        Implementors of action modules may find the following variables especially useful:\n\n        * Module parameters.  These are stored in self._task.args\n        \"\"\"\n\n        result = {}\n\n        if tmp is not None:\n            result['warning'] = ['ActionModule.run() no longer honors the tmp parameter. Action'\n                                 ' plugins should set self._connection._shell.tmpdir to share'\n                                 ' the tmpdir']\n        del tmp\n\n        if self._task.async_val and not self._supports_async:\n            raise AnsibleActionFail('async is not supported for this task.')\n        elif self._play_context.check_mode and not self._supports_check_mode:\n            raise AnsibleActionSkip('check mode is not supported for this task.')\n        elif self._task.async_val and self._play_context.check_mode:\n            raise AnsibleActionFail('check mode and async cannot be used on same task.')\n\n        # Error if invalid argument is passed\n        if self._VALID_ARGS:\n            task_opts = frozenset(self._task.args.keys())\n            bad_opts = task_opts.difference(self._VALID_ARGS)\n            if bad_opts:\n                raise AnsibleActionFail('Invalid options for %s: %s' % (self._task.action, ','.join(list(bad_opts))))\n\n        if self._connection._shell.tmpdir is None and self._early_needs_tmp_path():\n            self._make_tmp_path()\n\n        return result",
        "begin_line": 74,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase.get_plugin_option#130",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase.get_plugin_option(self, plugin, option, default=None)",
        "snippet": "    def get_plugin_option(self, plugin, option, default=None):\n        \"\"\"Helper to get an option from a plugin without having to use\n        the try/except dance everywhere to set a default\n        \"\"\"\n        try:\n            return plugin.get_option(option)\n        except (AttributeError, KeyError):\n            return default",
        "begin_line": 130,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase.get_become_option#139",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase.get_become_option(self, option, default=None)",
        "snippet": "    def get_become_option(self, option, default=None):\n        return self.get_plugin_option(self._connection.become, option, default=default)",
        "begin_line": 139,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase.get_shell_option#145",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase.get_shell_option(self, option, default=None)",
        "snippet": "    def get_shell_option(self, option, default=None):\n        return self.get_plugin_option(self._connection._shell, option, default=default)",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._configure_module#155",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._configure_module(self, module_name, module_args, task_vars=None)",
        "snippet": "    def _configure_module(self, module_name, module_args, task_vars=None):\n        '''\n        Handles the loading and templating of the module code through the\n        modify_module() function.\n        '''\n        if task_vars is None:\n            task_vars = dict()\n\n        # Search module path(s) for named module.\n        for mod_type in self._connection.module_implementation_preferences:\n            # Check to determine if PowerShell modules are supported, and apply\n            # some fixes (hacks) to module name + args.\n            if mod_type == '.ps1':\n                # FIXME: This should be temporary and moved to an exec subsystem plugin where we can define the mapping\n                # for each subsystem.\n                win_collection = 'ansible.windows'\n\n                # async_status, win_stat, win_file, win_copy, and win_ping are not just like their\n                # python counterparts but they are compatible enough for our\n                # internal usage\n                if module_name in ('stat', 'file', 'copy', 'ping') and self._task.action != module_name:\n                    module_name = '%s.win_%s' % (win_collection, module_name)\n                elif module_name in ['async_status']:\n                    module_name = '%s.%s' % (win_collection, module_name)\n\n                # Remove extra quotes surrounding path parameters before sending to module.\n                if module_name.split('.')[-1] in ['win_stat', 'win_file', 'win_copy', 'slurp'] and module_args and \\\n                        hasattr(self._connection._shell, '_unquote'):\n                    for key in ('src', 'dest', 'path'):\n                        if key in module_args:\n                            module_args[key] = self._connection._shell._unquote(module_args[key])\n\n            module_path = self._shared_loader_obj.module_loader.find_plugin(module_name, mod_type, collection_list=self._task.collections)\n            if module_path:\n                break\n        else:  # This is a for-else: http://bit.ly/1ElPkyg\n            raise AnsibleError(\"The module %s was not found in configured module paths\" % (module_name))\n\n        # insert shared code and arguments into the module\n        final_environment = dict()\n        self._compute_environment_string(final_environment)\n\n        become_kwargs = {}\n        if self._connection.become:\n            become_kwargs['become'] = True\n            become_kwargs['become_method'] = self._connection.become.name\n            become_kwargs['become_user'] = self._connection.become.get_option('become_user',\n                                                                              playcontext=self._play_context)\n            become_kwargs['become_password'] = self._connection.become.get_option('become_pass',\n                                                                                  playcontext=self._play_context)\n            become_kwargs['become_flags'] = self._connection.become.get_option('become_flags',\n                                                                               playcontext=self._play_context)\n\n        # modify_module will exit early if interpreter discovery is required; re-run after if necessary\n        for dummy in (1, 2):\n            try:\n                (module_data, module_style, module_shebang) = modify_module(module_name, module_path, module_args, self._templar,\n                                                                            task_vars=task_vars,\n                                                                            module_compression=self._play_context.module_compression,\n                                                                            async_timeout=self._task.async_val,\n                                                                            environment=final_environment,\n                                                                            **become_kwargs)\n                break\n            except InterpreterDiscoveryRequiredError as idre:\n                self._discovered_interpreter = AnsibleUnsafeText(discover_interpreter(\n                    action=self,\n                    interpreter_name=idre.interpreter_name,\n                    discovery_mode=idre.discovery_mode,\n                    task_vars=task_vars))\n\n                # update the local task_vars with the discovered interpreter (which might be None);\n                # we'll propagate back to the controller in the task result\n                discovered_key = 'discovered_interpreter_%s' % idre.interpreter_name\n                # store in local task_vars facts collection for the retry and any other usages in this worker\n                if task_vars.get('ansible_facts') is None:\n                    task_vars['ansible_facts'] = {}\n                task_vars['ansible_facts'][discovered_key] = self._discovered_interpreter\n                # preserve this so _execute_module can propagate back to controller as a fact\n                self._discovered_interpreter_key = discovered_key\n\n        return (module_style, module_shebang, module_data, module_path)",
        "begin_line": 155,
        "end_line": 235,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._compute_environment_string#237",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._compute_environment_string(self, raw_environment_out=None)",
        "snippet": "    def _compute_environment_string(self, raw_environment_out=None):\n        '''\n        Builds the environment string to be used when executing the remote task.\n        '''\n\n        final_environment = dict()\n        if self._task.environment is not None:\n            environments = self._task.environment\n            if not isinstance(environments, list):\n                environments = [environments]\n\n            # The order of environments matters to make sure we merge\n            # in the parent's values first so those in the block then\n            # task 'win' in precedence\n            for environment in environments:\n                if environment is None or len(environment) == 0:\n                    continue\n                temp_environment = self._templar.template(environment)\n                if not isinstance(temp_environment, dict):\n                    raise AnsibleError(\"environment must be a dictionary, received %s (%s)\" % (temp_environment, type(temp_environment)))\n                # very deliberately using update here instead of combine_vars, as\n                # these environment settings should not need to merge sub-dicts\n                final_environment.update(temp_environment)\n\n        if len(final_environment) > 0:\n            final_environment = self._templar.template(final_environment)\n\n        if isinstance(raw_environment_out, dict):\n            raw_environment_out.clear()\n            raw_environment_out.update(final_environment)\n\n        return self._connection._shell.env_prefix(**final_environment)",
        "begin_line": 237,
        "end_line": 268,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._early_needs_tmp_path#270",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._early_needs_tmp_path(self)",
        "snippet": "    def _early_needs_tmp_path(self):\n        '''\n        Determines if a tmp path should be created before the action is executed.\n        '''\n\n        return getattr(self, 'TRANSFERS_FILES', False)",
        "begin_line": 270,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._get_admin_users#297",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._get_admin_users(self)",
        "snippet": "    def _get_admin_users(self):\n        '''\n        Returns a list of admin users that are configured for the current shell\n        plugin\n        '''\n\n        return self.get_shell_option('admin_users', ['root'])",
        "begin_line": 297,
        "end_line": 303,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._get_remote_user#305",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._get_remote_user(self)",
        "snippet": "    def _get_remote_user(self):\n        ''' consistently get the 'remote_user' for the action plugin '''\n        # TODO: use 'current user running ansible' as fallback when moving away from play_context\n        # pwd.getpwuid(os.getuid()).pw_name\n        remote_user = None\n        try:\n            remote_user = self._connection.get_option('remote_user')\n        except KeyError:\n            # plugin does not have remote_user option, fallback to default and/play_context\n            remote_user = getattr(self._connection, 'default_user', None) or self._play_context.remote_user\n        except AttributeError:\n            # plugin does not use config system, fallback to old play_context\n            remote_user = self._play_context.remote_user\n        return remote_user",
        "begin_line": 305,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._is_become_unprivileged#320",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._is_become_unprivileged(self)",
        "snippet": "    def _is_become_unprivileged(self):\n        '''\n        The user is not the same as the connection user and is not part of the\n        shell configured admin users\n        '''\n        # if we don't use become then we know we aren't switching to a\n        # different unprivileged user\n        if not self._connection.become:\n            return False\n\n        # if we use become and the user is not an admin (or same user) then\n        # we need to return become_unprivileged as True\n        admin_users = self._get_admin_users()\n        remote_user = self._get_remote_user()\n        become_user = self.get_become_option('become_user')\n        return bool(become_user and become_user not in admin_users + [remote_user])",
        "begin_line": 320,
        "end_line": 335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._make_tmp_path#337",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._make_tmp_path(self, remote_user=None)",
        "snippet": "    def _make_tmp_path(self, remote_user=None):\n        '''\n        Create and return a temporary path on a remote box.\n        '''\n\n        # Network connection plugins (network_cli, netconf, etc.) execute on the controller, rather than the remote host.\n        # As such, we want to avoid using remote_user for paths  as remote_user may not line up with the local user\n        # This is a hack and should be solved by more intelligent handling of remote_tmp in 2.7\n        if getattr(self._connection, '_remote_is_local', False):\n            tmpdir = C.DEFAULT_LOCAL_TMP\n        else:\n            # NOTE: shell plugins should populate this setting anyways, but they dont do remote expansion, which\n            # we need for 'non posix' systems like cloud-init and solaris\n            tmpdir = self._remote_expand_user(self.get_shell_option('remote_tmp', default='~/.ansible/tmp'), sudoable=False)\n\n        become_unprivileged = self._is_become_unprivileged()\n        basefile = self._connection._shell._generate_temp_dir_name()\n        cmd = self._connection._shell.mkdtemp(basefile=basefile, system=become_unprivileged, tmpdir=tmpdir)\n        result = self._low_level_execute_command(cmd, sudoable=False)\n\n        # error handling on this seems a little aggressive?\n        if result['rc'] != 0:\n            if result['rc'] == 5:\n                output = 'Authentication failure.'\n            elif result['rc'] == 255 and self._connection.transport in ('ssh',):\n\n                if self._play_context.verbosity > 3:\n                    output = u'SSH encountered an unknown error. The output was:\\n%s%s' % (result['stdout'], result['stderr'])\n                else:\n                    output = (u'SSH encountered an unknown error during the connection. '\n                              'We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help diagnose the issue')\n\n            elif u'No space left on device' in result['stderr']:\n                output = result['stderr']\n            else:\n                output = ('Failed to create temporary directory.'\n                          'In some cases, you may have been able to authenticate and did not have permissions on the target directory. '\n                          'Consider changing the remote tmp path in ansible.cfg to a path rooted in \"/tmp\", for more error information use -vvv. '\n                          'Failed command was: %s, exited with result %d' % (cmd, result['rc']))\n            if 'stdout' in result and result['stdout'] != u'':\n                output = output + u\", stdout output: %s\" % result['stdout']\n            if self._play_context.verbosity > 3 and 'stderr' in result and result['stderr'] != u'':\n                output += u\", stderr output: %s\" % result['stderr']\n            raise AnsibleConnectionFailure(output)\n        else:\n            self._cleanup_remote_tmp = True\n\n        try:\n            stdout_parts = result['stdout'].strip().split('%s=' % basefile, 1)\n            rc = self._connection._shell.join_path(stdout_parts[-1], u'').splitlines()[-1]\n        except IndexError:\n            # stdout was empty or just space, set to / to trigger error in next if\n            rc = '/'\n\n        # Catch failure conditions, files should never be\n        # written to locations in /.\n        if rc == '/':\n            raise AnsibleError('failed to resolve remote temporary directory from %s: `%s` returned empty string' % (basefile, cmd))\n\n        self._connection._shell.tmpdir = rc\n\n        return rc",
        "begin_line": 337,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._should_remove_tmp_path#400",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._should_remove_tmp_path(self, tmp_path)",
        "snippet": "    def _should_remove_tmp_path(self, tmp_path):\n        '''Determine if temporary path should be deleted or kept by user request/config'''\n        return tmp_path and self._cleanup_remote_tmp and not C.DEFAULT_KEEP_REMOTE_FILES and \"-tmp-\" in tmp_path",
        "begin_line": 400,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._remove_tmp_path#404",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._remove_tmp_path(self, tmp_path)",
        "snippet": "    def _remove_tmp_path(self, tmp_path):\n        '''Remove a temporary path we created. '''\n\n        if tmp_path is None and self._connection._shell.tmpdir:\n            tmp_path = self._connection._shell.tmpdir\n\n        if self._should_remove_tmp_path(tmp_path):\n            cmd = self._connection._shell.remove(tmp_path, recurse=True)\n            # If we have gotten here we have a working ssh configuration.\n            # If ssh breaks we could leave tmp directories out on the remote system.\n            tmp_rm_res = self._low_level_execute_command(cmd, sudoable=False)\n\n            if tmp_rm_res.get('rc', 0) != 0:\n                display.warning('Error deleting remote temporary files (rc: %s, stderr: %s})'\n                                % (tmp_rm_res.get('rc'), tmp_rm_res.get('stderr', 'No error string available.')))\n            else:\n                self._connection._shell.tmpdir = None",
        "begin_line": 404,
        "end_line": 420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._transfer_file#422",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._transfer_file(self, local_path, remote_path)",
        "snippet": "    def _transfer_file(self, local_path, remote_path):\n        \"\"\"\n        Copy a file from the controller to a remote path\n\n        :arg local_path: Path on controller to transfer\n        :arg remote_path: Path on the remote system to transfer into\n\n        .. warning::\n            * When you use this function you likely want to use use fixup_perms2() on the\n              remote_path to make sure that the remote file is readable when the user becomes\n              a non-privileged user.\n            * If you use fixup_perms2() on the file and copy or move the file into place, you will\n              need to then remove filesystem acls on the file once it has been copied into place by\n              the module.  See how the copy module implements this for help.\n        \"\"\"\n        self._connection.put_file(local_path, remote_path)\n        return remote_path",
        "begin_line": 422,
        "end_line": 438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._transfer_data#440",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._transfer_data(self, remote_path, data)",
        "snippet": "    def _transfer_data(self, remote_path, data):\n        '''\n        Copies the module data out to the temporary module path.\n        '''\n\n        if isinstance(data, dict):\n            data = jsonify(data)\n\n        afd, afile = tempfile.mkstemp(dir=C.DEFAULT_LOCAL_TMP)\n        afo = os.fdopen(afd, 'wb')\n        try:\n            data = to_bytes(data, errors='surrogate_or_strict')\n            afo.write(data)\n        except Exception as e:\n            raise AnsibleError(\"failure writing module data to temporary file for transfer: %s\" % to_native(e))\n\n        afo.flush()\n        afo.close()\n\n        try:\n            self._transfer_file(afile, remote_path)\n        finally:\n            os.unlink(afile)\n\n        return remote_path",
        "begin_line": 440,
        "end_line": 464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._execute_remote_stat#576",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._execute_remote_stat(self, path, all_vars, follow, tmp=None, checksum=True)",
        "snippet": "    def _execute_remote_stat(self, path, all_vars, follow, tmp=None, checksum=True):\n        '''\n        Get information from remote file.\n        '''\n        if tmp is not None:\n            display.warning('_execute_remote_stat no longer honors the tmp parameter. Action'\n                            ' plugins should set self._connection._shell.tmpdir to share'\n                            ' the tmpdir')\n        del tmp  # No longer used\n\n        module_args = dict(\n            path=path,\n            follow=follow,\n            get_checksum=checksum,\n            checksum_algorithm='sha1',\n        )\n        mystat = self._execute_module(module_name='stat', module_args=module_args, task_vars=all_vars,\n                                      wrap_async=False)\n\n        if mystat.get('failed'):\n            msg = mystat.get('module_stderr')\n            if not msg:\n                msg = mystat.get('module_stdout')\n            if not msg:\n                msg = mystat.get('msg')\n            raise AnsibleError('Failed to get information on remote file (%s): %s' % (path, msg))\n\n        if not mystat['stat']['exists']:\n            # empty might be matched, 1 should never match, also backwards compatible\n            mystat['stat']['checksum'] = '1'\n\n        # happens sometimes when it is a dir and not on bsd\n        if 'checksum' not in mystat['stat']:\n            mystat['stat']['checksum'] = ''\n        elif not isinstance(mystat['stat']['checksum'], string_types):\n            raise AnsibleError(\"Invalid checksum returned by stat: expected a string type but got %s\" % type(mystat['stat']['checksum']))\n\n        return mystat['stat']",
        "begin_line": 576,
        "end_line": 613,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._remote_expand_user#644",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._remote_expand_user(self, path, sudoable=True, pathsep=None)",
        "snippet": "    def _remote_expand_user(self, path, sudoable=True, pathsep=None):\n        ''' takes a remote path and performs tilde/$HOME expansion on the remote host '''\n\n        # We only expand ~/path and ~username/path\n        if not path.startswith('~'):\n            return path\n\n        # Per Jborean, we don't have to worry about Windows as we don't have a notion of user's home\n        # dir there.\n        split_path = path.split(os.path.sep, 1)\n        expand_path = split_path[0]\n\n        if expand_path == '~':\n            # Network connection plugins (network_cli, netconf, etc.) execute on the controller, rather than the remote host.\n            # As such, we want to avoid using remote_user for paths  as remote_user may not line up with the local user\n            # This is a hack and should be solved by more intelligent handling of remote_tmp in 2.7\n            become_user = self.get_become_option('become_user')\n            if getattr(self._connection, '_remote_is_local', False):\n                pass\n            elif sudoable and self._connection.become and become_user:\n                expand_path = '~%s' % become_user\n            else:\n                # use remote user instead, if none set default to current user\n                expand_path = '~%s' % (self._get_remote_user() or '')\n\n        # use shell to construct appropriate command and execute\n        cmd = self._connection._shell.expand_user(expand_path)\n        data = self._low_level_execute_command(cmd, sudoable=False)\n\n        try:\n            initial_fragment = data['stdout'].strip().splitlines()[-1]\n        except IndexError:\n            initial_fragment = None\n\n        if not initial_fragment:\n            # Something went wrong trying to expand the path remotely. Try using pwd, if not, return\n            # the original string\n            cmd = self._connection._shell.pwd()\n            pwd = self._low_level_execute_command(cmd, sudoable=False).get('stdout', '').strip()\n            if pwd:\n                expanded = pwd\n            else:\n                expanded = path\n\n        elif len(split_path) > 1:\n            expanded = self._connection._shell.join_path(initial_fragment, *split_path[1:])\n        else:\n            expanded = initial_fragment\n\n        if '..' in os.path.dirname(expanded).split('/'):\n            raise AnsibleError(\"'%s' returned an invalid relative home directory path containing '..'\" % self._play_context.remote_addr)\n\n        return expanded",
        "begin_line": 644,
        "end_line": 696,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._strip_success_message#698",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._strip_success_message(self, data)",
        "snippet": "    def _strip_success_message(self, data):\n        '''\n        Removes the BECOME-SUCCESS message from the data.\n        '''\n        if data.strip().startswith('BECOME-SUCCESS-'):\n            data = re.sub(r'^((\\r)?\\n)?BECOME-SUCCESS.*(\\r)?\\n', '', data)\n        return data",
        "begin_line": 698,
        "end_line": 704,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._update_module_args#706",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._update_module_args(self, module_name, module_args, task_vars)",
        "snippet": "    def _update_module_args(self, module_name, module_args, task_vars):\n\n        # set check mode in the module arguments, if required\n        if self._play_context.check_mode:\n            if not self._supports_check_mode:\n                raise AnsibleError(\"check mode is not supported for this operation\")\n            module_args['_ansible_check_mode'] = True\n        else:\n            module_args['_ansible_check_mode'] = False\n\n        # set no log in the module arguments, if required\n        no_target_syslog = C.config.get_config_value('DEFAULT_NO_TARGET_SYSLOG', variables=task_vars)\n        module_args['_ansible_no_log'] = self._play_context.no_log or no_target_syslog\n\n        # set debug in the module arguments, if required\n        module_args['_ansible_debug'] = C.DEFAULT_DEBUG\n\n        # let module know we are in diff mode\n        module_args['_ansible_diff'] = self._play_context.diff\n\n        # let module know our verbosity\n        module_args['_ansible_verbosity'] = display.verbosity\n\n        # give the module information about the ansible version\n        module_args['_ansible_version'] = __version__\n\n        # give the module information about its name\n        module_args['_ansible_module_name'] = module_name\n\n        # set the syslog facility to be used in the module\n        module_args['_ansible_syslog_facility'] = task_vars.get('ansible_syslog_facility', C.DEFAULT_SYSLOG_FACILITY)\n\n        # let module know about filesystems that selinux treats specially\n        module_args['_ansible_selinux_special_fs'] = C.DEFAULT_SELINUX_SPECIAL_FS\n\n        # what to do when parameter values are converted to strings\n        module_args['_ansible_string_conversion_action'] = C.STRING_CONVERSION_ACTION\n\n        # give the module the socket for persistent connections\n        module_args['_ansible_socket'] = getattr(self._connection, 'socket_path')\n        if not module_args['_ansible_socket']:\n            module_args['_ansible_socket'] = task_vars.get('ansible_socket')\n\n        # make sure all commands use the designated shell executable\n        module_args['_ansible_shell_executable'] = self._play_context.executable\n\n        # make sure modules are aware if they need to keep the remote files\n        module_args['_ansible_keep_remote_files'] = C.DEFAULT_KEEP_REMOTE_FILES\n\n        # make sure all commands use the designated temporary directory if created\n        if self._is_become_unprivileged():  # force fallback on remote_tmp as user cannot normally write to dir\n            module_args['_ansible_tmpdir'] = None\n        else:\n            module_args['_ansible_tmpdir'] = self._connection._shell.tmpdir\n\n        # make sure the remote_tmp value is sent through in case modules needs to create their own\n        module_args['_ansible_remote_tmp'] = self.get_shell_option('remote_tmp', default='~/.ansible/tmp')",
        "begin_line": 706,
        "end_line": 762,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._execute_module#764",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._execute_module(self, module_name=None, module_args=None, tmp=None, task_vars=None, persist_files=False, delete_remote_tmp=None, wrap_async=False)",
        "snippet": "    def _execute_module(self, module_name=None, module_args=None, tmp=None, task_vars=None, persist_files=False, delete_remote_tmp=None, wrap_async=False):\n        '''\n        Transfer and run a module along with its arguments.\n        '''\n        if tmp is not None:\n            display.warning('_execute_module no longer honors the tmp parameter. Action plugins'\n                            ' should set self._connection._shell.tmpdir to share the tmpdir')\n        del tmp  # No longer used\n        if delete_remote_tmp is not None:\n            display.warning('_execute_module no longer honors the delete_remote_tmp parameter.'\n                            ' Action plugins should check self._connection._shell.tmpdir to'\n                            ' see if a tmpdir existed before they were called to determine'\n                            ' if they are responsible for removing it.')\n        del delete_remote_tmp  # No longer used\n\n        tmpdir = self._connection._shell.tmpdir\n\n        # We set the module_style to new here so the remote_tmp is created\n        # before the module args are built if remote_tmp is needed (async).\n        # If the module_style turns out to not be new and we didn't create the\n        # remote tmp here, it will still be created. This must be done before\n        # calling self._update_module_args() so the module wrapper has the\n        # correct remote_tmp value set\n        if not self._is_pipelining_enabled(\"new\", wrap_async) and tmpdir is None:\n            self._make_tmp_path()\n            tmpdir = self._connection._shell.tmpdir\n\n        if task_vars is None:\n            task_vars = dict()\n\n        # if a module name was not specified for this execution, use the action from the task\n        if module_name is None:\n            module_name = self._task.action\n        if module_args is None:\n            module_args = self._task.args\n\n        self._update_module_args(module_name, module_args, task_vars)\n\n        # FIXME: convert async_wrapper.py to not rely on environment variables\n        # make sure we get the right async_dir variable, backwards compatibility\n        # means we need to lookup the env value ANSIBLE_ASYNC_DIR first\n        remove_async_dir = None\n        if wrap_async or self._task.async_val:\n            env_async_dir = [e for e in self._task.environment if\n                             \"ANSIBLE_ASYNC_DIR\" in e]\n            if len(env_async_dir) > 0:\n                msg = \"Setting the async dir from the environment keyword \" \\\n                      \"ANSIBLE_ASYNC_DIR is deprecated. Set the async_dir \" \\\n                      \"shell option instead\"\n                self._display.deprecated(msg, \"2.12\")\n            else:\n                # ANSIBLE_ASYNC_DIR is not set on the task, we get the value\n                # from the shell option and temporarily add to the environment\n                # list for async_wrapper to pick up\n                async_dir = self.get_shell_option('async_dir', default=\"~/.ansible_async\")\n                remove_async_dir = len(self._task.environment)\n                self._task.environment.append({\"ANSIBLE_ASYNC_DIR\": async_dir})\n\n        # FUTURE: refactor this along with module build process to better encapsulate \"smart wrapper\" functionality\n        (module_style, shebang, module_data, module_path) = self._configure_module(module_name=module_name, module_args=module_args, task_vars=task_vars)\n        display.vvv(\"Using module file %s\" % module_path)\n        if not shebang and module_style != 'binary':\n            raise AnsibleError(\"module (%s) is missing interpreter line\" % module_name)\n\n        self._used_interpreter = shebang\n        remote_module_path = None\n\n        if not self._is_pipelining_enabled(module_style, wrap_async):\n            # we might need remote tmp dir\n            if tmpdir is None:\n                self._make_tmp_path()\n                tmpdir = self._connection._shell.tmpdir\n\n            remote_module_filename = self._connection._shell.get_remote_filename(module_path)\n            remote_module_path = self._connection._shell.join_path(tmpdir, 'AnsiballZ_%s' % remote_module_filename)\n\n        args_file_path = None\n        if module_style in ('old', 'non_native_want_json', 'binary'):\n            # we'll also need a tmp file to hold our module arguments\n            args_file_path = self._connection._shell.join_path(tmpdir, 'args')\n\n        if remote_module_path or module_style != 'new':\n            display.debug(\"transferring module to remote %s\" % remote_module_path)\n            if module_style == 'binary':\n                self._transfer_file(module_path, remote_module_path)\n            else:\n                self._transfer_data(remote_module_path, module_data)\n            if module_style == 'old':\n                # we need to dump the module args to a k=v string in a file on\n                # the remote system, which can be read and parsed by the module\n                args_data = \"\"\n                for k, v in iteritems(module_args):\n                    args_data += '%s=%s ' % (k, shlex_quote(text_type(v)))\n                self._transfer_data(args_file_path, args_data)\n            elif module_style in ('non_native_want_json', 'binary'):\n                self._transfer_data(args_file_path, json.dumps(module_args))\n            display.debug(\"done transferring module to remote\")\n\n        environment_string = self._compute_environment_string()\n\n        # remove the ANSIBLE_ASYNC_DIR env entry if we added a temporary one for\n        # the async_wrapper task - this is so the async_status plugin doesn't\n        # fire a deprecation warning when it runs after this task\n        if remove_async_dir is not None:\n            del self._task.environment[remove_async_dir]\n\n        remote_files = []\n        if tmpdir and remote_module_path:\n            remote_files = [tmpdir, remote_module_path]\n\n        if args_file_path:\n            remote_files.append(args_file_path)\n\n        sudoable = True\n        in_data = None\n        cmd = \"\"\n\n        if wrap_async and not self._connection.always_pipeline_modules:\n            # configure, upload, and chmod the async_wrapper module\n            (async_module_style, shebang, async_module_data, async_module_path) = self._configure_module(module_name='async_wrapper', module_args=dict(),\n                                                                                                         task_vars=task_vars)\n            async_module_remote_filename = self._connection._shell.get_remote_filename(async_module_path)\n            remote_async_module_path = self._connection._shell.join_path(tmpdir, async_module_remote_filename)\n            self._transfer_data(remote_async_module_path, async_module_data)\n            remote_files.append(remote_async_module_path)\n\n            async_limit = self._task.async_val\n            async_jid = str(random.randint(0, 999999999999))\n\n            # call the interpreter for async_wrapper directly\n            # this permits use of a script for an interpreter on non-Linux platforms\n            # TODO: re-implement async_wrapper as a regular module to avoid this special case\n            interpreter = shebang.replace('#!', '').strip()\n            async_cmd = [interpreter, remote_async_module_path, async_jid, async_limit, remote_module_path]\n\n            if environment_string:\n                async_cmd.insert(0, environment_string)\n\n            if args_file_path:\n                async_cmd.append(args_file_path)\n            else:\n                # maintain a fixed number of positional parameters for async_wrapper\n                async_cmd.append('_')\n\n            if not self._should_remove_tmp_path(tmpdir):\n                async_cmd.append(\"-preserve_tmp\")\n\n            cmd = \" \".join(to_text(x) for x in async_cmd)\n\n        else:\n\n            if self._is_pipelining_enabled(module_style):\n                in_data = module_data\n                display.vvv(\"Pipelining is enabled.\")\n            else:\n                cmd = remote_module_path\n\n            cmd = self._connection._shell.build_module_command(environment_string, shebang, cmd, arg_path=args_file_path).strip()\n\n        # Fix permissions of the tmpdir path and tmpdir files. This should be called after all\n        # files have been transferred.\n        if remote_files:\n            # remove none/empty\n            remote_files = [x for x in remote_files if x]\n            self._fixup_perms2(remote_files, self._get_remote_user())\n\n        # actually execute\n        res = self._low_level_execute_command(cmd, sudoable=sudoable, in_data=in_data)\n\n        # parse the main result\n        data = self._parse_returned_data(res)\n\n        # NOTE: INTERNAL KEYS ONLY ACCESSIBLE HERE\n        # get internal info before cleaning\n        if data.pop(\"_ansible_suppress_tmpdir_delete\", False):\n            self._cleanup_remote_tmp = False\n\n        # NOTE: yum returns results .. but that made it 'compatible' with squashing, so we allow mappings, for now\n        if 'results' in data and (not isinstance(data['results'], Sequence) or isinstance(data['results'], string_types)):\n            data['ansible_module_results'] = data['results']\n            del data['results']\n            display.warning(\"Found internal 'results' key in module return, renamed to 'ansible_module_results'.\")\n\n        # remove internal keys\n        remove_internal_keys(data)\n\n        if wrap_async:\n            # async_wrapper will clean up its tmpdir on its own so we want the controller side to\n            # forget about it now\n            self._connection._shell.tmpdir = None\n\n            # FIXME: for backwards compat, figure out if still makes sense\n            data['changed'] = True\n\n        # pre-split stdout/stderr into lines if needed\n        if 'stdout' in data and 'stdout_lines' not in data:\n            # if the value is 'False', a default won't catch it.\n            txt = data.get('stdout', None) or u''\n            data['stdout_lines'] = txt.splitlines()\n        if 'stderr' in data and 'stderr_lines' not in data:\n            # if the value is 'False', a default won't catch it.\n            txt = data.get('stderr', None) or u''\n            data['stderr_lines'] = txt.splitlines()\n\n        # propagate interpreter discovery results back to the controller\n        if self._discovered_interpreter_key:\n            if data.get('ansible_facts') is None:\n                data['ansible_facts'] = {}\n\n            data['ansible_facts'][self._discovered_interpreter_key] = self._discovered_interpreter\n\n        if self._discovery_warnings:\n            if data.get('warnings') is None:\n                data['warnings'] = []\n            data['warnings'].extend(self._discovery_warnings)\n\n        if self._discovery_deprecation_warnings:\n            if data.get('deprecations') is None:\n                data['deprecations'] = []\n            data['deprecations'].extend(self._discovery_deprecation_warnings)\n\n        # mark the entire module results untrusted as a template right here, since the current action could\n        # possibly template one of these values.\n        data = wrap_var(data)\n\n        display.debug(\"done with _execute_module (%s, %s)\" % (module_name, module_args))\n        return data",
        "begin_line": 764,
        "end_line": 990,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._parse_returned_data#992",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._parse_returned_data(self, res)",
        "snippet": "    def _parse_returned_data(self, res):\n        try:\n            filtered_output, warnings = _filter_non_json_lines(res.get('stdout', u''))\n            for w in warnings:\n                display.warning(w)\n\n            data = json.loads(filtered_output)\n            data['_ansible_parsed'] = True\n        except ValueError:\n            # not valid json, lets try to capture error\n            data = dict(failed=True, _ansible_parsed=False)\n            data['module_stdout'] = res.get('stdout', u'')\n            if 'stderr' in res:\n                data['module_stderr'] = res['stderr']\n                if res['stderr'].startswith(u'Traceback'):\n                    data['exception'] = res['stderr']\n\n            # in some cases a traceback will arrive on stdout instead of stderr, such as when using ssh with -tt\n            if 'exception' not in data and data['module_stdout'].startswith(u'Traceback'):\n                data['exception'] = data['module_stdout']\n\n            # The default\n            data['msg'] = \"MODULE FAILURE\"\n\n            # try to figure out if we are missing interpreter\n            if self._used_interpreter is not None:\n                match = re.compile('%s: (?:No such file or directory|not found)' % self._used_interpreter.lstrip('!#'))\n                if match.search(data['module_stderr']) or match.search(data['module_stdout']):\n                    data['msg'] = \"The module failed to execute correctly, you probably need to set the interpreter.\"\n\n            # always append hint\n            data['msg'] += '\\nSee stdout/stderr for the exact error'\n\n            if 'rc' in res:\n                data['rc'] = res['rc']\n        return data",
        "begin_line": 992,
        "end_line": 1027,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.action.__init__.ActionBase._low_level_execute_command#1030",
        "src_path": "lib/ansible/plugins/action/__init__.py",
        "class_name": "lib.ansible.plugins.action.__init__.ActionBase",
        "signature": "lib.ansible.plugins.action.__init__.ActionBase._low_level_execute_command(self, cmd, sudoable=True, in_data=None, executable=None, encoding_errors='surrogate_then_replace', chdir=None)",
        "snippet": "    def _low_level_execute_command(self, cmd, sudoable=True, in_data=None, executable=None, encoding_errors='surrogate_then_replace', chdir=None):\n        '''\n        This is the function which executes the low level shell command, which\n        may be commands to create/remove directories for temporary files, or to\n        run the module code or python directly when pipelining.\n\n        :kwarg encoding_errors: If the value returned by the command isn't\n            utf-8 then we have to figure out how to transform it to unicode.\n            If the value is just going to be displayed to the user (or\n            discarded) then the default of 'replace' is fine.  If the data is\n            used as a key or is going to be written back out to a file\n            verbatim, then this won't work.  May have to use some sort of\n            replacement strategy (python3 could use surrogateescape)\n        :kwarg chdir: cd into this directory before executing the command.\n        '''\n\n        display.debug(\"_low_level_execute_command(): starting\")\n        # if not cmd:\n        #     # this can happen with powershell modules when there is no analog to a Windows command (like chmod)\n        #     display.debug(\"_low_level_execute_command(): no command, exiting\")\n        #     return dict(stdout='', stderr='', rc=254)\n\n        if chdir:\n            display.debug(\"_low_level_execute_command(): changing cwd to %s for this command\" % chdir)\n            cmd = self._connection._shell.append_command('cd %s' % chdir, cmd)\n\n        # https://github.com/ansible/ansible/issues/68054\n        if executable:\n            self._connection._shell.executable = executable\n\n        ruser = self._get_remote_user()\n        buser = self.get_become_option('become_user')\n        if (sudoable and self._connection.become and  # if sudoable and have become\n                self._connection.transport.split('.')[-1] != 'network_cli' and  # if not using network_cli\n                (C.BECOME_ALLOW_SAME_USER or (buser != ruser or not any((ruser, buser))))):  # if we allow same user PE or users are different and either is set\n            display.debug(\"_low_level_execute_command(): using become for this command\")\n            cmd = self._connection.become.build_become_command(cmd, self._connection._shell)\n\n        if self._connection.allow_executable:\n            if executable is None:\n                executable = self._play_context.executable\n                # mitigation for SSH race which can drop stdout (https://github.com/ansible/ansible/issues/13876)\n                # only applied for the default executable to avoid interfering with the raw action\n                cmd = self._connection._shell.append_command(cmd, 'sleep 0')\n            if executable:\n                cmd = executable + ' -c ' + shlex_quote(cmd)\n\n        display.debug(\"_low_level_execute_command(): executing: %s\" % (cmd,))\n\n        # Change directory to basedir of task for command execution when connection is local\n        if self._connection.transport == 'local':\n            self._connection.cwd = to_bytes(self._loader.get_basedir(), errors='surrogate_or_strict')\n\n        rc, stdout, stderr = self._connection.exec_command(cmd, in_data=in_data, sudoable=sudoable)\n\n        # stdout and stderr may be either a file-like or a bytes object.\n        # Convert either one to a text type\n        if isinstance(stdout, binary_type):\n            out = to_text(stdout, errors=encoding_errors)\n        elif not isinstance(stdout, text_type):\n            out = to_text(b''.join(stdout.readlines()), errors=encoding_errors)\n        else:\n            out = stdout\n\n        if isinstance(stderr, binary_type):\n            err = to_text(stderr, errors=encoding_errors)\n        elif not isinstance(stderr, text_type):\n            err = to_text(b''.join(stderr.readlines()), errors=encoding_errors)\n        else:\n            err = stderr\n\n        if rc is None:\n            rc = 0\n\n        # be sure to remove the BECOME-SUCCESS message now\n        out = self._strip_success_message(out)\n\n        display.debug(u\"_low_level_execute_command() done: rc=%d, stdout=%s, stderr=%s\" % (rc, out, err))\n        return dict(rc=rc, stdout=out, stdout_lines=out.splitlines(), stderr=err, stderr_lines=err.splitlines())",
        "begin_line": 1030,
        "end_line": 1108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.included_file.IncludedFile.__init__#36",
        "src_path": "lib/ansible/playbook/included_file.py",
        "class_name": "lib.ansible.playbook.included_file.IncludedFile",
        "signature": "lib.ansible.playbook.included_file.IncludedFile.__init__(self, filename, args, vars, task, is_role=False)",
        "snippet": "    def __init__(self, filename, args, vars, task, is_role=False):\n        self._filename = filename\n        self._args = args\n        self._vars = vars\n        self._task = task\n        self._hosts = []\n        self._is_role = is_role",
        "begin_line": 36,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.included_file.IncludedFile.add_host#44",
        "src_path": "lib/ansible/playbook/included_file.py",
        "class_name": "lib.ansible.playbook.included_file.IncludedFile",
        "signature": "lib.ansible.playbook.included_file.IncludedFile.add_host(self, host)",
        "snippet": "    def add_host(self, host):\n        if host not in self._hosts:\n            self._hosts.append(host)\n            return\n        raise ValueError()",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.included_file.IncludedFile.__eq__#50",
        "src_path": "lib/ansible/playbook/included_file.py",
        "class_name": "lib.ansible.playbook.included_file.IncludedFile",
        "signature": "lib.ansible.playbook.included_file.IncludedFile.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return (other._filename == self._filename and\n                other._args == self._args and\n                other._vars == self._vars and\n                other._task._parent._uuid == self._task._parent._uuid)",
        "begin_line": 50,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.included_file.IncludedFile.__repr__#56",
        "src_path": "lib/ansible/playbook/included_file.py",
        "class_name": "lib.ansible.playbook.included_file.IncludedFile",
        "signature": "lib.ansible.playbook.included_file.IncludedFile.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return \"%s (args=%s vars=%s): %s\" % (self._filename, self._args, self._vars, self._hosts)",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.included_file.IncludedFile.process_include_results#60",
        "src_path": "lib/ansible/playbook/included_file.py",
        "class_name": "lib.ansible.playbook.included_file.IncludedFile",
        "signature": "lib.ansible.playbook.included_file.IncludedFile.process_include_results(results, iterator, loader, variable_manager)",
        "snippet": "    def process_include_results(results, iterator, loader, variable_manager):\n        included_files = []\n        task_vars_cache = {}\n\n        for res in results:\n\n            original_host = res._host\n            original_task = res._task\n\n            if original_task.action in ('include', 'include_tasks', 'include_role'):\n                if original_task.loop:\n                    if 'results' not in res._result:\n                        continue\n                    include_results = res._result['results']\n                else:\n                    include_results = [res._result]\n\n                for include_result in include_results:\n                    # if the task result was skipped or failed, continue\n                    if 'skipped' in include_result and include_result['skipped'] or 'failed' in include_result and include_result['failed']:\n                        continue\n\n                    cache_key = (iterator._play, original_host, original_task)\n                    try:\n                        task_vars = task_vars_cache[cache_key]\n                    except KeyError:\n                        task_vars = task_vars_cache[cache_key] = variable_manager.get_vars(play=iterator._play, host=original_host, task=original_task)\n\n                    include_args = include_result.get('include_args', dict())\n                    special_vars = {}\n                    loop_var = include_result.get('ansible_loop_var', 'item')\n                    index_var = include_result.get('ansible_index_var')\n                    if loop_var in include_result:\n                        task_vars[loop_var] = special_vars[loop_var] = include_result[loop_var]\n                    if index_var and index_var in include_result:\n                        task_vars[index_var] = special_vars[index_var] = include_result[index_var]\n                    if '_ansible_item_label' in include_result:\n                        task_vars['_ansible_item_label'] = special_vars['_ansible_item_label'] = include_result['_ansible_item_label']\n                    if 'ansible_loop' in include_result:\n                        task_vars['ansible_loop'] = special_vars['ansible_loop'] = include_result['ansible_loop']\n                    if original_task.no_log and '_ansible_no_log' not in include_args:\n                        task_vars['_ansible_no_log'] = special_vars['_ansible_no_log'] = original_task.no_log\n\n                    # get search path for this task to pass to lookup plugins that may be used in pathing to\n                    # the included file\n                    task_vars['ansible_search_path'] = original_task.get_search_path()\n\n                    # ensure basedir is always in (dwim already searches here but we need to display it)\n                    if loader.get_basedir() not in task_vars['ansible_search_path']:\n                        task_vars['ansible_search_path'].append(loader.get_basedir())\n\n                    templar = Templar(loader=loader, variables=task_vars)\n\n                    if original_task.action in ('include', 'include_tasks'):\n                        include_file = None\n                        if original_task:\n                            if original_task.static:\n                                continue\n\n                            if original_task._parent:\n                                # handle relative includes by walking up the list of parent include\n                                # tasks and checking the relative result to see if it exists\n                                parent_include = original_task._parent\n                                cumulative_path = None\n                                while parent_include is not None:\n                                    if not isinstance(parent_include, TaskInclude):\n                                        parent_include = parent_include._parent\n                                        continue\n                                    if isinstance(parent_include, IncludeRole):\n                                        parent_include_dir = parent_include._role_path\n                                    else:\n                                        try:\n                                            parent_include_dir = os.path.dirname(templar.template(parent_include.args.get('_raw_params')))\n                                        except AnsibleError as e:\n                                            parent_include_dir = ''\n                                            display.warning(\n                                                'Templating the path of the parent %s failed. The path to the '\n                                                'included file may not be found. '\n                                                'The error was: %s.' % (original_task.action, to_text(e))\n                                            )\n                                    if cumulative_path is not None and not os.path.isabs(cumulative_path):\n                                        cumulative_path = os.path.join(parent_include_dir, cumulative_path)\n                                    else:\n                                        cumulative_path = parent_include_dir\n                                    include_target = templar.template(include_result['include'])\n                                    if original_task._role:\n                                        new_basedir = os.path.join(original_task._role._role_path, 'tasks', cumulative_path)\n                                        candidates = [loader.path_dwim_relative(original_task._role._role_path, 'tasks', include_target),\n                                                      loader.path_dwim_relative(new_basedir, 'tasks', include_target)]\n                                        for include_file in candidates:\n                                            try:\n                                                # may throw OSError\n                                                os.stat(include_file)\n                                                # or select the task file if it exists\n                                                break\n                                            except OSError:\n                                                pass\n                                    else:\n                                        include_file = loader.path_dwim_relative(loader.get_basedir(), cumulative_path, include_target)\n\n                                    if os.path.exists(include_file):\n                                        break\n                                    else:\n                                        parent_include = parent_include._parent\n\n                        if include_file is None:\n                            if original_task._role:\n                                include_target = templar.template(include_result['include'])\n                                include_file = loader.path_dwim_relative(original_task._role._role_path, 'tasks', include_target)\n                            else:\n                                include_file = loader.path_dwim(include_result['include'])\n\n                        include_file = templar.template(include_file)\n                        inc_file = IncludedFile(include_file, include_args, special_vars, original_task)\n                    else:\n                        # template the included role's name here\n                        role_name = include_args.pop('name', include_args.pop('role', None))\n                        if role_name is not None:\n                            role_name = templar.template(role_name)\n\n                        new_task = original_task.copy()\n                        new_task._role_name = role_name\n                        for from_arg in new_task.FROM_ARGS:\n                            if from_arg in include_args:\n                                from_key = from_arg.replace('_from', '')\n                                new_task._from_files[from_key] = templar.template(include_args.pop(from_arg))\n\n                        inc_file = IncludedFile(role_name, include_args, special_vars, new_task, is_role=True)\n\n                    idx = 0\n                    orig_inc_file = inc_file\n                    while 1:\n                        try:\n                            pos = included_files[idx:].index(orig_inc_file)\n                            # pos is relative to idx since we are slicing\n                            # use idx + pos due to relative indexing\n                            inc_file = included_files[idx + pos]\n                        except ValueError:\n                            included_files.append(orig_inc_file)\n                            inc_file = orig_inc_file\n\n                        try:\n                            inc_file.add_host(original_host)\n                        except ValueError:\n                            # The host already exists for this include, advance forward, this is a new include\n                            idx += pos + 1\n                        else:\n                            break\n\n        return included_files",
        "begin_line": 60,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.__init__.ShellBase.__init__#36",
        "src_path": "lib/ansible/plugins/shell/__init__.py",
        "class_name": "lib.ansible.plugins.shell.__init__.ShellBase",
        "signature": "lib.ansible.plugins.shell.__init__.ShellBase.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(ShellBase, self).__init__()\n\n        self.env = {}\n        self.tmpdir = None\n        self.executable = None",
        "begin_line": 36,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.57002271006813e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.__init__.ShellBase.quote#225",
        "src_path": "lib/ansible/plugins/shell/__init__.py",
        "class_name": "lib.ansible.plugins.shell.__init__.ShellBase",
        "signature": "lib.ansible.plugins.shell.__init__.ShellBase.quote(self, cmd)",
        "snippet": "    def quote(self, cmd):\n        \"\"\"Returns a shell-escaped string that can be safely used as one token in a shell command line\"\"\"\n        return shlex_quote(cmd)",
        "begin_line": 225,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.shell.sh.ShellModule.checksum#47",
        "src_path": "lib/ansible/plugins/shell/sh.py",
        "class_name": "lib.ansible.plugins.shell.sh.ShellModule",
        "signature": "lib.ansible.plugins.shell.sh.ShellModule.checksum(self, path, python_interp)",
        "snippet": "    def checksum(self, path, python_interp):\n        # In the following test, each condition is a check and logical\n        # comparison (|| or &&) that sets the rc value.  Every check is run so\n        # the last check in the series to fail will be the rc that is returned.\n        #\n        # If a check fails we error before invoking the hash functions because\n        # hash functions may successfully take the hash of a directory on BSDs\n        # (UFS filesystem?) which is not what the rest of the ansible code expects\n        #\n        # If all of the available hashing methods fail we fail with an rc of 0.\n        # This logic is added to the end of the cmd at the bottom of this function.\n\n        # Return codes:\n        # checksum: success!\n        # 0: Unknown error\n        # 1: Remote file does not exist\n        # 2: No read permissions on the file\n        # 3: File is a directory\n        # 4: No python interpreter\n\n        # Quoting gets complex here.  We're writing a python string that's\n        # used by a variety of shells on the remote host to invoke a python\n        # \"one-liner\".\n        shell_escaped_path = shlex_quote(path)\n        test = \"rc=flag; [ -r %(p)s ] %(shell_or)s rc=2; [ -f %(p)s ] %(shell_or)s rc=1; [ -d %(p)s ] %(shell_and)s rc=3; %(i)s -V 2>/dev/null %(shell_or)s rc=4; [ x\\\"$rc\\\" != \\\"xflag\\\" ] %(shell_and)s echo \\\"${rc}  \\\"%(p)s %(shell_and)s exit 0\" % dict(p=shell_escaped_path, i=python_interp, shell_and=self._SHELL_AND, shell_or=self._SHELL_OR)  # NOQA\n        csums = [\n            u\"({0} -c 'import hashlib; BLOCKSIZE = 65536; hasher = hashlib.sha1();{2}afile = open(\\\"'{1}'\\\", \\\"rb\\\"){2}buf = afile.read(BLOCKSIZE){2}while len(buf) > 0:{2}\\thasher.update(buf){2}\\tbuf = afile.read(BLOCKSIZE){2}afile.close(){2}print(hasher.hexdigest())' 2>/dev/null)\".format(python_interp, shell_escaped_path, self._SHELL_EMBEDDED_PY_EOL),  # NOQA  Python > 2.4 (including python3)\n            u\"({0} -c 'import sha; BLOCKSIZE = 65536; hasher = sha.sha();{2}afile = open(\\\"'{1}'\\\", \\\"rb\\\"){2}buf = afile.read(BLOCKSIZE){2}while len(buf) > 0:{2}\\thasher.update(buf){2}\\tbuf = afile.read(BLOCKSIZE){2}afile.close(){2}print(hasher.hexdigest())' 2>/dev/null)\".format(python_interp, shell_escaped_path, self._SHELL_EMBEDDED_PY_EOL),  # NOQA  Python == 2.4\n        ]\n\n        cmd = (\" %s \" % self._SHELL_OR).join(csums)\n        cmd = \"%s; %s %s (echo \\'0  \\'%s)\" % (test, cmd, self._SHELL_OR, shell_escaped_path)\n        return cmd",
        "begin_line": 47,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.pkg_mgr.OpenBSDPkgMgrFactCollector.collect#48",
        "src_path": "lib/ansible/module_utils/facts/system/pkg_mgr.py",
        "class_name": "lib.ansible.module_utils.facts.system.pkg_mgr.OpenBSDPkgMgrFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.pkg_mgr.OpenBSDPkgMgrFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n\n        facts_dict['pkg_mgr'] = 'openbsd_pkg'\n        return facts_dict",
        "begin_line": 48,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector._check_rh_versions#62",
        "src_path": "lib/ansible/module_utils/facts/system/pkg_mgr.py",
        "class_name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector._check_rh_versions(self, pkg_mgr_name, collected_facts)",
        "snippet": "    def _check_rh_versions(self, pkg_mgr_name, collected_facts):\n        if collected_facts['ansible_distribution'] == 'Fedora':\n            if os.path.exists('/run/ostree-booted'):\n                return \"atomic_container\"\n            try:\n                if int(collected_facts['ansible_distribution_major_version']) < 23:\n                    for yum in [pkg_mgr for pkg_mgr in PKG_MGRS if pkg_mgr['name'] == 'yum']:\n                        if os.path.exists(yum['path']):\n                            pkg_mgr_name = 'yum'\n                            break\n                else:\n                    for dnf in [pkg_mgr for pkg_mgr in PKG_MGRS if pkg_mgr['name'] == 'dnf']:\n                        if os.path.exists(dnf['path']):\n                            pkg_mgr_name = 'dnf'\n                            break\n            except ValueError:\n                # If there's some new magical Fedora version in the future,\n                # just default to dnf\n                pkg_mgr_name = 'dnf'\n        elif collected_facts['ansible_distribution'] == 'Amazon':\n            pkg_mgr_name = 'yum'\n        else:\n            # If it's not one of the above and it's Red Hat family of distros, assume\n            # RHEL or a clone. For versions of RHEL < 8 that Ansible supports, the\n            # vendor supported official package manager is 'yum' and in RHEL 8+\n            # (as far as we know at the time of this writing) it is 'dnf'.\n            # If anyone wants to force a non-official package manager then they\n            # can define a provider to either the package or yum action plugins.\n            if int(collected_facts['ansible_distribution_major_version']) < 8:\n                pkg_mgr_name = 'yum'\n            else:\n                pkg_mgr_name = 'dnf'\n        return pkg_mgr_name",
        "begin_line": 62,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector._check_apt_flavor#96",
        "src_path": "lib/ansible/module_utils/facts/system/pkg_mgr.py",
        "class_name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector._check_apt_flavor(self, pkg_mgr_name)",
        "snippet": "    def _check_apt_flavor(self, pkg_mgr_name):\n        # Check if '/usr/bin/apt' is APT-RPM or an ordinary (dpkg-based) APT.\n        # There's rpm package on Debian, so checking if /usr/bin/rpm exists\n        # is not enough. Instead ask RPM if /usr/bin/apt-get belongs to some\n        # RPM package.\n        rpm_query = '/usr/bin/rpm -q --whatprovides /usr/bin/apt-get'.split()\n        if os.path.exists('/usr/bin/rpm'):\n            with open(os.devnull, 'w') as null:\n                try:\n                    subprocess.check_call(rpm_query, stdout=null, stderr=null)\n                    pkg_mgr_name = 'apt_rpm'\n                except subprocess.CalledProcessError:\n                    # No apt-get in RPM database. Looks like Debian/Ubuntu\n                    # with rpm package installed\n                    pkg_mgr_name = 'apt'\n        return pkg_mgr_name",
        "begin_line": 96,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector.collect#113",
        "src_path": "lib/ansible/module_utils/facts/system/pkg_mgr.py",
        "class_name": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.pkg_mgr.PkgMgrFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        collected_facts = collected_facts or {}\n\n        pkg_mgr_name = 'unknown'\n        for pkg in PKG_MGRS:\n            if os.path.exists(pkg['path']):\n                pkg_mgr_name = pkg['name']\n\n        # Handle distro family defaults when more than one package manager is\n        # installed or available to the distro, the ansible_fact entry should be\n        # the default package manager officially supported by the distro.\n        if collected_facts['ansible_os_family'] == \"RedHat\":\n            pkg_mgr_name = self._check_rh_versions(pkg_mgr_name, collected_facts)\n        elif collected_facts['ansible_os_family'] == 'Debian' and pkg_mgr_name != 'apt':\n            # It's possible to install yum, dnf, zypper, rpm, etc inside of\n            # Debian. Doing so does not mean the system wants to use them.\n            pkg_mgr_name = 'apt'\n        elif collected_facts['ansible_os_family'] == 'Altlinux':\n            if pkg_mgr_name == 'apt':\n                pkg_mgr_name = 'apt_rpm'\n\n        # Check if /usr/bin/apt-get is ordinary (dpkg-based) APT or APT-RPM\n        if pkg_mgr_name == 'apt':\n            pkg_mgr_name = self._check_apt_flavor(pkg_mgr_name)\n\n        facts_dict['pkg_mgr'] = pkg_mgr_name\n        return facts_dict",
        "begin_line": 113,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.env.LookupModule.run#53",
        "src_path": "lib/ansible/plugins/lookup/env.py",
        "class_name": "lib.ansible.plugins.lookup.env.LookupModule",
        "signature": "lib.ansible.plugins.lookup.env.LookupModule.run(self, terms, variables, **kwargs)",
        "snippet": "    def run(self, terms, variables, **kwargs):\n        ret = []\n\n        for term in terms:\n            var = term.split()[0]\n            ret.append(py3compat.environ.get(var, ''))\n\n        return ret",
        "begin_line": 53,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.date_time.DateTimeFactCollector.collect#31",
        "src_path": "lib/ansible/module_utils/facts/system/date_time.py",
        "class_name": "lib.ansible.module_utils.facts.system.date_time.DateTimeFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.date_time.DateTimeFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        date_time_facts = {}\n\n        now = datetime.datetime.now()\n        date_time_facts['year'] = now.strftime('%Y')\n        date_time_facts['month'] = now.strftime('%m')\n        date_time_facts['weekday'] = now.strftime('%A')\n        date_time_facts['weekday_number'] = now.strftime('%w')\n        date_time_facts['weeknumber'] = now.strftime('%W')\n        date_time_facts['day'] = now.strftime('%d')\n        date_time_facts['hour'] = now.strftime('%H')\n        date_time_facts['minute'] = now.strftime('%M')\n        date_time_facts['second'] = now.strftime('%S')\n        date_time_facts['epoch'] = now.strftime('%s')\n        if date_time_facts['epoch'] == '' or date_time_facts['epoch'][0] == '%':\n            # NOTE: in this case, the epoch wont match the rest of the date_time facts? ie, it's a few milliseconds later..? -akl\n            date_time_facts['epoch'] = str(int(time.time()))\n        date_time_facts['date'] = now.strftime('%Y-%m-%d')\n        date_time_facts['time'] = now.strftime('%H:%M:%S')\n        date_time_facts['iso8601_micro'] = now.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n        date_time_facts['iso8601'] = now.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n        date_time_facts['iso8601_basic'] = now.strftime(\"%Y%m%dT%H%M%S%f\")\n        date_time_facts['iso8601_basic_short'] = now.strftime(\"%Y%m%dT%H%M%S\")\n        date_time_facts['tz'] = time.strftime(\"%Z\")\n        date_time_facts['tz_offset'] = time.strftime(\"%z\")\n\n        facts_dict['date_time'] = date_time_facts\n        return facts_dict",
        "begin_line": 31,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.collectionsearch._ensure_default_collection#16",
        "src_path": "lib/ansible/playbook/collectionsearch.py",
        "class_name": "lib.ansible.playbook.collectionsearch",
        "signature": "lib.ansible.playbook.collectionsearch._ensure_default_collection(collection_list=None)",
        "snippet": "def _ensure_default_collection(collection_list=None):\n    default_collection = AnsibleCollectionLoader().default_collection\n\n    # Will be None when used as the default\n    if collection_list is None:\n        collection_list = []\n\n    # FIXME: exclude role tasks?\n    if default_collection and default_collection not in collection_list:\n        collection_list.insert(0, default_collection)\n\n    # if there's something in the list, ensure that builtin or legacy is always there too\n    if collection_list and 'ansible.builtin' not in collection_list and 'ansible.legacy' not in collection_list:\n        collection_list.append('ansible.legacy')\n\n    return collection_list",
        "begin_line": 16,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.collectionsearch.CollectionSearch._load_collections#40",
        "src_path": "lib/ansible/playbook/collectionsearch.py",
        "class_name": "lib.ansible.playbook.collectionsearch.CollectionSearch",
        "signature": "lib.ansible.playbook.collectionsearch.CollectionSearch._load_collections(self, attr, ds)",
        "snippet": "    def _load_collections(self, attr, ds):\n        # We are always a mixin with Base, so we can validate this untemplated\n        # field early on to guarantee we are dealing with a list.\n        ds = self.get_validated_value('collections', self._collections, ds, None)\n\n        # this will only be called if someone specified a value; call the shared value\n        _ensure_default_collection(collection_list=ds)\n\n        if not ds:  # don't return an empty collection list, just return None\n            return None\n\n        # This duplicates static attr checking logic from post_validate()\n        # because if the user attempts to template a collection name, it may\n        # error before it ever gets to the post_validate() warning (e.g. trying\n        # to import a role from the collection).\n        env = Environment()\n        for collection_name in ds:\n            if is_template(collection_name, env):\n                display.warning('\"collections\" is not templatable, but we found: %s, '\n                                'it will not be templated and will be used \"as is\".' % (collection_name))\n\n        return ds",
        "begin_line": 40,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.preprocess_vars#54",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager",
        "signature": "lib.ansible.vars.manager.preprocess_vars(a)",
        "snippet": "def preprocess_vars(a):\n    '''\n    Ensures that vars contained in the parameter passed in are\n    returned as a list of dictionaries, to ensure for instance\n    that vars loaded from a file conform to an expected state.\n    '''\n\n    if a is None:\n        return None\n    elif not isinstance(a, list):\n        data = [a]\n    else:\n        data = a\n\n    for item in data:\n        if not isinstance(item, MutableMapping):\n            raise AnsibleError(\"variable files must contain either a dictionary of variables, or a list of dictionaries. Got: %s (%s)\" % (a, type(a)))\n\n    return data",
        "begin_line": 54,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager.__init__#80",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager.__init__(self, loader=None, inventory=None, version_info=None)",
        "snippet": "    def __init__(self, loader=None, inventory=None, version_info=None):\n        self._nonpersistent_fact_cache = defaultdict(dict)\n        self._vars_cache = defaultdict(dict)\n        self._extra_vars = defaultdict(dict)\n        self._host_vars_files = defaultdict(dict)\n        self._group_vars_files = defaultdict(dict)\n        self._inventory = inventory\n        self._loader = loader\n        self._hostvars = None\n        self._omit_token = '__omit_place_holder__%s' % sha1(os.urandom(64)).hexdigest()\n\n        self._options_vars = load_options_vars(version_info)\n\n        # If the basedir is specified as the empty string then it results in cwd being used.\n        # This is not a safe location to load vars from.\n        basedir = self._options_vars.get('basedir', False)\n        self.safe_basedir = bool(basedir is False or basedir)\n\n        # load extra vars\n        self._extra_vars = load_extra_vars(loader=self._loader)\n\n        # load fact cache\n        try:\n            self._fact_cache = FactCache()\n        except AnsibleError as e:\n            # bad cache plugin is not fatal error\n            # fallback to a dict as in memory cache\n            display.warning(to_text(e))\n            self._fact_cache = {}",
        "begin_line": 80,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager.get_vars#146",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager.get_vars(self, play=None, host=None, task=None, include_hostvars=True, include_delegate_to=True, use_cache=True, _hosts=None, _hosts_all=None, stage='task')",
        "snippet": "    def get_vars(self, play=None, host=None, task=None, include_hostvars=True, include_delegate_to=True, use_cache=True,\n                 _hosts=None, _hosts_all=None, stage='task'):\n        '''\n        Returns the variables, with optional \"context\" given via the parameters\n        for the play, host, and task (which could possibly result in different\n        sets of variables being returned due to the additional context).\n\n        The order of precedence is:\n        - play->roles->get_default_vars (if there is a play context)\n        - group_vars_files[host] (if there is a host context)\n        - host_vars_files[host] (if there is a host context)\n        - host->get_vars (if there is a host context)\n        - fact_cache[host] (if there is a host context)\n        - play vars (if there is a play context)\n        - play vars_files (if there's no host context, ignore\n          file names that cannot be templated)\n        - task->get_vars (if there is a task context)\n        - vars_cache[host] (if there is a host context)\n        - extra vars\n\n        ``_hosts`` and ``_hosts_all`` should be considered private args, with only internal trusted callers relying\n        on the functionality they provide. These arguments may be removed at a later date without a deprecation\n        period and without warning.\n        '''\n\n        display.debug(\"in VariableManager get_vars()\")\n\n        all_vars = dict()\n        magic_variables = self._get_magic_variables(\n            play=play,\n            host=host,\n            task=task,\n            include_hostvars=include_hostvars,\n            include_delegate_to=include_delegate_to,\n            _hosts=_hosts,\n            _hosts_all=_hosts_all,\n        )\n\n        _vars_sources = {}\n\n        def _combine_and_track(data, new_data, source):\n            '''\n            Wrapper function to update var sources dict and call combine_vars()\n\n            See notes in the VarsWithSources docstring for caveats and limitations of the source tracking\n            '''\n            if C.DEFAULT_DEBUG:\n                # Populate var sources dict\n                for key in new_data:\n                    _vars_sources[key] = source\n            return combine_vars(data, new_data)\n\n        # default for all cases\n        basedirs = []\n        if self.safe_basedir:  # avoid adhoc/console loading cwd\n            basedirs = [self._loader.get_basedir()]\n\n        if play:\n            # first we compile any vars specified in defaults/main.yml\n            # for all roles within the specified play\n            for role in play.get_roles():\n                all_vars = _combine_and_track(all_vars, role.get_default_vars(), \"role '%s' defaults\" % role.name)\n\n        if task:\n            # set basedirs\n            if C.PLAYBOOK_VARS_ROOT == 'all':  # should be default\n                basedirs = task.get_search_path()\n            elif C.PLAYBOOK_VARS_ROOT in ('bottom', 'playbook_dir'):  # only option in 2.4.0\n                basedirs = [task.get_search_path()[0]]\n            elif C.PLAYBOOK_VARS_ROOT != 'top':\n                # preserves default basedirs, only option pre 2.3\n                raise AnsibleError('Unknown playbook vars logic: %s' % C.PLAYBOOK_VARS_ROOT)\n\n            # if we have a task in this context, and that task has a role, make\n            # sure it sees its defaults above any other roles, as we previously\n            # (v1) made sure each task had a copy of its roles default vars\n            if task._role is not None and (play or task.action == 'include_role'):\n                all_vars = _combine_and_track(all_vars, task._role.get_default_vars(dep_chain=task.get_dep_chain()),\n                                              \"role '%s' defaults\" % task._role.name)\n\n        if host:\n            # THE 'all' group and the rest of groups for a host, used below\n            all_group = self._inventory.groups.get('all')\n            host_groups = sort_groups([g for g in host.get_groups() if g.name not in ['all']])\n\n            def _get_plugin_vars(plugin, path, entities):\n                data = {}\n                try:\n                    data = plugin.get_vars(self._loader, path, entities)\n                except AttributeError:\n                    try:\n                        for entity in entities:\n                            if isinstance(entity, Host):\n                                data.update(plugin.get_host_vars(entity.name))\n                            else:\n                                data.update(plugin.get_group_vars(entity.name))\n                    except AttributeError:\n                        if hasattr(plugin, 'run'):\n                            raise AnsibleError(\"Cannot use v1 type vars plugin %s from %s\" % (plugin._load_name, plugin._original_path))\n                        else:\n                            raise AnsibleError(\"Invalid vars plugin %s from %s\" % (plugin._load_name, plugin._original_path))\n                return data\n\n            # internal functions that actually do the work\n            def _plugins_inventory(entities):\n                ''' merges all entities by inventory source '''\n                return get_vars_from_inventory_sources(self._loader, self._inventory._sources, entities, stage)\n\n            def _plugins_play(entities):\n                ''' merges all entities adjacent to play '''\n                data = {}\n                for path in basedirs:\n                    data = _combine_and_track(data, get_vars_from_path(self._loader, path, entities, stage), \"path '%s'\" % path)\n                return data\n\n            # configurable functions that are sortable via config, remember to add to _ALLOWED if expanding this list\n            def all_inventory():\n                return all_group.get_vars()\n\n            def all_plugins_inventory():\n                return _plugins_inventory([all_group])\n\n            def all_plugins_play():\n                return _plugins_play([all_group])\n\n            def groups_inventory():\n                ''' gets group vars from inventory '''\n                return get_group_vars(host_groups)\n\n            def groups_plugins_inventory():\n                ''' gets plugin sources from inventory for groups '''\n                return _plugins_inventory(host_groups)\n\n            def groups_plugins_play():\n                ''' gets plugin sources from play for groups '''\n                return _plugins_play(host_groups)\n\n            def plugins_by_groups():\n                '''\n                    merges all plugin sources by group,\n                    This should be used instead, NOT in combination with the other groups_plugins* functions\n                '''\n                data = {}\n                for group in host_groups:\n                    data[group] = _combine_and_track(data[group], _plugins_inventory(group), \"inventory group_vars for '%s'\" % group)\n                    data[group] = _combine_and_track(data[group], _plugins_play(group), \"playbook group_vars for '%s'\" % group)\n                return data\n\n            # Merge groups as per precedence config\n            # only allow to call the functions we want exposed\n            for entry in C.VARIABLE_PRECEDENCE:\n                if entry in self._ALLOWED:\n                    display.debug('Calling %s to load vars for %s' % (entry, host.name))\n                    all_vars = _combine_and_track(all_vars, locals()[entry](), \"group vars, precedence entry '%s'\" % entry)\n                else:\n                    display.warning('Ignoring unknown variable precedence entry: %s' % (entry))\n\n            # host vars, from inventory, inventory adjacent and play adjacent via plugins\n            all_vars = _combine_and_track(all_vars, host.get_vars(), \"host vars for '%s'\" % host)\n            all_vars = _combine_and_track(all_vars, _plugins_inventory([host]), \"inventory host_vars for '%s'\" % host)\n            all_vars = _combine_and_track(all_vars, _plugins_play([host]), \"playbook host_vars for '%s'\" % host)\n\n            # finally, the facts caches for this host, if it exists\n            # TODO: cleaning of facts should eventually become part of taskresults instead of vars\n            try:\n                facts = wrap_var(self._fact_cache.get(host.name, {}))\n                all_vars.update(namespace_facts(facts))\n\n                # push facts to main namespace\n                if C.INJECT_FACTS_AS_VARS:\n                    all_vars = _combine_and_track(all_vars, wrap_var(clean_facts(facts)), \"facts\")\n                else:\n                    # always 'promote' ansible_local\n                    all_vars = _combine_and_track(all_vars, wrap_var({'ansible_local': facts.get('ansible_local', {})}), \"facts\")\n            except KeyError:\n                pass\n\n        if play:\n            all_vars = _combine_and_track(all_vars, play.get_vars(), \"play vars\")\n\n            vars_files = play.get_vars_files()\n            try:\n                for vars_file_item in vars_files:\n                    # create a set of temporary vars here, which incorporate the extra\n                    # and magic vars so we can properly template the vars_files entries\n                    temp_vars = combine_vars(all_vars, self._extra_vars)\n                    temp_vars = combine_vars(temp_vars, magic_variables)\n                    templar = Templar(loader=self._loader, variables=temp_vars)\n\n                    # we assume each item in the list is itself a list, as we\n                    # support \"conditional includes\" for vars_files, which mimics\n                    # the with_first_found mechanism.\n                    vars_file_list = vars_file_item\n                    if not isinstance(vars_file_list, list):\n                        vars_file_list = [vars_file_list]\n\n                    # now we iterate through the (potential) files, and break out\n                    # as soon as we read one from the list. If none are found, we\n                    # raise an error, which is silently ignored at this point.\n                    try:\n                        for vars_file in vars_file_list:\n                            vars_file = templar.template(vars_file)\n                            if not (isinstance(vars_file, Sequence)):\n                                raise AnsibleError(\n                                    \"Invalid vars_files entry found: %r\\n\"\n                                    \"vars_files entries should be either a string type or \"\n                                    \"a list of string types after template expansion\" % vars_file\n                                )\n                            try:\n                                data = preprocess_vars(self._loader.load_from_file(vars_file, unsafe=True))\n                                if data is not None:\n                                    for item in data:\n                                        all_vars = _combine_and_track(all_vars, item, \"play vars_files from '%s'\" % vars_file)\n                                break\n                            except AnsibleFileNotFound:\n                                # we continue on loader failures\n                                continue\n                            except AnsibleParserError:\n                                raise\n                        else:\n                            # if include_delegate_to is set to False, we ignore the missing\n                            # vars file here because we're working on a delegated host\n                            if include_delegate_to:\n                                raise AnsibleFileNotFound(\"vars file %s was not found\" % vars_file_item)\n                    except (UndefinedError, AnsibleUndefinedVariable):\n                        if host is not None and self._fact_cache.get(host.name, dict()).get('module_setup') and task is not None:\n                            raise AnsibleUndefinedVariable(\"an undefined variable was found when attempting to template the vars_files item '%s'\"\n                                                           % vars_file_item, obj=vars_file_item)\n                        else:\n                            # we do not have a full context here, and the missing variable could be because of that\n                            # so just show a warning and continue\n                            display.vvv(\"skipping vars_file '%s' due to an undefined variable\" % vars_file_item)\n                            continue\n\n                    display.vvv(\"Read vars_file '%s'\" % vars_file_item)\n            except TypeError:\n                raise AnsibleParserError(\"Error while reading vars files - please supply a list of file names. \"\n                                         \"Got '%s' of type %s\" % (vars_files, type(vars_files)))\n\n            # By default, we now merge in all vars from all roles in the play,\n            # unless the user has disabled this via a config option\n            if not C.DEFAULT_PRIVATE_ROLE_VARS:\n                for role in play.get_roles():\n                    all_vars = _combine_and_track(all_vars, role.get_vars(include_params=False), \"role '%s' vars\" % role.name)\n\n        # next, we merge in the vars from the role, which will specifically\n        # follow the role dependency chain, and then we merge in the tasks\n        # vars (which will look at parent blocks/task includes)\n        if task:\n            if task._role:\n                all_vars = _combine_and_track(all_vars, task._role.get_vars(task.get_dep_chain(), include_params=False),\n                                              \"role '%s' vars\" % task._role.name)\n            all_vars = _combine_and_track(all_vars, task.get_vars(), \"task vars\")\n\n        # next, we merge in the vars cache (include vars) and nonpersistent\n        # facts cache (set_fact/register), in that order\n        if host:\n            # include_vars non-persistent cache\n            all_vars = _combine_and_track(all_vars, self._vars_cache.get(host.get_name(), dict()), \"include_vars\")\n            # fact non-persistent cache\n            all_vars = _combine_and_track(all_vars, self._nonpersistent_fact_cache.get(host.name, dict()), \"set_fact\")\n\n        # next, we merge in role params and task include params\n        if task:\n            if task._role:\n                all_vars = _combine_and_track(all_vars, task._role.get_role_params(task.get_dep_chain()), \"role '%s' params\" % task._role.name)\n\n            # special case for include tasks, where the include params\n            # may be specified in the vars field for the task, which should\n            # have higher precedence than the vars/np facts above\n            all_vars = _combine_and_track(all_vars, task.get_include_params(), \"include params\")\n\n        # extra vars\n        all_vars = _combine_and_track(all_vars, self._extra_vars, \"extra vars\")\n\n        # magic variables\n        all_vars = _combine_and_track(all_vars, magic_variables, \"magic vars\")\n\n        # special case for the 'environment' magic variable, as someone\n        # may have set it as a variable and we don't want to stomp on it\n        if task:\n            all_vars['environment'] = task.environment\n\n        # if we have a task and we're delegating to another host, figure out the\n        # variables for that host now so we don't have to rely on hostvars later\n        if task and task.delegate_to is not None and include_delegate_to:\n            all_vars['ansible_delegated_vars'], all_vars['_ansible_loop_cache'] = self._get_delegated_vars(play, task, all_vars)\n\n        # 'vars' magic var\n        if task or play:\n            # has to be copy, otherwise recursive ref\n            all_vars['vars'] = all_vars.copy()\n\n        display.debug(\"done with get_vars()\")\n        if C.DEFAULT_DEBUG:\n            # Use VarsWithSources wrapper class to display var sources\n            return VarsWithSources.new_vars_with_sources(all_vars, _vars_sources)\n        else:\n            return all_vars",
        "begin_line": 146,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager._combine_and_track#186",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager._combine_and_track(data, new_data, source)",
        "snippet": "        def _combine_and_track(data, new_data, source):\n            '''\n            Wrapper function to update var sources dict and call combine_vars()\n\n            See notes in the VarsWithSources docstring for caveats and limitations of the source tracking\n            '''\n            if C.DEFAULT_DEBUG:\n                # Populate var sources dict\n                for key in new_data:\n                    _vars_sources[key] = source\n            return combine_vars(data, new_data)",
        "begin_line": 186,
        "end_line": 196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager._get_magic_variables#446",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager._get_magic_variables(self, play, host, task, include_hostvars, include_delegate_to, _hosts=None, _hosts_all=None)",
        "snippet": "    def _get_magic_variables(self, play, host, task, include_hostvars, include_delegate_to,\n                             _hosts=None, _hosts_all=None):\n        '''\n        Returns a dictionary of so-called \"magic\" variables in Ansible,\n        which are special variables we set internally for use.\n        '''\n\n        variables = {}\n        variables['playbook_dir'] = os.path.abspath(self._loader.get_basedir())\n        variables['ansible_playbook_python'] = sys.executable\n        variables['ansible_config_file'] = C.CONFIG_FILE\n\n        if play:\n            # This is a list of all role names of all dependencies for all roles for this play\n            dependency_role_names = list(set([d.get_name() for r in play.roles for d in r.get_all_dependencies()]))\n            # This is a list of all role names of all roles for this play\n            play_role_names = [r.get_name() for r in play.roles]\n\n            # ansible_role_names includes all role names, dependent or directly referenced by the play\n            variables['ansible_role_names'] = list(set(dependency_role_names + play_role_names))\n            # ansible_play_role_names includes the names of all roles directly referenced by this play\n            # roles that are implicitly referenced via dependencies are not listed.\n            variables['ansible_play_role_names'] = play_role_names\n            # ansible_dependent_role_names includes the names of all roles that are referenced via dependencies\n            # dependencies that are also explicitly named as roles are included in this list\n            variables['ansible_dependent_role_names'] = dependency_role_names\n\n            # DEPRECATED: role_names should be deprecated in favor of ansible_role_names or ansible_play_role_names\n            variables['role_names'] = variables['ansible_play_role_names']\n\n            variables['ansible_play_name'] = play.get_name()\n\n        if task:\n            if task._role:\n                variables['role_name'] = task._role.get_name(include_role_fqcn=False)\n                variables['role_path'] = task._role._role_path\n                variables['role_uuid'] = text_type(task._role._uuid)\n                variables['ansible_collection_name'] = task._role._role_collection\n                variables['ansible_role_name'] = task._role.get_name()\n\n        if self._inventory is not None:\n            variables['groups'] = self._inventory.get_groups_dict()\n            if play:\n                templar = Templar(loader=self._loader)\n                if templar.is_template(play.hosts):\n                    pattern = 'all'\n                else:\n                    pattern = play.hosts or 'all'\n                # add the list of hosts in the play, as adjusted for limit/filters\n                if not _hosts_all:\n                    _hosts_all = [h.name for h in self._inventory.get_hosts(pattern=pattern, ignore_restrictions=True)]\n                if not _hosts:\n                    _hosts = [h.name for h in self._inventory.get_hosts()]\n\n                variables['ansible_play_hosts_all'] = _hosts_all[:]\n                variables['ansible_play_hosts'] = [x for x in variables['ansible_play_hosts_all'] if x not in play._removed_hosts]\n                variables['ansible_play_batch'] = [x for x in _hosts if x not in play._removed_hosts]\n\n                # DEPRECATED: play_hosts should be deprecated in favor of ansible_play_batch,\n                # however this would take work in the templating engine, so for now we'll add both\n                variables['play_hosts'] = variables['ansible_play_batch']\n\n        # the 'omit' value allows params to be left out if the variable they are based on is undefined\n        variables['omit'] = self._omit_token\n        # Set options vars\n        for option, option_value in iteritems(self._options_vars):\n            variables[option] = option_value\n\n        if self._hostvars is not None and include_hostvars:\n            variables['hostvars'] = self._hostvars\n\n        return variables",
        "begin_line": 446,
        "end_line": 517,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager.clear_facts#637",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager.clear_facts(self, hostname)",
        "snippet": "    def clear_facts(self, hostname):\n        '''\n        Clears the facts for a host\n        '''\n        self._fact_cache.pop(hostname, None)",
        "begin_line": 637,
        "end_line": 641,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.vars.manager.VariableManager.set_host_facts#643",
        "src_path": "lib/ansible/vars/manager.py",
        "class_name": "lib.ansible.vars.manager.VariableManager",
        "signature": "lib.ansible.vars.manager.VariableManager.set_host_facts(self, host, facts)",
        "snippet": "    def set_host_facts(self, host, facts):\n        '''\n        Sets or updates the given facts for a host in the fact cache.\n        '''\n\n        if not isinstance(facts, Mapping):\n            raise AnsibleAssertionError(\"the type of 'facts' to set for host_facts should be a Mapping but is a %s\" % type(facts))\n\n        try:\n            host_cache = self._fact_cache[host]\n        except KeyError:\n            # We get to set this as new\n            host_cache = facts\n        else:\n            if not isinstance(host_cache, MutableMapping):\n                raise TypeError('The object retrieved for {0} must be a MutableMapping but was'\n                                ' a {1}'.format(host, type(host_cache)))\n            # Update the existing facts\n            host_cache.update(facts)\n\n        # Save the facts back to the backing store\n        self._fact_cache[host] = host_cache",
        "begin_line": 643,
        "end_line": 664,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.known_hosts.sanity_check#175",
        "src_path": "lib/ansible/modules/system/known_hosts.py",
        "class_name": "lib.ansible.modules.system.known_hosts",
        "signature": "lib.ansible.modules.system.known_hosts.sanity_check(module, host, key, sshkeygen)",
        "snippet": "def sanity_check(module, host, key, sshkeygen):\n    '''Check supplied key is sensible\n\n    host and key are parameters provided by the user; If the host\n    provided is inconsistent with the key supplied, then this function\n    quits, providing an error to the user.\n    sshkeygen is the path to ssh-keygen, found earlier with get_bin_path\n    '''\n    # If no key supplied, we're doing a removal, and have nothing to check here.\n    if not key:\n        return\n    # Rather than parsing the key ourselves, get ssh-keygen to do it\n    # (this is essential for hashed keys, but otherwise useful, as the\n    # key question is whether ssh-keygen thinks the key matches the host).\n\n    # The approach is to write the key to a temporary file,\n    # and then attempt to look up the specified host in that file.\n\n    if re.search(r'\\S+(\\s+)?,(\\s+)?', host):\n        module.fail_json(msg=\"Comma separated list of names is not supported. \"\n                             \"Please pass a single name to lookup in the known_hosts file.\")\n\n    with tempfile.NamedTemporaryFile(mode='w+') as outf:\n        try:\n            outf.write(key)\n            outf.flush()\n        except IOError as e:\n            module.fail_json(msg=\"Failed to write to temporary file %s: %s\" %\n                             (outf.name, to_native(e)))\n\n        sshkeygen_command = [sshkeygen, '-F', host, '-f', outf.name]\n        rc, stdout, stderr = module.run_command(sshkeygen_command)\n\n    if stdout == '':  # host not found\n        module.fail_json(msg=\"Host parameter does not match hashed host field in supplied key\")",
        "begin_line": 175,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.known_hosts.compute_diff#301",
        "src_path": "lib/ansible/modules/system/known_hosts.py",
        "class_name": "lib.ansible.modules.system.known_hosts",
        "signature": "lib.ansible.modules.system.known_hosts.compute_diff(path, found_line, replace_or_add, state, key)",
        "snippet": "def compute_diff(path, found_line, replace_or_add, state, key):\n    diff = {\n        'before_header': path,\n        'after_header': path,\n        'before': '',\n        'after': '',\n    }\n    try:\n        inf = open(path, \"r\")\n    except IOError as e:\n        if e.errno == errno.ENOENT:\n            diff['before_header'] = '/dev/null'\n    else:\n        diff['before'] = inf.read()\n        inf.close()\n    lines = diff['before'].splitlines(1)\n    if (replace_or_add or state == 'absent') and found_line is not None and 1 <= found_line <= len(lines):\n        del lines[found_line - 1]\n    if state == 'present' and (replace_or_add or found_line is None):\n        lines.append(key)\n    diff['after'] = ''.join(lines)\n    return diff",
        "begin_line": 301,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.__init__#71",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.__init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None)",
        "snippet": "    def __init__(self, inventory, variable_manager, loader, passwords, stdout_callback=None, run_additional_callbacks=True, run_tree=False, forks=None):\n\n        self._inventory = inventory\n        self._variable_manager = variable_manager\n        self._loader = loader\n        self._stats = AggregateStats()\n        self.passwords = passwords\n        self._stdout_callback = stdout_callback\n        self._run_additional_callbacks = run_additional_callbacks\n        self._run_tree = run_tree\n        self._forks = forks or 5\n\n        self._callbacks_loaded = False\n        self._callback_plugins = []\n        self._start_at_done = False\n\n        # make sure any module paths (if specified) are added to the module_loader\n        if context.CLIARGS.get('module_path', False):\n            for path in context.CLIARGS['module_path']:\n                if path:\n                    module_loader.add_directory(path)\n\n        # a special flag to help us exit cleanly\n        self._terminated = False\n\n        # dictionaries to keep track of failed/unreachable hosts\n        self._failed_hosts = dict()\n        self._unreachable_hosts = dict()\n\n        try:\n            self._final_q = multiprocessing_context.Queue()\n        except OSError as e:\n            raise AnsibleError(\"Unable to use multiprocessing, this is normally caused by lack of access to /dev/shm: %s\" % to_native(e))\n\n        # A temporary file (opened pre-fork) used by connection\n        # plugins for inter-process locking.\n        self._connection_lockfile = tempfile.TemporaryFile()",
        "begin_line": 71,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager._initialize_processes#109",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager._initialize_processes(self, num)",
        "snippet": "    def _initialize_processes(self, num):\n        self._workers = []\n\n        for i in range(num):\n            self._workers.append(None)",
        "begin_line": 109,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.cleanup#252",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.cleanup(self)",
        "snippet": "    def cleanup(self):\n        display.debug(\"RUNNING CLEANUP\")\n        self.terminate()\n        self._final_q.close()\n        self._cleanup_processes()",
        "begin_line": 252,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager._cleanup_processes#258",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager._cleanup_processes(self)",
        "snippet": "    def _cleanup_processes(self):\n        if hasattr(self, '_workers'):\n            for attempts_remaining in range(C.WORKER_SHUTDOWN_POLL_COUNT - 1, -1, -1):\n                if not any(worker_prc and worker_prc.is_alive() for worker_prc in self._workers):\n                    break\n\n                if attempts_remaining:\n                    time.sleep(C.WORKER_SHUTDOWN_POLL_DELAY)\n                else:\n                    display.warning('One or more worker processes are still running and will be terminated.')\n\n            for worker_prc in self._workers:\n                if worker_prc and worker_prc.is_alive():\n                    try:\n                        worker_prc.terminate()\n                    except AttributeError:\n                        pass",
        "begin_line": 258,
        "end_line": 274,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_inventory#279",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_inventory(self)",
        "snippet": "    def get_inventory(self):\n        return self._inventory",
        "begin_line": 279,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_variable_manager#282",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_variable_manager(self)",
        "snippet": "    def get_variable_manager(self):\n        return self._variable_manager",
        "begin_line": 282,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_loader#285",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.get_loader(self)",
        "snippet": "    def get_loader(self):\n        return self._loader",
        "begin_line": 285,
        "end_line": 286,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.terminate#291",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.terminate(self)",
        "snippet": "    def terminate(self):\n        self._terminated = True",
        "begin_line": 291,
        "end_line": 292,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.executor.task_queue_manager.TaskQueueManager.send_callback#305",
        "src_path": "lib/ansible/executor/task_queue_manager.py",
        "class_name": "lib.ansible.executor.task_queue_manager.TaskQueueManager",
        "signature": "lib.ansible.executor.task_queue_manager.TaskQueueManager.send_callback(self, method_name, *args, **kwargs)",
        "snippet": "    def send_callback(self, method_name, *args, **kwargs):\n        for callback_plugin in [self._stdout_callback] + self._callback_plugins:\n            # a plugin that set self.disabled to True will not be called\n            # see osx_say.py example for such a plugin\n            if getattr(callback_plugin, 'disabled', False):\n                continue\n\n            # try to find v2 method, fallback to v1 method, ignore callback if no method found\n            methods = []\n            for possible in [method_name, 'v2_on_any']:\n                gotit = getattr(callback_plugin, possible, None)\n                if gotit is None:\n                    gotit = getattr(callback_plugin, possible.replace('v2_', ''), None)\n                if gotit is not None:\n                    methods.append(gotit)\n\n            # send clean copies\n            new_args = []\n            for arg in args:\n                # FIXME: add play/task cleaners\n                if isinstance(arg, TaskResult):\n                    new_args.append(arg.clean_copy())\n                # elif isinstance(arg, Play):\n                # elif isinstance(arg, Task):\n                else:\n                    new_args.append(arg)\n\n            for method in methods:\n                try:\n                    method(*new_args, **kwargs)\n                except Exception as e:\n                    # TODO: add config toggle to make this fatal or not?\n                    display.warning(u\"Failure using method (%s) in callback plugin (%s): %s\" % (to_text(method_name), to_text(callback_plugin), to_text(e)))\n                    from traceback import format_tb\n                    from sys import exc_info\n                    display.vvv('Callback Exception: \\n' + ' '.join(format_tb(exc_info()[2])))",
        "begin_line": 305,
        "end_line": 340,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.unique#49",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.unique(environment, a, case_sensitive=False, attribute=None)",
        "snippet": "def unique(environment, a, case_sensitive=False, attribute=None):\n\n    def _do_fail(e):\n        if case_sensitive or attribute:\n            raise AnsibleFilterError(\"Jinja2's unique filter failed and we cannot fall back to Ansible's version \"\n                                     \"as it does not support the parameters supplied\", orig_exc=e)\n\n    error = e = None\n    try:\n        if HAS_UNIQUE:\n            c = do_unique(environment, a, case_sensitive=case_sensitive, attribute=attribute)\n            if isinstance(a, Hashable):\n                c = set(c)\n            else:\n                c = list(c)\n    except TypeError as e:\n        error = e\n        _do_fail(e)\n    except Exception as e:\n        error = e\n        _do_fail(e)\n        display.warning('Falling back to Ansible unique filter as Jinja2 one failed: %s' % to_text(e))\n\n    if not HAS_UNIQUE or error:\n\n        # handle Jinja2 specific attributes when using Ansible's version\n        if case_sensitive or attribute:\n            raise AnsibleFilterError(\"Ansible's unique filter does not support case_sensitive nor attribute parameters, \"\n                                     \"you need a newer version of Jinja2 that provides their version of the filter.\")\n\n        if isinstance(a, Hashable):\n            c = set(a)\n        else:\n            c = []\n            for x in a:\n                if x not in c:\n                    c.append(x)\n    return c",
        "begin_line": 49,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff._do_fail#51",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff._do_fail(e)",
        "snippet": "    def _do_fail(e):\n        if case_sensitive or attribute:\n            raise AnsibleFilterError(\"Jinja2's unique filter failed and we cannot fall back to Ansible's version \"\n                                     \"as it does not support the parameters supplied\", orig_exc=e)",
        "begin_line": 51,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.183306055646482e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.intersect#90",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.intersect(environment, a, b)",
        "snippet": "def intersect(environment, a, b):\n    if isinstance(a, Hashable) and isinstance(b, Hashable):\n        c = set(a) & set(b)\n    else:\n        c = unique(environment, [x for x in a if x in b])\n    return c",
        "begin_line": 90,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.difference#99",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.difference(environment, a, b)",
        "snippet": "def difference(environment, a, b):\n    if isinstance(a, Hashable) and isinstance(b, Hashable):\n        c = set(a) - set(b)\n    else:\n        c = unique(environment, [x for x in a if x not in b])\n    return c",
        "begin_line": 99,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.symmetric_difference#108",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.symmetric_difference(environment, a, b)",
        "snippet": "def symmetric_difference(environment, a, b):\n    if isinstance(a, Hashable) and isinstance(b, Hashable):\n        c = set(a) ^ set(b)\n    else:\n        isect = intersect(environment, a, b)\n        c = [x for x in union(environment, a, b) if x not in isect]\n    return c",
        "begin_line": 108,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.union#118",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.union(environment, a, b)",
        "snippet": "def union(environment, a, b):\n    if isinstance(a, Hashable) and isinstance(b, Hashable):\n        c = set(a) | set(b)\n    else:\n        c = unique(environment, a + b)\n    return c",
        "begin_line": 118,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.min#126",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.min(a)",
        "snippet": "def min(a):\n    _min = __builtins__.get('min')\n    return _min(a)",
        "begin_line": 126,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.max#131",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.max(a)",
        "snippet": "def max(a):\n    _max = __builtins__.get('max')\n    return _max(a)",
        "begin_line": 131,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.logarithm#136",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.logarithm(x, base=math.e)",
        "snippet": "def logarithm(x, base=math.e):\n    try:\n        if base == 10:\n            return math.log10(x)\n        else:\n            return math.log(x, base)\n    except TypeError as e:\n        raise AnsibleFilterError('log() can only be used on numbers: %s' % to_native(e))",
        "begin_line": 136,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.power#146",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.power(x, y)",
        "snippet": "def power(x, y):\n    try:\n        return math.pow(x, y)\n    except TypeError as e:\n        raise AnsibleFilterError('pow() can only be used on numbers: %s' % to_native(e))",
        "begin_line": 146,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.inversepower#153",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.inversepower(x, base=2)",
        "snippet": "def inversepower(x, base=2):\n    try:\n        if base == 2:\n            return math.sqrt(x)\n        else:\n            return math.pow(x, 1.0 / float(base))\n    except (ValueError, TypeError) as e:\n        raise AnsibleFilterError('root() can only be used on numbers: %s' % to_native(e))",
        "begin_line": 153,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.human_readable#163",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.human_readable(size, isbits=False, unit=None)",
        "snippet": "def human_readable(size, isbits=False, unit=None):\n    ''' Return a human readable string '''\n    try:\n        return formatters.bytes_to_human(size, isbits, unit)\n    except Exception:\n        raise AnsibleFilterError(\"human_readable() can't interpret following string: %s\" % size)",
        "begin_line": 163,
        "end_line": 168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.human_to_bytes#171",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.human_to_bytes(size, default_unit=None, isbits=False)",
        "snippet": "def human_to_bytes(size, default_unit=None, isbits=False):\n    ''' Return bytes count from a human readable string '''\n    try:\n        return formatters.human_to_bytes(size, default_unit, isbits)\n    except Exception:\n        raise AnsibleFilterError(\"human_to_bytes() can't interpret following string: %s\" % size)",
        "begin_line": 171,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.rekey_on_member#179",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff",
        "signature": "lib.ansible.plugins.filter.mathstuff.rekey_on_member(data, key, duplicates='error')",
        "snippet": "def rekey_on_member(data, key, duplicates='error'):\n    \"\"\"\n    Rekey a dict of dicts on another member\n\n    May also create a dict from a list of dicts.\n\n    duplicates can be one of ``error`` or ``overwrite`` to specify whether to error out if the key\n    value would be duplicated or to overwrite previous entries if that's the case.\n    \"\"\"\n    if duplicates not in ('error', 'overwrite'):\n        raise AnsibleFilterError(\"duplicates parameter to rekey_on_member has unknown value: {0}\".format(duplicates))\n\n    new_obj = {}\n\n    if isinstance(data, Mapping):\n        iterate_over = data.values()\n    elif isinstance(data, Iterable) and not isinstance(data, (text_type, binary_type)):\n        iterate_over = data\n    else:\n        raise AnsibleFilterError(\"Type is not a valid list, set, or dict\")\n\n    for item in iterate_over:\n        if not isinstance(item, Mapping):\n            raise AnsibleFilterError(\"List item is not a valid dict\")\n\n        try:\n            key_elem = item[key]\n        except KeyError:\n            raise AnsibleFilterError(\"Key {0} was not found\".format(key))\n        except Exception as e:\n            raise AnsibleFilterError(to_native(e))\n\n        # Note: if new_obj[key_elem] exists it will always be a non-empty dict (it will at\n        # minimum contain {key: key_elem}\n        if new_obj.get(key_elem, None):\n            if duplicates == 'error':\n                raise AnsibleFilterError(\"Key {0} is not unique, cannot correctly turn into dict\".format(key_elem))\n            elif duplicates == 'overwrite':\n                new_obj[key_elem] = item\n        else:\n            new_obj[key_elem] = item\n\n    return new_obj",
        "begin_line": 179,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.filter.mathstuff.FilterModule.filters#227",
        "src_path": "lib/ansible/plugins/filter/mathstuff.py",
        "class_name": "lib.ansible.plugins.filter.mathstuff.FilterModule",
        "signature": "lib.ansible.plugins.filter.mathstuff.FilterModule.filters(self)",
        "snippet": "    def filters(self):\n        filters = {\n            # general math\n            'min': min,\n            'max': max,\n\n            # exponents and logarithms\n            'log': logarithm,\n            'pow': power,\n            'root': inversepower,\n\n            # set theory\n            'unique': unique,\n            'intersect': intersect,\n            'difference': difference,\n            'symmetric_difference': symmetric_difference,\n            'union': union,\n\n            # combinatorial\n            'product': itertools.product,\n            'permutations': itertools.permutations,\n            'combinations': itertools.combinations,\n\n            # computer theory\n            'human_readable': human_readable,\n            'human_to_bytes': human_to_bytes,\n            'rekey_on_member': rekey_on_member,\n\n            # zip\n            'zip': zip,\n            'zip_longest': zip_longest,\n\n        }\n\n        return filters",
        "begin_line": 227,
        "end_line": 261,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006131207847946045,
            "pseudo_dstar_susp": 0.0006131207847946045,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0006131207847946045,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.cli.galaxy._display_header#53",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy",
        "signature": "lib.ansible.cli.galaxy._display_header(path, h1, h2, w1=10, w2=7)",
        "snippet": "def _display_header(path, h1, h2, w1=10, w2=7):\n    display.display('\\n# {0}\\n{1:{cwidth}} {2:{vwidth}}\\n{3} {4}\\n'.format(\n        path,\n        h1,\n        h2,\n        '-' * max([len(h1), w1]),  # Make sure that the number of dashes is at least the width of the header\n        '-' * max([len(h2), w2]),\n        cwidth=w1,\n        vwidth=w2,\n    ))",
        "begin_line": 53,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.__init__#103",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.__init__(self, args)",
        "snippet": "    def __init__(self, args):\n        # Inject role into sys.argv[1] as a backwards compatibility step\n        if len(args) > 1 and args[1] not in ['-h', '--help', '--version'] and 'role' not in args and 'collection' not in args:\n            # TODO: Should we add a warning here and eventually deprecate the implicit role subcommand choice\n            # Remove this in Ansible 2.13 when we also remove -v as an option on the root parser for ansible-galaxy.\n            idx = 2 if args[1].startswith('-v') else 1\n            args.insert(idx, 'role')\n\n        self.api_servers = []\n        self.galaxy = None\n        super(GalaxyCLI, self).__init__(args)",
        "begin_line": 103,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012048192771084338,
            "pseudo_dstar_susp": 0.001519756838905775,
            "pseudo_tarantula_susp": 0.0010080645161290322,
            "pseudo_op2_susp": 0.001519756838905775,
            "pseudo_barinel_susp": 0.0010080645161290322
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.init_parser#115",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.init_parser(self)",
        "snippet": "    def init_parser(self):\n        ''' create an options parser for bin/ansible '''\n\n        super(GalaxyCLI, self).init_parser(\n            desc=\"Perform various Role and Collection related operations.\",\n        )\n\n        # Common arguments that apply to more than 1 action\n        common = opt_help.argparse.ArgumentParser(add_help=False)\n        common.add_argument('-s', '--server', dest='api_server', help='The Galaxy API server URL')\n        common.add_argument('--token', '--api-key', dest='api_key',\n                            help='The Ansible Galaxy API key which can be found at '\n                                 'https://galaxy.ansible.com/me/preferences. You can also use ansible-galaxy login to '\n                                 'retrieve this key or set the token for the GALAXY_SERVER_LIST entry.')\n        common.add_argument('-c', '--ignore-certs', action='store_true', dest='ignore_certs',\n                            default=C.GALAXY_IGNORE_CERTS, help='Ignore SSL certificate validation errors.')\n        opt_help.add_verbosity_options(common)\n\n        force = opt_help.argparse.ArgumentParser(add_help=False)\n        force.add_argument('-f', '--force', dest='force', action='store_true', default=False,\n                           help='Force overwriting an existing role or collection')\n\n        github = opt_help.argparse.ArgumentParser(add_help=False)\n        github.add_argument('github_user', help='GitHub username')\n        github.add_argument('github_repo', help='GitHub repository')\n\n        offline = opt_help.argparse.ArgumentParser(add_help=False)\n        offline.add_argument('--offline', dest='offline', default=False, action='store_true',\n                             help=\"Don't query the galaxy API when creating roles\")\n\n        default_roles_path = C.config.get_configuration_definition('DEFAULT_ROLES_PATH').get('default', '')\n        roles_path = opt_help.argparse.ArgumentParser(add_help=False)\n        roles_path.add_argument('-p', '--roles-path', dest='roles_path', type=opt_help.unfrack_path(pathsep=True),\n                                default=C.DEFAULT_ROLES_PATH, action=opt_help.PrependListAction,\n                                help='The path to the directory containing your roles. The default is the first '\n                                     'writable one configured via DEFAULT_ROLES_PATH: %s ' % default_roles_path)\n\n        collections_path = opt_help.argparse.ArgumentParser(add_help=False)\n        collections_path.add_argument('-p', '--collection-path', dest='collections_path', type=opt_help.unfrack_path(pathsep=True),\n                                      default=C.COLLECTIONS_PATHS, action=opt_help.PrependListAction,\n                                      help=\"One or more directories to search for collections in addition \"\n                                      \"to the default COLLECTIONS_PATHS. Separate multiple paths \"\n                                      \"with '{0}'.\".format(os.path.pathsep))\n\n        # Add sub parser for the Galaxy role type (role or collection)\n        type_parser = self.parser.add_subparsers(metavar='TYPE', dest='type')\n        type_parser.required = True\n\n        # Add sub parser for the Galaxy collection actions\n        collection = type_parser.add_parser('collection', help='Manage an Ansible Galaxy collection.')\n        collection_parser = collection.add_subparsers(metavar='COLLECTION_ACTION', dest='action')\n        collection_parser.required = True\n        self.add_download_options(collection_parser, parents=[common])\n        self.add_init_options(collection_parser, parents=[common, force])\n        self.add_build_options(collection_parser, parents=[common, force])\n        self.add_publish_options(collection_parser, parents=[common])\n        self.add_install_options(collection_parser, parents=[common, force])\n        self.add_list_options(collection_parser, parents=[common, collections_path])\n        self.add_verify_options(collection_parser, parents=[common, collections_path])\n\n        # Add sub parser for the Galaxy role actions\n        role = type_parser.add_parser('role', help='Manage an Ansible Galaxy role.')\n        role_parser = role.add_subparsers(metavar='ROLE_ACTION', dest='action')\n        role_parser.required = True\n        self.add_init_options(role_parser, parents=[common, force, offline])\n        self.add_remove_options(role_parser, parents=[common, roles_path])\n        self.add_delete_options(role_parser, parents=[common, github])\n        self.add_list_options(role_parser, parents=[common, roles_path])\n        self.add_search_options(role_parser, parents=[common])\n        self.add_import_options(role_parser, parents=[common, github])\n        self.add_setup_options(role_parser, parents=[common, roles_path])\n        self.add_login_options(role_parser, parents=[common])\n        self.add_info_options(role_parser, parents=[common, roles_path, offline])\n        self.add_install_options(role_parser, parents=[common, force, roles_path])",
        "begin_line": 115,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_download_options#190",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_download_options(self, parser, parents=None)",
        "snippet": "    def add_download_options(self, parser, parents=None):\n        download_parser = parser.add_parser('download', parents=parents,\n                                            help='Download collections and their dependencies as a tarball for an '\n                                                 'offline install.')\n        download_parser.set_defaults(func=self.execute_download)\n\n        download_parser.add_argument('args', help='Collection(s)', metavar='collection', nargs='*')\n\n        download_parser.add_argument('-n', '--no-deps', dest='no_deps', action='store_true', default=False,\n                                     help=\"Don't download collection(s) listed as dependencies.\")\n\n        download_parser.add_argument('-p', '--download-path', dest='download_path',\n                                     default='./collections',\n                                     help='The directory to download the collections to.')\n        download_parser.add_argument('-r', '--requirements-file', dest='requirements',\n                                     help='A file containing a list of collections to be downloaded.')\n        download_parser.add_argument('--pre', dest='allow_pre_release', action='store_true',\n                                     help='Include pre-release versions. Semantic versioning pre-releases are ignored by default')",
        "begin_line": 190,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_init_options#209",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_init_options(self, parser, parents=None)",
        "snippet": "    def add_init_options(self, parser, parents=None):\n        galaxy_type = 'collection' if parser.metavar == 'COLLECTION_ACTION' else 'role'\n\n        init_parser = parser.add_parser('init', parents=parents,\n                                        help='Initialize new {0} with the base structure of a '\n                                             '{0}.'.format(galaxy_type))\n        init_parser.set_defaults(func=self.execute_init)\n\n        init_parser.add_argument('--init-path', dest='init_path', default='./',\n                                 help='The path in which the skeleton {0} will be created. The default is the '\n                                      'current working directory.'.format(galaxy_type))\n        init_parser.add_argument('--{0}-skeleton'.format(galaxy_type), dest='{0}_skeleton'.format(galaxy_type),\n                                 default=C.GALAXY_ROLE_SKELETON,\n                                 help='The path to a {0} skeleton that the new {0} should be based '\n                                      'upon.'.format(galaxy_type))\n\n        obj_name_kwargs = {}\n        if galaxy_type == 'collection':\n            obj_name_kwargs['type'] = validate_collection_name\n        init_parser.add_argument('{0}_name'.format(galaxy_type), help='{0} name'.format(galaxy_type.capitalize()),\n                                 **obj_name_kwargs)\n\n        if galaxy_type == 'role':\n            init_parser.add_argument('--type', dest='role_type', action='store', default='default',\n                                     help=\"Initialize using an alternate role type. Valid types include: 'container', \"\n                                          \"'apb' and 'network'.\")",
        "begin_line": 209,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_remove_options#236",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_remove_options(self, parser, parents=None)",
        "snippet": "    def add_remove_options(self, parser, parents=None):\n        remove_parser = parser.add_parser('remove', parents=parents, help='Delete roles from roles_path.')\n        remove_parser.set_defaults(func=self.execute_remove)\n\n        remove_parser.add_argument('args', help='Role(s)', metavar='role', nargs='+')",
        "begin_line": 236,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_delete_options#242",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_delete_options(self, parser, parents=None)",
        "snippet": "    def add_delete_options(self, parser, parents=None):\n        delete_parser = parser.add_parser('delete', parents=parents,\n                                          help='Removes the role from Galaxy. It does not remove or alter the actual '\n                                               'GitHub repository.')\n        delete_parser.set_defaults(func=self.execute_delete)",
        "begin_line": 242,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_list_options#248",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_list_options(self, parser, parents=None)",
        "snippet": "    def add_list_options(self, parser, parents=None):\n        galaxy_type = 'role'\n        if parser.metavar == 'COLLECTION_ACTION':\n            galaxy_type = 'collection'\n\n        list_parser = parser.add_parser('list', parents=parents,\n                                        help='Show the name and version of each {0} installed in the {0}s_path.'.format(galaxy_type))\n\n        list_parser.set_defaults(func=self.execute_list)\n\n        list_parser.add_argument(galaxy_type, help=galaxy_type.capitalize(), nargs='?', metavar=galaxy_type)",
        "begin_line": 248,
        "end_line": 258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_search_options#260",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_search_options(self, parser, parents=None)",
        "snippet": "    def add_search_options(self, parser, parents=None):\n        search_parser = parser.add_parser('search', parents=parents,\n                                          help='Search the Galaxy database by tags, platforms, author and multiple '\n                                               'keywords.')\n        search_parser.set_defaults(func=self.execute_search)\n\n        search_parser.add_argument('--platforms', dest='platforms', help='list of OS platforms to filter by')\n        search_parser.add_argument('--galaxy-tags', dest='galaxy_tags', help='list of galaxy tags to filter by')\n        search_parser.add_argument('--author', dest='author', help='GitHub username')\n        search_parser.add_argument('args', help='Search terms', metavar='searchterm', nargs='*')",
        "begin_line": 260,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_import_options#271",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_import_options(self, parser, parents=None)",
        "snippet": "    def add_import_options(self, parser, parents=None):\n        import_parser = parser.add_parser('import', parents=parents, help='Import a role into a galaxy server')\n        import_parser.set_defaults(func=self.execute_import)\n\n        import_parser.add_argument('--no-wait', dest='wait', action='store_false', default=True,\n                                   help=\"Don't wait for import results.\")\n        import_parser.add_argument('--branch', dest='reference',\n                                   help='The name of a branch to import. Defaults to the repository\\'s default branch '\n                                        '(usually master)')\n        import_parser.add_argument('--role-name', dest='role_name',\n                                   help='The name the role should have, if different than the repo name')\n        import_parser.add_argument('--status', dest='check_status', action='store_true', default=False,\n                                   help='Check the status of the most recent import request for given github_'\n                                        'user/github_repo.')",
        "begin_line": 271,
        "end_line": 284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_setup_options#286",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_setup_options(self, parser, parents=None)",
        "snippet": "    def add_setup_options(self, parser, parents=None):\n        setup_parser = parser.add_parser('setup', parents=parents,\n                                         help='Manage the integration between Galaxy and the given source.')\n        setup_parser.set_defaults(func=self.execute_setup)\n\n        setup_parser.add_argument('--remove', dest='remove_id', default=None,\n                                  help='Remove the integration matching the provided ID value. Use --list to see '\n                                       'ID values.')\n        setup_parser.add_argument('--list', dest=\"setup_list\", action='store_true', default=False,\n                                  help='List all of your integrations.')\n        setup_parser.add_argument('source', help='Source')\n        setup_parser.add_argument('github_user', help='GitHub username')\n        setup_parser.add_argument('github_repo', help='GitHub repository')\n        setup_parser.add_argument('secret', help='Secret')",
        "begin_line": 286,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_login_options#301",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_login_options(self, parser, parents=None)",
        "snippet": "    def add_login_options(self, parser, parents=None):\n        login_parser = parser.add_parser('login', parents=parents,\n                                         help=\"Login to api.github.com server in order to use ansible-galaxy role sub \"\n                                              \"command such as 'import', 'delete', 'publish', and 'setup'\")\n        login_parser.set_defaults(func=self.execute_login)\n\n        login_parser.add_argument('--github-token', dest='token', default=None,\n                                  help='Identify with github token rather than username and password.')",
        "begin_line": 301,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_info_options#310",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_info_options(self, parser, parents=None)",
        "snippet": "    def add_info_options(self, parser, parents=None):\n        info_parser = parser.add_parser('info', parents=parents, help='View more details about a specific role.')\n        info_parser.set_defaults(func=self.execute_info)\n\n        info_parser.add_argument('args', nargs='+', help='role', metavar='role_name[,version]')",
        "begin_line": 310,
        "end_line": 314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_verify_options#316",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_verify_options(self, parser, parents=None)",
        "snippet": "    def add_verify_options(self, parser, parents=None):\n        galaxy_type = 'collection'\n        verify_parser = parser.add_parser('verify', parents=parents, help='Compare checksums with the collection(s) '\n                                          'found on the server and the installed copy. This does not verify dependencies.')\n        verify_parser.set_defaults(func=self.execute_verify)\n\n        verify_parser.add_argument('args', metavar='{0}_name'.format(galaxy_type), nargs='*', help='The collection(s) name or '\n                                   'path/url to a tar.gz collection artifact. This is mutually exclusive with --requirements-file.')\n        verify_parser.add_argument('-i', '--ignore-errors', dest='ignore_errors', action='store_true', default=False,\n                                   help='Ignore errors during verification and continue with the next specified collection.')\n        verify_parser.add_argument('-r', '--requirements-file', dest='requirements',\n                                   help='A file containing a list of collections to be verified.')",
        "begin_line": 316,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_install_options#329",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_install_options(self, parser, parents=None)",
        "snippet": "    def add_install_options(self, parser, parents=None):\n        galaxy_type = 'collection' if parser.metavar == 'COLLECTION_ACTION' else 'role'\n\n        args_kwargs = {}\n        if galaxy_type == 'collection':\n            args_kwargs['help'] = 'The collection(s) name or path/url to a tar.gz collection artifact. This is ' \\\n                                  'mutually exclusive with --requirements-file.'\n            ignore_errors_help = 'Ignore errors during installation and continue with the next specified ' \\\n                                 'collection. This will not ignore dependency conflict errors.'\n        else:\n            args_kwargs['help'] = 'Role name, URL or tar file'\n            ignore_errors_help = 'Ignore errors and continue with the next specified role.'\n\n        install_parser = parser.add_parser('install', parents=parents,\n                                           help='Install {0}(s) from file(s), URL(s) or Ansible '\n                                                'Galaxy'.format(galaxy_type))\n        install_parser.set_defaults(func=self.execute_install)\n\n        install_parser.add_argument('args', metavar='{0}_name'.format(galaxy_type), nargs='*', **args_kwargs)\n        install_parser.add_argument('-i', '--ignore-errors', dest='ignore_errors', action='store_true', default=False,\n                                    help=ignore_errors_help)\n\n        install_exclusive = install_parser.add_mutually_exclusive_group()\n        install_exclusive.add_argument('-n', '--no-deps', dest='no_deps', action='store_true', default=False,\n                                       help=\"Don't download {0}s listed as dependencies.\".format(galaxy_type))\n        install_exclusive.add_argument('--force-with-deps', dest='force_with_deps', action='store_true', default=False,\n                                       help=\"Force overwriting an existing {0} and its \"\n                                            \"dependencies.\".format(galaxy_type))\n\n        if galaxy_type == 'collection':\n            install_parser.add_argument('-p', '--collections-path', dest='collections_path',\n                                        default=C.COLLECTIONS_PATHS[0],\n                                        help='The path to the directory containing your collections.')\n            install_parser.add_argument('-r', '--requirements-file', dest='requirements',\n                                        help='A file containing a list of collections to be installed.')\n            install_parser.add_argument('--pre', dest='allow_pre_release', action='store_true',\n                                        help='Include pre-release versions. Semantic versioning pre-releases are ignored by default')\n        else:\n            install_parser.add_argument('-r', '--role-file', dest='role_file',\n                                        help='A file containing a list of roles to be imported.')\n            install_parser.add_argument('-g', '--keep-scm-meta', dest='keep_scm_meta', action='store_true',\n                                        default=False,\n                                        help='Use tar instead of the scm archive option when packaging the role.')",
        "begin_line": 329,
        "end_line": 371,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_build_options#373",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_build_options(self, parser, parents=None)",
        "snippet": "    def add_build_options(self, parser, parents=None):\n        build_parser = parser.add_parser('build', parents=parents,\n                                         help='Build an Ansible collection artifact that can be publish to Ansible '\n                                              'Galaxy.')\n        build_parser.set_defaults(func=self.execute_build)\n\n        build_parser.add_argument('args', metavar='collection', nargs='*', default=('.',),\n                                  help='Path to the collection(s) directory to build. This should be the directory '\n                                       'that contains the galaxy.yml file. The default is the current working '\n                                       'directory.')\n        build_parser.add_argument('--output-path', dest='output_path', default='./',\n                                  help='The path in which the collection is built to. The default is the current '\n                                       'working directory.')",
        "begin_line": 373,
        "end_line": 385,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.add_publish_options#387",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.add_publish_options(self, parser, parents=None)",
        "snippet": "    def add_publish_options(self, parser, parents=None):\n        publish_parser = parser.add_parser('publish', parents=parents,\n                                           help='Publish a collection artifact to Ansible Galaxy.')\n        publish_parser.set_defaults(func=self.execute_publish)\n\n        publish_parser.add_argument('args', metavar='collection_path',\n                                    help='The path to the collection tarball to publish.')\n        publish_parser.add_argument('--no-wait', dest='wait', action='store_false', default=True,\n                                    help=\"Don't wait for import validation results.\")\n        publish_parser.add_argument('--import-timeout', dest='import_timeout', type=int, default=0,\n                                    help=\"The time to wait for the collection import process to finish.\")",
        "begin_line": 387,
        "end_line": 397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.0018281535648994515,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.0018281535648994515,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.post_process_args#399",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        options = super(GalaxyCLI, self).post_process_args(options)\n        display.verbosity = options.verbosity\n        return options",
        "begin_line": 399,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017211703958691911,
            "pseudo_dstar_susp": 0.002325581395348837,
            "pseudo_tarantula_susp": 0.0014104372355430183,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0014104372355430183
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.run#404",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.run(self)",
        "snippet": "    def run(self):\n\n        super(GalaxyCLI, self).run()\n\n        self.galaxy = Galaxy()\n\n        def server_config_def(section, key, required):\n            return {\n                'description': 'The %s of the %s Galaxy server' % (key, section),\n                'ini': [\n                    {\n                        'section': 'galaxy_server.%s' % section,\n                        'key': key,\n                    }\n                ],\n                'env': [\n                    {'name': 'ANSIBLE_GALAXY_SERVER_%s_%s' % (section.upper(), key.upper())},\n                ],\n                'required': required,\n            }\n        server_def = [('url', True), ('username', False), ('password', False), ('token', False),\n                      ('auth_url', False)]\n\n        config_servers = []\n\n        # Need to filter out empty strings or non truthy values as an empty server list env var is equal to [''].\n        server_list = [s for s in C.GALAXY_SERVER_LIST or [] if s]\n        for server_key in server_list:\n            # Config definitions are looked up dynamically based on the C.GALAXY_SERVER_LIST entry. We look up the\n            # section [galaxy_server.<server>] for the values url, username, password, and token.\n            config_dict = dict((k, server_config_def(server_key, k, req)) for k, req in server_def)\n            defs = AnsibleLoader(yaml.safe_dump(config_dict)).get_single_data()\n            C.config.initialize_plugin_configuration_definitions('galaxy_server', server_key, defs)\n\n            server_options = C.config.get_plugin_options('galaxy_server', server_key)\n            # auth_url is used to create the token, but not directly by GalaxyAPI, so\n            # it doesn't need to be passed as kwarg to GalaxyApi\n            auth_url = server_options.pop('auth_url', None)\n            token_val = server_options['token'] or NoTokenSentinel\n            username = server_options['username']\n\n            # default case if no auth info is provided.\n            server_options['token'] = None\n\n            if username:\n                server_options['token'] = BasicAuthToken(username,\n                                                         server_options['password'])\n            else:\n                if token_val:\n                    if auth_url:\n                        server_options['token'] = KeycloakToken(access_token=token_val,\n                                                                auth_url=auth_url,\n                                                                validate_certs=not context.CLIARGS['ignore_certs'])\n                    else:\n                        # The galaxy v1 / github / django / 'Token'\n                        server_options['token'] = GalaxyToken(token=token_val)\n\n            config_servers.append(GalaxyAPI(self.galaxy, server_key, **server_options))\n\n        cmd_server = context.CLIARGS['api_server']\n        cmd_token = GalaxyToken(token=context.CLIARGS['api_key'])\n        if cmd_server:\n            # Cmd args take precedence over the config entry but fist check if the arg was a name and use that config\n            # entry, otherwise create a new API entry for the server specified.\n            config_server = next((s for s in config_servers if s.name == cmd_server), None)\n            if config_server:\n                self.api_servers.append(config_server)\n            else:\n                self.api_servers.append(GalaxyAPI(self.galaxy, 'cmd_arg', cmd_server, token=cmd_token))\n        else:\n            self.api_servers = config_servers\n\n        # Default to C.GALAXY_SERVER if no servers were defined\n        if len(self.api_servers) == 0:\n            self.api_servers.append(GalaxyAPI(self.galaxy, 'default', C.GALAXY_SERVER, token=cmd_token))\n\n        context.CLIARGS['func']()",
        "begin_line": 404,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002079002079002079,
            "pseudo_dstar_susp": 0.006993006993006993,
            "pseudo_tarantula_susp": 0.0014534883720930232,
            "pseudo_op2_susp": 0.006993006993006993,
            "pseudo_barinel_susp": 0.0014534883720930232
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.server_config_def#410",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.server_config_def(section, key, required)",
        "snippet": "        def server_config_def(section, key, required):\n            return {\n                'description': 'The %s of the %s Galaxy server' % (key, section),\n                'ini': [\n                    {\n                        'section': 'galaxy_server.%s' % section,\n                        'key': key,\n                    }\n                ],\n                'env': [\n                    {'name': 'ANSIBLE_GALAXY_SERVER_%s_%s' % (section.upper(), key.upper())},\n                ],\n                'required': required,\n            }",
        "begin_line": 410,
        "end_line": 423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017211703958691911,
            "pseudo_dstar_susp": 0.002325581395348837,
            "pseudo_tarantula_susp": 0.0014104372355430183,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0014104372355430183
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.api#483",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.api(self)",
        "snippet": "    def api(self):\n        return self.api_servers[0]",
        "begin_line": 483,
        "end_line": 484,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI._parse_requirements_file#486",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI._parse_requirements_file(self, requirements_file, allow_old_format=True)",
        "snippet": "    def _parse_requirements_file(self, requirements_file, allow_old_format=True):\n        \"\"\"\n        Parses an Ansible requirement.yml file and returns all the roles and/or collections defined in it. There are 2\n        requirements file format:\n\n            # v1 (roles only)\n            - src: The source of the role, required if include is not set. Can be Galaxy role name, URL to a SCM repo or tarball.\n              name: Downloads the role to the specified name, defaults to Galaxy name from Galaxy or name of repo if src is a URL.\n              scm: If src is a URL, specify the SCM. Only git or hd are supported and defaults ot git.\n              version: The version of the role to download. Can also be tag, commit, or branch name and defaults to master.\n              include: Path to additional requirements.yml files.\n\n            # v2 (roles and collections)\n            ---\n            roles:\n            # Same as v1 format just under the roles key\n\n            collections:\n            - namespace.collection\n            - name: namespace.collection\n              version: version identifier, multiple identifiers are separated by ','\n              source: the URL or a predefined source name that relates to C.GALAXY_SERVER_LIST\n\n        :param requirements_file: The path to the requirements file.\n        :param allow_old_format: Will fail if a v1 requirements file is found and this is set to False.\n        :return: a dict containing roles and collections to found in the requirements file.\n        \"\"\"\n        requirements = {\n            'roles': [],\n            'collections': [],\n        }\n\n        b_requirements_file = to_bytes(requirements_file, errors='surrogate_or_strict')\n        if not os.path.exists(b_requirements_file):\n            raise AnsibleError(\"The requirements file '%s' does not exist.\" % to_native(requirements_file))\n\n        display.vvv(\"Reading requirement file at '%s'\" % requirements_file)\n        with open(b_requirements_file, 'rb') as req_obj:\n            try:\n                file_requirements = yaml.safe_load(req_obj)\n            except YAMLError as err:\n                raise AnsibleError(\n                    \"Failed to parse the requirements yml at '%s' with the following error:\\n%s\"\n                    % (to_native(requirements_file), to_native(err)))\n\n        if file_requirements is None:\n            raise AnsibleError(\"No requirements found in file '%s'\" % to_native(requirements_file))\n\n        def parse_role_req(requirement):\n            if \"include\" not in requirement:\n                role = RoleRequirement.role_yaml_parse(requirement)\n                display.vvv(\"found role %s in yaml file\" % to_text(role))\n                if \"name\" not in role and \"src\" not in role:\n                    raise AnsibleError(\"Must specify name or src for role\")\n                return [GalaxyRole(self.galaxy, self.api, **role)]\n            else:\n                b_include_path = to_bytes(requirement[\"include\"], errors=\"surrogate_or_strict\")\n                if not os.path.isfile(b_include_path):\n                    raise AnsibleError(\"Failed to find include requirements file '%s' in '%s'\"\n                                       % (to_native(b_include_path), to_native(requirements_file)))\n\n                with open(b_include_path, 'rb') as f_include:\n                    try:\n                        return [GalaxyRole(self.galaxy, self.api, **r) for r in\n                                (RoleRequirement.role_yaml_parse(i) for i in yaml.safe_load(f_include))]\n                    except Exception as e:\n                        raise AnsibleError(\"Unable to load data from include requirements file: %s %s\"\n                                           % (to_native(requirements_file), to_native(e)))\n\n        if isinstance(file_requirements, list):\n            # Older format that contains only roles\n            if not allow_old_format:\n                raise AnsibleError(\"Expecting requirements file to be a dict with the key 'collections' that contains \"\n                                   \"a list of collections to install\")\n\n            for role_req in file_requirements:\n                requirements['roles'] += parse_role_req(role_req)\n\n        else:\n            # Newer format with a collections and/or roles key\n            extra_keys = set(file_requirements.keys()).difference(set(['roles', 'collections']))\n            if extra_keys:\n                raise AnsibleError(\"Expecting only 'roles' and/or 'collections' as base keys in the requirements \"\n                                   \"file. Found: %s\" % (to_native(\", \".join(extra_keys))))\n\n            for role_req in file_requirements.get('roles') or []:\n                requirements['roles'] += parse_role_req(role_req)\n\n            for collection_req in file_requirements.get('collections') or []:\n                if isinstance(collection_req, dict):\n                    req_name = collection_req.get('name', None)\n                    if req_name is None:\n                        raise AnsibleError(\"Collections requirement entry should contain the key name.\")\n\n                    req_version = collection_req.get('version', '*')\n                    req_source = collection_req.get('source', None)\n                    if req_source:\n                        # Try and match up the requirement source with our list of Galaxy API servers defined in the\n                        # config, otherwise create a server with that URL without any auth.\n                        req_source = next(iter([a for a in self.api_servers if req_source in [a.name, a.api_server]]),\n                                          GalaxyAPI(self.galaxy, \"explicit_requirement_%s\" % req_name, req_source))\n\n                    requirements['collections'].append((req_name, req_version, req_source))\n                else:\n                    requirements['collections'].append((collection_req, '*', None))\n\n        return requirements",
        "begin_line": 486,
        "end_line": 592,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011560693641618498,
            "pseudo_dstar_susp": 0.0010741138560687433,
            "pseudo_tarantula_susp": 0.0029154518950437317,
            "pseudo_op2_susp": 0.0010741138560687433,
            "pseudo_barinel_susp": 0.0029154518950437317
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.parse_role_req#534",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.parse_role_req(requirement)",
        "snippet": "        def parse_role_req(requirement):\n            if \"include\" not in requirement:\n                role = RoleRequirement.role_yaml_parse(requirement)\n                display.vvv(\"found role %s in yaml file\" % to_text(role))\n                if \"name\" not in role and \"src\" not in role:\n                    raise AnsibleError(\"Must specify name or src for role\")\n                return [GalaxyRole(self.galaxy, self.api, **role)]\n            else:\n                b_include_path = to_bytes(requirement[\"include\"], errors=\"surrogate_or_strict\")\n                if not os.path.isfile(b_include_path):\n                    raise AnsibleError(\"Failed to find include requirements file '%s' in '%s'\"\n                                       % (to_native(b_include_path), to_native(requirements_file)))\n\n                with open(b_include_path, 'rb') as f_include:\n                    try:\n                        return [GalaxyRole(self.galaxy, self.api, **r) for r in\n                                (RoleRequirement.role_yaml_parse(i) for i in yaml.safe_load(f_include))]\n                    except Exception as e:\n                        raise AnsibleError(\"Unable to load data from include requirements file: %s %s\"\n                                           % (to_native(requirements_file), to_native(e)))",
        "begin_line": 534,
        "end_line": 553,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009784735812133072,
            "pseudo_dstar_susp": 0.0009633911368015414,
            "pseudo_tarantula_susp": 0.001466275659824047,
            "pseudo_op2_susp": 0.0009633911368015414,
            "pseudo_barinel_susp": 0.001466275659824047
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI._resolve_path#630",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI._resolve_path(path)",
        "snippet": "    def _resolve_path(path):\n        return os.path.abspath(os.path.expanduser(os.path.expandvars(path)))",
        "begin_line": 630,
        "end_line": 631,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002074688796680498,
            "pseudo_dstar_susp": 0.0024096385542168677,
            "pseudo_tarantula_susp": 0.0021141649048625794,
            "pseudo_op2_susp": 0.0024096385542168677,
            "pseudo_barinel_susp": 0.002150537634408602
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI._get_skeleton_galaxy_yml#634",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI._get_skeleton_galaxy_yml(template_path, inject_data)",
        "snippet": "    def _get_skeleton_galaxy_yml(template_path, inject_data):\n        with open(to_bytes(template_path, errors='surrogate_or_strict'), 'rb') as template_obj:\n            meta_template = to_text(template_obj.read(), errors='surrogate_or_strict')\n\n        galaxy_meta = get_collections_galaxy_meta_info()\n\n        required_config = []\n        optional_config = []\n        for meta_entry in galaxy_meta:\n            config_list = required_config if meta_entry.get('required', False) else optional_config\n\n            value = inject_data.get(meta_entry['key'], None)\n            if not value:\n                meta_type = meta_entry.get('type', 'str')\n\n                if meta_type == 'str':\n                    value = ''\n                elif meta_type == 'list':\n                    value = []\n                elif meta_type == 'dict':\n                    value = {}\n\n            meta_entry['value'] = value\n            config_list.append(meta_entry)\n\n        link_pattern = re.compile(r\"L\\(([^)]+),\\s+([^)]+)\\)\")\n        const_pattern = re.compile(r\"C\\(([^)]+)\\)\")\n\n        def comment_ify(v):\n            if isinstance(v, list):\n                v = \". \".join([l.rstrip('.') for l in v])\n\n            v = link_pattern.sub(r\"\\1 <\\2>\", v)\n            v = const_pattern.sub(r\"'\\1'\", v)\n\n            return textwrap.fill(v, width=117, initial_indent=\"# \", subsequent_indent=\"# \", break_on_hyphens=False)\n\n        loader = DataLoader()\n        templar = Templar(loader, variables={'required_config': required_config, 'optional_config': optional_config})\n        templar.environment.filters['comment_ify'] = comment_ify\n\n        meta_value = templar.template(meta_template)\n\n        return meta_value",
        "begin_line": 634,
        "end_line": 677,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.comment_ify#662",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.comment_ify(v)",
        "snippet": "        def comment_ify(v):\n            if isinstance(v, list):\n                v = \". \".join([l.rstrip('.') for l in v])\n\n            v = link_pattern.sub(r\"\\1 <\\2>\", v)\n            v = const_pattern.sub(r\"'\\1'\", v)\n\n            return textwrap.fill(v, width=117, initial_indent=\"# \", subsequent_indent=\"# \", break_on_hyphens=False)",
        "begin_line": 662,
        "end_line": 669,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI._require_one_of_collections_requirements#679",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI._require_one_of_collections_requirements(self, collections, requirements_file)",
        "snippet": "    def _require_one_of_collections_requirements(self, collections, requirements_file):\n        if collections and requirements_file:\n            raise AnsibleError(\"The positional collection_name arg and --requirements-file are mutually exclusive.\")\n        elif not collections and not requirements_file:\n            raise AnsibleError(\"You must specify a collection name or a requirements file.\")\n        elif requirements_file:\n            requirements_file = GalaxyCLI._resolve_path(requirements_file)\n            requirements = self._parse_requirements_file(requirements_file, allow_old_format=False)['collections']\n        else:\n            requirements = []\n            for collection_input in collections:\n                requirement = None\n                if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \\\n                        urlparse(collection_input).scheme.lower() in ['http', 'https']:\n                    # Arg is a file path or URL to a collection\n                    name = collection_input\n                else:\n                    name, dummy, requirement = collection_input.partition(':')\n                requirements.append((name, requirement or '*', None))\n        return requirements",
        "begin_line": 679,
        "end_line": 698,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00205761316872428,
            "pseudo_dstar_susp": 0.0013869625520110957,
            "pseudo_tarantula_susp": 0.0029154518950437317,
            "pseudo_op2_susp": 0.0013869625520110957,
            "pseudo_barinel_susp": 0.0029154518950437317
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.execute_build#720",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.execute_build(self)",
        "snippet": "    def execute_build(self):\n        \"\"\"\n        Build an Ansible Galaxy collection artifact that can be stored in a central repository like Ansible Galaxy.\n        By default, this command builds from the current working directory. You can optionally pass in the\n        collection input path (where the ``galaxy.yml`` file is).\n        \"\"\"\n        force = context.CLIARGS['force']\n        output_path = GalaxyCLI._resolve_path(context.CLIARGS['output_path'])\n        b_output_path = to_bytes(output_path, errors='surrogate_or_strict')\n\n        if not os.path.exists(b_output_path):\n            os.makedirs(b_output_path)\n        elif os.path.isfile(b_output_path):\n            raise AnsibleError(\"- the output collection directory %s is a file - aborting\" % to_native(output_path))\n\n        for collection_path in context.CLIARGS['args']:\n            collection_path = GalaxyCLI._resolve_path(collection_path)\n            build_collection(collection_path, output_path, force)",
        "begin_line": 720,
        "end_line": 737,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009784735812133072,
            "pseudo_dstar_susp": 0.0009633911368015414,
            "pseudo_tarantula_susp": 0.001466275659824047,
            "pseudo_op2_susp": 0.0009633911368015414,
            "pseudo_barinel_susp": 0.001466275659824047
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.execute_init#761",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.execute_init(self)",
        "snippet": "    def execute_init(self):\n        \"\"\"\n        Creates the skeleton framework of a role or collection that complies with the Galaxy metadata format.\n        Requires a role or collection name. The collection name must be in the format ``<namespace>.<collection>``.\n        \"\"\"\n\n        galaxy_type = context.CLIARGS['type']\n        init_path = context.CLIARGS['init_path']\n        force = context.CLIARGS['force']\n        obj_skeleton = context.CLIARGS['{0}_skeleton'.format(galaxy_type)]\n\n        obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]\n\n        inject_data = dict(\n            description='your {0} description'.format(galaxy_type),\n            ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),\n        )\n        if galaxy_type == 'role':\n            inject_data.update(dict(\n                author='your name',\n                company='your company (optional)',\n                license='license (GPL-2.0-or-later, MIT, etc)',\n                role_name=obj_name,\n                role_type=context.CLIARGS['role_type'],\n                issue_tracker_url='http://example.com/issue/tracker',\n                repository_url='http://example.com/repository',\n                documentation_url='http://docs.example.com',\n                homepage_url='http://example.com',\n                min_ansible_version=ansible_version[:3],  # x.y\n                dependencies=[],\n            ))\n\n            obj_path = os.path.join(init_path, obj_name)\n        elif galaxy_type == 'collection':\n            namespace, collection_name = obj_name.split('.', 1)\n\n            inject_data.update(dict(\n                namespace=namespace,\n                collection_name=collection_name,\n                version='1.0.0',\n                readme='README.md',\n                authors=['your name <example@domain.com>'],\n                license=['GPL-2.0-or-later'],\n                repository='http://example.com/repository',\n                documentation='http://docs.example.com',\n                homepage='http://example.com',\n                issues='http://example.com/issue/tracker',\n                build_ignore=[],\n            ))\n\n            obj_path = os.path.join(init_path, namespace, collection_name)\n\n        b_obj_path = to_bytes(obj_path, errors='surrogate_or_strict')\n\n        if os.path.exists(b_obj_path):\n            if os.path.isfile(obj_path):\n                raise AnsibleError(\"- the path %s already exists, but is a file - aborting\" % to_native(obj_path))\n            elif not force:\n                raise AnsibleError(\"- the directory %s already exists. \"\n                                   \"You can use --force to re-initialize this directory,\\n\"\n                                   \"however it will reset any main.yml files that may have\\n\"\n                                   \"been modified there already.\" % to_native(obj_path))\n\n        if obj_skeleton is not None:\n            own_skeleton = False\n            skeleton_ignore_expressions = C.GALAXY_ROLE_SKELETON_IGNORE\n        else:\n            own_skeleton = True\n            obj_skeleton = self.galaxy.default_role_skeleton_path\n            skeleton_ignore_expressions = ['^.*/.git_keep$']\n\n        obj_skeleton = os.path.expanduser(obj_skeleton)\n        skeleton_ignore_re = [re.compile(x) for x in skeleton_ignore_expressions]\n\n        if not os.path.exists(obj_skeleton):\n            raise AnsibleError(\"- the skeleton path '{0}' does not exist, cannot init {1}\".format(\n                to_native(obj_skeleton), galaxy_type)\n            )\n\n        loader = DataLoader()\n        templar = Templar(loader, variables=inject_data)\n\n        # create role directory\n        if not os.path.exists(b_obj_path):\n            os.makedirs(b_obj_path)\n\n        for root, dirs, files in os.walk(obj_skeleton, topdown=True):\n            rel_root = os.path.relpath(root, obj_skeleton)\n            rel_dirs = rel_root.split(os.sep)\n            rel_root_dir = rel_dirs[0]\n            if galaxy_type == 'collection':\n                # A collection can contain templates in playbooks/*/templates and roles/*/templates\n                in_templates_dir = rel_root_dir in ['playbooks', 'roles'] and 'templates' in rel_dirs\n            else:\n                in_templates_dir = rel_root_dir == 'templates'\n\n            dirs = [d for d in dirs if not any(r.match(d) for r in skeleton_ignore_re)]\n\n            for f in files:\n                filename, ext = os.path.splitext(f)\n\n                if any(r.match(os.path.join(rel_root, f)) for r in skeleton_ignore_re):\n                    continue\n\n                if galaxy_type == 'collection' and own_skeleton and rel_root == '.' and f == 'galaxy.yml.j2':\n                    # Special use case for galaxy.yml.j2 in our own default collection skeleton. We build the options\n                    # dynamically which requires special options to be set.\n\n                    # The templated data's keys must match the key name but the inject data contains collection_name\n                    # instead of name. We just make a copy and change the key back to name for this file.\n                    template_data = inject_data.copy()\n                    template_data['name'] = template_data.pop('collection_name')\n\n                    meta_value = GalaxyCLI._get_skeleton_galaxy_yml(os.path.join(root, rel_root, f), template_data)\n                    b_dest_file = to_bytes(os.path.join(obj_path, rel_root, filename), errors='surrogate_or_strict')\n                    with open(b_dest_file, 'wb') as galaxy_obj:\n                        galaxy_obj.write(to_bytes(meta_value, errors='surrogate_or_strict'))\n                elif ext == \".j2\" and not in_templates_dir:\n                    src_template = os.path.join(root, f)\n                    dest_file = os.path.join(obj_path, rel_root, filename)\n                    template_data = to_text(loader._get_file_contents(src_template)[0], errors='surrogate_or_strict')\n                    b_rendered = to_bytes(templar.template(template_data), errors='surrogate_or_strict')\n                    with open(dest_file, 'wb') as df:\n                        df.write(b_rendered)\n                else:\n                    f_rel_path = os.path.relpath(os.path.join(root, f), obj_skeleton)\n                    shutil.copyfile(os.path.join(root, f), os.path.join(obj_path, f_rel_path))\n\n            for d in dirs:\n                b_dir_path = to_bytes(os.path.join(obj_path, rel_root, d), errors='surrogate_or_strict')\n                if not os.path.exists(b_dir_path):\n                    os.makedirs(b_dir_path)\n\n        display.display(\"- %s %s was created successfully\" % (galaxy_type.title(), obj_name))",
        "begin_line": 761,
        "end_line": 894,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009900990099009901,
            "pseudo_dstar_susp": 0.0012468827930174563,
            "pseudo_tarantula_susp": 0.00089126559714795,
            "pseudo_op2_susp": 0.0012468827930174563,
            "pseudo_barinel_susp": 0.00089126559714795
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.execute_verify#938",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.execute_verify(self)",
        "snippet": "    def execute_verify(self):\n\n        collections = context.CLIARGS['args']\n        search_paths = context.CLIARGS['collections_path']\n        ignore_certs = context.CLIARGS['ignore_certs']\n        ignore_errors = context.CLIARGS['ignore_errors']\n        requirements_file = context.CLIARGS['requirements']\n\n        requirements = self._require_one_of_collections_requirements(collections, requirements_file)\n\n        resolved_paths = [validate_collection_path(GalaxyCLI._resolve_path(path)) for path in search_paths]\n\n        verify_collections(requirements, resolved_paths, self.api_servers, (not ignore_certs), ignore_errors,\n                           allow_pre_release=True)\n\n        return 0",
        "begin_line": 938,
        "end_line": 953,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.galaxy.GalaxyCLI.execute_install#955",
        "src_path": "lib/ansible/cli/galaxy.py",
        "class_name": "lib.ansible.cli.galaxy.GalaxyCLI",
        "signature": "lib.ansible.cli.galaxy.GalaxyCLI.execute_install(self)",
        "snippet": "    def execute_install(self):\n        \"\"\"\n        Install one or more roles(``ansible-galaxy role install``), or one or more collections(``ansible-galaxy collection install``).\n        You can pass in a list (roles or collections) or use the file\n        option listed below (these are mutually exclusive). If you pass in a list, it\n        can be a name (which will be downloaded via the galaxy API and github), or it can be a local tar archive file.\n        \"\"\"\n        if context.CLIARGS['type'] == 'collection':\n            collections = context.CLIARGS['args']\n            force = context.CLIARGS['force']\n            output_path = context.CLIARGS['collections_path']\n            ignore_certs = context.CLIARGS['ignore_certs']\n            ignore_errors = context.CLIARGS['ignore_errors']\n            requirements_file = context.CLIARGS['requirements']\n            no_deps = context.CLIARGS['no_deps']\n            force_deps = context.CLIARGS['force_with_deps']\n\n            if collections and requirements_file:\n                raise AnsibleError(\"The positional collection_name arg and --requirements-file are mutually exclusive.\")\n            elif not collections and not requirements_file:\n                raise AnsibleError(\"You must specify a collection name or a requirements file.\")\n\n            if requirements_file:\n                requirements_file = GalaxyCLI._resolve_path(requirements_file)\n            requirements = self._require_one_of_collections_requirements(collections, requirements_file)\n\n            output_path = GalaxyCLI._resolve_path(output_path)\n            collections_path = C.COLLECTIONS_PATHS\n\n            if len([p for p in collections_path if p.startswith(output_path)]) == 0:\n                display.warning(\"The specified collections path '%s' is not part of the configured Ansible \"\n                                \"collections paths '%s'. The installed collection won't be picked up in an Ansible \"\n                                \"run.\" % (to_text(output_path), to_text(\":\".join(collections_path))))\n\n            output_path = validate_collection_path(output_path)\n            b_output_path = to_bytes(output_path, errors='surrogate_or_strict')\n            if not os.path.exists(b_output_path):\n                os.makedirs(b_output_path)\n\n            install_collections(requirements, output_path, self.api_servers, (not ignore_certs), ignore_errors,\n                                no_deps, force, force_deps, context.CLIARGS['allow_pre_release'])\n\n            return 0\n\n        role_file = context.CLIARGS['role_file']\n\n        if not context.CLIARGS['args'] and role_file is None:\n            # the user needs to specify one of either --role-file or specify a single user/role name\n            raise AnsibleOptionsError(\"- you must specify a user/role name or a roles file\")\n\n        no_deps = context.CLIARGS['no_deps']\n        force_deps = context.CLIARGS['force_with_deps']\n\n        force = context.CLIARGS['force'] or force_deps\n\n        roles_left = []\n        if role_file:\n            if not (role_file.endswith('.yaml') or role_file.endswith('.yml')):\n                raise AnsibleError(\"Invalid role requirements file, it must end with a .yml or .yaml extension\")\n\n            roles_left = self._parse_requirements_file(role_file)['roles']\n        else:\n            # roles were specified directly, so we'll just go out grab them\n            # (and their dependencies, unless the user doesn't want us to).\n            for rname in context.CLIARGS['args']:\n                role = RoleRequirement.role_yaml_parse(rname.strip())\n                roles_left.append(GalaxyRole(self.galaxy, self.api, **role))\n\n        for role in roles_left:\n            # only process roles in roles files when names matches if given\n            if role_file and context.CLIARGS['args'] and role.name not in context.CLIARGS['args']:\n                display.vvv('Skipping role %s' % role.name)\n                continue\n\n            display.vvv('Processing role %s ' % role.name)\n\n            # query the galaxy API for the role data\n\n            if role.install_info is not None:\n                if role.install_info['version'] != role.version or force:\n                    if force:\n                        display.display('- changing role %s from %s to %s' %\n                                        (role.name, role.install_info['version'], role.version or \"unspecified\"))\n                        role.remove()\n                    else:\n                        display.warning('- %s (%s) is already installed - use --force to change version to %s' %\n                                        (role.name, role.install_info['version'], role.version or \"unspecified\"))\n                        continue\n                else:\n                    if not force:\n                        display.display('- %s is already installed, skipping.' % str(role))\n                        continue\n\n            try:\n                installed = role.install()\n            except AnsibleError as e:\n                display.warning(u\"- %s was NOT installed successfully: %s \" % (role.name, to_text(e)))\n                self.exit_without_ignore()\n                continue\n\n            # install dependencies, if we want them\n            if not no_deps and installed:\n                if not role.metadata:\n                    display.warning(\"Meta file %s is empty. Skipping dependencies.\" % role.path)\n                else:\n                    role_dependencies = (role.metadata.get('dependencies') or []) + role.requirements\n                    for dep in role_dependencies:\n                        display.debug('Installing dep %s' % dep)\n                        dep_req = RoleRequirement()\n                        dep_info = dep_req.role_yaml_parse(dep)\n                        dep_role = GalaxyRole(self.galaxy, self.api, **dep_info)\n                        if '.' not in dep_role.name and '.' not in dep_role.src and dep_role.scm is None:\n                            # we know we can skip this, as it's not going to\n                            # be found on galaxy.ansible.com\n                            continue\n                        if dep_role.install_info is None:\n                            if dep_role not in roles_left:\n                                display.display('- adding dependency: %s' % to_text(dep_role))\n                                roles_left.append(dep_role)\n                            else:\n                                display.display('- dependency %s already pending installation.' % dep_role.name)\n                        else:\n                            if dep_role.install_info['version'] != dep_role.version:\n                                if force_deps:\n                                    display.display('- changing dependant role %s from %s to %s' %\n                                                    (dep_role.name, dep_role.install_info['version'], dep_role.version or \"unspecified\"))\n                                    dep_role.remove()\n                                    roles_left.append(dep_role)\n                                else:\n                                    display.warning('- dependency %s (%s) from role %s differs from already installed version (%s), skipping' %\n                                                    (to_text(dep_role), dep_role.version, role.name, dep_role.install_info['version']))\n                            else:\n                                if force_deps:\n                                    roles_left.append(dep_role)\n                                else:\n                                    display.display('- dependency %s is already installed, skipping.' % dep_role.name)\n\n            if not installed:\n                display.warning(\"- %s was NOT installed successfully.\" % role.name)\n                self.exit_without_ignore()\n\n        return 0",
        "begin_line": 955,
        "end_line": 1096,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005025125628140704,
            "pseudo_dstar_susp": 0.005208333333333333,
            "pseudo_tarantula_susp": 0.0035842293906810036,
            "pseudo_op2_susp": 0.005208333333333333,
            "pseudo_barinel_susp": 0.0035842293906810036
        }
    },
    {
        "name": "lib.ansible.utils.plugin_docs.merge_fragment#27",
        "src_path": "lib/ansible/utils/plugin_docs.py",
        "class_name": "lib.ansible.utils.plugin_docs",
        "signature": "lib.ansible.utils.plugin_docs.merge_fragment(target, source)",
        "snippet": "def merge_fragment(target, source):\n\n    for key, value in source.items():\n        if key in target:\n            # assumes both structures have same type\n            if isinstance(target[key], MutableMapping):\n                value.update(target[key])\n            elif isinstance(target[key], MutableSet):\n                value.add(target[key])\n            elif isinstance(target[key], MutableSequence):\n                value = sorted(frozenset(value + target[key]))\n            else:\n                raise Exception(\"Attempt to extend a documentation fragement, invalid type for %s\" % key)\n        target[key] = value",
        "begin_line": 27,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.plugin_docs.add_fragments#43",
        "src_path": "lib/ansible/utils/plugin_docs.py",
        "class_name": "lib.ansible.utils.plugin_docs",
        "signature": "lib.ansible.utils.plugin_docs.add_fragments(doc, filename, fragment_loader)",
        "snippet": "def add_fragments(doc, filename, fragment_loader):\n\n    fragments = doc.pop('extends_documentation_fragment', [])\n\n    if isinstance(fragments, string_types):\n        fragments = [fragments]\n\n    unknown_fragments = []\n\n    # doc_fragments are allowed to specify a fragment var other than DOCUMENTATION\n    # with a . separator; this is complicated by collections-hosted doc_fragments that\n    # use the same separator. Assume it's collection-hosted normally first, try to load\n    # as-specified. If failure, assume the right-most component is a var, split it off,\n    # and retry the load.\n    for fragment_slug in fragments:\n        fragment_name = fragment_slug\n        fragment_var = 'DOCUMENTATION'\n\n        fragment_class = fragment_loader.get(fragment_name)\n        if fragment_class is None and '.' in fragment_slug:\n            splitname = fragment_slug.rsplit('.', 1)\n            fragment_name = splitname[0]\n            fragment_var = splitname[1].upper()\n            fragment_class = fragment_loader.get(fragment_name)\n\n        if fragment_class is None:\n            unknown_fragments.append(fragment_slug)\n            continue\n\n        fragment_yaml = getattr(fragment_class, fragment_var, None)\n        if fragment_yaml is None:\n            if fragment_var != 'DOCUMENTATION':\n                # if it's asking for something specific that's missing, that's an error\n                unknown_fragments.append(fragment_slug)\n                continue\n            else:\n                fragment_yaml = '{}'  # TODO: this is still an error later since we require 'options' below...\n\n        fragment = AnsibleLoader(fragment_yaml, file_name=filename).get_single_data()\n\n        if 'notes' in fragment:\n            notes = fragment.pop('notes')\n            if notes:\n                if 'notes' not in doc:\n                    doc['notes'] = []\n                doc['notes'].extend(notes)\n\n        if 'seealso' in fragment:\n            seealso = fragment.pop('seealso')\n            if seealso:\n                if 'seealso' not in doc:\n                    doc['seealso'] = []\n                doc['seealso'].extend(seealso)\n\n        if 'options' not in fragment:\n            raise Exception(\"missing options in fragment (%s), possibly misformatted?: %s\" % (fragment_name, filename))\n\n        # ensure options themselves are directly merged\n        if 'options' in doc:\n            try:\n                merge_fragment(doc['options'], fragment.pop('options'))\n            except Exception as e:\n                raise AnsibleError(\"%s options (%s) of unknown type: %s\" % (to_native(e), fragment_name, filename))\n        else:\n            doc['options'] = fragment.pop('options')\n\n        # merge rest of the sections\n        try:\n            merge_fragment(doc, fragment)\n        except Exception as e:\n            raise AnsibleError(\"%s (%s) of unknown type: %s\" % (to_native(e), fragment_name, filename))\n\n    if unknown_fragments:\n        raise AnsibleError('unknown doc_fragment(s) in file {0}: {1}'.format(filename, to_native(', '.join(unknown_fragments))))",
        "begin_line": 43,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.plugin_docs.get_docstring#119",
        "src_path": "lib/ansible/utils/plugin_docs.py",
        "class_name": "lib.ansible.utils.plugin_docs",
        "signature": "lib.ansible.utils.plugin_docs.get_docstring(filename, fragment_loader, verbose=False, ignore_errors=False)",
        "snippet": "def get_docstring(filename, fragment_loader, verbose=False, ignore_errors=False):\n    \"\"\"\n    DOCUMENTATION can be extended using documentation fragments loaded by the PluginLoader from the doc_fragments plugins.\n    \"\"\"\n\n    data = read_docstring(filename, verbose=verbose, ignore_errors=ignore_errors)\n\n    # add fragments to documentation\n    if data.get('doc', False):\n        add_fragments(data['doc'], filename, fragment_loader=fragment_loader)\n\n    return data['doc'], data['plainexamples'], data['returndocs'], data['metadata']",
        "begin_line": 119,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.plugin_docs.get_versioned_doclink#133",
        "src_path": "lib/ansible/utils/plugin_docs.py",
        "class_name": "lib.ansible.utils.plugin_docs",
        "signature": "lib.ansible.utils.plugin_docs.get_versioned_doclink(path)",
        "snippet": "def get_versioned_doclink(path):\n    \"\"\"\n    returns a versioned documentation link for the current Ansible major.minor version; used to generate\n    in-product warning/error links to the configured DOCSITE_ROOT_URL\n    (eg, https://docs.ansible.com/ansible/2.8/somepath/doc.html)\n\n    :param path: relative path to a document under docs/docsite/rst;\n    :return: absolute URL to the specified doc for the current version of Ansible\n    \"\"\"\n    path = to_native(path)\n    try:\n        base_url = C.config.get_config_value('DOCSITE_ROOT_URL')\n        if not base_url.endswith('/'):\n            base_url += '/'\n        if path.startswith('/'):\n            path = path[1:]\n        split_ver = ansible_version.split('.')\n        if len(split_ver) < 3:\n            raise RuntimeError('invalid version ({0})'.format(ansible_version))\n\n        doc_version = '{0}.{1}'.format(split_ver[0], split_ver[1])\n\n        # check to see if it's a X.Y.0 non-rc prerelease or dev release, if so, assume devel (since the X.Y doctree\n        # isn't published until beta-ish)\n        if split_ver[2].startswith('0'):\n            # exclude rc; we should have the X.Y doctree live by rc1\n            if any((pre in split_ver[2]) for pre in ['a', 'b']) or len(split_ver) > 3 and 'dev' in split_ver[3]:\n                doc_version = 'devel'\n\n        return '{0}{1}/{2}'.format(base_url, doc_version, path)\n    except Exception as ex:\n        return '(unable to create versioned doc link for path {0}: {1})'.format(path, to_native(ex))",
        "begin_line": 133,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000970873786407767,
            "pseudo_dstar_susp": 0.0012391573729863693,
            "pseudo_tarantula_susp": 0.0007855459544383347,
            "pseudo_op2_susp": 0.0012391573729863693,
            "pseudo_barinel_susp": 0.0007855459544383347
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__init__#48",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__init__(self, specifier)",
        "snippet": "    def __init__(self, specifier):\n        self.specifier = specifier",
        "begin_line": 48,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009293680297397769,
            "pseudo_dstar_susp": 0.0012376237623762376,
            "pseudo_tarantula_susp": 0.0007633587786259542,
            "pseudo_op2_susp": 0.0012376237623762376,
            "pseudo_barinel_susp": 0.0007633587786259542
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__repr__#51",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return repr(self.specifier)",
        "begin_line": 51,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__eq__#54",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        if isinstance(other, _Alpha):\n            return self.specifier == other.specifier\n        elif isinstance(other, str):\n            return self.specifier == other\n\n        return False",
        "begin_line": 54,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__ne__#62",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        return not self.__eq__(other)",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__lt__#65",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__lt__(self, other)",
        "snippet": "    def __lt__(self, other):\n        if isinstance(other, _Alpha):\n            return self.specifier < other.specifier\n        elif isinstance(other, str):\n            return self.specifier < other\n        elif isinstance(other, _Numeric):\n            return False\n\n        raise ValueError",
        "begin_line": 65,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__gt__#75",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__gt__(self, other)",
        "snippet": "    def __gt__(self, other):\n        return not self.__lt__(other)",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.001002004008016032,
            "pseudo_dstar_susp": 0.000980392156862745,
            "pseudo_tarantula_susp": 0.0015220700152207,
            "pseudo_op2_susp": 0.000980392156862745,
            "pseudo_barinel_susp": 0.0015220700152207
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__le__#78",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__le__(self, other)",
        "snippet": "    def __le__(self, other):\n        return self.__lt__(other) or self.__eq__(other)",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Alpha.__ge__#81",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Alpha",
        "signature": "lib.ansible.utils.version._Alpha.__ge__(self, other)",
        "snippet": "    def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__init__#91",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__init__(self, specifier)",
        "snippet": "    def __init__(self, specifier):\n        self.specifier = int(specifier)",
        "begin_line": 91,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001075268817204301,
            "pseudo_dstar_susp": 0.0012594458438287153,
            "pseudo_tarantula_susp": 0.0014858841010401188,
            "pseudo_op2_susp": 0.0012594458438287153,
            "pseudo_barinel_susp": 0.0014858841010401188
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__repr__#94",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return repr(self.specifier)",
        "begin_line": 94,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__eq__#97",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        if isinstance(other, _Numeric):\n            return self.specifier == other.specifier\n        elif isinstance(other, int):\n            return self.specifier == other\n\n        return False",
        "begin_line": 97,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__ne__#105",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        return not self.__eq__(other)",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__lt__#108",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__lt__(self, other)",
        "snippet": "    def __lt__(self, other):\n        if isinstance(other, _Numeric):\n            return self.specifier < other.specifier\n        elif isinstance(other, int):\n            return self.specifier < other\n        elif isinstance(other, _Alpha):\n            return True\n\n        raise ValueError",
        "begin_line": 108,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__gt__#118",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__gt__(self, other)",
        "snippet": "    def __gt__(self, other):\n        return not self.__lt__(other)",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__le__#121",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__le__(self, other)",
        "snippet": "    def __le__(self, other):\n        return self.__lt__(other) or self.__eq__(other)",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version._Numeric.__ge__#124",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version._Numeric",
        "signature": "lib.ansible.utils.version._Numeric.__ge__(self, other)",
        "snippet": "    def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__init__#136",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__init__(self, vstring=None)",
        "snippet": "    def __init__(self, vstring=None):\n        self.vstring = vstring\n        self.major = None\n        self.minor = None\n        self.patch = None\n        self.prerelease = ()\n        self.buildmetadata = ()\n\n        if vstring:\n            self.parse(vstring)",
        "begin_line": 136,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005555555555555556,
            "pseudo_dstar_susp": 0.0005555555555555556,
            "pseudo_tarantula_susp": 0.0005555555555555556,
            "pseudo_op2_susp": 0.0005555555555555556,
            "pseudo_barinel_susp": 0.0005555555555555556
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.from_loose_version#151",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.from_loose_version(loose_version)",
        "snippet": "    def from_loose_version(loose_version):\n        \"\"\"This method is designed to take a ``LooseVersion``\n        and attempt to construct a ``SemanticVersion`` from it\n\n        This is useful where you want to do simple version math\n        without requiring users to provide a compliant semver.\n        \"\"\"\n        if not isinstance(loose_version, LooseVersion):\n            raise ValueError(\"%r is not a LooseVersion\" % loose_version)\n\n        try:\n            version = loose_version.version[:]\n        except AttributeError:\n            raise ValueError(\"%r is not a LooseVersion\" % loose_version)\n\n        extra_idx = 3\n        for marker in ('-', '+'):\n            try:\n                idx = version.index(marker)\n            except ValueError:\n                continue\n            else:\n                if idx < extra_idx:\n                    extra_idx = idx\n        version = version[:extra_idx]\n\n        if version and set(type(v) for v in version) != set((int,)):\n            raise ValueError(\"Non integer values in %r\" % loose_version)\n\n        # Extra is everything to the right of the core version\n        extra = re.search('[+-].+$', loose_version.vstring)\n\n        version = version + [0] * (3 - len(version))\n        return SemanticVersion(\n            '%s%s' % (\n                '.'.join(str(v) for v in version),\n                extra.group(0) if extra else ''\n            )\n        )",
        "begin_line": 151,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008833922261484099,
            "pseudo_dstar_susp": 0.0012330456226880395,
            "pseudo_tarantula_susp": 0.0007451564828614009,
            "pseudo_op2_susp": 0.0012330456226880395,
            "pseudo_barinel_susp": 0.0007451564828614009
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.parse#191",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.parse(self, vstring)",
        "snippet": "    def parse(self, vstring):\n        match = SEMVER_RE.match(vstring)\n        if not match:\n            raise ValueError(\"invalid semantic version '%s'\" % vstring)\n\n        (major, minor, patch, prerelease, buildmetadata) = match.group(1, 2, 3, 4, 5)\n        self.major = int(major)\n        self.minor = int(minor)\n        self.patch = int(patch)\n\n        if prerelease:\n            self.prerelease = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in prerelease.split('.'))\n        if buildmetadata:\n            self.buildmetadata = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in buildmetadata.split('.'))",
        "begin_line": 191,
        "end_line": 204,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005602240896358543,
            "pseudo_dstar_susp": 0.0005602240896358543,
            "pseudo_tarantula_susp": 0.0005602240896358543,
            "pseudo_op2_susp": 0.0005602240896358543,
            "pseudo_barinel_susp": 0.0005602240896358543
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.core#207",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.core(self)",
        "snippet": "    def core(self):\n        return self.major, self.minor, self.patch",
        "begin_line": 207,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006131207847946045,
            "pseudo_dstar_susp": 0.0006131207847946045,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0006131207847946045,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.is_prerelease#211",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.is_prerelease(self)",
        "snippet": "    def is_prerelease(self):\n        return bool(self.prerelease)",
        "begin_line": 211,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007002801120448179,
            "pseudo_dstar_susp": 0.0007002801120448179,
            "pseudo_tarantula_susp": 0.0007057163020465773,
            "pseudo_op2_susp": 0.0007002801120448179,
            "pseudo_barinel_susp": 0.0007057163020465773
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.is_stable#215",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.is_stable(self)",
        "snippet": "    def is_stable(self):\n        # Major version zero (0.y.z) is for initial development. Anything MAY change at any time.\n        # The public API SHOULD NOT be considered stable.\n        # https://semver.org/#spec-item-4\n        return not (self.major == 0 or self.is_prerelease)",
        "begin_line": 215,
        "end_line": 219,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion._cmp#221",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion._cmp(self, other)",
        "snippet": "    def _cmp(self, other):\n        if isinstance(other, str):\n            other = SemanticVersion(other)\n\n        if self.core != other.core:\n            # if the core version doesn't match\n            # prerelease and buildmetadata doesn't matter\n            if self.core < other.core:\n                return -1\n            else:\n                return 1\n\n        if not any((self.prerelease, other.prerelease)):\n            return 0\n\n        if self.prerelease and not other.prerelease:\n            return -1\n        elif not self.prerelease and other.prerelease:\n            return 1\n        else:\n            if self.prerelease < other.prerelease:\n                return -1\n            elif self.prerelease > other.prerelease:\n                return 1\n\n        # Build metadata MUST be ignored when determining version precedence\n        # https://semver.org/#spec-item-10\n        # With the above in mind it is ignored here\n\n        # If we have made it here, things should be equal\n        return 0",
        "begin_line": 221,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007380073800738007,
            "pseudo_dstar_susp": 0.0007374631268436578,
            "pseudo_tarantula_susp": 0.0007507507507507507,
            "pseudo_op2_susp": 0.0007374631268436578,
            "pseudo_barinel_susp": 0.0007507507507507507
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__eq__#256",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self._cmp(other) == 0",
        "begin_line": 256,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007209805335255948,
            "pseudo_dstar_susp": 0.0007209805335255948,
            "pseudo_tarantula_susp": 0.0007283321194464676,
            "pseudo_op2_susp": 0.0007209805335255948,
            "pseudo_barinel_susp": 0.0007283321194464676
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__ne__#259",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__ne__(self, other)",
        "snippet": "    def __ne__(self, other):\n        return not self.__eq__(other)",
        "begin_line": 259,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__lt__#262",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__lt__(self, other)",
        "snippet": "    def __lt__(self, other):\n        return self._cmp(other) < 0",
        "begin_line": 262,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__le__#265",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__le__(self, other)",
        "snippet": "    def __le__(self, other):\n        return self._cmp(other) <= 0",
        "begin_line": 265,
        "end_line": 266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__gt__#268",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__gt__(self, other)",
        "snippet": "    def __gt__(self, other):\n        return self._cmp(other) > 0",
        "begin_line": 268,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.version.SemanticVersion.__ge__#271",
        "src_path": "lib/ansible/utils/version.py",
        "class_name": "lib.ansible.utils.version.SemanticVersion",
        "signature": "lib.ansible.utils.version.SemanticVersion.__ge__(self, other)",
        "snippet": "    def __ge__(self, other):\n        return self._cmp(other) >= 0",
        "begin_line": 271,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.lookup.items.LookupModule.run#71",
        "src_path": "lib/ansible/plugins/lookup/items.py",
        "class_name": "lib.ansible.plugins.lookup.items.LookupModule",
        "signature": "lib.ansible.plugins.lookup.items.LookupModule.run(self, terms, **kwargs)",
        "snippet": "    def run(self, terms, **kwargs):\n\n        return self._flatten(terms)",
        "begin_line": 71,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.__init__#33",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.__init__(self, config=None)",
        "snippet": "    def __init__(self, config=None):\n        if config:\n            paths = config.get_config_value('COLLECTIONS_PATHS')\n        else:\n            paths = os.environ.get('ANSIBLE_COLLECTIONS_PATHS', '').split(os.pathsep)\n\n        if isinstance(paths, string_types):\n            paths = [paths]\n        elif paths is None:\n            paths = []\n\n        # expand any placeholders in configured paths\n        paths = [\n            to_native(os.path.expanduser(p), errors='surrogate_or_strict')\n            for p in paths\n        ]\n\n        # Append all ``ansible_collections`` dirs from sys.path to the end\n        for path in sys.path:\n            if (\n                    path not in paths and\n                    os.path.isdir(to_bytes(\n                        os.path.join(path, 'ansible_collections'),\n                        errors='surrogate_or_strict',\n                    ))\n            ):\n                paths.append(path)\n\n        self._n_configured_paths = paths\n\n        self._n_playbook_paths = []\n        self._default_collection = None\n        # pre-inject grafted package maps so we can force them to use the right loader instead of potentially delegating to a \"normal\" loader\n        for syn_pkg_def in (p for p in iteritems(_SYNTHETIC_PACKAGES) if p[1].get('graft')):\n            pkg_name = syn_pkg_def[0]\n            pkg_def = syn_pkg_def[1]\n\n            newmod = ModuleType(pkg_name)\n            newmod.__package__ = pkg_name\n            newmod.__file__ = '<ansible_synthetic_collection_package>'\n            pkg_type = pkg_def.get('type')\n\n            # TODO: need to rethink map style so we can just delegate all the loading\n\n            if pkg_type == 'flatmap':\n                newmod.__loader__ = AnsibleFlatMapLoader(import_module(pkg_def['flatmap']))\n            newmod.__path__ = []\n\n            sys.modules[pkg_name] = newmod",
        "begin_line": 33,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005235602094240838,
            "pseudo_dstar_susp": 0.0013123359580052493,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0013123359580052493,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.n_collection_paths#84",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.n_collection_paths(self)",
        "snippet": "    def n_collection_paths(self):\n        return self._n_playbook_paths + self._n_configured_paths",
        "begin_line": 84,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.default_collection#112",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.default_collection(self)",
        "snippet": "    def default_collection(self):\n        return self._default_collection",
        "begin_line": 112,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.988608568034104e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.find_module#115",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.find_module(self, fullname, path=None)",
        "snippet": "    def find_module(self, fullname, path=None):\n        if self._find_module(fullname, path, load=False)[0]:\n            return self\n\n        return None",
        "begin_line": 115,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007993605115907274,
            "pseudo_dstar_susp": 0.0007968127490039841,
            "pseudo_tarantula_susp": 0.0008340283569641367,
            "pseudo_op2_susp": 0.0007968127490039841,
            "pseudo_barinel_susp": 0.0008340283569641367
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.load_module#121",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader.load_module(self, fullname)",
        "snippet": "    def load_module(self, fullname):\n        mod = self._find_module(fullname, None, load=True)[1]\n\n        if not mod:\n            raise ImportError('module {0} not found'.format(fullname))\n\n        return mod",
        "begin_line": 121,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader._find_module#129",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader._find_module(self, fullname, path, load)",
        "snippet": "    def _find_module(self, fullname, path, load):\n        # this loader is only concerned with items under the Ansible Collections namespace hierarchy, ignore others\n        if not fullname.startswith('ansible_collections.') and fullname != 'ansible_collections':\n            return False, None\n\n        if sys.modules.get(fullname):\n            if not load:\n                return True, None\n\n            return True, sys.modules[fullname]\n\n        newmod = None\n\n        # this loader implements key functionality for Ansible collections\n        # * implicit distributed namespace packages for the root Ansible namespace (no pkgutil.extend_path hackery reqd)\n        # * implicit package support for Python 2.7 (no need for __init__.py in collections, except to use standard Py2.7 tooling)\n        # * preventing controller-side code injection during collection loading\n        # * (default loader would execute arbitrary package code from all __init__.py's)\n\n        parent_pkg_name = '.'.join(fullname.split('.')[:-1])\n\n        parent_pkg = sys.modules.get(parent_pkg_name)\n\n        if parent_pkg_name and not parent_pkg:\n            raise ImportError('parent package {0} not found'.format(parent_pkg_name))\n\n        # are we at or below the collection level? eg a.mynamespace.mycollection.something.else\n        # if so, we don't want distributed namespace behavior; first mynamespace.mycollection on the path is where\n        # we'll load everything from (ie, don't fall back to another mynamespace.mycollection lower on the path)\n        sub_collection = fullname.count('.') > 1\n\n        synpkg_def = _SYNTHETIC_PACKAGES.get(fullname)\n        synpkg_remainder = ''\n\n        if not synpkg_def:\n            # if the parent is a grafted package, we have some special work to do, otherwise just look for stuff on disk\n            parent_synpkg_def = _SYNTHETIC_PACKAGES.get(parent_pkg_name)\n            if parent_synpkg_def and parent_synpkg_def.get('graft'):\n                synpkg_def = parent_synpkg_def\n                synpkg_remainder = '.' + fullname.rpartition('.')[2]\n\n        # FUTURE: collapse as much of this back to on-demand as possible (maybe stub packages that get replaced when actually loaded?)\n        if synpkg_def:\n            pkg_type = synpkg_def.get('type')\n            if not pkg_type:\n                raise KeyError('invalid synthetic package type (no package \"type\" specified)')\n            if pkg_type == 'map':\n                map_package = synpkg_def.get('map')\n\n                if not map_package:\n                    raise KeyError('invalid synthetic map package definition (no target \"map\" defined)')\n\n                if not load:\n                    return True, None\n\n                mod = import_module(map_package + synpkg_remainder)\n\n                sys.modules[fullname] = mod\n\n                return True, mod\n            elif pkg_type == 'flatmap':\n                raise NotImplementedError()\n            elif pkg_type == 'pkg_only':\n                if not load:\n                    return True, None\n\n                newmod = ModuleType(fullname)\n                newmod.__package__ = fullname\n                newmod.__file__ = '<ansible_synthetic_collection_package>'\n                newmod.__loader__ = self\n                newmod.__path__ = []\n\n                if not synpkg_def.get('allow_external_subpackages'):\n                    # if external subpackages are NOT allowed, we're done\n                    sys.modules[fullname] = newmod\n                    return True, newmod\n\n                # if external subpackages ARE allowed, check for on-disk implementations and return a normal\n                # package if we find one, otherwise return the one we created here\n\n        if not parent_pkg:  # top-level package, look for NS subpackages on all collection paths\n            package_paths = [self._extend_path_with_ns(p, fullname) for p in self.n_collection_paths]\n        else:  # subpackage; search in all subpaths (we'll limit later inside a collection)\n            package_paths = [self._extend_path_with_ns(p, fullname) for p in parent_pkg.__path__]\n\n        for candidate_child_path in package_paths:\n            code_object = None\n            is_package = True\n            location = None\n            # check for implicit sub-package first\n            if os.path.isdir(to_bytes(candidate_child_path)):\n                # Py3.x implicit namespace packages don't have a file location, so they don't support get_data\n                # (which assumes the parent dir or that the loader has an internal mapping); so we have to provide\n                # a bogus leaf file on the __file__ attribute for pkgutil.get_data to strip off\n                location = os.path.join(candidate_child_path, '__synthetic__')\n            else:\n                for source_path in [os.path.join(candidate_child_path, '__init__.py'),\n                                    candidate_child_path + '.py']:\n                    if not os.path.isfile(to_bytes(source_path)):\n                        continue\n\n                    if not load:\n                        return True, None\n\n                    with open(to_bytes(source_path), 'rb') as fd:\n                        source = fd.read()\n\n                    code_object = compile(source=source, filename=source_path, mode='exec', flags=0, dont_inherit=True)\n                    location = source_path\n                    is_package = source_path.endswith('__init__.py')\n                    break\n\n                if not location:\n                    continue\n\n            newmod = ModuleType(fullname)\n            newmod.__file__ = location\n            newmod.__loader__ = self\n\n            if is_package:\n                if sub_collection:  # we never want to search multiple instances of the same collection; use first found\n                    newmod.__path__ = [candidate_child_path]\n                else:\n                    newmod.__path__ = package_paths\n\n                newmod.__package__ = fullname\n            else:\n                newmod.__package__ = parent_pkg_name\n\n            sys.modules[fullname] = newmod\n\n            if code_object:\n                # FIXME: decide cases where we don't actually want to exec the code?\n                exec(code_object, newmod.__dict__)\n\n            return True, newmod\n\n        # even if we didn't find one on disk, fall back to a synthetic package if we have one...\n        if newmod:\n            sys.modules[fullname] = newmod\n            return True, newmod\n\n        # FIXME: need to handle the \"no dirs present\" case for at least the root and synthetic internal collections like ansible.builtin\n\n        return False, None",
        "begin_line": 129,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001984126984126984,
            "pseudo_dstar_susp": 0.001282051282051282,
            "pseudo_tarantula_susp": 0.004424778761061947,
            "pseudo_op2_susp": 0.001282051282051282,
            "pseudo_barinel_susp": 0.004424778761061947
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader._extend_path_with_ns#276",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionLoader._extend_path_with_ns(path, ns)",
        "snippet": "    def _extend_path_with_ns(path, ns):\n        ns_path_add = ns.rsplit('.', 1)[-1]\n\n        return os.path.join(path, ns_path_add)",
        "begin_line": 276,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0018587360594795538,
            "pseudo_dstar_susp": 0.001128668171557562,
            "pseudo_tarantula_susp": 0.004424778761061947,
            "pseudo_op2_susp": 0.001128668171557562,
            "pseudo_barinel_susp": 0.004424778761061947
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleFlatMapLoader.__init__#289",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleFlatMapLoader",
        "signature": "lib.ansible.utils.collection_loader.AnsibleFlatMapLoader.__init__(self, root_package)",
        "snippet": "    def __init__(self, root_package):\n        self._root_package = root_package\n        self._dirtree = None",
        "begin_line": 289,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002932551319648094,
            "pseudo_dstar_susp": 0.0011792452830188679,
            "pseudo_tarantula_susp": 0.017543859649122806,
            "pseudo_op2_susp": 0.0011792452830188679,
            "pseudo_barinel_susp": 0.017543859649122806
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.__init__#361",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.__init__(self, collection_name, subdirs, resource, ref_type)",
        "snippet": "    def __init__(self, collection_name, subdirs, resource, ref_type):\n        \"\"\"\n        Create an AnsibleCollectionRef from components\n        :param collection_name: a collection name of the form 'namespace.collectionname'\n        :param subdirs: optional subdir segments to be appended below the plugin type (eg, 'subdir1.subdir2')\n        :param resource: the name of the resource being references (eg, 'mymodule', 'someaction', 'a_role')\n        :param ref_type: the type of the reference, eg 'module', 'role', 'doc_fragment'\n        \"\"\"\n        collection_name = to_text(collection_name, errors='strict')\n        if subdirs is not None:\n            subdirs = to_text(subdirs, errors='strict')\n        resource = to_text(resource, errors='strict')\n        ref_type = to_text(ref_type, errors='strict')\n\n        if not self.is_valid_collection_name(collection_name):\n            raise ValueError('invalid collection name (must be of the form namespace.collection): {0}'.format(to_native(collection_name)))\n\n        if ref_type not in self.VALID_REF_TYPES:\n            raise ValueError('invalid collection ref_type: {0}'.format(ref_type))\n\n        self.collection = collection_name\n        if subdirs:\n            if not re.match(self.VALID_SUBDIRS_RE, subdirs):\n                raise ValueError('invalid subdirs entry: {0} (must be empty/None or of the form subdir1.subdir2)'.format(to_native(subdirs)))\n            self.subdirs = subdirs\n        else:\n            self.subdirs = u''\n\n        self.resource = resource\n        self.ref_type = ref_type\n\n        package_components = [u'ansible_collections', self.collection]\n\n        if self.ref_type == u'role':\n            package_components.append(u'roles')\n        else:\n            # we assume it's a plugin\n            package_components += [u'plugins', self.ref_type]\n\n        if self.subdirs:\n            package_components.append(self.subdirs)\n\n        if self.ref_type == u'role':\n            # roles are their own resource\n            package_components.append(self.resource)\n\n        self.n_python_package_name = to_native('.'.join(package_components))",
        "begin_line": 361,
        "end_line": 407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.from_fqcr#410",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.from_fqcr(ref, ref_type)",
        "snippet": "    def from_fqcr(ref, ref_type):\n        \"\"\"\n        Parse a string as a fully-qualified collection reference, raises ValueError if invalid\n        :param ref: collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n        :param ref_type: the type of the reference, eg 'module', 'role', 'doc_fragment'\n        :return: a populated AnsibleCollectionRef object\n        \"\"\"\n        # assuming the fq_name is of the form (ns).(coll).(optional_subdir_N).(resource_name),\n        # we split the resource name off the right, split ns and coll off the left, and we're left with any optional\n        # subdirs that need to be added back below the plugin-specific subdir we'll add. So:\n        # ns.coll.resource -> ansible_collections.ns.coll.plugins.(plugintype).resource\n        # ns.coll.subdir1.resource -> ansible_collections.ns.coll.plugins.subdir1.(plugintype).resource\n        # ns.coll.rolename -> ansible_collections.ns.coll.roles.rolename\n        if not AnsibleCollectionRef.is_valid_fqcr(ref):\n            raise ValueError('{0} is not a valid collection reference'.format(to_native(ref)))\n\n        ref = to_text(ref, errors='strict')\n        ref_type = to_text(ref_type, errors='strict')\n\n        resource_splitname = ref.rsplit(u'.', 1)\n        package_remnant = resource_splitname[0]\n        resource = resource_splitname[1]\n\n        # split the left two components of the collection package name off, anything remaining is plugin-type\n        # specific subdirs to be added back on below the plugin type\n        package_splitname = package_remnant.split(u'.', 2)\n        if len(package_splitname) == 3:\n            subdirs = package_splitname[2]\n        else:\n            subdirs = u''\n\n        collection_name = u'.'.join(package_splitname[0:2])\n\n        return AnsibleCollectionRef(collection_name, subdirs, resource, ref_type)",
        "begin_line": 410,
        "end_line": 443,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.try_parse_fqcr#446",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.try_parse_fqcr(ref, ref_type)",
        "snippet": "    def try_parse_fqcr(ref, ref_type):\n        \"\"\"\n        Attempt to parse a string as a fully-qualified collection reference, returning None on failure (instead of raising an error)\n        :param ref: collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n        :param ref_type: the type of the reference, eg 'module', 'role', 'doc_fragment'\n        :return: a populated AnsibleCollectionRef object on successful parsing, else None\n        \"\"\"\n        try:\n            return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n        except ValueError:\n            pass",
        "begin_line": 446,
        "end_line": 456,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.is_valid_fqcr#478",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.is_valid_fqcr(ref, ref_type=None)",
        "snippet": "    def is_valid_fqcr(ref, ref_type=None):\n        \"\"\"\n        Validates if is string is a well-formed fully-qualified collection reference (does not look up the collection itself)\n        :param ref: candidate collection reference to validate (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n        :param ref_type: optional reference type to enable deeper validation, eg 'module', 'role', 'doc_fragment'\n        :return: True if the collection ref passed is well-formed, False otherwise\n        \"\"\"\n\n        ref = to_text(ref)\n\n        if not ref_type:\n            return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n\n        return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))",
        "begin_line": 478,
        "end_line": 491,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.is_valid_collection_name#494",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader.AnsibleCollectionRef",
        "signature": "lib.ansible.utils.collection_loader.AnsibleCollectionRef.is_valid_collection_name(collection_name)",
        "snippet": "    def is_valid_collection_name(collection_name):\n        \"\"\"\n        Validates if the given string is a well-formed collection name (does not look up the collection itself)\n        :param collection_name: candidate collection name to validate (a valid name is of the form 'ns.collname')\n        :return: True if the collection name passed is well-formed, False otherwise\n        \"\"\"\n\n        collection_name = to_text(collection_name)\n\n        return bool(re.match(AnsibleCollectionRef.VALID_COLLECTION_NAME_RE, collection_name))",
        "begin_line": 494,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006978367062107466,
            "pseudo_dstar_susp": 0.0006978367062107466,
            "pseudo_tarantula_susp": 0.0007022471910112359,
            "pseudo_op2_susp": 0.0006978367062107466,
            "pseudo_barinel_susp": 0.0007022471910112359
        }
    },
    {
        "name": "lib.ansible.utils.collection_loader.get_collection_role_path#506",
        "src_path": "lib/ansible/utils/collection_loader.py",
        "class_name": "lib.ansible.utils.collection_loader",
        "signature": "lib.ansible.utils.collection_loader.get_collection_role_path(role_name, collection_list=None)",
        "snippet": "def get_collection_role_path(role_name, collection_list=None):\n    acr = AnsibleCollectionRef.try_parse_fqcr(role_name, 'role')\n\n    if acr:\n        # looks like a valid qualified collection ref; skip the collection_list\n        collection_list = [acr.collection]\n        subdirs = acr.subdirs\n        resource = acr.resource\n    elif not collection_list:\n        return None  # not a FQ role and no collection search list spec'd, nothing to do\n    else:\n        resource = role_name  # treat as unqualified, loop through the collection search list to try and resolve\n        subdirs = ''\n\n    for collection_name in collection_list:\n        try:\n            acr = AnsibleCollectionRef(collection_name=collection_name, subdirs=subdirs, resource=resource, ref_type='role')\n            # FIXME: error handling/logging; need to catch any import failures and move along\n\n            # FIXME: this line shouldn't be necessary, but py2 pkgutil.get_data is delegating back to built-in loader when it shouldn't\n            pkg = import_module(acr.n_python_package_name)\n\n            if pkg is not None:\n                # the package is now loaded, get the collection's package and ask where it lives\n                path = os.path.dirname(to_bytes(sys.modules[acr.n_python_package_name].__file__, errors='surrogate_or_strict'))\n                return resource, to_text(path, errors='surrogate_or_strict'), collection_name\n\n        except IOError:\n            continue\n        except Exception as ex:\n            # FIXME: pick out typical import errors first, then error logging\n            continue\n\n    return None",
        "begin_line": 506,
        "end_line": 539,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.random_password#40",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt",
        "signature": "lib.ansible.utils.encrypt.random_password(length=DEFAULT_PASSWORD_LENGTH, chars=C.DEFAULT_PASSWORD_CHARS)",
        "snippet": "def random_password(length=DEFAULT_PASSWORD_LENGTH, chars=C.DEFAULT_PASSWORD_CHARS):\n    '''Return a random password string of length containing only chars\n\n    :kwarg length: The number of characters in the new password.  Defaults to 20.\n    :kwarg chars: The characters to choose from.  The default is all ascii\n        letters, ascii digits, and these symbols ``.,:-_``\n    '''\n    if not isinstance(chars, text_type):\n        raise AnsibleAssertionError('%s (%s) is not a text_type' % (chars, type(chars)))\n\n    random_generator = random.SystemRandom()\n    return u''.join(random_generator.choice(chars) for dummy in range(length))",
        "begin_line": 40,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.91981090000892e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.random_salt#54",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt",
        "signature": "lib.ansible.utils.encrypt.random_salt(length=8)",
        "snippet": "def random_salt(length=8):\n    \"\"\"Return a text string suitable for use as a salt for the hash functions we use to encrypt passwords.\n    \"\"\"\n    # Note passlib salt values must be pure ascii so we can't let the user\n    # configure this\n    salt_chars = string.ascii_letters + string.digits + u'./'\n    return random_password(length=length, chars=salt_chars)",
        "begin_line": 54,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.BaseHash.__init__#72",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.BaseHash",
        "signature": "lib.ansible.utils.encrypt.BaseHash.__init__(self, algorithm)",
        "snippet": "    def __init__(self, algorithm):\n        self.algorithm = algorithm",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.CryptHash.__init__#77",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.CryptHash",
        "signature": "lib.ansible.utils.encrypt.CryptHash.__init__(self, algorithm)",
        "snippet": "    def __init__(self, algorithm):\n        super(CryptHash, self).__init__(algorithm)\n\n        if sys.platform.startswith('darwin'):\n            raise AnsibleError(\"crypt.crypt not supported on Mac OS X/Darwin, install passlib python module\")\n\n        if algorithm not in self.algorithms:\n            raise AnsibleError(\"crypt.crypt does not support '%s' algorithm\" % self.algorithm)\n        self.algo_data = self.algorithms[algorithm]",
        "begin_line": 77,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.CryptHash.hash#87",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.CryptHash",
        "signature": "lib.ansible.utils.encrypt.CryptHash.hash(self, secret, salt=None, salt_size=None, rounds=None)",
        "snippet": "    def hash(self, secret, salt=None, salt_size=None, rounds=None):\n        salt = self._salt(salt, salt_size)\n        rounds = self._rounds(rounds)\n        return self._hash(secret, salt, rounds)",
        "begin_line": 87,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.CryptHash._salt#92",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.CryptHash",
        "signature": "lib.ansible.utils.encrypt.CryptHash._salt(self, salt, salt_size)",
        "snippet": "    def _salt(self, salt, salt_size):\n        salt_size = salt_size or self.algo_data.salt_size\n        return salt or random_salt(salt_size)",
        "begin_line": 92,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.CryptHash._rounds#96",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.CryptHash",
        "signature": "lib.ansible.utils.encrypt.CryptHash._rounds(self, rounds)",
        "snippet": "    def _rounds(self, rounds):\n        if rounds == self.algo_data.implicit_rounds:\n            # Passlib does not include the rounds if it is the same as implicit_rounds.\n            # Make crypt lib behave the same, by not explicitly specifying the rounds in that case.\n            return None\n        else:\n            return rounds",
        "begin_line": 96,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.CryptHash._hash#104",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.CryptHash",
        "signature": "lib.ansible.utils.encrypt.CryptHash._hash(self, secret, salt, rounds)",
        "snippet": "    def _hash(self, secret, salt, rounds):\n        if rounds is None:\n            saltstring = \"$%s$%s\" % (self.algo_data.crypt_id, salt)\n        else:\n            saltstring = \"$%s$rounds=%d$%s\" % (self.algo_data.crypt_id, rounds, salt)\n        result = crypt.crypt(secret, saltstring)\n\n        # crypt.crypt returns None if it cannot parse saltstring\n        # None as result would be interpreted by the some modules (user module)\n        # as no password at all.\n        if not result:\n            raise AnsibleError(\"crypt.crypt does not support '%s' algorithm\" % self.algorithm)\n\n        return result",
        "begin_line": 104,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.PasslibHash.__init__#121",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt.PasslibHash",
        "signature": "lib.ansible.utils.encrypt.PasslibHash.__init__(self, algorithm)",
        "snippet": "    def __init__(self, algorithm):\n        super(PasslibHash, self).__init__(algorithm)\n\n        if not PASSLIB_AVAILABLE:\n            raise AnsibleError(\"passlib must be installed to hash with '%s'\" % algorithm)\n\n        try:\n            self.crypt_algo = getattr(passlib.hash, algorithm)\n        except Exception:\n            raise AnsibleError(\"passlib does not support '%s' algorithm\" % algorithm)",
        "begin_line": 121,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.passlib_or_crypt#189",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt",
        "signature": "lib.ansible.utils.encrypt.passlib_or_crypt(secret, algorithm, salt=None, salt_size=None, rounds=None)",
        "snippet": "def passlib_or_crypt(secret, algorithm, salt=None, salt_size=None, rounds=None):\n    if PASSLIB_AVAILABLE:\n        return PasslibHash(algorithm).hash(secret, salt=salt, salt_size=salt_size, rounds=rounds)\n    else:\n        return CryptHash(algorithm).hash(secret, salt=salt, salt_size=salt_size, rounds=rounds)",
        "begin_line": 189,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.encrypt.do_encrypt#196",
        "src_path": "lib/ansible/utils/encrypt.py",
        "class_name": "lib.ansible.utils.encrypt",
        "signature": "lib.ansible.utils.encrypt.do_encrypt(result, encrypt, salt_size=None, salt=None)",
        "snippet": "def do_encrypt(result, encrypt, salt_size=None, salt=None):\n    return passlib_or_crypt(result, encrypt, salt_size=salt_size, salt=salt)",
        "begin_line": 196,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.parsing.quoting.is_quoted#23",
        "src_path": "lib/ansible/parsing/quoting.py",
        "class_name": "lib.ansible.parsing.quoting",
        "signature": "lib.ansible.parsing.quoting.is_quoted(data)",
        "snippet": "def is_quoted(data):\n    return len(data) > 1 and data[0] == data[-1] and data[0] in ('\"', \"'\") and data[-2] != '\\\\'",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005586592178770949,
            "pseudo_dstar_susp": 0.0005586592178770949,
            "pseudo_tarantula_susp": 0.0005586592178770949,
            "pseudo_op2_susp": 0.0005586592178770949,
            "pseudo_barinel_susp": 0.0005586592178770949
        }
    },
    {
        "name": "lib.ansible.parsing.quoting.unquote#27",
        "src_path": "lib/ansible/parsing/quoting.py",
        "class_name": "lib.ansible.parsing.quoting",
        "signature": "lib.ansible.parsing.quoting.unquote(data)",
        "snippet": "def unquote(data):\n    ''' removes first and last quotes from a string, if the string starts and ends with the same quotes '''\n    if is_quoted(data):\n        return data[1:-1]\n    return data",
        "begin_line": 27,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005617977528089888,
            "pseudo_dstar_susp": 0.0005617977528089888,
            "pseudo_tarantula_susp": 0.0005617977528089888,
            "pseudo_op2_susp": 0.0005617977528089888,
            "pseudo_barinel_susp": 0.0005617977528089888
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.base.Network.__init__#35",
        "src_path": "lib/ansible/module_utils/facts/network/base.py",
        "class_name": "lib.ansible.module_utils.facts.network.base.Network",
        "signature": "lib.ansible.module_utils.facts.network.base.Network.__init__(self, module, load_on_init=False)",
        "snippet": "    def __init__(self, module, load_on_init=False):\n        self.module = module",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.451564828614009e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.base.Network.populate#39",
        "src_path": "lib/ansible/module_utils/facts/network/base.py",
        "class_name": "lib.ansible.module_utils.facts.network.base.Network",
        "signature": "lib.ansible.module_utils.facts.network.base.Network.populate(self, collected_facts=None)",
        "snippet": "    def populate(self, collected_facts=None):\n        return {}",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.network.base.NetworkCollector.collect#60",
        "src_path": "lib/ansible/module_utils/facts/network/base.py",
        "class_name": "lib.ansible.module_utils.facts.network.base.NetworkCollector",
        "signature": "lib.ansible.module_utils.facts.network.base.NetworkCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        collected_facts = collected_facts or {}\n        if not module:\n            return {}\n\n        # Network munges cached_facts by side effect, so give it a copy\n        facts_obj = self._fact_class(module)\n\n        facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n        return facts_dict",
        "begin_line": 60,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._get_proc_cmdline#30",
        "src_path": "lib/ansible/module_utils/facts/system/cmdline.py",
        "class_name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._get_proc_cmdline(self)",
        "snippet": "    def _get_proc_cmdline(self):\n        return get_file_content('/proc/cmdline')",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._parse_proc_cmdline#33",
        "src_path": "lib/ansible/module_utils/facts/system/cmdline.py",
        "class_name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._parse_proc_cmdline(self, data)",
        "snippet": "    def _parse_proc_cmdline(self, data):\n        cmdline_dict = {}\n        try:\n            for piece in shlex.split(data, posix=False):\n                item = piece.split('=', 1)\n                if len(item) == 1:\n                    cmdline_dict[item[0]] = True\n                else:\n                    cmdline_dict[item[0]] = item[1]\n        except ValueError:\n            pass\n\n        return cmdline_dict",
        "begin_line": 33,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._parse_proc_cmdline_facts#47",
        "src_path": "lib/ansible/module_utils/facts/system/cmdline.py",
        "class_name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector._parse_proc_cmdline_facts(self, data)",
        "snippet": "    def _parse_proc_cmdline_facts(self, data):\n        cmdline_dict = {}\n        try:\n            for piece in shlex.split(data, posix=False):\n                item = piece.split('=', 1)\n                if len(item) == 1:\n                    cmdline_dict[item[0]] = True\n                else:\n                    if item[0] in cmdline_dict:\n                        if isinstance(cmdline_dict[item[0]], list):\n                            cmdline_dict[item[0]].append(item[1])\n                        else:\n                            new_list = [cmdline_dict[item[0]], item[1]]\n                            cmdline_dict[item[0]] = new_list\n                    else:\n                        cmdline_dict[item[0]] = item[1]\n        except ValueError:\n            pass\n\n        return cmdline_dict",
        "begin_line": 47,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector.collect#68",
        "src_path": "lib/ansible/module_utils/facts/system/cmdline.py",
        "class_name": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.cmdline.CmdLineFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        cmdline_facts = {}\n\n        data = self._get_proc_cmdline()\n\n        if not data:\n            return cmdline_facts\n\n        cmdline_facts['cmdline'] = self._parse_proc_cmdline(data)\n        cmdline_facts['proc_cmdline'] = self._parse_proc_cmdline_facts(data)\n\n        return cmdline_facts",
        "begin_line": 68,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase.__init__#64",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase.__init__(self, display=None, options=None)",
        "snippet": "    def __init__(self, display=None, options=None):\n        if display:\n            self._display = display\n        else:\n            self._display = global_display\n\n        if self._display.verbosity >= 4:\n            name = getattr(self, 'CALLBACK_NAME', 'unnamed')\n            ctype = getattr(self, 'CALLBACK_TYPE', 'old')\n            version = getattr(self, 'CALLBACK_VERSION', '1.0')\n            self._display.vvvv('Loading callback plugin %s of type %s, v%s from %s' % (name, ctype, version, sys.modules[self.__module__].__file__))\n\n        self.disabled = False\n\n        self._plugin_options = {}\n        if options is not None:\n            self.set_options(options)\n\n        self._hide_in_debug = ('changed', 'failed', 'skipped', 'invocation', 'skip_reason')",
        "begin_line": 64,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._dump_results#105",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._dump_results(self, result, indent=None, sort_keys=True, keep_invocation=False)",
        "snippet": "    def _dump_results(self, result, indent=None, sort_keys=True, keep_invocation=False):\n\n        if not indent and (result.get('_ansible_verbose_always') or self._display.verbosity > 2):\n            indent = 4\n\n        # All result keys stating with _ansible_ are internal, so remove them from the result before we output anything.\n        abridged_result = strip_internal_keys(module_response_deepcopy(result))\n\n        # remove invocation unless specifically wanting it\n        if not keep_invocation and self._display.verbosity < 3 and 'invocation' in result:\n            del abridged_result['invocation']\n\n        # remove diff information from screen output\n        if self._display.verbosity < 3 and 'diff' in result:\n            del abridged_result['diff']\n\n        # remove exception from screen output\n        if 'exception' in abridged_result:\n            del abridged_result['exception']\n\n        try:\n            jsonified_results = json.dumps(abridged_result, cls=AnsibleJSONEncoder, indent=indent, ensure_ascii=False, sort_keys=sort_keys)\n        except TypeError:\n            # Python3 bug: throws an exception when keys are non-homogenous types:\n            # https://bugs.python.org/issue25457\n            # sort into an OrderedDict and then json.dumps() that instead\n            if not OrderedDict:\n                raise\n            jsonified_results = json.dumps(OrderedDict(sorted(abridged_result.items(), key=to_text)),\n                                           cls=AnsibleJSONEncoder, indent=indent,\n                                           ensure_ascii=False, sort_keys=False)\n        return jsonified_results",
        "begin_line": 105,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._serialize_diff#164",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._serialize_diff(self, diff)",
        "snippet": "    def _serialize_diff(self, diff):\n        return json.dumps(diff, sort_keys=True, indent=4, separators=(u',', u': ')) + u'\\n'",
        "begin_line": 164,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._get_diff#167",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._get_diff(self, difflist)",
        "snippet": "    def _get_diff(self, difflist):\n\n        if not isinstance(difflist, list):\n            difflist = [difflist]\n\n        ret = []\n        for diff in difflist:\n            if 'dst_binary' in diff:\n                ret.append(u\"diff skipped: destination file appears to be binary\\n\")\n            if 'src_binary' in diff:\n                ret.append(u\"diff skipped: source file appears to be binary\\n\")\n            if 'dst_larger' in diff:\n                ret.append(u\"diff skipped: destination file size is greater than %d\\n\" % diff['dst_larger'])\n            if 'src_larger' in diff:\n                ret.append(u\"diff skipped: source file size is greater than %d\\n\" % diff['src_larger'])\n            if 'before' in diff and 'after' in diff:\n                # format complex structures into 'files'\n                for x in ['before', 'after']:\n                    if isinstance(diff[x], MutableMapping):\n                        diff[x] = self._serialize_diff(diff[x])\n                    elif diff[x] is None:\n                        diff[x] = ''\n                if 'before_header' in diff:\n                    before_header = u\"before: %s\" % diff['before_header']\n                else:\n                    before_header = u'before'\n                if 'after_header' in diff:\n                    after_header = u\"after: %s\" % diff['after_header']\n                else:\n                    after_header = u'after'\n                before_lines = diff['before'].splitlines(True)\n                after_lines = diff['after'].splitlines(True)\n                if before_lines and not before_lines[-1].endswith(u'\\n'):\n                    before_lines[-1] += u'\\n\\\\ No newline at end of file\\n'\n                if after_lines and not after_lines[-1].endswith('\\n'):\n                    after_lines[-1] += u'\\n\\\\ No newline at end of file\\n'\n                differ = difflib.unified_diff(before_lines,\n                                              after_lines,\n                                              fromfile=before_header,\n                                              tofile=after_header,\n                                              fromfiledate=u'',\n                                              tofiledate=u'',\n                                              n=C.DIFF_CONTEXT)\n                difflines = list(differ)\n                if len(difflines) >= 3 and sys.version_info[:2] == (2, 6):\n                    # difflib in Python 2.6 adds trailing spaces after\n                    # filenames in the -- before/++ after headers.\n                    difflines[0] = difflines[0].replace(u' \\n', u'\\n')\n                    difflines[1] = difflines[1].replace(u' \\n', u'\\n')\n                    # it also treats empty files differently\n                    difflines[2] = difflines[2].replace(u'-1,0', u'-0,0').replace(u'+1,0', u'+0,0')\n                has_diff = False\n                for line in difflines:\n                    has_diff = True\n                    if line.startswith(u'+'):\n                        line = stringc(line, C.COLOR_DIFF_ADD)\n                    elif line.startswith(u'-'):\n                        line = stringc(line, C.COLOR_DIFF_REMOVE)\n                    elif line.startswith(u'@@'):\n                        line = stringc(line, C.COLOR_DIFF_LINES)\n                    ret.append(line)\n                if has_diff:\n                    ret.append('\\n')\n            if 'prepared' in diff:\n                ret.append(diff['prepared'])\n        return u''.join(ret)",
        "begin_line": 167,
        "end_line": 232,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._get_item_label#234",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._get_item_label(self, result)",
        "snippet": "    def _get_item_label(self, result):\n        ''' retrieves the value to be displayed as a label for an item entry from a result object'''\n        if result.get('_ansible_no_log', False):\n            item = \"(censored due to no_log)\"\n        else:\n            item = result.get('_ansible_item_label', result.get('item'))\n        return item",
        "begin_line": 234,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._get_item#242",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._get_item(self, result)",
        "snippet": "    def _get_item(self, result):\n        ''' here for backwards compat, really should have always been named: _get_item_label'''\n        cback = getattr(self, 'NAME', os.path.basename(__file__))\n        self._display.deprecated(\"The %s callback plugin should be updated to use the _get_item_label method instead\" % cback, version=\"2.11\")\n        return self._get_item_label(result)",
        "begin_line": 242,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase._clean_results#252",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase._clean_results(self, result, task_name)",
        "snippet": "    def _clean_results(self, result, task_name):\n        ''' removes data from results for display '''\n\n        # mostly controls that debug only outputs what it was meant to\n        if task_name == 'debug':\n            if 'msg' in result:\n                # msg should be alone\n                for key in list(result.keys()):\n                    if key not in _DEBUG_ALLOWED_KEYS and not key.startswith('_'):\n                        result.pop(key)\n            else:\n                # 'var' value as field, so eliminate others and what is left should be varname\n                for hidme in self._hide_in_debug:\n                    result.pop(hidme, None)",
        "begin_line": 252,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase.on_any#270",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase.on_any(self, *args, **kwargs)",
        "snippet": "    def on_any(self, *args, **kwargs):\n        pass",
        "begin_line": 270,
        "end_line": 271,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.callback.__init__.CallbackBase.v2_on_any#334",
        "src_path": "lib/ansible/plugins/callback/__init__.py",
        "class_name": "lib.ansible.plugins.callback.__init__.CallbackBase",
        "signature": "lib.ansible.plugins.callback.__init__.CallbackBase.v2_on_any(self, *args, **kwargs)",
        "snippet": "    def v2_on_any(self, *args, **kwargs):\n        self.on_any(args, kwargs)",
        "begin_line": 334,
        "end_line": 335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.count_terms#26",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.count_terms(terms, module_parameters)",
        "snippet": "def count_terms(terms, module_parameters):\n    \"\"\"Count the number of occurrences of a key in a given dictionary\n\n    :arg terms: String or iterable of values to check\n    :arg module_parameters: Dictionary of module parameters\n\n    :returns: An integer that is the number of occurrences of the terms values\n        in the provided dictionary.\n    \"\"\"\n\n    if not is_iterable(terms):\n        terms = [terms]\n\n    return len(set(terms).intersection(module_parameters))",
        "begin_line": 26,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_mutually_exclusive#42",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_mutually_exclusive(terms, module_parameters)",
        "snippet": "def check_mutually_exclusive(terms, module_parameters):\n    \"\"\"Check mutually exclusive terms against argument parameters\n\n    Accepts a single list or list of lists that are groups of terms that should be\n    mutually exclusive with one another\n\n    :arg terms: List of mutually exclusive module parameters\n    :arg module_parameters: Dictionary of module parameters\n\n    :returns: Empty list or raises TypeError if the check fails.\n    \"\"\"\n\n    results = []\n    if terms is None:\n        return results\n\n    for check in terms:\n        count = count_terms(check, module_parameters)\n        if count > 1:\n            results.append(check)\n\n    if results:\n        full_list = ['|'.join(check) for check in results]\n        msg = \"parameters are mutually exclusive: %s\" % ', '.join(full_list)\n        raise TypeError(to_native(msg))\n\n    return results",
        "begin_line": 42,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_required_together#101",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_required_together(terms, module_parameters)",
        "snippet": "def check_required_together(terms, module_parameters):\n    \"\"\"Check each list of terms to ensure every parameter in each list exists\n    in the given module parameters\n\n    Accepts a list of lists or tuples\n\n    :arg terms: List of lists of terms to check. Each list should include\n        parameters that are all required when at least one is specified\n        in the module_parameters.\n    :arg module_parameters: Dictionary of module parameters\n\n    :returns: Empty list or raises TypeError if the check fails.\n    \"\"\"\n\n    results = []\n    if terms is None:\n        return results\n\n    for term in terms:\n        counts = [count_terms(field, module_parameters) for field in term]\n        non_zero = [c for c in counts if c > 0]\n        if len(non_zero) > 0:\n            if 0 in counts:\n                results.append(term)\n    if results:\n        for term in results:\n            msg = \"parameters are required together: %s\" % ', '.join(term)\n            raise TypeError(to_native(msg))\n\n    return results",
        "begin_line": 101,
        "end_line": 130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_required_arguments#169",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_required_arguments(argument_spec, module_parameters)",
        "snippet": "def check_required_arguments(argument_spec, module_parameters):\n    \"\"\"Check all paramaters in argument_spec and return a list of parameters\n    that are required but not present in module_parameters\n\n    Raises TypeError if the check fails\n\n    :arg argument_spec: Argument spec dicitionary containing all parameters\n        and their specification\n    :arg module_paramaters: Dictionary of module parameters\n\n    :returns: Empty list or raises TypeError if the check fails.\n    \"\"\"\n\n    missing = []\n    if argument_spec is None:\n        return missing\n\n    for (k, v) in argument_spec.items():\n        required = v.get('required', False)\n        if required and k not in module_parameters:\n            missing.append(k)\n\n    if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\n        raise TypeError(to_native(msg))\n\n    return missing",
        "begin_line": 169,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_required_if#198",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_required_if(requirements, module_parameters)",
        "snippet": "def check_required_if(requirements, module_parameters):\n    \"\"\"Check parameters that are conditionally required\n\n    Raises TypeError if the check fails\n\n    :arg requirements: List of lists specifying a parameter, value, parameters\n        required when the given parameter is the specified value, and optionally\n        a boolean indicating any or all parameters are required.\n\n        Example:\n            required_if=[\n                ['state', 'present', ('path',), True],\n                ['someint', 99, ('bool_param', 'string_param')],\n            ]\n\n    :arg module_paramaters: Dictionary of module parameters\n\n    :returns: Empty list or raises TypeError if the check fails.\n        The results attribute of the exception contains a list of dictionaries.\n        Each dictionary is the result of evaluting each item in requirements.\n        Each return dictionary contains the following keys:\n\n            :key missing: List of parameters that are required but missing\n            :key requires: 'any' or 'all'\n            :key paramater: Parameter name that has the requirement\n            :key value: Original value of the paramater\n            :key requirements: Original required parameters\n\n        Example:\n            [\n                {\n                    'parameter': 'someint',\n                    'value': 99\n                    'requirements': ('bool_param', 'string_param'),\n                    'missing': ['string_param'],\n                    'requires': 'all',\n                }\n            ]\n\n    \"\"\"\n    results = []\n    if requirements is None:\n        return results\n\n    for req in requirements:\n        missing = {}\n        missing['missing'] = []\n        max_missing_count = 0\n        is_one_of = False\n        if len(req) == 4:\n            key, val, requirements, is_one_of = req\n        else:\n            key, val, requirements = req\n\n        # is_one_of is True at least one requirement should be\n        # present, else all requirements should be present.\n        if is_one_of:\n            max_missing_count = len(requirements)\n            missing['requires'] = 'any'\n        else:\n            missing['requires'] = 'all'\n\n        if key in module_parameters and module_parameters[key] == val:\n            for check in requirements:\n                count = count_terms(check, module_parameters)\n                if count == 0:\n                    missing['missing'].append(check)\n        if len(missing['missing']) and len(missing['missing']) >= max_missing_count:\n            missing['parameter'] = key\n            missing['value'] = val\n            missing['requirements'] = requirements\n            results.append(missing)\n\n    if results:\n        for missing in results:\n            msg = \"%s is %s but %s of the following are missing: %s\" % (\n                missing['parameter'], missing['value'], missing['requires'], ', '.join(missing['missing']))\n            raise TypeError(to_native(msg))\n\n    return results",
        "begin_line": 198,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_str#335",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_str(value, allow_conversion=True)",
        "snippet": "def check_type_str(value, allow_conversion=True):\n    \"\"\"Verify that the value is a string or convert to a string.\n\n    Since unexpected changes can sometimes happen when converting to a string,\n    ``allow_conversion`` controls whether or not the value will be converted or a\n    TypeError will be raised if the value is not a string and would be converted\n\n    :arg value: Value to validate or convert to a string\n    :arg allow_conversion: Whether to convert the string and return it or raise\n        a TypeError\n\n    :returns: Original value if it is a string, the value converted to a string\n        if allow_conversion=True, or raises a TypeError if allow_conversion=False.\n    \"\"\"\n    if isinstance(value, string_types):\n        return value\n\n    if allow_conversion:\n        return to_native(value, errors='surrogate_or_strict')\n\n    msg = \"'{0!r}' is not a string and conversion is not allowed\".format(value)\n    raise TypeError(to_native(msg))",
        "begin_line": 335,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_list#359",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_list(value)",
        "snippet": "def check_type_list(value):\n    \"\"\"Verify that the value is a list or convert to a list\n\n    A comma separated string will be split into a list. Rases a TypeError if\n    unable to convert to a list.\n\n    :arg value: Value to validate or convert to a list\n\n    :returns: Original value if it is already a list, single item list if a\n        float, int or string without commas, or a multi-item list if a\n        comma-delimited string.\n    \"\"\"\n    if isinstance(value, list):\n        return value\n\n    if isinstance(value, string_types):\n        return value.split(\",\")\n    elif isinstance(value, int) or isinstance(value, float):\n        return [str(value)]\n\n    raise TypeError('%s cannot be converted to a list' % type(value))",
        "begin_line": 359,
        "end_line": 379,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_dict#382",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_dict(value)",
        "snippet": "def check_type_dict(value):\n    \"\"\"Verify that value is a dict or convert it to a dict and return it.\n\n    Raises TypeError if unable to convert to a dict\n\n    :arg value: Dict or string to convert to a dict. Accepts 'k1=v2, k2=v2'.\n\n    :returns: value converted to a dictionary\n    \"\"\"\n    if isinstance(value, dict):\n        return value\n\n    if isinstance(value, string_types):\n        if value.startswith(\"{\"):\n            try:\n                return json.loads(value)\n            except Exception:\n                (result, exc) = safe_eval(value, dict(), include_exceptions=True)\n                if exc is not None:\n                    raise TypeError('unable to evaluate string as dictionary')\n                return result\n        elif '=' in value:\n            fields = []\n            field_buffer = []\n            in_quote = False\n            in_escape = False\n            for c in value.strip():\n                if in_escape:\n                    field_buffer.append(c)\n                    in_escape = False\n                elif c == '\\\\':\n                    in_escape = True\n                elif not in_quote and c in ('\\'', '\"'):\n                    in_quote = c\n                elif in_quote and in_quote == c:\n                    in_quote = False\n                elif not in_quote and c in (',', ' '):\n                    field = ''.join(field_buffer)\n                    if field:\n                        fields.append(field)\n                    field_buffer = []\n                else:\n                    field_buffer.append(c)\n\n            field = ''.join(field_buffer)\n            if field:\n                fields.append(field)\n            return dict(x.split(\"=\", 1) for x in fields)\n        else:\n            raise TypeError(\"dictionary requested, could not parse JSON or key=value\")\n\n    raise TypeError('%s cannot be converted to a dict' % type(value))",
        "begin_line": 382,
        "end_line": 433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_bool#436",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_bool(value)",
        "snippet": "def check_type_bool(value):\n    \"\"\"Verify that the value is a bool or convert it to a bool and return it.\n\n    Raises TypeError if unable to convert to a bool\n\n    :arg value: String, int, or float to convert to bool. Valid booleans include:\n         '1', 'on', 1, '0', 0, 'n', 'f', 'false', 'true', 'y', 't', 'yes', 'no', 'off'\n\n    :returns: Boolean True or False\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n\n    if isinstance(value, string_types) or isinstance(value, (int, float)):\n        return boolean(value)\n\n    raise TypeError('%s cannot be converted to a bool' % type(value))",
        "begin_line": 436,
        "end_line": 452,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_int#455",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_int(value)",
        "snippet": "def check_type_int(value):\n    \"\"\"Verify that the value is an integer and return it or convert the value\n    to an integer and return it\n\n    Raises TypeError if unable to convert to an int\n\n    :arg value: String or int to convert of verify\n\n    :return: Int of given value\n    \"\"\"\n    if isinstance(value, integer_types):\n        return value\n\n    if isinstance(value, string_types):\n        try:\n            return int(value)\n        except ValueError:\n            pass\n\n    raise TypeError('%s cannot be converted to an int' % type(value))",
        "begin_line": 455,
        "end_line": 474,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_float#477",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_float(value)",
        "snippet": "def check_type_float(value):\n    \"\"\"Verify that value is a float or convert it to a float and return it\n\n    Raises TypeError if unable to convert to a float\n\n    :arg value: Float, int, str, or bytes to verify or convert and return.\n\n    :returns: Float of given value.\n    \"\"\"\n    if isinstance(value, float):\n        return value\n\n    if isinstance(value, (binary_type, text_type, int)):\n        try:\n            return float(value)\n        except ValueError:\n            pass\n\n    raise TypeError('%s cannot be converted to a float' % type(value))",
        "begin_line": 477,
        "end_line": 495,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_path#498",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_path(value)",
        "snippet": "def check_type_path(value,):\n    \"\"\"Verify the provided value is a string or convert it to a string,\n    then return the expanded path\n    \"\"\"\n    value = check_type_str(value)\n    return os.path.expanduser(os.path.expandvars(value))",
        "begin_line": 498,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_raw#506",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_raw(value)",
        "snippet": "def check_type_raw(value):\n    \"\"\"Returns the raw value\n    \"\"\"\n    return value",
        "begin_line": 506,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_bytes#512",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_bytes(value)",
        "snippet": "def check_type_bytes(value):\n    \"\"\"Convert a human-readable string value to bytes\n\n    Raises TypeError if unable to covert the value\n    \"\"\"\n    try:\n        return human_to_bytes(value)\n    except ValueError:\n        raise TypeError('%s cannot be converted to a Byte value' % type(value))",
        "begin_line": 512,
        "end_line": 520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_bits#523",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_bits(value)",
        "snippet": "def check_type_bits(value):\n    \"\"\"Convert a human-readable string bits value to bits in integer.\n\n    Example: check_type_bits('1Mb') returns integer 1048576.\n\n    Raises TypeError if unable to covert the value.\n    \"\"\"\n    try:\n        return human_to_bytes(value, isbits=True)\n    except ValueError:\n        raise TypeError('%s cannot be converted to a Bit value' % type(value))",
        "begin_line": 523,
        "end_line": 533,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.validation.check_type_jsonarg#536",
        "src_path": "lib/ansible/module_utils/common/validation.py",
        "class_name": "lib.ansible.module_utils.common.validation",
        "signature": "lib.ansible.module_utils.common.validation.check_type_jsonarg(value)",
        "snippet": "def check_type_jsonarg(value):\n    \"\"\"Return a jsonified string. Sometimes the controller turns a json string\n    into a dict/list so transform it back into json here\n\n    Raises TypeError if unable to covert the value\n\n    \"\"\"\n    if isinstance(value, (text_type, binary_type)):\n        return value.strip()\n    elif isinstance(value, (list, tuple, dict)):\n        return jsonify(value)\n    raise TypeError('%s cannot be converted to a json string' % type(value))",
        "begin_line": 536,
        "end_line": 547,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.modules.system.systemd.parse_systemctl_show#285",
        "src_path": "lib/ansible/modules/system/systemd.py",
        "class_name": "lib.ansible.modules.system.systemd",
        "signature": "lib.ansible.modules.system.systemd.parse_systemctl_show(lines)",
        "snippet": "def parse_systemctl_show(lines):\n    # The output of 'systemctl show' can contain values that span multiple lines. At first glance it\n    # appears that such values are always surrounded by {}, so the previous version of this code\n    # assumed that any value starting with { was a multi-line value; it would then consume lines\n    # until it saw a line that ended with }. However, it is possible to have a single-line value\n    # that starts with { but does not end with } (this could happen in the value for Description=,\n    # for example), and the previous version of this code would then consume all remaining lines as\n    # part of that value. Cryptically, this would lead to Ansible reporting that the service file\n    # couldn't be found.\n    #\n    # To avoid this issue, the following code only accepts multi-line values for keys whose names\n    # start with Exec (e.g., ExecStart=), since these are the only keys whose values are known to\n    # span multiple lines.\n    parsed = {}\n    multival = []\n    k = None\n    for line in lines:\n        if k is None:\n            if '=' in line:\n                k, v = line.split('=', 1)\n                if k.startswith('Exec') and v.lstrip().startswith('{'):\n                    if not v.rstrip().endswith('}'):\n                        multival.append(v)\n                        continue\n                parsed[k] = v.strip()\n                k = None\n        else:\n            multival.append(line)\n            if line.rstrip().endswith('}'):\n                parsed[k] = '\\n'.join(multival).strip()\n                multival = []\n                k = None\n    return parsed",
        "begin_line": 285,
        "end_line": 317,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.to_safe_group_name#32",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group",
        "signature": "lib.ansible.inventory.group.to_safe_group_name(name, replacer='_', force=False, silent=False)",
        "snippet": "def to_safe_group_name(name, replacer=\"_\", force=False, silent=False):\n    # Converts 'bad' characters in a string to underscores (or provided replacer) so they can be used as Ansible hosts or groups\n\n    warn = ''\n    if name:  # when deserializing we might not have name yet\n        invalid_chars = C.INVALID_VARIABLE_NAMES.findall(name)\n        if invalid_chars:\n            msg = 'invalid character(s) \"%s\" in group name (%s)' % (to_text(set(invalid_chars)), to_text(name))\n            if C.TRANSFORM_INVALID_GROUP_CHARS not in ('never', 'ignore') or force:\n                name = C.INVALID_VARIABLE_NAMES.sub(replacer, name)\n                if not (silent or C.TRANSFORM_INVALID_GROUP_CHARS == 'silently'):\n                    display.vvvv('Replacing ' + msg)\n                    warn = 'Invalid characters were found in group names and automatically replaced, use -vvvv to see details'\n            else:\n                if C.TRANSFORM_INVALID_GROUP_CHARS == 'never':\n                    display.vvvv('Not replacing %s' % msg)\n                    warn = True\n                    warn = 'Invalid characters were found in group names but not replaced, use -vvvv to see details'\n\n                # remove this message after 2.10 AND changing the default to 'always'\n                group_chars_setting, group_chars_origin = C.config.get_config_value_and_origin('TRANSFORM_INVALID_GROUP_CHARS')\n                if group_chars_origin == 'default':\n                    display.deprecated('The TRANSFORM_INVALID_GROUP_CHARS settings is set to allow bad characters in group names by default,'\n                                       ' this will change, but still be user configurable on deprecation', version='2.10')\n\n    if warn:\n        display.warning(warn)\n\n    return name",
        "begin_line": 32,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.__init__#68",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.__init__(self, name=None)",
        "snippet": "    def __init__(self, name=None):\n\n        self.depth = 0\n        self.name = to_safe_group_name(name)\n        self.hosts = []\n        self._hosts = None\n        self.vars = {}\n        self.child_groups = []\n        self.parent_groups = []\n        self._hosts_cache = None\n        self.priority = 1",
        "begin_line": 68,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.serialize#92",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.serialize(self)",
        "snippet": "    def serialize(self):\n        parent_groups = []\n        for parent in self.parent_groups:\n            parent_groups.append(parent.serialize())\n\n        self._hosts = None\n\n        result = dict(\n            name=self.name,\n            vars=self.vars.copy(),\n            parent_groups=parent_groups,\n            depth=self.depth,\n            hosts=self.hosts,\n        )\n\n        return result",
        "begin_line": 92,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.deserialize#109",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.deserialize(self, data)",
        "snippet": "    def deserialize(self, data):\n        self.__init__()\n        self.name = data.get('name')\n        self.vars = data.get('vars', dict())\n        self.depth = data.get('depth', 0)\n        self.hosts = data.get('hosts', [])\n        self._hosts = None\n\n        parent_groups = data.get('parent_groups', [])\n        for parent_data in parent_groups:\n            g = Group()\n            g.deserialize(parent_data)\n            self.parent_groups.append(g)",
        "begin_line": 109,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group._walk_relationship#123",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group._walk_relationship(self, rel, include_self=False, preserve_ordering=False)",
        "snippet": "    def _walk_relationship(self, rel, include_self=False, preserve_ordering=False):\n        '''\n        Given `rel` that is an iterable property of Group,\n        consitituting a directed acyclic graph among all groups,\n        Returns a set of all groups in full tree\n        A   B    C\n        |  / |  /\n        | /  | /\n        D -> E\n        |  /    vertical connections\n        | /     are directed upward\n        F\n        Called on F, returns set of (A, B, C, D, E)\n        '''\n        seen = set([])\n        unprocessed = set(getattr(self, rel))\n        if include_self:\n            unprocessed.add(self)\n        if preserve_ordering:\n            ordered = [self] if include_self else []\n            ordered.extend(getattr(self, rel))\n\n        while unprocessed:\n            seen.update(unprocessed)\n            new_unprocessed = set([])\n\n            for new_item in chain.from_iterable(getattr(g, rel) for g in unprocessed):\n                new_unprocessed.add(new_item)\n                if preserve_ordering:\n                    if new_item not in seen:\n                        ordered.append(new_item)\n\n            new_unprocessed.difference_update(seen)\n            unprocessed = new_unprocessed\n\n        if preserve_ordering:\n            return ordered\n        return seen",
        "begin_line": 123,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.get_ancestors#162",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.get_ancestors(self)",
        "snippet": "    def get_ancestors(self):\n        return self._walk_relationship('parent_groups')",
        "begin_line": 162,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.246376811594203e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.get_descendants#165",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.get_descendants(self, **kwargs)",
        "snippet": "    def get_descendants(self, **kwargs):\n        return self._walk_relationship('child_groups', **kwargs)",
        "begin_line": 165,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.637668983426258e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.host_names#169",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.host_names(self)",
        "snippet": "    def host_names(self):\n        if self._hosts is None:\n            self._hosts = set(self.hosts)\n        return self._hosts",
        "begin_line": 169,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.add_child_group#177",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.add_child_group(self, group)",
        "snippet": "    def add_child_group(self, group):\n\n        if self == group:\n            raise Exception(\"can't add group to itself\")\n\n        # don't add if it's already there\n        if group not in self.child_groups:\n\n            # prepare list of group's new ancestors this edge creates\n            start_ancestors = group.get_ancestors()\n            new_ancestors = self.get_ancestors()\n            if group in new_ancestors:\n                raise AnsibleError(\"Adding group '%s' as child to '%s' creates a recursive dependency loop.\" % (to_native(group.name), to_native(self.name)))\n            new_ancestors.add(self)\n            new_ancestors.difference_update(start_ancestors)\n\n            self.child_groups.append(group)\n\n            # update the depth of the child\n            group.depth = max([self.depth + 1, group.depth])\n\n            # update the depth of the grandchildren\n            group._check_children_depth()\n\n            # now add self to child's parent_groups list, but only if there\n            # isn't already a group with the same name\n            if self.name not in [g.name for g in group.parent_groups]:\n                group.parent_groups.append(self)\n                for h in group.get_hosts():\n                    h.populate_ancestors(additions=new_ancestors)\n\n            self.clear_hosts_cache()",
        "begin_line": 177,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group._check_children_depth#210",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group._check_children_depth(self)",
        "snippet": "    def _check_children_depth(self):\n\n        depth = self.depth\n        start_depth = self.depth  # self.depth could change over loop\n        seen = set([])\n        unprocessed = set(self.child_groups)\n\n        while unprocessed:\n            seen.update(unprocessed)\n            depth += 1\n            to_process = unprocessed.copy()\n            unprocessed = set([])\n            for g in to_process:\n                if g.depth < depth:\n                    g.depth = depth\n                    unprocessed.update(g.child_groups)\n            if depth - start_depth > len(seen):\n                raise AnsibleError(\"The group named '%s' has a recursive dependency loop.\" % to_native(self.name))",
        "begin_line": 210,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.add_host#229",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.add_host(self, host)",
        "snippet": "    def add_host(self, host):\n        if host.name not in self.host_names:\n            self.hosts.append(host)\n            self._hosts.add(host.name)\n            host.add_group(self)\n            self.clear_hosts_cache()",
        "begin_line": 229,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.472422265525714e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.remove_host#236",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.remove_host(self, host)",
        "snippet": "    def remove_host(self, host):\n\n        if host.name in self.host_names:\n            self.hosts.remove(host)\n            self._hosts.remove(host.name)\n            host.remove_group(self)\n            self.clear_hosts_cache()",
        "begin_line": 236,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.clear_hosts_cache#254",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.clear_hosts_cache(self)",
        "snippet": "    def clear_hosts_cache(self):\n\n        self._hosts_cache = None\n        for g in self.get_ancestors():\n            g._hosts_cache = None",
        "begin_line": 254,
        "end_line": 258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.788890841975742e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group.get_hosts#260",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group.get_hosts(self)",
        "snippet": "    def get_hosts(self):\n\n        if self._hosts_cache is None:\n            self._hosts_cache = self._get_hosts()\n        return self._hosts_cache",
        "begin_line": 260,
        "end_line": 264,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.637668983426258e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.group.Group._get_hosts#266",
        "src_path": "lib/ansible/inventory/group.py",
        "class_name": "lib.ansible.inventory.group.Group",
        "signature": "lib.ansible.inventory.group.Group._get_hosts(self)",
        "snippet": "    def _get_hosts(self):\n\n        hosts = []\n        seen = {}\n        for kid in self.get_descendants(include_self=True, preserve_ordering=True):\n            kid_hosts = kid.hosts\n            for kk in kid_hosts:\n                if kk not in seen:\n                    seen[kk] = 1\n                    if self.name == 'all' and kk.implicit:\n                        continue\n                    hosts.append(kk)\n        return hosts",
        "begin_line": 266,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.419743782969103e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.helpers.pct_to_int#25",
        "src_path": "lib/ansible/utils/helpers.py",
        "class_name": "lib.ansible.utils.helpers",
        "signature": "lib.ansible.utils.helpers.pct_to_int(value, num_items, min_value=1)",
        "snippet": "def pct_to_int(value, num_items, min_value=1):\n    '''\n    Converts a given value to a percentage if specified as \"x%\",\n    otherwise converts the given value to an integer.\n    '''\n    if isinstance(value, string_types) and value.endswith('%'):\n        value_pct = int(value.replace(\"%\", \"\"))\n        return int((value_pct / 100.0) * num_items) or min_value\n    else:\n        return int(value)",
        "begin_line": 25,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.utils.helpers.deduplicate_list#46",
        "src_path": "lib/ansible/utils/helpers.py",
        "class_name": "lib.ansible.utils.helpers",
        "signature": "lib.ansible.utils.helpers.deduplicate_list(original_list)",
        "snippet": "def deduplicate_list(original_list):\n    \"\"\"\n    Creates a deduplicated list with the order in which each item is first found.\n    \"\"\"\n    seen = set()\n    return [x for x in original_list if x not in seen and not seen.add(x)]",
        "begin_line": 46,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.get_plugin_class#40",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__",
        "signature": "lib.ansible.plugins.__init__.get_plugin_class(obj)",
        "snippet": "def get_plugin_class(obj):\n    if isinstance(obj, string_types):\n        return obj.lower().replace('module', '')\n    else:\n        return obj.__class__.__name__.lower().replace('module', '')",
        "begin_line": 40,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.374507997655137e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.AnsiblePlugin.__init__#52",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__.AnsiblePlugin",
        "signature": "lib.ansible.plugins.__init__.AnsiblePlugin.__init__(self)",
        "snippet": "    def __init__(self):\n        self._options = {}",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 6.994963626189144e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.AnsiblePlugin.get_option#55",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__.AnsiblePlugin",
        "signature": "lib.ansible.plugins.__init__.AnsiblePlugin.get_option(self, option, hostvars=None)",
        "snippet": "    def get_option(self, option, hostvars=None):\n        if option not in self._options:\n            try:\n                option_value = C.config.get_config_value(option, plugin_type=get_plugin_class(self), plugin_name=self._load_name, variables=hostvars)\n            except AnsibleError as e:\n                raise KeyError(to_native(e))\n            self.set_option(option, option_value)\n        return self._options.get(option)",
        "begin_line": 55,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010117361392148928,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.AnsiblePlugin.set_option#64",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__.AnsiblePlugin",
        "signature": "lib.ansible.plugins.__init__.AnsiblePlugin.set_option(self, option, value)",
        "snippet": "    def set_option(self, option, value):\n        self._options[option] = value",
        "begin_line": 64,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.AnsiblePlugin.set_options#67",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__.AnsiblePlugin",
        "signature": "lib.ansible.plugins.__init__.AnsiblePlugin.set_options(self, task_keys=None, var_options=None, direct=None)",
        "snippet": "    def set_options(self, task_keys=None, var_options=None, direct=None):\n        '''\n        Sets the _options attribute with the configuration/keyword information for this plugin\n\n        :arg task_keys: Dict with playbook keywords that affect this option\n        :arg var_options: Dict with either 'connection variables'\n        :arg direct: Dict with 'direct assignment'\n        '''\n        self._options = C.config.get_plugin_options(get_plugin_class(self), self._load_name, keys=task_keys, variables=var_options, direct=direct)\n\n        # allow extras/wildcards from vars that are not directly consumed in configuration\n        # this is needed to support things like winrm that can have extended protocol options we don't directly handle\n        if self.allow_extras and var_options and '_extras' in var_options:\n            self.set_option('_extras', var_options['_extras'])",
        "begin_line": 67,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.__init__.AnsiblePlugin.has_option#82",
        "src_path": "lib/ansible/plugins/__init__.py",
        "class_name": "lib.ansible.plugins.__init__.AnsiblePlugin",
        "signature": "lib.ansible.plugins.__init__.AnsiblePlugin.has_option(self, option)",
        "snippet": "    def has_option(self, option):\n        if not self._options:\n            self.set_options()\n        return option in self._options",
        "begin_line": 82,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.local.LocalFactCollector.collect#36",
        "src_path": "lib/ansible/module_utils/facts/system/local.py",
        "class_name": "lib.ansible.module_utils.facts.system.local.LocalFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.local.LocalFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        local_facts = {}\n        local_facts['local'] = {}\n\n        if not module:\n            return local_facts\n\n        fact_path = module.params.get('fact_path', None)\n\n        if not fact_path or not os.path.exists(fact_path):\n            return local_facts\n\n        local = {}\n        for fn in sorted(glob.glob(fact_path + '/*.fact')):\n            # where it will sit under local facts\n            fact_base = os.path.basename(fn).replace('.fact', '')\n            if stat.S_IXUSR & os.stat(fn)[stat.ST_MODE]:\n                # run it\n                # try to read it as json first\n                # if that fails read it with ConfigParser\n                # if that fails, skip it\n                try:\n                    rc, out, err = module.run_command(fn)\n                except UnicodeError:\n                    fact = 'error loading fact - output of running %s was not utf-8' % fn\n                    local[fact_base] = fact\n                    local_facts['local'] = local\n                    module.warn(fact)\n                    return local_facts\n            else:\n                out = get_file_content(fn, default='')\n\n            # load raw json\n            fact = 'loading %s' % fact_base\n            try:\n                fact = json.loads(out)\n            except ValueError:\n                # load raw ini\n                cp = configparser.ConfigParser()\n                try:\n                    cp.readfp(StringIO(out))\n                except configparser.Error:\n                    fact = \"error loading fact - please check content\"\n                    module.warn(fact)\n                else:\n                    fact = {}\n                    for sect in cp.sections():\n                        if sect not in fact:\n                            fact[sect] = {}\n                        for opt in cp.options(sect):\n                            val = cp.get(sect, opt)\n                            fact[sect][opt] = val\n\n            local[fact_base] = fact\n\n        local_facts['local'] = local\n        return local_facts",
        "begin_line": 36,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ensure_connect#30",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__",
        "signature": "lib.ansible.plugins.connection.__init__.ensure_connect(func)",
        "snippet": "def ensure_connect(func):\n    @wraps(func)\n    def wrapped(self, *args, **kwargs):\n        if not self._connected:\n            self._connect()\n        return func(self, *args, **kwargs)\n    return wrapped",
        "begin_line": 30,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.wrapped#32",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__",
        "signature": "lib.ansible.plugins.connection.__init__.wrapped(self, *args, **kwargs)",
        "snippet": "    def wrapped(self, *args, **kwargs):\n        if not self._connected:\n            self._connect()\n        return func(self, *args, **kwargs)",
        "begin_line": 32,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00013163090693694878,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.__init__#61",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.__init__(self, play_context, new_stdin, shell=None, *args, **kwargs)",
        "snippet": "    def __init__(self, play_context, new_stdin, shell=None, *args, **kwargs):\n\n        super(ConnectionBase, self).__init__()\n\n        # All these hasattrs allow subclasses to override these parameters\n        if not hasattr(self, '_play_context'):\n            # Backwards compat: self._play_context isn't really needed, using set_options/get_option\n            self._play_context = play_context\n        if not hasattr(self, '_new_stdin'):\n            self._new_stdin = new_stdin\n        if not hasattr(self, '_display'):\n            # Backwards compat: self._display isn't really needed, just import the global display and use that.\n            self._display = display\n        if not hasattr(self, '_connected'):\n            self._connected = False\n\n        self.success_key = None\n        self.prompt = None\n        self._connected = False\n        self._socket_path = None\n\n        # helper plugins\n        self._shell = shell\n\n        # we always must have shell\n        if not self._shell:\n            shell_type = play_context.shell if play_context.shell else getattr(self, '_shell_type', None)\n            self._shell = get_shell_plugin(shell_type=shell_type, executable=self._play_context.executable)\n\n        self.become = None",
        "begin_line": 61,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.671064743786438e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.set_become_plugin#92",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.set_become_plugin(self, plugin)",
        "snippet": "    def set_become_plugin(self, plugin):\n        self.become = plugin",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase._split_ssh_args#106",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase._split_ssh_args(argstring)",
        "snippet": "    def _split_ssh_args(argstring):\n        \"\"\"\n        Takes a string like '-o Foo=1 -o Bar=\"foo bar\"' and returns a\n        list ['-o', 'Foo=1', '-o', 'Bar=foo bar'] that can be added to\n        the argument list. The list will not contain any empty elements.\n        \"\"\"\n        try:\n            # Python 2.6.x shlex doesn't handle unicode type so we have to\n            # convert args to byte string for that case.  More efficient to\n            # try without conversion first but python2.6 doesn't throw an\n            # exception, it merely mangles the output:\n            # >>> shlex.split(u't e')\n            # ['t\\x00\\x00\\x00', '\\x00\\x00\\x00e\\x00\\x00\\x00']\n            return [to_text(x.strip()) for x in shlex.split(to_bytes(argstring)) if x.strip()]\n        except AttributeError:\n            # In Python3, shlex.split doesn't work on a byte string.\n            return [to_text(x.strip()) for x in shlex.split(argstring) if x.strip()]",
        "begin_line": 106,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.exec_command#135",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.exec_command(self, cmd, in_data=None, sudoable=True)",
        "snippet": "    def exec_command(self, cmd, in_data=None, sudoable=True):\n        \"\"\"Run a command on the remote host.\n\n        :arg cmd: byte string containing the command\n        :kwarg in_data: If set, this data is passed to the command's stdin.\n            This is used to implement pipelining.  Currently not all\n            connection plugins implement pipelining.\n        :kwarg sudoable: Tell the connection plugin if we're executing\n            a command via a privilege escalation mechanism.  This may affect\n            how the connection plugin returns data.  Note that not all\n            connections can handle privilege escalation.\n        :returns: a tuple of (return code, stdout, stderr)  The return code is\n            an int while stdout and stderr are both byte strings.\n\n        When a command is executed, it goes through multiple commands to get\n        there.  It looks approximately like this::\n\n            [LocalShell] ConnectionCommand [UsersLoginShell (*)] ANSIBLE_SHELL_EXECUTABLE [(BecomeCommand ANSIBLE_SHELL_EXECUTABLE)] Command\n        :LocalShell: Is optional.  It is run locally to invoke the\n            ``Connection Command``.  In most instances, the\n            ``ConnectionCommand`` can be invoked directly instead.  The ssh\n            connection plugin which can have values that need expanding\n            locally specified via ssh_args is the sole known exception to\n            this.  Shell metacharacters in the command itself should be\n            processed on the remote machine, not on the local machine so no\n            shell is needed on the local machine.  (Example, ``/bin/sh``)\n        :ConnectionCommand: This is the command that connects us to the remote\n            machine to run the rest of the command.  ``ansible_user``,\n            ``ansible_ssh_host`` and so forth are fed to this piece of the\n            command to connect to the correct host (Examples ``ssh``,\n            ``chroot``)\n        :UsersLoginShell: This shell may or may not be created depending on\n            the ConnectionCommand used by the connection plugin.  This is the\n            shell that the ``ansible_user`` has configured as their login\n            shell.  In traditional UNIX parlance, this is the last field of\n            a user's ``/etc/passwd`` entry   We do not specifically try to run\n            the ``UsersLoginShell`` when we connect.  Instead it is implicit\n            in the actions that the ``ConnectionCommand`` takes when it\n            connects to a remote machine.  ``ansible_shell_type`` may be set\n            to inform ansible of differences in how the ``UsersLoginShell``\n            handles things like quoting if a shell has different semantics\n            than the Bourne shell.\n        :ANSIBLE_SHELL_EXECUTABLE: This is the shell set via the inventory var\n            ``ansible_shell_executable`` or via\n            ``constants.DEFAULT_EXECUTABLE`` if the inventory var is not set.\n            We explicitly invoke this shell so that we have predictable\n            quoting rules at this point.  ``ANSIBLE_SHELL_EXECUTABLE`` is only\n            settable by the user because some sudo setups may only allow\n            invoking a specific shell.  (For instance, ``/bin/bash`` may be\n            allowed but ``/bin/sh``, our default, may not).  We invoke this\n            twice, once after the ``ConnectionCommand`` and once after the\n            ``BecomeCommand``.  After the ConnectionCommand, this is run by\n            the ``UsersLoginShell``.  After the ``BecomeCommand`` we specify\n            that the ``ANSIBLE_SHELL_EXECUTABLE`` is being invoked directly.\n        :BecomeComand ANSIBLE_SHELL_EXECUTABLE: Is the command that performs\n            privilege escalation.  Setting this up is performed by the action\n            plugin prior to running ``exec_command``. So we just get passed\n            :param:`cmd` which has the BecomeCommand already added.\n            (Examples: sudo, su)  If we have a BecomeCommand then we will\n            invoke a ANSIBLE_SHELL_EXECUTABLE shell inside of it so that we\n            have a consistent view of quoting.\n        :Command: Is the command we're actually trying to run remotely.\n            (Examples: mkdir -p $HOME/.ansible, python $HOME/.ansible/tmp-script-file)\n        \"\"\"\n        pass",
        "begin_line": 135,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.put_file#203",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.put_file(self, in_path, out_path)",
        "snippet": "    def put_file(self, in_path, out_path):\n        \"\"\"Transfer a file from local to remote\"\"\"\n        pass",
        "begin_line": 203,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.fetch_file#209",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.fetch_file(self, in_path, out_path)",
        "snippet": "    def fetch_file(self, in_path, out_path):\n        \"\"\"Fetch a file from remote to local; callers are expected to have pre-created the directory chain for out_path\"\"\"\n        pass",
        "begin_line": 209,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.connection.__init__.ConnectionBase.check_password_prompt#244",
        "src_path": "lib/ansible/plugins/connection/__init__.py",
        "class_name": "lib.ansible.plugins.connection.__init__.ConnectionBase",
        "signature": "lib.ansible.plugins.connection.__init__.ConnectionBase.check_password_prompt(self, b_output)",
        "snippet": "    def check_password_prompt(self, b_output):\n        display.deprecated(\n            \"Connection.check_password_prompt is deprecated, calling code should be using become plugins instead\",\n            version=\"2.12\"\n        )\n        return self.become.check_password_prompt(b_output)",
        "begin_line": 244,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleError.__init__#53",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleError",
        "signature": "lib.ansible.errors.__init__.AnsibleError.__init__(self, message='', obj=None, show_content=True, suppress_extended_error=False, orig_exc=None)",
        "snippet": "    def __init__(self, message=\"\", obj=None, show_content=True, suppress_extended_error=False, orig_exc=None):\n        super(AnsibleError, self).__init__(message)\n\n        # we import this here to prevent an import loop problem,\n        # since the objects code also imports ansible.errors\n        from ansible.parsing.yaml.objects import AnsibleBaseYAMLObject\n\n        self._obj = obj\n        self._show_content = show_content\n        if obj and isinstance(obj, AnsibleBaseYAMLObject):\n            extended_error = self._get_extended_error()\n            if extended_error and not suppress_extended_error:\n                self.message = '%s\\n\\n%s' % (to_native(message), to_native(extended_error))\n            else:\n                self.message = '%s' % to_native(message)\n        else:\n            self.message = '%s' % to_native(message)\n        if orig_exc:\n            self.orig_exc = orig_exc",
        "begin_line": 53,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001095290251916758,
            "pseudo_dstar_susp": 0.002564102564102564,
            "pseudo_tarantula_susp": 0.0007598784194528875,
            "pseudo_op2_susp": 0.002564102564102564,
            "pseudo_barinel_susp": 0.0007598784194528875
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleError.__str__#73",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleError",
        "signature": "lib.ansible.errors.__init__.AnsibleError.__str__(self)",
        "snippet": "    def __str__(self):\n        return self.message",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011918951132300357,
            "pseudo_dstar_susp": 0.002577319587628866,
            "pseudo_tarantula_susp": 0.0008635578583765112,
            "pseudo_op2_susp": 0.002577319587628866,
            "pseudo_barinel_susp": 0.0008635578583765112
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleError.__repr__#76",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleError",
        "signature": "lib.ansible.errors.__init__.AnsibleError.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return self.message",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleError._get_error_lines_from_file#79",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleError",
        "signature": "lib.ansible.errors.__init__.AnsibleError._get_error_lines_from_file(self, file_name, line_number)",
        "snippet": "    def _get_error_lines_from_file(self, file_name, line_number):\n        '''\n        Returns the line in the file which corresponds to the reported error\n        location, as well as the line preceding it (if the error did not\n        occur on the first line), to provide context to the error.\n        '''\n\n        target_line = ''\n        prev_line = ''\n\n        with open(file_name, 'r') as f:\n            lines = f.readlines()\n\n            target_line = lines[line_number]\n            if line_number > 0:\n                prev_line = lines[line_number - 1]\n\n        return (target_line, prev_line)",
        "begin_line": 79,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleError._get_extended_error#98",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleError",
        "signature": "lib.ansible.errors.__init__.AnsibleError._get_extended_error(self)",
        "snippet": "    def _get_extended_error(self):\n        '''\n        Given an object reporting the location of the exception in a file, return\n        detailed information regarding it including:\n\n          * the line which caused the error as well as the one preceding it\n          * causes and suggested remedies for common syntax errors\n\n        If this error was created with show_content=False, the reporting of content\n        is suppressed, as the file contents may be sensitive (ie. vault data).\n        '''\n\n        error_message = ''\n\n        try:\n            (src_file, line_number, col_number) = self._obj.ansible_pos\n            error_message += YAML_POSITION_DETAILS % (src_file, line_number, col_number)\n            if src_file not in ('<string>', '<unicode>') and self._show_content:\n                (target_line, prev_line) = self._get_error_lines_from_file(src_file, line_number - 1)\n                target_line = to_text(target_line)\n                prev_line = to_text(prev_line)\n                if target_line:\n                    stripped_line = target_line.replace(\" \", \"\")\n\n                    # Check for k=v syntax in addition to YAML syntax and set the appropriate error position,\n                    # arrow index\n                    if re.search(r'\\w+(\\s+)?=(\\s+)?[\\w/-]+', prev_line):\n                        error_position = prev_line.rstrip().find('=')\n                        arrow_line = (\" \" * error_position) + \"^ here\"\n                        error_message = YAML_POSITION_DETAILS % (src_file, line_number - 1, error_position + 1)\n                        error_message += \"\\nThe offending line appears to be:\\n\\n%s\\n%s\\n\\n\" % (prev_line.rstrip(), arrow_line)\n                        error_message += YAML_AND_SHORTHAND_ERROR\n                    else:\n                        arrow_line = (\" \" * (col_number - 1)) + \"^ here\"\n                        error_message += \"\\nThe offending line appears to be:\\n\\n%s\\n%s\\n%s\\n\" % (prev_line.rstrip(), target_line.rstrip(), arrow_line)\n\n                    # TODO: There may be cases where there is a valid tab in a line that has other errors.\n                    if '\\t' in target_line:\n                        error_message += YAML_COMMON_LEADING_TAB_ERROR\n                    # common error/remediation checking here:\n                    # check for unquoted vars starting lines\n                    if ('{{' in target_line and '}}' in target_line) and ('\"{{' not in target_line or \"'{{\" not in target_line):\n                        error_message += YAML_COMMON_UNQUOTED_VARIABLE_ERROR\n                    # check for common dictionary mistakes\n                    elif \":{{\" in stripped_line and \"}}\" in stripped_line:\n                        error_message += YAML_COMMON_DICT_ERROR\n                    # check for common unquoted colon mistakes\n                    elif (len(target_line) and\n                            len(target_line) > 1 and\n                            len(target_line) > col_number and\n                            target_line[col_number] == \":\" and\n                            target_line.count(':') > 1):\n                        error_message += YAML_COMMON_UNQUOTED_COLON_ERROR\n                    # otherwise, check for some common quoting mistakes\n                    else:\n                        # FIXME: This needs to split on the first ':' to account for modules like lineinfile\n                        # that may have lines that contain legitimate colons, e.g., line: 'i ALL= (ALL) NOPASSWD: ALL'\n                        # and throw off the quote matching logic.\n                        parts = target_line.split(\":\")\n                        if len(parts) > 1:\n                            middle = parts[1].strip()\n                            match = False\n                            unbalanced = False\n\n                            if middle.startswith(\"'\") and not middle.endswith(\"'\"):\n                                match = True\n                            elif middle.startswith('\"') and not middle.endswith('\"'):\n                                match = True\n\n                            if (len(middle) > 0 and\n                                    middle[0] in ['\"', \"'\"] and\n                                    middle[-1] in ['\"', \"'\"] and\n                                    target_line.count(\"'\") > 2 or\n                                    target_line.count('\"') > 2):\n                                unbalanced = True\n\n                            if match:\n                                error_message += YAML_COMMON_PARTIALLY_QUOTED_LINE_ERROR\n                            if unbalanced:\n                                error_message += YAML_COMMON_UNBALANCED_QUOTES_ERROR\n\n        except (IOError, TypeError):\n            error_message += '\\n(could not open file to display line)'\n        except IndexError:\n            error_message += '\\n(specified line no longer in file, maybe it changed?)'\n\n        return error_message",
        "begin_line": 98,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.errors.__init__.AnsibleFileNotFound.__init__#255",
        "src_path": "lib/ansible/errors/__init__.py",
        "class_name": "lib.ansible.errors.__init__.AnsibleFileNotFound",
        "signature": "lib.ansible.errors.__init__.AnsibleFileNotFound.__init__(self, message='', obj=None, show_content=True, suppress_extended_error=False, orig_exc=None, paths=None, file_name=None)",
        "snippet": "    def __init__(self, message=\"\", obj=None, show_content=True, suppress_extended_error=False, orig_exc=None, paths=None, file_name=None):\n\n        self.file_name = file_name\n        self.paths = paths\n\n        if message:\n            message += \"\\n\"\n        if self.file_name:\n            message += \"Could not find or access '%s'\" % to_text(self.file_name)\n        else:\n            message += \"Could not find file\"\n\n        if self.paths and isinstance(self.paths, Sequence):\n            searched = to_text('\\n\\t'.join(self.paths))\n            if message:\n                message += \"\\n\"\n            message += \"Searched in:\\n\\t%s\" % searched\n\n        message += \" on the Ansible Controller.\\nIf you are using a module and expect the file to exist on the remote, see the remote_src option\"\n\n        super(AnsibleFileNotFound, self).__init__(message=message, obj=obj, show_content=show_content,\n                                                  suppress_extended_error=suppress_extended_error, orig_exc=orig_exc)",
        "begin_line": 255,
        "end_line": 276,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.__init__#42",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.__init__(self)",
        "snippet": "    def __init__(self):\n\n        self.groups = {}\n        self.hosts = {}\n\n        # provides 'groups' magic var, host object has group_names\n        self._groups_dict_cache = {}\n\n        # current localhost, implicit or explicit\n        self.localhost = None\n\n        self.current_source = None\n\n        # Always create the 'all' and 'ungrouped' groups,\n        for group in ('all', 'ungrouped'):\n            self.add_group(group)\n        self.add_child('all', 'ungrouped')",
        "begin_line": 42,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.304268393954493e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.reconcile_inventory#101",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.reconcile_inventory(self)",
        "snippet": "    def reconcile_inventory(self):\n        ''' Ensure inventory basic rules, run after updates '''\n\n        display.debug('Reconcile groups and hosts in inventory.')\n        self.current_source = None\n\n        group_names = set()\n        # set group vars from group_vars/ files and vars plugins\n        for g in self.groups:\n            group = self.groups[g]\n            group_names.add(group.name)\n\n            # ensure all groups inherit from 'all'\n            if group.name != 'all' and not group.get_ancestors():\n                self.add_child('all', group.name)\n\n        host_names = set()\n        # get host vars from host_vars/ files and vars plugins\n        for host in self.hosts.values():\n            host_names.add(host.name)\n\n            mygroups = host.get_groups()\n\n            if self.groups['ungrouped'] in mygroups:\n                # clear ungrouped of any incorrectly stored by parser\n                if set(mygroups).difference(set([self.groups['all'], self.groups['ungrouped']])):\n                    self.groups['ungrouped'].remove_host(host)\n\n            elif not host.implicit:\n                # add ungrouped hosts to ungrouped, except implicit\n                length = len(mygroups)\n                if length == 0 or (length == 1 and self.groups['all'] in mygroups):\n                    self.add_child('ungrouped', host.name)\n\n            # special case for implicit hosts\n            if host.implicit:\n                host.vars = combine_vars(self.groups['all'].get_vars(), host.vars)\n\n        # warn if overloading identifier as both group and host\n        for conflict in group_names.intersection(host_names):\n            display.warning(\"Found both group and host with same name: %s\" % conflict)\n\n        self._groups_dict_cache = {}",
        "begin_line": 101,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.get_host#145",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.get_host(self, hostname)",
        "snippet": "    def get_host(self, hostname):\n        ''' fetch host object using name deal with implicit localhost '''\n\n        matching_host = self.hosts.get(hostname, None)\n\n        # if host is not in hosts dict\n        if matching_host is None and hostname in C.LOCALHOST:\n            # might need to create implicit localhost\n            matching_host = self._create_implicit_localhost(hostname)\n\n        return matching_host",
        "begin_line": 145,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.add_group#157",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.add_group(self, group)",
        "snippet": "    def add_group(self, group):\n        ''' adds a group to inventory if not there already, returns named actually used '''\n\n        if group:\n            if not isinstance(group, string_types):\n                raise AnsibleError(\"Invalid group name supplied, expected a string but got %s for %s\" % (type(group), group))\n            if group not in self.groups:\n                g = Group(group)\n                if g.name not in self.groups:\n                    self.groups[g.name] = g\n                    self._groups_dict_cache = {}\n                    display.debug(\"Added group %s to inventory\" % group)\n                group = g.name\n            else:\n                display.debug(\"group %s already in inventory\" % group)\n        else:\n            raise AnsibleError(\"Invalid empty/false group name provided: %s\" % group)\n\n        return group",
        "begin_line": 157,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.add_host#188",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.add_host(self, host, group=None, port=None)",
        "snippet": "    def add_host(self, host, group=None, port=None):\n        ''' adds a host to inventory and possibly a group if not there already '''\n\n        if host:\n            if not isinstance(host, string_types):\n                raise AnsibleError(\"Invalid host name supplied, expected a string but got %s for %s\" % (type(host), host))\n\n            # TODO: add to_safe_host_name\n            g = None\n            if group:\n                if group in self.groups:\n                    g = self.groups[group]\n                else:\n                    raise AnsibleError(\"Could not find group %s in inventory\" % group)\n\n            if host not in self.hosts:\n                h = Host(host, port)\n                self.hosts[host] = h\n                if self.current_source:  # set to 'first source' in which host was encountered\n                    self.set_variable(host, 'inventory_file', self.current_source)\n                    self.set_variable(host, 'inventory_dir', basedir(self.current_source))\n                else:\n                    self.set_variable(host, 'inventory_file', None)\n                    self.set_variable(host, 'inventory_dir', None)\n                display.debug(\"Added host %s to inventory\" % (host))\n\n                # set default localhost from inventory to avoid creating an implicit one. Last localhost defined 'wins'.\n                if host in C.LOCALHOST:\n                    if self.localhost is None:\n                        self.localhost = self.hosts[host]\n                        display.vvvv(\"Set default localhost to %s\" % h)\n                    else:\n                        display.warning(\"A duplicate localhost-like entry was found (%s). First found localhost was %s\" % (h, self.localhost.name))\n            else:\n                h = self.hosts[host]\n\n            if g:\n                g.add_host(h)\n                self._groups_dict_cache = {}\n                display.debug(\"Added host %s to group %s\" % (host, group))\n        else:\n            raise AnsibleError(\"Invalid empty host name provided: %s\" % host)\n\n        return host",
        "begin_line": 188,
        "end_line": 231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00010734220695577502,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.set_variable#242",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.set_variable(self, entity, varname, value)",
        "snippet": "    def set_variable(self, entity, varname, value):\n        ''' sets a variable for an inventory object '''\n\n        if entity in self.groups:\n            inv_object = self.groups[entity]\n        elif entity in self.hosts:\n            inv_object = self.hosts[entity]\n        else:\n            raise AnsibleError(\"Could not identify group or host named %s\" % entity)\n\n        inv_object.set_variable(varname, value)\n        display.debug('set %s for %s' % (varname, entity))",
        "begin_line": 242,
        "end_line": 253,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 8.628127696289905e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.inventory.data.InventoryData.add_child#255",
        "src_path": "lib/ansible/inventory/data.py",
        "class_name": "lib.ansible.inventory.data.InventoryData",
        "signature": "lib.ansible.inventory.data.InventoryData.add_child(self, group, child)",
        "snippet": "    def add_child(self, group, child):\n        ''' Add host or group to group '''\n\n        if group in self.groups:\n            g = self.groups[group]\n            if child in self.groups:\n                g.add_child_group(self.groups[child])\n            elif child in self.hosts:\n                g.add_host(self.hosts[child])\n            else:\n                raise AnsibleError(\"%s is not a known host nor group\" % child)\n            self._groups_dict_cache = {}\n            display.debug('Group %s now contains %s' % (group, child))\n        else:\n            raise AnsibleError(\"%s is not a known group\" % group)",
        "begin_line": 255,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.plugins.inventory.constructed.InventoryModule.__init__#87",
        "src_path": "lib/ansible/plugins/inventory/constructed.py",
        "class_name": "lib.ansible.plugins.inventory.constructed.InventoryModule",
        "signature": "lib.ansible.plugins.inventory.constructed.InventoryModule.__init__(self)",
        "snippet": "    def __init__(self):\n\n        super(InventoryModule, self).__init__()\n\n        self._cache = FactCache()",
        "begin_line": 87,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.724788485850433e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.facts.system.apparmor.ApparmorFactCollector.collect#30",
        "src_path": "lib/ansible/module_utils/facts/system/apparmor.py",
        "class_name": "lib.ansible.module_utils.facts.system.apparmor.ApparmorFactCollector",
        "signature": "lib.ansible.module_utils.facts.system.apparmor.ApparmorFactCollector.collect(self, module=None, collected_facts=None)",
        "snippet": "    def collect(self, module=None, collected_facts=None):\n        facts_dict = {}\n        apparmor_facts = {}\n        if os.path.exists('/sys/kernel/security/apparmor'):\n            apparmor_facts['status'] = 'enabled'\n        else:\n            apparmor_facts['status'] = 'disabled'\n\n        facts_dict['apparmor'] = apparmor_facts\n        return facts_dict",
        "begin_line": 30,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 9.147457006952067e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.module_utils.common.warnings.warn#14",
        "src_path": "lib/ansible/module_utils/common/warnings.py",
        "class_name": "lib.ansible.module_utils.common.warnings",
        "signature": "lib.ansible.module_utils.common.warnings.warn(warning)",
        "snippet": "def warn(warning):\n    if isinstance(warning, string_types):\n        _global_warnings.append(warning)\n    else:\n        raise TypeError(\"warn requires a string not a %s\" % type(warning))",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005050505050505051,
            "pseudo_dstar_susp": 0.0013698630136986301,
            "pseudo_tarantula_susp": 0.006369426751592357,
            "pseudo_op2_susp": 0.0013698630136986301,
            "pseudo_barinel_susp": 0.006369426751592357
        }
    },
    {
        "name": "lib.ansible.module_utils.common.warnings.deprecate#21",
        "src_path": "lib/ansible/module_utils/common/warnings.py",
        "class_name": "lib.ansible.module_utils.common.warnings",
        "signature": "lib.ansible.module_utils.common.warnings.deprecate(msg, version=None)",
        "snippet": "def deprecate(msg, version=None):\n    if isinstance(msg, string_types):\n        _global_deprecations.append({'msg': msg, 'version': version})\n    else:\n        raise TypeError(\"deprecate requires a string not a %s\" % type(msg))",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005434782608695652,
            "pseudo_dstar_susp": 0.0013717421124828531,
            "pseudo_tarantula_susp": 0.006802721088435374,
            "pseudo_op2_susp": 0.0013717421124828531,
            "pseudo_barinel_susp": 0.006802721088435374
        }
    },
    {
        "name": "lib.ansible.module_utils.common.warnings.get_warning_messages#28",
        "src_path": "lib/ansible/module_utils/common/warnings.py",
        "class_name": "lib.ansible.module_utils.common.warnings",
        "signature": "lib.ansible.module_utils.common.warnings.get_warning_messages()",
        "snippet": "def get_warning_messages():\n    \"\"\"Return a tuple of warning messages accumulated over this run\"\"\"\n    return tuple(_global_warnings)",
        "begin_line": 28,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001081081081081081,
            "pseudo_dstar_susp": 0.001026694045174538,
            "pseudo_tarantula_susp": 0.0023148148148148147,
            "pseudo_op2_susp": 0.001026694045174538,
            "pseudo_barinel_susp": 0.0023148148148148147
        }
    },
    {
        "name": "lib.ansible.module_utils.common.warnings.get_deprecation_messages#33",
        "src_path": "lib/ansible/module_utils/common/warnings.py",
        "class_name": "lib.ansible.module_utils.common.warnings",
        "signature": "lib.ansible.module_utils.common.warnings.get_deprecation_messages()",
        "snippet": "def get_deprecation_messages():\n    \"\"\"Return a tuple of deprecations accumulated over this run\"\"\"\n    return tuple(_global_deprecations)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001081081081081081,
            "pseudo_dstar_susp": 0.001026694045174538,
            "pseudo_tarantula_susp": 0.0023148148148148147,
            "pseudo_op2_susp": 0.001026694045174538,
            "pseudo_barinel_susp": 0.0023148148148148147
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.__init__#39",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.__init__(self, args)",
        "snippet": "    def __init__(self, args):\n\n        self.b_vault_pass = None\n        self.b_new_vault_pass = None\n        self.encrypt_string_read_stdin = False\n\n        self.encrypt_secret = None\n        self.encrypt_vault_id = None\n        self.new_encrypt_secret = None\n        self.new_encrypt_vault_id = None\n\n        super(VaultCLI, self).__init__(args)",
        "begin_line": 39,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.init_parser#52",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.init_parser(self)",
        "snippet": "    def init_parser(self):\n        super(VaultCLI, self).init_parser(\n            desc=\"encryption/decryption utility for Ansible data files\",\n            epilog=\"\\nSee '%s <command> --help' for more information on a specific command.\\n\\n\" % os.path.basename(sys.argv[0])\n        )\n\n        common = opt_help.argparse.ArgumentParser(add_help=False)\n        opt_help.add_vault_options(common)\n        opt_help.add_verbosity_options(common)\n\n        subparsers = self.parser.add_subparsers(dest='action')\n        subparsers.required = True\n\n        output = opt_help.argparse.ArgumentParser(add_help=False)\n        output.add_argument('--output', default=None, dest='output_file',\n                            help='output file name for encrypt or decrypt; use - for stdout',\n                            type=opt_help.unfrack_path())\n\n        # For encrypting actions, we can also specify which of multiple vault ids should be used for encrypting\n        vault_id = opt_help.argparse.ArgumentParser(add_help=False)\n        vault_id.add_argument('--encrypt-vault-id', default=[], dest='encrypt_vault_id',\n                              action='store', type=str,\n                              help='the vault id used to encrypt (required if more than vault-id is provided)')\n\n        create_parser = subparsers.add_parser('create', help='Create new vault encrypted file', parents=[vault_id, common])\n        create_parser.set_defaults(func=self.execute_create)\n        create_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')\n\n        decrypt_parser = subparsers.add_parser('decrypt', help='Decrypt vault encrypted file', parents=[output, common])\n        decrypt_parser.set_defaults(func=self.execute_decrypt)\n        decrypt_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')\n\n        edit_parser = subparsers.add_parser('edit', help='Edit vault encrypted file', parents=[vault_id, common])\n        edit_parser.set_defaults(func=self.execute_edit)\n        edit_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')\n\n        view_parser = subparsers.add_parser('view', help='View vault encrypted file', parents=[common])\n        view_parser.set_defaults(func=self.execute_view)\n        view_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')\n\n        encrypt_parser = subparsers.add_parser('encrypt', help='Encrypt YAML file', parents=[common, output, vault_id])\n        encrypt_parser.set_defaults(func=self.execute_encrypt)\n        encrypt_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')\n\n        enc_str_parser = subparsers.add_parser('encrypt_string', help='Encrypt a string', parents=[common, output, vault_id])\n        enc_str_parser.set_defaults(func=self.execute_encrypt_string)\n        enc_str_parser.add_argument('args', help='String to encrypt', metavar='string_to_encrypt', nargs='*')\n        enc_str_parser.add_argument('-p', '--prompt', dest='encrypt_string_prompt',\n                                    action='store_true',\n                                    help=\"Prompt for the string to encrypt\")\n        enc_str_parser.add_argument('-n', '--name', dest='encrypt_string_names',\n                                    action='append',\n                                    help=\"Specify the variable name\")\n        enc_str_parser.add_argument('--stdin-name', dest='encrypt_string_stdin_name',\n                                    default=None,\n                                    help=\"Specify the variable name for stdin\")\n\n        rekey_parser = subparsers.add_parser('rekey', help='Re-key a vault encrypted file', parents=[common, vault_id])\n        rekey_parser.set_defaults(func=self.execute_rekey)\n        rekey_new_group = rekey_parser.add_mutually_exclusive_group()\n        rekey_new_group.add_argument('--new-vault-password-file', default=None, dest='new_vault_password_file',\n                                     help=\"new vault password file for rekey\", type=opt_help.unfrack_path())\n        rekey_new_group.add_argument('--new-vault-id', default=None, dest='new_vault_id', type=str,\n                                     help='the new vault identity to use for rekey')\n        rekey_parser.add_argument('args', help='Filename', metavar='file_name', nargs='*')",
        "begin_line": 52,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.post_process_args#118",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.post_process_args(self, options)",
        "snippet": "    def post_process_args(self, options):\n        options = super(VaultCLI, self).post_process_args(options)\n\n        display.verbosity = options.verbosity\n\n        if options.vault_ids:\n            for vault_id in options.vault_ids:\n                if u';' in vault_id:\n                    raise AnsibleOptionsError(\"'%s' is not a valid vault id. The character ';' is not allowed in vault ids\" % vault_id)\n\n        if getattr(options, 'output_file', None) and len(options.args) > 1:\n            raise AnsibleOptionsError(\"At most one input file may be used with the --output option\")\n\n        if options.action == 'encrypt_string':\n            if '-' in options.args or not options.args or options.encrypt_string_stdin_name:\n                self.encrypt_string_read_stdin = True\n\n            # TODO: prompting from stdin and reading from stdin seem mutually exclusive, but verify that.\n            if options.encrypt_string_prompt and self.encrypt_string_read_stdin:\n                raise AnsibleOptionsError('The --prompt option is not supported if also reading input from stdin')\n\n        return options",
        "begin_line": 118,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.run#141",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.run(self)",
        "snippet": "    def run(self):\n        super(VaultCLI, self).run()\n        loader = DataLoader()\n\n        # set default restrictive umask\n        old_umask = os.umask(0o077)\n\n        vault_ids = list(context.CLIARGS['vault_ids'])\n\n        # there are 3 types of actions, those that just 'read' (decrypt, view) and only\n        # need to ask for a password once, and those that 'write' (create, encrypt) that\n        # ask for a new password and confirm it, and 'read/write (rekey) that asks for the\n        # old password, then asks for a new one and confirms it.\n\n        default_vault_ids = C.DEFAULT_VAULT_IDENTITY_LIST\n        vault_ids = default_vault_ids + vault_ids\n\n        action = context.CLIARGS['action']\n\n        # TODO: instead of prompting for these before, we could let VaultEditor\n        #       call a callback when it needs it.\n        if action in ['decrypt', 'view', 'rekey', 'edit']:\n            vault_secrets = self.setup_vault_secrets(loader, vault_ids=vault_ids,\n                                                     vault_password_files=list(context.CLIARGS['vault_password_files']),\n                                                     ask_vault_pass=context.CLIARGS['ask_vault_pass'])\n            if not vault_secrets:\n                raise AnsibleOptionsError(\"A vault password is required to use Ansible's Vault\")\n\n        if action in ['encrypt', 'encrypt_string', 'create']:\n\n            encrypt_vault_id = None\n            # no --encrypt-vault-id context.CLIARGS['encrypt_vault_id'] for 'edit'\n            if action not in ['edit']:\n                encrypt_vault_id = context.CLIARGS['encrypt_vault_id'] or C.DEFAULT_VAULT_ENCRYPT_IDENTITY\n\n            vault_secrets = None\n            vault_secrets = \\\n                self.setup_vault_secrets(loader,\n                                         vault_ids=vault_ids,\n                                         vault_password_files=list(context.CLIARGS['vault_password_files']),\n                                         ask_vault_pass=context.CLIARGS['ask_vault_pass'],\n                                         create_new_password=True)\n\n            if len(vault_secrets) > 1 and not encrypt_vault_id:\n                raise AnsibleOptionsError(\"The vault-ids %s are available to encrypt. Specify the vault-id to encrypt with --encrypt-vault-id\" %\n                                          ','.join([x[0] for x in vault_secrets]))\n\n            if not vault_secrets:\n                raise AnsibleOptionsError(\"A vault password is required to use Ansible's Vault\")\n\n            encrypt_secret = match_encrypt_secret(vault_secrets,\n                                                  encrypt_vault_id=encrypt_vault_id)\n\n            # only one secret for encrypt for now, use the first vault_id and use its first secret\n            # TODO: exception if more than one?\n            self.encrypt_vault_id = encrypt_secret[0]\n            self.encrypt_secret = encrypt_secret[1]\n\n        if action in ['rekey']:\n            encrypt_vault_id = context.CLIARGS['encrypt_vault_id'] or C.DEFAULT_VAULT_ENCRYPT_IDENTITY\n            # print('encrypt_vault_id: %s' % encrypt_vault_id)\n            # print('default_encrypt_vault_id: %s' % default_encrypt_vault_id)\n\n            # new_vault_ids should only ever be one item, from\n            # load the default vault ids if we are using encrypt-vault-id\n            new_vault_ids = []\n            if encrypt_vault_id:\n                new_vault_ids = default_vault_ids\n            if context.CLIARGS['new_vault_id']:\n                new_vault_ids.append(context.CLIARGS['new_vault_id'])\n\n            new_vault_password_files = []\n            if context.CLIARGS['new_vault_password_file']:\n                new_vault_password_files.append(context.CLIARGS['new_vault_password_file'])\n\n            new_vault_secrets = \\\n                self.setup_vault_secrets(loader,\n                                         vault_ids=new_vault_ids,\n                                         vault_password_files=new_vault_password_files,\n                                         ask_vault_pass=context.CLIARGS['ask_vault_pass'],\n                                         create_new_password=True)\n\n            if not new_vault_secrets:\n                raise AnsibleOptionsError(\"A new vault password is required to use Ansible's Vault rekey\")\n\n            # There is only one new_vault_id currently and one new_vault_secret, or we\n            # use the id specified in --encrypt-vault-id\n            new_encrypt_secret = match_encrypt_secret(new_vault_secrets,\n                                                      encrypt_vault_id=encrypt_vault_id)\n\n            self.new_encrypt_vault_id = new_encrypt_secret[0]\n            self.new_encrypt_secret = new_encrypt_secret[1]\n\n        loader.set_vault_secrets(vault_secrets)\n\n        # FIXME: do we need to create VaultEditor here? its not reused\n        vault = VaultLib(vault_secrets)\n        self.editor = VaultEditor(vault)\n\n        context.CLIARGS['func']()\n\n        # and restore umask\n        os.umask(old_umask)",
        "begin_line": 141,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_encrypt#245",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_encrypt(self)",
        "snippet": "    def execute_encrypt(self):\n        ''' encrypt the supplied file using the provided vault secret '''\n\n        if not context.CLIARGS['args'] and sys.stdin.isatty():\n            display.display(\"Reading plaintext input from stdin\", stderr=True)\n\n        for f in context.CLIARGS['args'] or ['-']:\n            # Fixme: use the correct vau\n            self.editor.encrypt_file(f, self.encrypt_secret,\n                                     vault_id=self.encrypt_vault_id,\n                                     output_file=context.CLIARGS['output_file'])\n\n        if sys.stdout.isatty():\n            display.display(\"Encryption successful\", stderr=True)",
        "begin_line": 245,
        "end_line": 258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.format_ciphertext_yaml#261",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.format_ciphertext_yaml(b_ciphertext, indent=None, name=None)",
        "snippet": "    def format_ciphertext_yaml(b_ciphertext, indent=None, name=None):\n        indent = indent or 10\n\n        block_format_var_name = \"\"\n        if name:\n            block_format_var_name = \"%s: \" % name\n\n        block_format_header = \"%s!vault |\" % block_format_var_name\n        lines = []\n        vault_ciphertext = to_text(b_ciphertext)\n\n        lines.append(block_format_header)\n        for line in vault_ciphertext.splitlines():\n            lines.append('%s%s' % (' ' * indent, line))\n\n        yaml_ciphertext = '\\n'.join(lines)\n        return yaml_ciphertext",
        "begin_line": 261,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00011741223435481978,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_encrypt_string#279",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_encrypt_string(self)",
        "snippet": "    def execute_encrypt_string(self):\n        ''' encrypt the supplied string using the provided vault secret '''\n        b_plaintext = None\n\n        # Holds tuples (the_text, the_source_of_the_string, the variable name if its provided).\n        b_plaintext_list = []\n\n        # remove the non-option '-' arg (used to indicate 'read from stdin') from the candidate args so\n        # we don't add it to the plaintext list\n        args = [x for x in context.CLIARGS['args'] if x != '-']\n\n        # We can prompt and read input, or read from stdin, but not both.\n        if context.CLIARGS['encrypt_string_prompt']:\n            msg = \"String to encrypt: \"\n\n            name = None\n            name_prompt_response = display.prompt('Variable name (enter for no name): ')\n\n            # TODO: enforce var naming rules?\n            if name_prompt_response != \"\":\n                name = name_prompt_response\n\n            # TODO: could prompt for which vault_id to use for each plaintext string\n            #       currently, it will just be the default\n            # could use private=True for shadowed input if useful\n            prompt_response = display.prompt(msg)\n\n            if prompt_response == '':\n                raise AnsibleOptionsError('The plaintext provided from the prompt was empty, not encrypting')\n\n            b_plaintext = to_bytes(prompt_response)\n            b_plaintext_list.append((b_plaintext, self.FROM_PROMPT, name))\n\n        # read from stdin\n        if self.encrypt_string_read_stdin:\n            if sys.stdout.isatty():\n                display.display(\"Reading plaintext input from stdin. (ctrl-d to end input)\", stderr=True)\n\n            stdin_text = sys.stdin.read()\n            if stdin_text == '':\n                raise AnsibleOptionsError('stdin was empty, not encrypting')\n\n            if sys.stdout.isatty() and not stdin_text.endswith(\"\\n\"):\n                display.display(\"\\n\")\n\n            b_plaintext = to_bytes(stdin_text)\n\n            # defaults to None\n            name = context.CLIARGS['encrypt_string_stdin_name']\n            b_plaintext_list.append((b_plaintext, self.FROM_STDIN, name))\n\n        # use any leftover args as strings to encrypt\n        # Try to match args up to --name options\n        if context.CLIARGS.get('encrypt_string_names', False):\n            name_and_text_list = list(zip(context.CLIARGS['encrypt_string_names'], args))\n\n            # Some but not enough --name's to name each var\n            if len(args) > len(name_and_text_list):\n                # Trying to avoid ever showing the plaintext in the output, so this warning is vague to avoid that.\n                display.display('The number of --name options do not match the number of args.',\n                                stderr=True)\n                display.display('The last named variable will be \"%s\". The rest will not have'\n                                ' names.' % context.CLIARGS['encrypt_string_names'][-1],\n                                stderr=True)\n\n            # Add the rest of the args without specifying a name\n            for extra_arg in args[len(name_and_text_list):]:\n                name_and_text_list.append((None, extra_arg))\n\n        # if no --names are provided, just use the args without a name.\n        else:\n            name_and_text_list = [(None, x) for x in args]\n\n        # Convert the plaintext text objects to bytestrings and collect\n        for name_and_text in name_and_text_list:\n            name, plaintext = name_and_text\n\n            if plaintext == '':\n                raise AnsibleOptionsError('The plaintext provided from the command line args was empty, not encrypting')\n\n            b_plaintext = to_bytes(plaintext)\n            b_plaintext_list.append((b_plaintext, self.FROM_ARGS, name))\n\n        # TODO: specify vault_id per string?\n        # Format the encrypted strings and any corresponding stderr output\n        outputs = self._format_output_vault_strings(b_plaintext_list, vault_id=self.encrypt_vault_id)\n\n        for output in outputs:\n            err = output.get('err', None)\n            out = output.get('out', '')\n            if err:\n                sys.stderr.write(err)\n            print(out)\n\n        if sys.stdout.isatty():\n            display.display(\"Encryption successful\", stderr=True)",
        "begin_line": 279,
        "end_line": 374,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI._format_output_vault_strings#378",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI._format_output_vault_strings(self, b_plaintext_list, vault_id=None)",
        "snippet": "    def _format_output_vault_strings(self, b_plaintext_list, vault_id=None):\n        # If we are only showing one item in the output, we don't need to included commented\n        # delimiters in the text\n        show_delimiter = False\n        if len(b_plaintext_list) > 1:\n            show_delimiter = True\n\n        # list of dicts {'out': '', 'err': ''}\n        output = []\n\n        # Encrypt the plaintext, and format it into a yaml block that can be pasted into a playbook.\n        # For more than one input, show some differentiating info in the stderr output so we can tell them\n        # apart. If we have a var name, we include that in the yaml\n        for index, b_plaintext_info in enumerate(b_plaintext_list):\n            # (the text itself, which input it came from, its name)\n            b_plaintext, src, name = b_plaintext_info\n\n            b_ciphertext = self.editor.encrypt_bytes(b_plaintext, self.encrypt_secret,\n                                                     vault_id=vault_id)\n\n            # block formatting\n            yaml_text = self.format_ciphertext_yaml(b_ciphertext, name=name)\n\n            err_msg = None\n            if show_delimiter:\n                human_index = index + 1\n                if name:\n                    err_msg = '# The encrypted version of variable (\"%s\", the string #%d from %s).\\n' % (name, human_index, src)\n                else:\n                    err_msg = '# The encrypted version of the string #%d from %s.)\\n' % (human_index, src)\n            output.append({'out': yaml_text, 'err': err_msg})\n\n        return output",
        "begin_line": 378,
        "end_line": 410,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_decrypt#412",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_decrypt(self)",
        "snippet": "    def execute_decrypt(self):\n        ''' decrypt the supplied file using the provided vault secret '''\n\n        if not context.CLIARGS['args'] and sys.stdin.isatty():\n            display.display(\"Reading ciphertext input from stdin\", stderr=True)\n\n        for f in context.CLIARGS['args'] or ['-']:\n            self.editor.decrypt_file(f, output_file=context.CLIARGS['output_file'])\n\n        if sys.stdout.isatty():\n            display.display(\"Decryption successful\", stderr=True)",
        "begin_line": 412,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_create#424",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_create(self)",
        "snippet": "    def execute_create(self):\n        ''' create and open a file in an editor that will be encrypted with the provided vault secret when closed'''\n\n        if len(context.CLIARGS['args']) != 1:\n            raise AnsibleOptionsError(\"ansible-vault create can take only one filename argument\")\n\n        self.editor.create_file(context.CLIARGS['args'][0], self.encrypt_secret,\n                                vault_id=self.encrypt_vault_id)",
        "begin_line": 424,
        "end_line": 431,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_edit#433",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_edit(self)",
        "snippet": "    def execute_edit(self):\n        ''' open and decrypt an existing vaulted file in an editor, that will be encrypted again when closed'''\n        for f in context.CLIARGS['args']:\n            self.editor.edit_file(f)",
        "begin_line": 433,
        "end_line": 436,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_view#438",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_view(self)",
        "snippet": "    def execute_view(self):\n        ''' open, decrypt and view an existing vaulted file using a pager using the supplied vault secret '''\n\n        for f in context.CLIARGS['args']:\n            # Note: vault should return byte strings because it could encrypt\n            # and decrypt binary files.  We are responsible for changing it to\n            # unicode here because we are displaying it and therefore can make\n            # the decision that the display doesn't have to be precisely what\n            # the input was (leave that to decrypt instead)\n            plaintext = self.editor.plaintext(f)\n            self.pager(to_text(plaintext))",
        "begin_line": 438,
        "end_line": 448,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.cli.vault.VaultCLI.execute_rekey#450",
        "src_path": "lib/ansible/cli/vault.py",
        "class_name": "lib.ansible.cli.vault.VaultCLI",
        "signature": "lib.ansible.cli.vault.VaultCLI.execute_rekey(self)",
        "snippet": "    def execute_rekey(self):\n        ''' re-encrypt a vaulted file with a new secret, the previous secret is required '''\n        for f in context.CLIARGS['args']:\n            # FIXME: plumb in vault_id, use the default new_vault_secret for now\n            self.editor.rekey_file(f, self.new_encrypt_secret,\n                                   self.new_encrypt_vault_id)\n\n        display.display(\"Rekey successful\", stderr=True)",
        "begin_line": 450,
        "end_line": 457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 0.00026469031233456857,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.include.RoleInclude.__init__#46",
        "src_path": "lib/ansible/playbook/role/include.py",
        "class_name": "lib.ansible.playbook.role.include.RoleInclude",
        "signature": "lib.ansible.playbook.role.include.RoleInclude.__init__(self, play=None, role_basedir=None, variable_manager=None, loader=None, collection_list=None)",
        "snippet": "    def __init__(self, play=None, role_basedir=None, variable_manager=None, loader=None, collection_list=None):\n        super(RoleInclude, self).__init__(play=play, role_basedir=role_basedir, variable_manager=variable_manager,\n                                          loader=loader, collection_list=collection_list)",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    },
    {
        "name": "lib.ansible.playbook.role.include.RoleInclude.load#51",
        "src_path": "lib/ansible/playbook/role/include.py",
        "class_name": "lib.ansible.playbook.role.include.RoleInclude",
        "signature": "lib.ansible.playbook.role.include.RoleInclude.load(data, play, current_role_path=None, parent_role=None, variable_manager=None, loader=None, collection_list=None)",
        "snippet": "    def load(data, play, current_role_path=None, parent_role=None, variable_manager=None, loader=None, collection_list=None):\n\n        if not (isinstance(data, string_types) or isinstance(data, dict) or isinstance(data, AnsibleBaseYAMLObject)):\n            raise AnsibleParserError(\"Invalid role definition: %s\" % to_native(data))\n\n        if isinstance(data, string_types) and ',' in data:\n            raise AnsibleError(\"Invalid old style role requirement: %s\" % data)\n\n        ri = RoleInclude(play=play, role_basedir=current_role_path, variable_manager=variable_manager, loader=loader, collection_list=collection_list)\n        return ri.load_data(data, variable_manager=variable_manager, loader=loader)",
        "begin_line": 51,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00012132977432661975,
            "pseudo_dstar_susp": 0.00012132977432661975,
            "pseudo_tarantula_susp": 0.00012132977432661975,
            "pseudo_op2_susp": 7.880220646178094e-05,
            "pseudo_barinel_susp": 0.00012132977432661975
        }
    }
]