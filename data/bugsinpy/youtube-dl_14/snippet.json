[
    {
        "name": "youtube_dl.extractor.bbc.MediaSelectionError.__init__#300",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.MediaSelectionError",
        "signature": "youtube_dl.extractor.bbc.MediaSelectionError.__init__(self, id)",
        "snippet": "        def __init__(self, id):\n            self.id = id",
        "begin_line": 300,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._extract_medias#316",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._extract_medias(self, media_selection)",
        "snippet": "    def _extract_medias(self, media_selection):\n        error = media_selection.find('./{%s}error' % self._MEDIASELECTION_NS)\n        if error is None:\n            media_selection.find('./{%s}error' % self._EMP_PLAYLIST_NS)\n        if error is not None:\n            raise BBCCoUkIE.MediaSelectionError(error.get('id'))\n        return self._findall_ns(media_selection, './{%s}media')",
        "begin_line": 316,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003566333808844508,
            "pseudo_dstar_susp": 0.00036886757654002215,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00036886757654002215,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._extract_connections#324",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._extract_connections(self, media)",
        "snippet": "    def _extract_connections(self, media):\n        return self._findall_ns(media, './{%s}connection')",
        "begin_line": 324,
        "end_line": 325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003566333808844508,
            "pseudo_dstar_susp": 0.00036886757654002215,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00036886757654002215,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._download_media_selector#351",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._download_media_selector(self, programme_id)",
        "snippet": "    def _download_media_selector(self, programme_id):\n        last_exception = None\n        for mediaselector_url in self._MEDIASELECTOR_URLS:\n            try:\n                return self._download_media_selector_url(\n                    mediaselector_url % programme_id, programme_id)\n            except BBCCoUkIE.MediaSelectionError as e:\n                if e.id in ('notukerror', 'geolocation', 'selectionunavailable'):\n                    last_exception = e\n                    continue\n                self._raise_extractor_error(e)\n        self._raise_extractor_error(last_exception)",
        "begin_line": 351,
        "end_line": 362,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003566333808844508,
            "pseudo_dstar_susp": 0.00036886757654002215,
            "pseudo_tarantula_susp": 0.000591715976331361,
            "pseudo_op2_susp": 0.00036886757654002215,
            "pseudo_barinel_susp": 0.000591715976331361
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._download_media_selector_url#364",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._download_media_selector_url(self, url, programme_id=None)",
        "snippet": "    def _download_media_selector_url(self, url, programme_id=None):\n        media_selection = self._download_xml(\n            url, programme_id, 'Downloading media selection XML',\n            expected_status=(403, 404))\n        return self._process_media_selector(media_selection, programme_id)",
        "begin_line": 364,
        "end_line": 368,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003566333808844508,
            "pseudo_dstar_susp": 0.00036886757654002215,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00036886757654002215,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._process_media_selector#370",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._process_media_selector(self, media_selection, programme_id)",
        "snippet": "    def _process_media_selector(self, media_selection, programme_id):\n        formats = []\n        subtitles = None\n        urls = []\n\n        for media in self._extract_medias(media_selection):\n            kind = media.get('kind')\n            if kind in ('video', 'audio'):\n                bitrate = int_or_none(media.get('bitrate'))\n                encoding = media.get('encoding')\n                service = media.get('service')\n                width = int_or_none(media.get('width'))\n                height = int_or_none(media.get('height'))\n                file_size = int_or_none(media.get('media_file_size'))\n                for connection in self._extract_connections(media):\n                    href = connection.get('href')\n                    if href in urls:\n                        continue\n                    if href:\n                        urls.append(href)\n                    conn_kind = connection.get('kind')\n                    protocol = connection.get('protocol')\n                    supplier = connection.get('supplier')\n                    transfer_format = connection.get('transferFormat')\n                    format_id = supplier or conn_kind or protocol\n                    if service:\n                        format_id = '%s_%s' % (service, format_id)\n                    # ASX playlist\n                    if supplier == 'asx':\n                        for i, ref in enumerate(self._extract_asx_playlist(connection, programme_id)):\n                            formats.append({\n                                'url': ref,\n                                'format_id': 'ref%s_%s' % (i, format_id),\n                            })\n                    elif transfer_format == 'dash':\n                        formats.extend(self._extract_mpd_formats(\n                            href, programme_id, mpd_id=format_id, fatal=False))\n                    elif transfer_format == 'hls':\n                        formats.extend(self._extract_m3u8_formats(\n                            href, programme_id, ext='mp4', entry_protocol='m3u8_native',\n                            m3u8_id=format_id, fatal=False))\n                        if re.search(self._USP_RE, href):\n                            usp_formats = self._extract_m3u8_formats(\n                                re.sub(self._USP_RE, r'/\\1.ism/\\1.m3u8', href),\n                                programme_id, ext='mp4', entry_protocol='m3u8_native',\n                                m3u8_id=format_id, fatal=False)\n                            for f in usp_formats:\n                                if f.get('height') and f['height'] > 720:\n                                    continue\n                                formats.append(f)\n                    elif transfer_format == 'hds':\n                        formats.extend(self._extract_f4m_formats(\n                            href, programme_id, f4m_id=format_id, fatal=False))\n                    else:\n                        if not service and not supplier and bitrate:\n                            format_id += '-%d' % bitrate\n                        fmt = {\n                            'format_id': format_id,\n                            'filesize': file_size,\n                        }\n                        if kind == 'video':\n                            fmt.update({\n                                'width': width,\n                                'height': height,\n                                'tbr': bitrate,\n                                'vcodec': encoding,\n                            })\n                        else:\n                            fmt.update({\n                                'abr': bitrate,\n                                'acodec': encoding,\n                                'vcodec': 'none',\n                            })\n                        if protocol in ('http', 'https'):\n                            # Direct link\n                            fmt.update({\n                                'url': href,\n                            })\n                        elif protocol == 'rtmp':\n                            application = connection.get('application', 'ondemand')\n                            auth_string = connection.get('authString')\n                            identifier = connection.get('identifier')\n                            server = connection.get('server')\n                            fmt.update({\n                                'url': '%s://%s/%s?%s' % (protocol, server, application, auth_string),\n                                'play_path': identifier,\n                                'app': '%s?%s' % (application, auth_string),\n                                'page_url': 'http://www.bbc.co.uk',\n                                'player_url': 'http://www.bbc.co.uk/emp/releases/iplayer/revisions/617463_618125_4/617463_618125_4_emp.swf',\n                                'rtmp_live': False,\n                                'ext': 'flv',\n                            })\n                        else:\n                            continue\n                        formats.append(fmt)\n            elif kind == 'captions':\n                subtitles = self.extract_subtitles(media, programme_id)\n        return formats, subtitles",
        "begin_line": 370,
        "end_line": 467,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003566333808844508,
            "pseudo_dstar_susp": 0.00036886757654002215,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00036886757654002215,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._download_playlist#469",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._download_playlist(self, playlist_id)",
        "snippet": "    def _download_playlist(self, playlist_id):\n        try:\n            playlist = self._download_json(\n                'http://www.bbc.co.uk/programmes/%s/playlist.json' % playlist_id,\n                playlist_id, 'Downloading playlist JSON')\n\n            version = playlist.get('defaultAvailableVersion')\n            if version:\n                smp_config = version['smpConfig']\n                title = smp_config['title']\n                description = smp_config['summary']\n                for item in smp_config['items']:\n                    kind = item['kind']\n                    if kind not in ('programme', 'radioProgramme'):\n                        continue\n                    programme_id = item.get('vpid')\n                    duration = int_or_none(item.get('duration'))\n                    formats, subtitles = self._download_media_selector(programme_id)\n                return programme_id, title, description, duration, formats, subtitles\n        except ExtractorError as ee:\n            if not (isinstance(ee.cause, compat_HTTPError) and ee.cause.code == 404):\n                raise\n\n        # fallback to legacy playlist\n        return self._process_legacy_playlist(playlist_id)",
        "begin_line": 469,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003489183531053733,
            "pseudo_dstar_susp": 0.00034340659340659343,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00034340659340659343,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkIE._real_extract#551",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        group_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, group_id, 'Downloading video page')\n\n        error = self._search_regex(\n            r'<div\\b[^>]+\\bclass=[\"\\']smp__message delta[\"\\'][^>]*>([^<]+)<',\n            webpage, 'error', default=None)\n        if error:\n            raise ExtractorError(error, expected=True)\n\n        programme_id = None\n        duration = None\n\n        tviplayer = self._search_regex(\n            r'mediator\\.bind\\(({.+?})\\s*,\\s*document\\.getElementById',\n            webpage, 'player', default=None)\n\n        if tviplayer:\n            player = self._parse_json(tviplayer, group_id).get('player', {})\n            duration = int_or_none(player.get('duration'))\n            programme_id = player.get('vpid')\n\n        if not programme_id:\n            programme_id = self._search_regex(\n                r'\"vpid\"\\s*:\\s*\"(%s)\"' % self._ID_REGEX, webpage, 'vpid', fatal=False, default=None)\n\n        if programme_id:\n            formats, subtitles = self._download_media_selector(programme_id)\n            title = self._og_search_title(webpage, default=None) or self._html_search_regex(\n                (r'<h2[^>]+id=\"parent-title\"[^>]*>(.+?)</h2>',\n                 r'<div[^>]+class=\"info\"[^>]*>\\s*<h1>(.+?)</h1>'), webpage, 'title')\n            description = self._search_regex(\n                (r'<p class=\"[^\"]*medium-description[^\"]*\">([^<]+)</p>',\n                 r'<div[^>]+class=\"info_+synopsis\"[^>]*>([^<]+)</div>'),\n                webpage, 'description', default=None)\n            if not description:\n                description = self._html_search_meta('description', webpage)\n        else:\n            programme_id, title, description, duration, formats, subtitles = self._download_playlist(group_id)\n\n        self._sort_formats(formats)\n\n        return {\n            'id': programme_id,\n            'title': title,\n            'description': description,\n            'thumbnail': self._og_search_thumbnail(webpage, default=None),\n            'duration': duration,\n            'formats': formats,\n            'subtitles': subtitles,\n        }",
        "begin_line": 551,
        "end_line": 602,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004492362982929021,
            "pseudo_dstar_susp": 0.00044464206313917296,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00044464206313917296,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCIE.suitable#833",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCIE",
        "signature": "youtube_dl.extractor.bbc.BBCIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        EXCLUDE_IE = (BBCCoUkIE, BBCCoUkArticleIE, BBCCoUkIPlayerPlaylistIE, BBCCoUkPlaylistIE)\n        return (False if any(ie.suitable(url) for ie in EXCLUDE_IE)\n                else super(BBCIE, cls).suitable(url))",
        "begin_line": 833,
        "end_line": 836,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCIE._real_extract#879",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCIE",
        "signature": "youtube_dl.extractor.bbc.BBCIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        json_ld_info = self._search_json_ld(webpage, playlist_id, default={})\n        timestamp = json_ld_info.get('timestamp')\n\n        playlist_title = json_ld_info.get('title')\n        if not playlist_title:\n            playlist_title = self._og_search_title(\n                webpage, default=None) or self._html_search_regex(\n                r'<title>(.+?)</title>', webpage, 'playlist title', default=None)\n            if playlist_title:\n                playlist_title = re.sub(r'(.+)\\s*-\\s*BBC.*?$', r'\\1', playlist_title).strip()\n\n        playlist_description = json_ld_info.get(\n            'description') or self._og_search_description(webpage, default=None)\n\n        if not timestamp:\n            timestamp = parse_iso8601(self._search_regex(\n                [r'<meta[^>]+property=\"article:published_time\"[^>]+content=\"([^\"]+)\"',\n                 r'itemprop=\"datePublished\"[^>]+datetime=\"([^\"]+)\"',\n                 r'\"datePublished\":\\s*\"([^\"]+)'],\n                webpage, 'date', default=None))\n\n        entries = []\n\n        # article with multiple videos embedded with playlist.sxml (e.g.\n        # http://www.bbc.com/sport/0/football/34475836)\n        playlists = re.findall(r'<param[^>]+name=\"playlist\"[^>]+value=\"([^\"]+)\"', webpage)\n        playlists.extend(re.findall(r'data-media-id=\"([^\"]+/playlist\\.sxml)\"', webpage))\n        if playlists:\n            entries = [\n                self._extract_from_playlist_sxml(playlist_url, playlist_id, timestamp)\n                for playlist_url in playlists]\n\n        # news article with multiple videos embedded with data-playable\n        data_playables = re.findall(r'data-playable=([\"\\'])({.+?})\\1', webpage)\n        if data_playables:\n            for _, data_playable_json in data_playables:\n                data_playable = self._parse_json(\n                    unescapeHTML(data_playable_json), playlist_id, fatal=False)\n                if not data_playable:\n                    continue\n                settings = data_playable.get('settings', {})\n                if settings:\n                    # data-playable with video vpid in settings.playlistObject.items (e.g.\n                    # http://www.bbc.com/news/world-us-canada-34473351)\n                    playlist_object = settings.get('playlistObject', {})\n                    if playlist_object:\n                        items = playlist_object.get('items')\n                        if items and isinstance(items, list):\n                            title = playlist_object['title']\n                            description = playlist_object.get('summary')\n                            duration = int_or_none(items[0].get('duration'))\n                            programme_id = items[0].get('vpid')\n                            formats, subtitles = self._download_media_selector(programme_id)\n                            self._sort_formats(formats)\n                            entries.append({\n                                'id': programme_id,\n                                'title': title,\n                                'description': description,\n                                'timestamp': timestamp,\n                                'duration': duration,\n                                'formats': formats,\n                                'subtitles': subtitles,\n                            })\n                    else:\n                        # data-playable without vpid but with a playlist.sxml URLs\n                        # in otherSettings.playlist (e.g.\n                        # http://www.bbc.com/turkce/multimedya/2015/10/151010_vid_ankara_patlama_ani)\n                        playlist = data_playable.get('otherSettings', {}).get('playlist', {})\n                        if playlist:\n                            entry = None\n                            for key in ('streaming', 'progressiveDownload'):\n                                playlist_url = playlist.get('%sUrl' % key)\n                                if not playlist_url:\n                                    continue\n                                try:\n                                    info = self._extract_from_playlist_sxml(\n                                        playlist_url, playlist_id, timestamp)\n                                    if not entry:\n                                        entry = info\n                                    else:\n                                        entry['title'] = info['title']\n                                        entry['formats'].extend(info['formats'])\n                                except Exception as e:\n                                    # Some playlist URL may fail with 500, at the same time\n                                    # the other one may work fine (e.g.\n                                    # http://www.bbc.com/turkce/haberler/2015/06/150615_telabyad_kentin_cogu)\n                                    if isinstance(e.cause, compat_HTTPError) and e.cause.code == 500:\n                                        continue\n                                    raise\n                            if entry:\n                                self._sort_formats(entry['formats'])\n                                entries.append(entry)\n\n        if entries:\n            return self.playlist_result(entries, playlist_id, playlist_title, playlist_description)\n\n        # http://www.bbc.co.uk/learningenglish/chinese/features/lingohack/ep-181227\n        group_id = self._search_regex(\n            r'<div[^>]+\\bclass=[\"\\']video[\"\\'][^>]+\\bdata-pid=[\"\\'](%s)' % self._ID_REGEX,\n            webpage, 'group id', default=None)\n        if playlist_id:\n            return self.url_result(\n                'https://www.bbc.co.uk/programmes/%s' % group_id,\n                ie=BBCCoUkIE.ie_key())\n\n        # single video story (e.g. http://www.bbc.com/travel/story/20150625-sri-lankas-spicy-secret)\n        programme_id = self._search_regex(\n            [r'data-(?:video-player|media)-vpid=\"(%s)\"' % self._ID_REGEX,\n             r'<param[^>]+name=\"externalIdentifier\"[^>]+value=\"(%s)\"' % self._ID_REGEX,\n             r'videoId\\s*:\\s*[\"\\'](%s)[\"\\']' % self._ID_REGEX],\n            webpage, 'vpid', default=None)\n\n        if programme_id:\n            formats, subtitles = self._download_media_selector(programme_id)\n            self._sort_formats(formats)\n            # digitalData may be missing (e.g. http://www.bbc.com/autos/story/20130513-hyundais-rock-star)\n            digital_data = self._parse_json(\n                self._search_regex(\n                    r'var\\s+digitalData\\s*=\\s*({.+?});?\\n', webpage, 'digital data', default='{}'),\n                programme_id, fatal=False)\n            page_info = digital_data.get('page', {}).get('pageInfo', {})\n            title = page_info.get('pageName') or self._og_search_title(webpage)\n            description = page_info.get('description') or self._og_search_description(webpage)\n            timestamp = parse_iso8601(page_info.get('publicationDate')) or timestamp\n            return {\n                'id': programme_id,\n                'title': title,\n                'description': description,\n                'timestamp': timestamp,\n                'formats': formats,\n                'subtitles': subtitles,\n            }\n\n        # Morph based embed (e.g. http://www.bbc.co.uk/sport/live/olympics/36895975)\n        # There are several setPayload calls may be present but the video\n        # seems to be always related to the first one\n        morph_payload = self._parse_json(\n            self._search_regex(\n                r'Morph\\.setPayload\\([^,]+,\\s*({.+?})\\);',\n                webpage, 'morph payload', default='{}'),\n            playlist_id, fatal=False)\n        if morph_payload:\n            components = try_get(morph_payload, lambda x: x['body']['components'], list) or []\n            for component in components:\n                if not isinstance(component, dict):\n                    continue\n                lead_media = try_get(component, lambda x: x['props']['leadMedia'], dict)\n                if not lead_media:\n                    continue\n                identifiers = lead_media.get('identifiers')\n                if not identifiers or not isinstance(identifiers, dict):\n                    continue\n                programme_id = identifiers.get('vpid') or identifiers.get('playablePid')\n                if not programme_id:\n                    continue\n                title = lead_media.get('title') or self._og_search_title(webpage)\n                formats, subtitles = self._download_media_selector(programme_id)\n                self._sort_formats(formats)\n                description = lead_media.get('summary')\n                uploader = lead_media.get('masterBrand')\n                uploader_id = lead_media.get('mid')\n                duration = None\n                duration_d = lead_media.get('duration')\n                if isinstance(duration_d, dict):\n                    duration = parse_duration(dict_get(\n                        duration_d, ('rawDuration', 'formattedDuration', 'spokenDuration')))\n                return {\n                    'id': programme_id,\n                    'title': title,\n                    'description': description,\n                    'duration': duration,\n                    'uploader': uploader,\n                    'uploader_id': uploader_id,\n                    'formats': formats,\n                    'subtitles': subtitles,\n                }\n\n        preload_state = self._parse_json(self._search_regex(\n            r'window\\.__PRELOADED_STATE__\\s*=\\s*({.+?});', webpage,\n            'preload state', default='{}'), playlist_id, fatal=False)\n        if preload_state:\n            current_programme = preload_state.get('programmes', {}).get('current') or {}\n            programme_id = current_programme.get('id')\n            if current_programme and programme_id and current_programme.get('type') == 'playable_item':\n                title = current_programme.get('titles', {}).get('tertiary') or playlist_title\n                formats, subtitles = self._download_media_selector(programme_id)\n                self._sort_formats(formats)\n                synopses = current_programme.get('synopses') or {}\n                network = current_programme.get('network') or {}\n                duration = int_or_none(\n                    current_programme.get('duration', {}).get('value'))\n                thumbnail = None\n                image_url = current_programme.get('image_url')\n                if image_url:\n                    thumbnail = image_url.replace('{recipe}', '1920x1920')\n                return {\n                    'id': programme_id,\n                    'title': title,\n                    'description': dict_get(synopses, ('long', 'medium', 'short')),\n                    'thumbnail': thumbnail,\n                    'duration': duration,\n                    'uploader': network.get('short_title'),\n                    'uploader_id': network.get('id'),\n                    'formats': formats,\n                    'subtitles': subtitles,\n                }\n\n        bbc3_config = self._parse_json(\n            self._search_regex(\n                r'(?s)bbcthreeConfig\\s*=\\s*({.+?})\\s*;\\s*<', webpage,\n                'bbcthree config', default='{}'),\n            playlist_id, transform_source=js_to_json, fatal=False)\n        if bbc3_config:\n            bbc3_playlist = try_get(\n                bbc3_config, lambda x: x['payload']['content']['bbcMedia']['playlist'],\n                dict)\n            if bbc3_playlist:\n                playlist_title = bbc3_playlist.get('title') or playlist_title\n                thumbnail = bbc3_playlist.get('holdingImageURL')\n                entries = []\n                for bbc3_item in bbc3_playlist['items']:\n                    programme_id = bbc3_item.get('versionID')\n                    if not programme_id:\n                        continue\n                    formats, subtitles = self._download_media_selector(programme_id)\n                    self._sort_formats(formats)\n                    entries.append({\n                        'id': programme_id,\n                        'title': playlist_title,\n                        'thumbnail': thumbnail,\n                        'timestamp': timestamp,\n                        'formats': formats,\n                        'subtitles': subtitles,\n                    })\n                return self.playlist_result(\n                    entries, playlist_id, playlist_title, playlist_description)\n\n        def extract_all(pattern):\n            return list(filter(None, map(\n                lambda s: self._parse_json(s, playlist_id, fatal=False),\n                re.findall(pattern, webpage))))\n\n        # Multiple video article (e.g.\n        # http://www.bbc.co.uk/blogs/adamcurtis/entries/3662a707-0af9-3149-963f-47bea720b460)\n        EMBED_URL = r'https?://(?:www\\.)?bbc\\.co\\.uk/(?:[^/]+/)+%s(?:\\b[^\"]+)?' % self._ID_REGEX\n        entries = []\n        for match in extract_all(r'new\\s+SMP\\(({.+?})\\)'):\n            embed_url = match.get('playerSettings', {}).get('externalEmbedUrl')\n            if embed_url and re.match(EMBED_URL, embed_url):\n                entries.append(embed_url)\n        entries.extend(re.findall(\n            r'setPlaylist\\(\"(%s)\"\\)' % EMBED_URL, webpage))\n        if entries:\n            return self.playlist_result(\n                [self.url_result(entry_, 'BBCCoUk') for entry_ in entries],\n                playlist_id, playlist_title, playlist_description)\n\n        # Multiple video article (e.g. http://www.bbc.com/news/world-europe-32668511)\n        medias = extract_all(r\"data-media-meta='({[^']+})'\")\n\n        if not medias:\n            # Single video article (e.g. http://www.bbc.com/news/video_and_audio/international)\n            media_asset = self._search_regex(\n                r'mediaAssetPage\\.init\\(\\s*({.+?}), \"/',\n                webpage, 'media asset', default=None)\n            if media_asset:\n                media_asset_page = self._parse_json(media_asset, playlist_id, fatal=False)\n                medias = []\n                for video in media_asset_page.get('videos', {}).values():\n                    medias.extend(video.values())\n\n        if not medias:\n            # Multiple video playlist with single `now playing` entry (e.g.\n            # http://www.bbc.com/news/video_and_audio/must_see/33767813)\n            vxp_playlist = self._parse_json(\n                self._search_regex(\n                    r'<script[^>]+class=\"vxp-playlist-data\"[^>]+type=\"application/json\"[^>]*>([^<]+)</script>',\n                    webpage, 'playlist data'),\n                playlist_id)\n            playlist_medias = []\n            for item in vxp_playlist:\n                media = item.get('media')\n                if not media:\n                    continue\n                playlist_medias.append(media)\n                # Download single video if found media with asset id matching the video id from URL\n                if item.get('advert', {}).get('assetId') == playlist_id:\n                    medias = [media]\n                    break\n            # Fallback to the whole playlist\n            if not medias:\n                medias = playlist_medias\n\n        entries = []\n        for num, media_meta in enumerate(medias, start=1):\n            formats, subtitles = self._extract_from_media_meta(media_meta, playlist_id)\n            if not formats:\n                continue\n            self._sort_formats(formats)\n\n            video_id = media_meta.get('externalId')\n            if not video_id:\n                video_id = playlist_id if len(medias) == 1 else '%s-%s' % (playlist_id, num)\n\n            title = media_meta.get('caption')\n            if not title:\n                title = playlist_title if len(medias) == 1 else '%s - Video %s' % (playlist_title, num)\n\n            duration = int_or_none(media_meta.get('durationInSeconds')) or parse_duration(media_meta.get('duration'))\n\n            images = []\n            for image in media_meta.get('images', {}).values():\n                images.extend(image.values())\n            if 'image' in media_meta:\n                images.append(media_meta['image'])\n\n            thumbnails = [{\n                'url': image.get('href'),\n                'width': int_or_none(image.get('width')),\n                'height': int_or_none(image.get('height')),\n            } for image in images]\n\n            entries.append({\n                'id': video_id,\n                'title': title,\n                'thumbnails': thumbnails,\n                'duration': duration,\n                'timestamp': timestamp,\n                'formats': formats,\n                'subtitles': subtitles,\n            })\n\n        return self.playlist_result(entries, playlist_id, playlist_title, playlist_description)",
        "begin_line": 879,
        "end_line": 1216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005820721769499418,
            "pseudo_dstar_susp": 0.000510986203372509,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkArticleIE._real_extract#1235",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkArticleIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkArticleIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        title = self._og_search_title(webpage)\n        description = self._og_search_description(webpage).strip()\n\n        entries = [self.url_result(programme_url) for programme_url in re.findall(\n            r'<div[^>]+typeof=\"Clip\"[^>]+resource=\"([^\"]+)\"', webpage)]\n\n        return self.playlist_result(entries, playlist_id, title, description)",
        "begin_line": 1235,
        "end_line": 1246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00031735956839098697,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE._entries#1250",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE._entries(self, webpage, url, playlist_id)",
        "snippet": "    def _entries(self, webpage, url, playlist_id):\n        single_page = 'page' in compat_urlparse.parse_qs(\n            compat_urlparse.urlparse(url).query)\n        for page_num in itertools.count(2):\n            for video_id in re.findall(\n                    self._VIDEO_ID_TEMPLATE % BBCCoUkIE._ID_REGEX, webpage):\n                yield self.url_result(\n                    self._URL_TEMPLATE % video_id, BBCCoUkIE.ie_key())\n            if single_page:\n                return\n            next_page = self._search_regex(\n                r'<li[^>]+class=([\"\\'])pagination_+next\\1[^>]*><a[^>]+href=([\"\\'])(?P<url>(?:(?!\\2).)+)\\2',\n                webpage, 'next page url', default=None, group='url')\n            if not next_page:\n                break\n            webpage = self._download_webpage(\n                compat_urlparse.urljoin(url, next_page), playlist_id,\n                'Downloading page %d' % page_num, page_num)",
        "begin_line": 1250,
        "end_line": 1267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045330915684496827,
            "pseudo_dstar_susp": 0.0004278990158322636,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004278990158322636,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE._real_extract#1269",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkPlaylistBaseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        title, description = self._extract_title_and_description(webpage)\n\n        return self.playlist_result(\n            self._entries(webpage, url, playlist_id),\n            playlist_id, title, description)",
        "begin_line": 1269,
        "end_line": 1278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046040515653775324,
            "pseudo_dstar_susp": 0.00044483985765124553,
            "pseudo_tarantula_susp": 0.0017953321364452424,
            "pseudo_op2_susp": 0.00044483985765124553,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistIE._extract_title_and_description#1356",
        "src_path": "youtube_dl/extractor/bbc.py",
        "class_name": "youtube_dl.extractor.bbc.BBCCoUkPlaylistIE",
        "signature": "youtube_dl.extractor.bbc.BBCCoUkPlaylistIE._extract_title_and_description(self, webpage)",
        "snippet": "    def _extract_title_and_description(self, webpage):\n        title = self._og_search_title(webpage, fatal=False)\n        description = self._og_search_description(webpage)\n        return title, description",
        "begin_line": 1356,
        "end_line": 1359,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045330915684496827,
            "pseudo_dstar_susp": 0.0004278990158322636,
            "pseudo_tarantula_susp": 0.00202020202020202,
            "pseudo_op2_susp": 0.0004278990158322636,
            "pseudo_barinel_susp": 0.00202020202020202
        }
    },
    {
        "name": "youtube_dl.extractor.motherless.MotherlessGroupIE.suitable#151",
        "src_path": "youtube_dl/extractor/motherless.py",
        "class_name": "youtube_dl.extractor.motherless.MotherlessGroupIE",
        "signature": "youtube_dl.extractor.motherless.MotherlessGroupIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if MotherlessIE.suitable(url)\n                else super(MotherlessGroupIE, cls).suitable(url))",
        "begin_line": 151,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00033090668431502316,
            "pseudo_tarantula_susp": 0.0002752546105147261,
            "pseudo_op2_susp": 0.00033090668431502316,
            "pseudo_barinel_susp": 0.0002729257641921397
        }
    },
    {
        "name": "youtube_dl.extractor.washingtonpost.WashingtonPostArticleIE.suitable#162",
        "src_path": "youtube_dl/extractor/washingtonpost.py",
        "class_name": "youtube_dl.extractor.washingtonpost.WashingtonPostArticleIE",
        "signature": "youtube_dl.extractor.washingtonpost.WashingtonPostArticleIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if WashingtonPostIE.suitable(url) else super(WashingtonPostArticleIE, cls).suitable(url)",
        "begin_line": 162,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.udemy.UdemyCourseIE.suitable#434",
        "src_path": "youtube_dl/extractor/udemy.py",
        "class_name": "youtube_dl.extractor.udemy.UdemyCourseIE",
        "signature": "youtube_dl.extractor.udemy.UdemyCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if UdemyIE.suitable(url) else super(UdemyCourseIE, cls).suitable(url)",
        "begin_line": 434,
        "end_line": 435,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011810558639423645,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.options._hide_login_info#23",
        "src_path": "youtube_dl/options.py",
        "class_name": "youtube_dl.options",
        "signature": "youtube_dl.options._hide_login_info(opts)",
        "snippet": "def _hide_login_info(opts):\n    PRIVATE_OPTS = set(['-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'])\n    eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')\n\n    def _scrub_eq(o):\n        m = eqre.match(o)\n        if m:\n            return m.group('key') + '=PRIVATE'\n        else:\n            return o\n\n    opts = list(map(_scrub_eq, opts))\n    for idx, opt in enumerate(opts):\n        if opt in PRIVATE_OPTS and idx + 1 < len(opts):\n            opts[idx + 1] = 'PRIVATE'\n    return opts",
        "begin_line": 23,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.options._scrub_eq#27",
        "src_path": "youtube_dl/options.py",
        "class_name": "youtube_dl.options",
        "signature": "youtube_dl.options._scrub_eq(o)",
        "snippet": "    def _scrub_eq(o):\n        m = eqre.match(o)\n        if m:\n            return m.group('key') + '=PRIVATE'\n        else:\n            return o",
        "begin_line": 27,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.options.parseOpts#41",
        "src_path": "youtube_dl/options.py",
        "class_name": "youtube_dl.options",
        "signature": "youtube_dl.options.parseOpts(overrideArguments=None)",
        "snippet": "def parseOpts(overrideArguments=None):\n    def _readOptions(filename_bytes, default=[]):\n        try:\n            optionf = open(filename_bytes)\n        except IOError:\n            return default  # silently skip if file is not present\n        try:\n            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n            contents = optionf.read()\n            if sys.version_info < (3,):\n                contents = contents.decode(preferredencoding())\n            res = compat_shlex_split(contents, comments=True)\n        finally:\n            optionf.close()\n        return res\n\n    def _readUserConf():\n        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')\n        if xdg_config_home:\n            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')\n            if not os.path.isfile(userConfFile):\n                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')\n        else:\n            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')\n            if not os.path.isfile(userConfFile):\n                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')\n        userConf = _readOptions(userConfFile, None)\n\n        if userConf is None:\n            appdata_dir = compat_getenv('appdata')\n            if appdata_dir:\n                userConf = _readOptions(\n                    os.path.join(appdata_dir, 'youtube-dl', 'config'),\n                    default=None)\n                if userConf is None:\n                    userConf = _readOptions(\n                        os.path.join(appdata_dir, 'youtube-dl', 'config.txt'),\n                        default=None)\n\n        if userConf is None:\n            userConf = _readOptions(\n                os.path.join(compat_expanduser('~'), 'youtube-dl.conf'),\n                default=None)\n        if userConf is None:\n            userConf = _readOptions(\n                os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'),\n                default=None)\n\n        if userConf is None:\n            userConf = []\n\n        return userConf\n\n    def _format_option_string(option):\n        ''' ('-o', '--option') -> -o, --format METAVAR'''\n\n        opts = []\n\n        if option._short_opts:\n            opts.append(option._short_opts[0])\n        if option._long_opts:\n            opts.append(option._long_opts[0])\n        if len(opts) > 1:\n            opts.insert(1, ', ')\n\n        if option.takes_value():\n            opts.append(' %s' % option.metavar)\n\n        return ''.join(opts)\n\n    def _comma_separated_values_options_callback(option, opt_str, value, parser):\n        setattr(parser.values, option.dest, value.split(','))\n\n    # No need to wrap help messages if we're on a wide console\n    columns = compat_get_terminal_size().columns\n    max_width = columns if columns else 80\n    max_help_position = 80\n\n    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)\n    fmt.format_option_strings = _format_option_string\n\n    kw = {\n        'version': __version__,\n        'formatter': fmt,\n        'usage': '%prog [OPTIONS] URL [URL...]',\n        'conflict_handler': 'resolve',\n    }\n\n    parser = optparse.OptionParser(**compat_kwargs(kw))\n\n    general = optparse.OptionGroup(parser, 'General Options')\n    general.add_option(\n        '-h', '--help',\n        action='help',\n        help='Print this help text and exit')\n    general.add_option(\n        '--version',\n        action='version',\n        help='Print program version and exit')\n    general.add_option(\n        '-U', '--update',\n        action='store_true', dest='update_self',\n        help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')\n    general.add_option(\n        '-i', '--ignore-errors',\n        action='store_true', dest='ignoreerrors', default=False,\n        help='Continue on download errors, for example to skip unavailable videos in a playlist')\n    general.add_option(\n        '--abort-on-error',\n        action='store_false', dest='ignoreerrors',\n        help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')\n    general.add_option(\n        '--dump-user-agent',\n        action='store_true', dest='dump_user_agent', default=False,\n        help='Display the current browser identification')\n    general.add_option(\n        '--list-extractors',\n        action='store_true', dest='list_extractors', default=False,\n        help='List all supported extractors')\n    general.add_option(\n        '--extractor-descriptions',\n        action='store_true', dest='list_extractor_descriptions', default=False,\n        help='Output descriptions of all supported extractors')\n    general.add_option(\n        '--force-generic-extractor',\n        action='store_true', dest='force_generic_extractor', default=False,\n        help='Force extraction to use the generic extractor')\n    general.add_option(\n        '--default-search',\n        dest='default_search', metavar='PREFIX',\n        help='Use this prefix for unqualified URLs. For example \"gvsearch2:\" downloads two videos from google videos for youtube-dl \"large apple\". Use the value \"auto\" to let youtube-dl guess (\"auto_warning\" to emit a warning when guessing). \"error\" just throws an error. The default value \"fixup_error\" repairs broken URLs, but emits an error if this is not possible instead of searching.')\n    general.add_option(\n        '--ignore-config',\n        action='store_true',\n        help='Do not read configuration files. '\n        'When given in the global configuration file /etc/youtube-dl.conf: '\n        'Do not read the user configuration in ~/.config/youtube-dl/config '\n        '(%APPDATA%/youtube-dl/config.txt on Windows)')\n    general.add_option(\n        '--config-location',\n        dest='config_location', metavar='PATH',\n        help='Location of the configuration file; either the path to the config or its containing directory.')\n    general.add_option(\n        '--flat-playlist',\n        action='store_const', dest='extract_flat', const='in_playlist',\n        default=False,\n        help='Do not extract the videos of a playlist, only list them.')\n    general.add_option(\n        '--mark-watched',\n        action='store_true', dest='mark_watched', default=False,\n        help='Mark videos watched (YouTube only)')\n    general.add_option(\n        '--no-mark-watched',\n        action='store_false', dest='mark_watched', default=False,\n        help='Do not mark videos watched (YouTube only)')\n    general.add_option(\n        '--no-color', '--no-colors',\n        action='store_true', dest='no_color',\n        default=False,\n        help='Do not emit color codes in output')\n\n    network = optparse.OptionGroup(parser, 'Network Options')\n    network.add_option(\n        '--proxy', dest='proxy',\n        default=None, metavar='URL',\n        help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable '\n             'SOCKS proxy, specify a proper scheme. For example '\n             'socks5://127.0.0.1:1080/. Pass in an empty string (--proxy \"\") '\n             'for direct connection')\n    network.add_option(\n        '--socket-timeout',\n        dest='socket_timeout', type=float, default=None, metavar='SECONDS',\n        help='Time to wait before giving up, in seconds')\n    network.add_option(\n        '--source-address',\n        metavar='IP', dest='source_address', default=None,\n        help='Client-side IP address to bind to',\n    )\n    network.add_option(\n        '-4', '--force-ipv4',\n        action='store_const', const='0.0.0.0', dest='source_address',\n        help='Make all connections via IPv4',\n    )\n    network.add_option(\n        '-6', '--force-ipv6',\n        action='store_const', const='::', dest='source_address',\n        help='Make all connections via IPv6',\n    )\n\n    geo = optparse.OptionGroup(parser, 'Geo Restriction')\n    geo.add_option(\n        '--geo-verification-proxy',\n        dest='geo_verification_proxy', default=None, metavar='URL',\n        help='Use this proxy to verify the IP address for some geo-restricted sites. '\n        'The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')\n    geo.add_option(\n        '--cn-verification-proxy',\n        dest='cn_verification_proxy', default=None, metavar='URL',\n        help=optparse.SUPPRESS_HELP)\n    geo.add_option(\n        '--geo-bypass',\n        action='store_true', dest='geo_bypass', default=True,\n        help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')\n    geo.add_option(\n        '--no-geo-bypass',\n        action='store_false', dest='geo_bypass', default=True,\n        help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')\n    geo.add_option(\n        '--geo-bypass-country', metavar='CODE',\n        dest='geo_bypass_country', default=None,\n        help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')\n    geo.add_option(\n        '--geo-bypass-ip-block', metavar='IP_BLOCK',\n        dest='geo_bypass_ip_block', default=None,\n        help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')\n\n    selection = optparse.OptionGroup(parser, 'Video Selection')\n    selection.add_option(\n        '--playlist-start',\n        dest='playliststart', metavar='NUMBER', default=1, type=int,\n        help='Playlist video to start at (default is %default)')\n    selection.add_option(\n        '--playlist-end',\n        dest='playlistend', metavar='NUMBER', default=None, type=int,\n        help='Playlist video to end at (default is last)')\n    selection.add_option(\n        '--playlist-items',\n        dest='playlist_items', metavar='ITEM_SPEC', default=None,\n        help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: \"--playlist-items 1,2,5,8\" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: \"--playlist-items 1-3,7,10-13\", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')\n    selection.add_option(\n        '--match-title',\n        dest='matchtitle', metavar='REGEX',\n        help='Download only matching titles (regex or caseless sub-string)')\n    selection.add_option(\n        '--reject-title',\n        dest='rejecttitle', metavar='REGEX',\n        help='Skip download for matching titles (regex or caseless sub-string)')\n    selection.add_option(\n        '--max-downloads',\n        dest='max_downloads', metavar='NUMBER', type=int, default=None,\n        help='Abort after downloading NUMBER files')\n    selection.add_option(\n        '--min-filesize',\n        metavar='SIZE', dest='min_filesize', default=None,\n        help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')\n    selection.add_option(\n        '--max-filesize',\n        metavar='SIZE', dest='max_filesize', default=None,\n        help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')\n    selection.add_option(\n        '--date',\n        metavar='DATE', dest='date', default=None,\n        help='Download only videos uploaded in this date')\n    selection.add_option(\n        '--datebefore',\n        metavar='DATE', dest='datebefore', default=None,\n        help='Download only videos uploaded on or before this date (i.e. inclusive)')\n    selection.add_option(\n        '--dateafter',\n        metavar='DATE', dest='dateafter', default=None,\n        help='Download only videos uploaded on or after this date (i.e. inclusive)')\n    selection.add_option(\n        '--min-views',\n        metavar='COUNT', dest='min_views', default=None, type=int,\n        help='Do not download any videos with less than COUNT views')\n    selection.add_option(\n        '--max-views',\n        metavar='COUNT', dest='max_views', default=None, type=int,\n        help='Do not download any videos with more than COUNT views')\n    selection.add_option(\n        '--match-filter',\n        metavar='FILTER', dest='match_filter', default=None,\n        help=(\n            'Generic video filter. '\n            'Specify any key (see the \"OUTPUT TEMPLATE\" for a list of available keys) to '\n            'match if the key is present, '\n            '!key to check if the key is not present, '\n            'key > NUMBER (like \"comment_count > 12\", also works with '\n            '>=, <, <=, !=, =) to compare against a number, '\n            'key = \\'LITERAL\\' (like \"uploader = \\'Mike Smith\\'\", also works with !=) '\n            'to match against a string literal '\n            'and & to require multiple matches. '\n            'Values which are not known are excluded unless you '\n            'put a question mark (?) after the operator. '\n            'For example, to only match videos that have been liked more than '\n            '100 times and disliked less than 50 times (or the dislike '\n            'functionality is not available at the given service), but who '\n            'also have a description, use --match-filter '\n            '\"like_count > 100 & dislike_count <? 50 & description\" .'\n        ))\n    selection.add_option(\n        '--no-playlist',\n        action='store_true', dest='noplaylist', default=False,\n        help='Download only the video, if the URL refers to a video and a playlist.')\n    selection.add_option(\n        '--yes-playlist',\n        action='store_false', dest='noplaylist', default=False,\n        help='Download the playlist, if the URL refers to a video and a playlist.')\n    selection.add_option(\n        '--age-limit',\n        metavar='YEARS', dest='age_limit', default=None, type=int,\n        help='Download only videos suitable for the given age')\n    selection.add_option(\n        '--download-archive', metavar='FILE',\n        dest='download_archive',\n        help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')\n    selection.add_option(\n        '--include-ads',\n        dest='include_ads', action='store_true',\n        help='Download advertisements as well (experimental)')\n\n    authentication = optparse.OptionGroup(parser, 'Authentication Options')\n    authentication.add_option(\n        '-u', '--username',\n        dest='username', metavar='USERNAME',\n        help='Login with this account ID')\n    authentication.add_option(\n        '-p', '--password',\n        dest='password', metavar='PASSWORD',\n        help='Account password. If this option is left out, youtube-dl will ask interactively.')\n    authentication.add_option(\n        '-2', '--twofactor',\n        dest='twofactor', metavar='TWOFACTOR',\n        help='Two-factor authentication code')\n    authentication.add_option(\n        '-n', '--netrc',\n        action='store_true', dest='usenetrc', default=False,\n        help='Use .netrc authentication data')\n    authentication.add_option(\n        '--video-password',\n        dest='videopassword', metavar='PASSWORD',\n        help='Video password (vimeo, smotri, youku)')\n\n    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')\n    adobe_pass.add_option(\n        '--ap-mso',\n        dest='ap_mso', metavar='MSO',\n        help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')\n    adobe_pass.add_option(\n        '--ap-username',\n        dest='ap_username', metavar='USERNAME',\n        help='Multiple-system operator account login')\n    adobe_pass.add_option(\n        '--ap-password',\n        dest='ap_password', metavar='PASSWORD',\n        help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')\n    adobe_pass.add_option(\n        '--ap-list-mso',\n        action='store_true', dest='ap_list_mso', default=False,\n        help='List all supported multiple-system operators')\n\n    video_format = optparse.OptionGroup(parser, 'Video Format Options')\n    video_format.add_option(\n        '-f', '--format',\n        action='store', dest='format', metavar='FORMAT', default=None,\n        help='Video format code, see the \"FORMAT SELECTION\" for all the info')\n    video_format.add_option(\n        '--all-formats',\n        action='store_const', dest='format', const='all',\n        help='Download all available video formats')\n    video_format.add_option(\n        '--prefer-free-formats',\n        action='store_true', dest='prefer_free_formats', default=False,\n        help='Prefer free video formats unless a specific one is requested')\n    video_format.add_option(\n        '-F', '--list-formats',\n        action='store_true', dest='listformats',\n        help='List all available formats of requested videos')\n    video_format.add_option(\n        '--youtube-include-dash-manifest',\n        action='store_true', dest='youtube_include_dash_manifest', default=True,\n        help=optparse.SUPPRESS_HELP)\n    video_format.add_option(\n        '--youtube-skip-dash-manifest',\n        action='store_false', dest='youtube_include_dash_manifest',\n        help='Do not download the DASH manifests and related data on YouTube videos')\n    video_format.add_option(\n        '--merge-output-format',\n        action='store', dest='merge_output_format', metavar='FORMAT', default=None,\n        help=(\n            'If a merge is required (e.g. bestvideo+bestaudio), '\n            'output to given container format. One of mkv, mp4, ogg, webm, flv. '\n            'Ignored if no merge is required'))\n\n    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')\n    subtitles.add_option(\n        '--write-sub', '--write-srt',\n        action='store_true', dest='writesubtitles', default=False,\n        help='Write subtitle file')\n    subtitles.add_option(\n        '--write-auto-sub', '--write-automatic-sub',\n        action='store_true', dest='writeautomaticsub', default=False,\n        help='Write automatically generated subtitle file (YouTube only)')\n    subtitles.add_option(\n        '--all-subs',\n        action='store_true', dest='allsubtitles', default=False,\n        help='Download all the available subtitles of the video')\n    subtitles.add_option(\n        '--list-subs',\n        action='store_true', dest='listsubtitles', default=False,\n        help='List all available subtitles for the video')\n    subtitles.add_option(\n        '--sub-format',\n        action='store', dest='subtitlesformat', metavar='FORMAT', default='best',\n        help='Subtitle format, accepts formats preference, for example: \"srt\" or \"ass/srt/best\"')\n    subtitles.add_option(\n        '--sub-lang', '--sub-langs', '--srt-lang',\n        action='callback', dest='subtitleslangs', metavar='LANGS', type='str',\n        default=[], callback=_comma_separated_values_options_callback,\n        help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')\n\n    downloader = optparse.OptionGroup(parser, 'Download Options')\n    downloader.add_option(\n        '-r', '--limit-rate', '--rate-limit',\n        dest='ratelimit', metavar='RATE',\n        help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')\n    downloader.add_option(\n        '-R', '--retries',\n        dest='retries', metavar='RETRIES', default=10,\n        help='Number of retries (default is %default), or \"infinite\".')\n    downloader.add_option(\n        '--fragment-retries',\n        dest='fragment_retries', metavar='RETRIES', default=10,\n        help='Number of retries for a fragment (default is %default), or \"infinite\" (DASH, hlsnative and ISM)')\n    downloader.add_option(\n        '--skip-unavailable-fragments',\n        action='store_true', dest='skip_unavailable_fragments', default=True,\n        help='Skip unavailable fragments (DASH, hlsnative and ISM)')\n    downloader.add_option(\n        '--abort-on-unavailable-fragment',\n        action='store_false', dest='skip_unavailable_fragments',\n        help='Abort downloading when some fragment is not available')\n    downloader.add_option(\n        '--keep-fragments',\n        action='store_true', dest='keep_fragments', default=False,\n        help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')\n    downloader.add_option(\n        '--buffer-size',\n        dest='buffersize', metavar='SIZE', default='1024',\n        help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')\n    downloader.add_option(\n        '--no-resize-buffer',\n        action='store_true', dest='noresizebuffer', default=False,\n        help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')\n    downloader.add_option(\n        '--http-chunk-size',\n        dest='http_chunk_size', metavar='SIZE', default=None,\n        help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). '\n             'May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')\n    downloader.add_option(\n        '--test',\n        action='store_true', dest='test', default=False,\n        help=optparse.SUPPRESS_HELP)\n    downloader.add_option(\n        '--playlist-reverse',\n        action='store_true',\n        help='Download playlist videos in reverse order')\n    downloader.add_option(\n        '--playlist-random',\n        action='store_true',\n        help='Download playlist videos in random order')\n    downloader.add_option(\n        '--xattr-set-filesize',\n        dest='xattr_set_filesize', action='store_true',\n        help='Set file xattribute ytdl.filesize with expected file size')\n    downloader.add_option(\n        '--hls-prefer-native',\n        dest='hls_prefer_native', action='store_true', default=None,\n        help='Use the native HLS downloader instead of ffmpeg')\n    downloader.add_option(\n        '--hls-prefer-ffmpeg',\n        dest='hls_prefer_native', action='store_false', default=None,\n        help='Use ffmpeg instead of the native HLS downloader')\n    downloader.add_option(\n        '--hls-use-mpegts',\n        dest='hls_use_mpegts', action='store_true',\n        help='Use the mpegts container for HLS videos, allowing to play the '\n             'video while downloading (some players may not be able to play it)')\n    downloader.add_option(\n        '--external-downloader',\n        dest='external_downloader', metavar='COMMAND',\n        help='Use the specified external downloader. '\n             'Currently supports %s' % ','.join(list_external_downloaders()))\n    downloader.add_option(\n        '--external-downloader-args',\n        dest='external_downloader_args', metavar='ARGS',\n        help='Give these arguments to the external downloader')\n\n    workarounds = optparse.OptionGroup(parser, 'Workarounds')\n    workarounds.add_option(\n        '--encoding',\n        dest='encoding', metavar='ENCODING',\n        help='Force the specified encoding (experimental)')\n    workarounds.add_option(\n        '--no-check-certificate',\n        action='store_true', dest='no_check_certificate', default=False,\n        help='Suppress HTTPS certificate validation')\n    workarounds.add_option(\n        '--prefer-insecure',\n        '--prefer-unsecure', action='store_true', dest='prefer_insecure',\n        help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')\n    workarounds.add_option(\n        '--user-agent',\n        metavar='UA', dest='user_agent',\n        help='Specify a custom user agent')\n    workarounds.add_option(\n        '--referer',\n        metavar='URL', dest='referer', default=None,\n        help='Specify a custom referer, use if the video access is restricted to one domain',\n    )\n    workarounds.add_option(\n        '--add-header',\n        metavar='FIELD:VALUE', dest='headers', action='append',\n        help='Specify a custom HTTP header and its value, separated by a colon \\':\\'. You can use this option multiple times',\n    )\n    workarounds.add_option(\n        '--bidi-workaround',\n        dest='bidi_workaround', action='store_true',\n        help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')\n    workarounds.add_option(\n        '--sleep-interval', '--min-sleep-interval', metavar='SECONDS',\n        dest='sleep_interval', type=float,\n        help=(\n            'Number of seconds to sleep before each download when used alone '\n            'or a lower bound of a range for randomized sleep before each download '\n            '(minimum possible number of seconds to sleep) when used along with '\n            '--max-sleep-interval.'))\n    workarounds.add_option(\n        '--max-sleep-interval', metavar='SECONDS',\n        dest='max_sleep_interval', type=float,\n        help=(\n            'Upper bound of a range for randomized sleep before each download '\n            '(maximum possible number of seconds to sleep). Must only be used '\n            'along with --min-sleep-interval.'))\n\n    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')\n    verbosity.add_option(\n        '-q', '--quiet',\n        action='store_true', dest='quiet', default=False,\n        help='Activate quiet mode')\n    verbosity.add_option(\n        '--no-warnings',\n        dest='no_warnings', action='store_true', default=False,\n        help='Ignore warnings')\n    verbosity.add_option(\n        '-s', '--simulate',\n        action='store_true', dest='simulate', default=False,\n        help='Do not download the video and do not write anything to disk')\n    verbosity.add_option(\n        '--skip-download',\n        action='store_true', dest='skip_download', default=False,\n        help='Do not download the video')\n    verbosity.add_option(\n        '-g', '--get-url',\n        action='store_true', dest='geturl', default=False,\n        help='Simulate, quiet but print URL')\n    verbosity.add_option(\n        '-e', '--get-title',\n        action='store_true', dest='gettitle', default=False,\n        help='Simulate, quiet but print title')\n    verbosity.add_option(\n        '--get-id',\n        action='store_true', dest='getid', default=False,\n        help='Simulate, quiet but print id')\n    verbosity.add_option(\n        '--get-thumbnail',\n        action='store_true', dest='getthumbnail', default=False,\n        help='Simulate, quiet but print thumbnail URL')\n    verbosity.add_option(\n        '--get-description',\n        action='store_true', dest='getdescription', default=False,\n        help='Simulate, quiet but print video description')\n    verbosity.add_option(\n        '--get-duration',\n        action='store_true', dest='getduration', default=False,\n        help='Simulate, quiet but print video length')\n    verbosity.add_option(\n        '--get-filename',\n        action='store_true', dest='getfilename', default=False,\n        help='Simulate, quiet but print output filename')\n    verbosity.add_option(\n        '--get-format',\n        action='store_true', dest='getformat', default=False,\n        help='Simulate, quiet but print output format')\n    verbosity.add_option(\n        '-j', '--dump-json',\n        action='store_true', dest='dumpjson', default=False,\n        help='Simulate, quiet but print JSON information. See the \"OUTPUT TEMPLATE\" for a description of available keys.')\n    verbosity.add_option(\n        '-J', '--dump-single-json',\n        action='store_true', dest='dump_single_json', default=False,\n        help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')\n    verbosity.add_option(\n        '--print-json',\n        action='store_true', dest='print_json', default=False,\n        help='Be quiet and print the video information as JSON (video is still being downloaded).',\n    )\n    verbosity.add_option(\n        '--newline',\n        action='store_true', dest='progress_with_newline', default=False,\n        help='Output progress bar as new lines')\n    verbosity.add_option(\n        '--no-progress',\n        action='store_true', dest='noprogress', default=False,\n        help='Do not print progress bar')\n    verbosity.add_option(\n        '--console-title',\n        action='store_true', dest='consoletitle', default=False,\n        help='Display progress in console titlebar')\n    verbosity.add_option(\n        '-v', '--verbose',\n        action='store_true', dest='verbose', default=False,\n        help='Print various debugging information')\n    verbosity.add_option(\n        '--dump-pages', '--dump-intermediate-pages',\n        action='store_true', dest='dump_intermediate_pages', default=False,\n        help='Print downloaded pages encoded using base64 to debug problems (very verbose)')\n    verbosity.add_option(\n        '--write-pages',\n        action='store_true', dest='write_pages', default=False,\n        help='Write downloaded intermediary pages to files in the current directory to debug problems')\n    verbosity.add_option(\n        '--youtube-print-sig-code',\n        action='store_true', dest='youtube_print_sig_code', default=False,\n        help=optparse.SUPPRESS_HELP)\n    verbosity.add_option(\n        '--print-traffic', '--dump-headers',\n        dest='debug_printtraffic', action='store_true', default=False,\n        help='Display sent and read HTTP traffic')\n    verbosity.add_option(\n        '-C', '--call-home',\n        dest='call_home', action='store_true', default=False,\n        help='Contact the youtube-dl server for debugging')\n    verbosity.add_option(\n        '--no-call-home',\n        dest='call_home', action='store_false', default=False,\n        help='Do NOT contact the youtube-dl server for debugging')\n\n    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')\n    filesystem.add_option(\n        '-a', '--batch-file',\n        dest='batchfile', metavar='FILE',\n        help=\"File containing URLs to download ('-' for stdin), one URL per line. \"\n             \"Lines starting with '#', ';' or ']' are considered as comments and ignored.\")\n    filesystem.add_option(\n        '--id', default=False,\n        action='store_true', dest='useid', help='Use only video ID in file name')\n    filesystem.add_option(\n        '-o', '--output',\n        dest='outtmpl', metavar='TEMPLATE',\n        help=('Output filename template, see the \"OUTPUT TEMPLATE\" for all the info'))\n    filesystem.add_option(\n        '--autonumber-size',\n        dest='autonumber_size', metavar='NUMBER', type=int,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '--autonumber-start',\n        dest='autonumber_start', metavar='NUMBER', default=1, type=int,\n        help='Specify the start value for %(autonumber)s (default is %default)')\n    filesystem.add_option(\n        '--restrict-filenames',\n        action='store_true', dest='restrictfilenames', default=False,\n        help='Restrict filenames to only ASCII characters, and avoid \"&\" and spaces in filenames')\n    filesystem.add_option(\n        '-A', '--auto-number',\n        action='store_true', dest='autonumber', default=False,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '-t', '--title',\n        action='store_true', dest='usetitle', default=False,\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '-l', '--literal', default=False,\n        action='store_true', dest='usetitle',\n        help=optparse.SUPPRESS_HELP)\n    filesystem.add_option(\n        '-w', '--no-overwrites',\n        action='store_true', dest='nooverwrites', default=False,\n        help='Do not overwrite files')\n    filesystem.add_option(\n        '-c', '--continue',\n        action='store_true', dest='continue_dl', default=True,\n        help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')\n    filesystem.add_option(\n        '--no-continue',\n        action='store_false', dest='continue_dl',\n        help='Do not resume partially downloaded files (restart from beginning)')\n    filesystem.add_option(\n        '--no-part',\n        action='store_true', dest='nopart', default=False,\n        help='Do not use .part files - write directly into output file')\n    filesystem.add_option(\n        '--no-mtime',\n        action='store_false', dest='updatetime', default=True,\n        help='Do not use the Last-modified header to set the file modification time')\n    filesystem.add_option(\n        '--write-description',\n        action='store_true', dest='writedescription', default=False,\n        help='Write video description to a .description file')\n    filesystem.add_option(\n        '--write-info-json',\n        action='store_true', dest='writeinfojson', default=False,\n        help='Write video metadata to a .info.json file')\n    filesystem.add_option(\n        '--write-annotations',\n        action='store_true', dest='writeannotations', default=False,\n        help='Write video annotations to a .annotations.xml file')\n    filesystem.add_option(\n        '--load-info-json', '--load-info',\n        dest='load_info_filename', metavar='FILE',\n        help='JSON file containing the video information (created with the \"--write-info-json\" option)')\n    filesystem.add_option(\n        '--cookies',\n        dest='cookiefile', metavar='FILE',\n        help='File to read cookies from and dump cookie jar in')\n    filesystem.add_option(\n        '--cache-dir', dest='cachedir', default=None, metavar='DIR',\n        help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')\n    filesystem.add_option(\n        '--no-cache-dir', action='store_const', const=False, dest='cachedir',\n        help='Disable filesystem caching')\n    filesystem.add_option(\n        '--rm-cache-dir',\n        action='store_true', dest='rm_cachedir',\n        help='Delete all filesystem cache files')\n\n    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')\n    thumbnail.add_option(\n        '--write-thumbnail',\n        action='store_true', dest='writethumbnail', default=False,\n        help='Write thumbnail image to disk')\n    thumbnail.add_option(\n        '--write-all-thumbnails',\n        action='store_true', dest='write_all_thumbnails', default=False,\n        help='Write all thumbnail image formats to disk')\n    thumbnail.add_option(\n        '--list-thumbnails',\n        action='store_true', dest='list_thumbnails', default=False,\n        help='Simulate and list all available thumbnail formats')\n\n    postproc = optparse.OptionGroup(parser, 'Post-processing Options')\n    postproc.add_option(\n        '-x', '--extract-audio',\n        action='store_true', dest='extractaudio', default=False,\n        help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')\n    postproc.add_option(\n        '--audio-format', metavar='FORMAT', dest='audioformat', default='best',\n        help='Specify audio format: \"best\", \"aac\", \"flac\", \"mp3\", \"m4a\", \"opus\", \"vorbis\", or \"wav\"; \"%default\" by default; No effect without -x')\n    postproc.add_option(\n        '--audio-quality', metavar='QUALITY',\n        dest='audioquality', default='5',\n        help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')\n    postproc.add_option(\n        '--recode-video',\n        metavar='FORMAT', dest='recodevideo', default=None,\n        help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')\n    postproc.add_option(\n        '--postprocessor-args',\n        dest='postprocessor_args', metavar='ARGS',\n        help='Give these arguments to the postprocessor')\n    postproc.add_option(\n        '-k', '--keep-video',\n        action='store_true', dest='keepvideo', default=False,\n        help='Keep the video file on disk after the post-processing; the video is erased by default')\n    postproc.add_option(\n        '--no-post-overwrites',\n        action='store_true', dest='nopostoverwrites', default=False,\n        help='Do not overwrite post-processed files; the post-processed files are overwritten by default')\n    postproc.add_option(\n        '--embed-subs',\n        action='store_true', dest='embedsubtitles', default=False,\n        help='Embed subtitles in the video (only for mp4, webm and mkv videos)')\n    postproc.add_option(\n        '--embed-thumbnail',\n        action='store_true', dest='embedthumbnail', default=False,\n        help='Embed thumbnail in the audio as cover art')\n    postproc.add_option(\n        '--add-metadata',\n        action='store_true', dest='addmetadata', default=False,\n        help='Write metadata to the video file')\n    postproc.add_option(\n        '--metadata-from-title',\n        metavar='FORMAT', dest='metafromtitle',\n        help='Parse additional metadata like song title / artist from the video title. '\n             'The format syntax is the same as --output. Regular expression with '\n             'named capture groups may also be used. '\n             'The parsed parameters replace existing values. '\n             'Example: --metadata-from-title \"%(artist)s - %(title)s\" matches a title like '\n             '\"Coldplay - Paradise\". '\n             'Example (regex): --metadata-from-title \"(?P<artist>.+?) - (?P<title>.+)\"')\n    postproc.add_option(\n        '--xattrs',\n        action='store_true', dest='xattrs', default=False,\n        help='Write metadata to the video file\\'s xattrs (using dublin core and xdg standards)')\n    postproc.add_option(\n        '--fixup',\n        metavar='POLICY', dest='fixup', default='detect_or_warn',\n        help='Automatically correct known faults of the file. '\n             'One of never (do nothing), warn (only emit a warning), '\n             'detect_or_warn (the default; fix file if we can, warn otherwise)')\n    postproc.add_option(\n        '--prefer-avconv',\n        action='store_false', dest='prefer_ffmpeg',\n        help='Prefer avconv over ffmpeg for running the postprocessors')\n    postproc.add_option(\n        '--prefer-ffmpeg',\n        action='store_true', dest='prefer_ffmpeg',\n        help='Prefer ffmpeg over avconv for running the postprocessors (default)')\n    postproc.add_option(\n        '--ffmpeg-location', '--avconv-location', metavar='PATH',\n        dest='ffmpeg_location',\n        help='Location of the ffmpeg/avconv binary; either the path to the binary or its containing directory.')\n    postproc.add_option(\n        '--exec',\n        metavar='CMD', dest='exec_cmd',\n        help='Execute a command on the file after downloading and post-processing, similar to find\\'s -exec syntax. Example: --exec \\'adb push {} /sdcard/Music/ && rm {}\\'')\n    postproc.add_option(\n        '--convert-subs', '--convert-subtitles',\n        metavar='FORMAT', dest='convertsubtitles', default=None,\n        help='Convert the subtitles to other format (currently supported: srt|ass|vtt|lrc)')\n\n    parser.add_option_group(general)\n    parser.add_option_group(network)\n    parser.add_option_group(geo)\n    parser.add_option_group(selection)\n    parser.add_option_group(downloader)\n    parser.add_option_group(filesystem)\n    parser.add_option_group(thumbnail)\n    parser.add_option_group(verbosity)\n    parser.add_option_group(workarounds)\n    parser.add_option_group(video_format)\n    parser.add_option_group(subtitles)\n    parser.add_option_group(authentication)\n    parser.add_option_group(adobe_pass)\n    parser.add_option_group(postproc)\n\n    if overrideArguments is not None:\n        opts, args = parser.parse_args(overrideArguments)\n        if opts.verbose:\n            write_string('[debug] Override config: ' + repr(overrideArguments) + '\\n')\n    else:\n        def compat_conf(conf):\n            if sys.version_info < (3,):\n                return [a.decode(preferredencoding(), 'replace') for a in conf]\n            return conf\n\n        command_line_conf = compat_conf(sys.argv[1:])\n        opts, args = parser.parse_args(command_line_conf)\n\n        system_conf = user_conf = custom_conf = []\n\n        if '--config-location' in command_line_conf:\n            location = compat_expanduser(opts.config_location)\n            if os.path.isdir(location):\n                location = os.path.join(location, 'youtube-dl.conf')\n            if not os.path.exists(location):\n                parser.error('config-location %s does not exist.' % location)\n            custom_conf = _readOptions(location)\n        elif '--ignore-config' in command_line_conf:\n            pass\n        else:\n            system_conf = _readOptions('/etc/youtube-dl.conf')\n            if '--ignore-config' not in system_conf:\n                user_conf = _readUserConf()\n\n        argv = system_conf + user_conf + custom_conf + command_line_conf\n        opts, args = parser.parse_args(argv)\n        if opts.verbose:\n            for conf_label, conf in (\n                    ('System config', system_conf),\n                    ('User config', user_conf),\n                    ('Custom config', custom_conf),\n                    ('Command-line args', command_line_conf)):\n                write_string('[debug] %s: %s\\n' % (conf_label, repr(_hide_login_info(conf))))\n\n    return parser, opts, args",
        "begin_line": 41,
        "end_line": 916,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018083182640144665,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP.__init__#9",
        "src_path": "youtube_dl/postprocessor/metadatafromtitle.py",
        "class_name": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP",
        "signature": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP.__init__(self, downloader, titleformat)",
        "snippet": "    def __init__(self, downloader, titleformat):\n        super(MetadataFromTitlePP, self).__init__(downloader)\n        self._titleformat = titleformat\n        self._titleregex = (self.format_to_regex(titleformat)\n                            if re.search(r'%\\(\\w+\\)s', titleformat)\n                            else titleformat)",
        "begin_line": 9,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP.format_to_regex#16",
        "src_path": "youtube_dl/postprocessor/metadatafromtitle.py",
        "class_name": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP",
        "signature": "youtube_dl.postprocessor.metadatafromtitle.MetadataFromTitlePP.format_to_regex(self, fmt)",
        "snippet": "    def format_to_regex(self, fmt):\n        r\"\"\"\n        Converts a string like\n           '%(title)s - %(artist)s'\n        to a regex like\n           '(?P<title>.+)\\ \\-\\ (?P<artist>.+)'\n        \"\"\"\n        lastpos = 0\n        regex = ''\n        # replace %(..)s with regex group and escape other string parts\n        for match in re.finditer(r'%\\((\\w+)\\)s', fmt):\n            regex += re.escape(fmt[lastpos:match.start()])\n            regex += r'(?P<' + match.group(1) + '>.+)'\n            lastpos = match.end()\n        if lastpos < len(fmt):\n            regex += re.escape(fmt[lastpos:])\n        return regex",
        "begin_line": 16,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.bokecc.BokeCCBaseIE._extract_bokecc_formats#12",
        "src_path": "youtube_dl/extractor/bokecc.py",
        "class_name": "youtube_dl.extractor.bokecc.BokeCCBaseIE",
        "signature": "youtube_dl.extractor.bokecc.BokeCCBaseIE._extract_bokecc_formats(self, webpage, video_id, format_id=None)",
        "snippet": "    def _extract_bokecc_formats(self, webpage, video_id, format_id=None):\n        player_params_str = self._html_search_regex(\n            r'<(?:script|embed)[^>]+src=(?P<q>[\"\\'])(?:https?:)?//p\\.bokecc\\.com/(?:player|flash/player\\.swf)\\?(?P<query>.+?)(?P=q)',\n            webpage, 'player params', group='query')\n\n        player_params = compat_parse_qs(player_params_str)\n\n        info_xml = self._download_xml(\n            'http://p.bokecc.com/servlet/playinfo?uid=%s&vid=%s&m=1' % (\n                player_params['siteid'][0], player_params['vid'][0]), video_id)\n\n        formats = [{\n            'format_id': format_id,\n            'url': quality.find('./copy').attrib['playurl'],\n            'preference': int(quality.attrib['value']),\n        } for quality in info_xml.findall('./video/quality')]\n\n        self._sort_formats(formats)\n\n        return formats",
        "begin_line": 12,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00036469730123997083,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.00036469730123997083,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.extractor.tunein.TuneInBaseIE._extract_urls#15",
        "src_path": "youtube_dl/extractor/tunein.py",
        "class_name": "youtube_dl.extractor.tunein.TuneInBaseIE",
        "signature": "youtube_dl.extractor.tunein.TuneInBaseIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        return re.findall(\n            r'<iframe[^>]+src=[\"\\'](?P<url>(?:https?://)?tunein\\.com/embed/player/[pst]\\d+)',\n            webpage)",
        "begin_line": 15,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.tunein.TuneInStationIE.suitable#95",
        "src_path": "youtube_dl/extractor/tunein.py",
        "class_name": "youtube_dl.extractor.tunein.TuneInStationIE",
        "signature": "youtube_dl.extractor.tunein.TuneInStationIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if TuneInClipIE.suitable(url) else super(TuneInStationIE, cls).suitable(url)",
        "begin_line": 95,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.aenetworks.AENetworksIE._real_extract#140",
        "src_path": "youtube_dl/extractor/aenetworks.py",
        "class_name": "youtube_dl.extractor.aenetworks.AENetworksIE",
        "signature": "youtube_dl.extractor.aenetworks.AENetworksIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        domain, show_path, movie_display_id, special_display_id, collection_display_id = re.match(self._VALID_URL, url).groups()\n        display_id = show_path or movie_display_id or special_display_id or collection_display_id\n        webpage = self._download_webpage(url, display_id, headers=self.geo_verification_headers())\n        if show_path:\n            url_parts = show_path.split('/')\n            url_parts_len = len(url_parts)\n            if url_parts_len == 1:\n                entries = []\n                for season_url_path in re.findall(r'(?s)<li[^>]+data-href=\"(/shows/%s/season-\\d+)\"' % url_parts[0], webpage):\n                    entries.append(self.url_result(\n                        compat_urlparse.urljoin(url, season_url_path), 'AENetworks'))\n                if entries:\n                    return self.playlist_result(\n                        entries, self._html_search_meta('aetn:SeriesId', webpage),\n                        self._html_search_meta('aetn:SeriesTitle', webpage))\n                else:\n                    # single season\n                    url_parts_len = 2\n            if url_parts_len == 2:\n                entries = []\n                for episode_item in re.findall(r'(?s)<[^>]+class=\"[^\"]*(?:episode|program)-item[^\"]*\"[^>]*>', webpage):\n                    episode_attributes = extract_attributes(episode_item)\n                    episode_url = compat_urlparse.urljoin(\n                        url, episode_attributes['data-canonical'])\n                    entries.append(self.url_result(\n                        episode_url, 'AENetworks',\n                        episode_attributes.get('data-videoid') or episode_attributes.get('data-video-id')))\n                return self.playlist_result(\n                    entries, self._html_search_meta('aetn:SeasonId', webpage))\n\n        video_id = self._html_search_meta('aetn:VideoID', webpage)\n        media_url = self._search_regex(\n            [r\"media_url\\s*=\\s*'(?P<url>[^']+)'\",\n             r'data-media-url=(?P<url>(?:https?:)?//[^\\s>]+)',\n             r'data-media-url=([\"\\'])(?P<url>(?:(?!\\1).)+?)\\1'],\n            webpage, 'video url', group='url')\n        theplatform_metadata = self._download_theplatform_metadata(self._search_regex(\n            r'https?://link\\.theplatform\\.com/s/([^?]+)', media_url, 'theplatform_path'), video_id)\n        info = self._parse_theplatform_metadata(theplatform_metadata)\n        auth = None\n        if theplatform_metadata.get('AETN$isBehindWall'):\n            requestor_id = self._DOMAIN_TO_REQUESTOR_ID[domain]\n            resource = self._get_mvpd_resource(\n                requestor_id, theplatform_metadata['title'],\n                theplatform_metadata.get('AETN$PPL_pplProgramId') or theplatform_metadata.get('AETN$PPL_pplProgramId_OLD'),\n                theplatform_metadata['ratings'][0]['rating'])\n            auth = self._extract_mvpd_auth(\n                url, video_id, requestor_id, resource)\n        info.update(self._search_json_ld(webpage, video_id, fatal=False))\n        info.update(self._extract_aen_smil(media_url, video_id, auth))\n        return info",
        "begin_line": 140,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0027548209366391185,
            "pseudo_dstar_susp": 0.003215434083601286,
            "pseudo_tarantula_susp": 0.000877963125548727,
            "pseudo_op2_susp": 0.003215434083601286,
            "pseudo_barinel_susp": 0.000877963125548727
        }
    },
    {
        "name": "youtube_dl.postprocessor.common.PostProcessor.__init__#34",
        "src_path": "youtube_dl/postprocessor/common.py",
        "class_name": "youtube_dl.postprocessor.common.PostProcessor",
        "signature": "youtube_dl.postprocessor.common.PostProcessor.__init__(self, downloader=None)",
        "snippet": "    def __init__(self, downloader=None):\n        self._downloader = downloader",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.postprocessor.common.PostProcessor.set_downloader#37",
        "src_path": "youtube_dl/postprocessor/common.py",
        "class_name": "youtube_dl.postprocessor.common.PostProcessor",
        "signature": "youtube_dl.postprocessor.common.PostProcessor.set_downloader(self, downloader)",
        "snippet": "    def set_downloader(self, downloader):\n        \"\"\"Sets the downloader for this PP.\"\"\"\n        self._downloader = downloader",
        "begin_line": 37,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mangomolo.MangomoloBaseIE._get_real_id#15",
        "src_path": "youtube_dl/extractor/mangomolo.py",
        "class_name": "youtube_dl.extractor.mangomolo.MangomoloBaseIE",
        "signature": "youtube_dl.extractor.mangomolo.MangomoloBaseIE._get_real_id(self, page_id)",
        "snippet": "    def _get_real_id(self, page_id):\n        return page_id",
        "begin_line": 15,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007849293563579278,
            "pseudo_dstar_susp": 0.0007942811755361397,
            "pseudo_tarantula_susp": 0.0015151515151515152,
            "pseudo_op2_susp": 0.0007942811755361397,
            "pseudo_barinel_susp": 0.0015151515151515152
        }
    },
    {
        "name": "youtube_dl.extractor.mangomolo.MangomoloBaseIE._real_extract#18",
        "src_path": "youtube_dl/extractor/mangomolo.py",
        "class_name": "youtube_dl.extractor.mangomolo.MangomoloBaseIE",
        "signature": "youtube_dl.extractor.mangomolo.MangomoloBaseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        page_id = self._get_real_id(self._match_id(url))\n        webpage = self._download_webpage(\n            'https://player.mangomolo.com/v1/%s?%s' % (self._TYPE, url.split('?')[1]), page_id)\n        hidden_inputs = self._hidden_inputs(webpage)\n        m3u8_entry_protocol = 'm3u8' if self._IS_LIVE else 'm3u8_native'\n\n        format_url = self._html_search_regex(\n            [\n                r'(?:file|src)\\s*:\\s*\"(https?://[^\"]+?/playlist\\.m3u8)',\n                r'<a[^>]+href=\"(rtsp://[^\"]+)\"'\n            ], webpage, 'format url')\n        formats = self._extract_wowza_formats(\n            format_url, page_id, m3u8_entry_protocol, ['smil'])\n        self._sort_formats(formats)\n\n        return {\n            'id': page_id,\n            'title': self._live_title(page_id) if self._IS_LIVE else page_id,\n            'uploader_id': hidden_inputs.get('userid'),\n            'duration': int_or_none(hidden_inputs.get('duration')),\n            'is_live': self._IS_LIVE,\n            'formats': formats,\n        }",
        "begin_line": 18,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009587727708533077,
            "pseudo_dstar_susp": 0.0009560229445506692,
            "pseudo_tarantula_susp": 0.0009124087591240876,
            "pseudo_op2_susp": 0.0009560229445506692,
            "pseudo_barinel_susp": 0.0009124087591240876
        }
    },
    {
        "name": "youtube_dl.extractor.vlive.VLiveIE.suitable#64",
        "src_path": "youtube_dl/extractor/vlive.py",
        "class_name": "youtube_dl.extractor.vlive.VLiveIE",
        "signature": "youtube_dl.extractor.vlive.VLiveIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if VLivePlaylistIE.suitable(url) else super(VLiveIE, cls).suitable(url)",
        "begin_line": 64,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.r7.R7ArticleIE.suitable#100",
        "src_path": "youtube_dl/extractor/r7.py",
        "class_name": "youtube_dl.extractor.r7.R7ArticleIE",
        "signature": "youtube_dl.extractor.r7.R7ArticleIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if R7IE.suitable(url) else super(R7ArticleIE, cls).suitable(url)",
        "begin_line": 100,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.acast.ACastIE._real_extract#55",
        "src_path": "youtube_dl/extractor/acast.py",
        "class_name": "youtube_dl.extractor.acast.ACastIE",
        "signature": "youtube_dl.extractor.acast.ACastIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        channel, display_id = re.match(self._VALID_URL, url).groups()\n        s = self._download_json(\n            'https://feeder.acast.com/api/v1/shows/%s/episodes/%s' % (channel, display_id),\n            display_id)\n        media_url = s['url']\n        if re.search(r'[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12}', display_id):\n            episode_url = s.get('episodeUrl')\n            if episode_url:\n                display_id = episode_url\n            else:\n                channel, display_id = re.match(self._VALID_URL, s['link']).groups()\n        cast_data = self._download_json(\n            'https://play-api.acast.com/splash/%s/%s' % (channel, display_id),\n            display_id)['result']\n        e = cast_data['episode']\n        title = e.get('name') or s['title']\n        return {\n            'id': compat_str(e['id']),\n            'display_id': display_id,\n            'url': media_url,\n            'title': title,\n            'description': e.get('summary') or clean_html(e.get('description') or s.get('description')),\n            'thumbnail': e.get('image'),\n            'timestamp': unified_timestamp(e.get('publishingDate') or s.get('publishDate')),\n            'duration': float_or_none(e.get('duration') or s.get('duration')),\n            'filesize': int_or_none(e.get('contentLength')),\n            'creator': try_get(cast_data, lambda x: x['show']['author'], compat_str),\n            'series': try_get(cast_data, lambda x: x['show']['name'], compat_str),\n            'season_number': int_or_none(e.get('seasonNumber')),\n            'episode': title,\n            'episode_number': int_or_none(e.get('episodeNumber')),\n        }",
        "begin_line": 55,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.04,
            "pseudo_dstar_susp": 0.037037037037037035,
            "pseudo_tarantula_susp": 0.0008613264427217916,
            "pseudo_op2_susp": 0.037037037037037035,
            "pseudo_barinel_susp": 0.0008613264427217916
        }
    },
    {
        "name": "youtube_dl.extractor.acast.ACastChannelIE.suitable#116",
        "src_path": "youtube_dl/extractor/acast.py",
        "class_name": "youtube_dl.extractor.acast.ACastChannelIE",
        "signature": "youtube_dl.extractor.acast.ACastChannelIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ACastIE.suitable(url) else super(ACastChannelIE, cls).suitable(url)",
        "begin_line": 116,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.aes.aes_ctr_decrypt#11",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_ctr_decrypt(data, key, counter)",
        "snippet": "def aes_ctr_decrypt(data, key, counter):\n    \"\"\"\n    Decrypt with aes in counter mode\n\n    @param {int[]} data        cipher\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {instance} counter  Instance whose next_value function (@returns {int[]}  16-Byte block)\n                               returns the next counter block\n    @returns {int[]}           decrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    decrypted_data = []\n    for i in range(block_count):\n        counter_block = counter.next_value()\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block += [0] * (BLOCK_SIZE_BYTES - len(block))\n\n        cipher_counter_block = aes_encrypt(counter_block, expanded_key)\n        decrypted_data += xor(block, cipher_counter_block)\n    decrypted_data = decrypted_data[:len(data)]\n\n    return decrypted_data",
        "begin_line": 11,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.aes_cbc_decrypt#37",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_cbc_decrypt(data, key, iv)",
        "snippet": "def aes_cbc_decrypt(data, key, iv):\n    \"\"\"\n    Decrypt with aes in CBC mode\n\n    @param {int[]} data        cipher\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {int[]} iv          16-Byte IV\n    @returns {int[]}           decrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    decrypted_data = []\n    previous_cipher_block = iv\n    for i in range(block_count):\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block += [0] * (BLOCK_SIZE_BYTES - len(block))\n\n        decrypted_block = aes_decrypt(block, expanded_key)\n        decrypted_data += xor(decrypted_block, previous_cipher_block)\n        previous_cipher_block = block\n    decrypted_data = decrypted_data[:len(data)]\n\n    return decrypted_data",
        "begin_line": 37,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.aes_cbc_encrypt#63",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_cbc_encrypt(data, key, iv)",
        "snippet": "def aes_cbc_encrypt(data, key, iv):\n    \"\"\"\n    Encrypt with aes in CBC mode. Using PKCS#7 padding\n\n    @param {int[]} data        cleartext\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {int[]} iv          16-Byte IV\n    @returns {int[]}           encrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    encrypted_data = []\n    previous_cipher_block = iv\n    for i in range(block_count):\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        remaining_length = BLOCK_SIZE_BYTES - len(block)\n        block += [remaining_length] * remaining_length\n        mixed_block = xor(block, previous_cipher_block)\n\n        encrypted_block = aes_encrypt(mixed_block, expanded_key)\n        encrypted_data += encrypted_block\n\n        previous_cipher_block = encrypted_block\n\n    return encrypted_data",
        "begin_line": 63,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.key_expansion#91",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.key_expansion(data)",
        "snippet": "def key_expansion(data):\n    \"\"\"\n    Generate key schedule\n\n    @param {int[]} data  16/24/32-Byte cipher key\n    @returns {int[]}     176/208/240-Byte expanded key\n    \"\"\"\n    data = data[:]  # copy\n    rcon_iteration = 1\n    key_size_bytes = len(data)\n    expanded_key_size_bytes = (key_size_bytes // 4 + 7) * BLOCK_SIZE_BYTES\n\n    while len(data) < expanded_key_size_bytes:\n        temp = data[-4:]\n        temp = key_schedule_core(temp, rcon_iteration)\n        rcon_iteration += 1\n        data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        for _ in range(3):\n            temp = data[-4:]\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        if key_size_bytes == 32:\n            temp = data[-4:]\n            temp = sub_bytes(temp)\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        for _ in range(3 if key_size_bytes == 32 else 2 if key_size_bytes == 24 else 0):\n            temp = data[-4:]\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n    data = data[:expanded_key_size_bytes]\n\n    return data",
        "begin_line": 91,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.aes_encrypt#126",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_encrypt(data, expanded_key)",
        "snippet": "def aes_encrypt(data, expanded_key):\n    \"\"\"\n    Encrypt one block with aes\n\n    @param {int[]} data          16-Byte state\n    @param {int[]} expanded_key  176/208/240-Byte expanded key\n    @returns {int[]}             16-Byte cipher\n    \"\"\"\n    rounds = len(expanded_key) // BLOCK_SIZE_BYTES - 1\n\n    data = xor(data, expanded_key[:BLOCK_SIZE_BYTES])\n    for i in range(1, rounds + 1):\n        data = sub_bytes(data)\n        data = shift_rows(data)\n        if i != rounds:\n            data = mix_columns(data)\n        data = xor(data, expanded_key[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES])\n\n    return data",
        "begin_line": 126,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.aes_decrypt#147",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_decrypt(data, expanded_key)",
        "snippet": "def aes_decrypt(data, expanded_key):\n    \"\"\"\n    Decrypt one block with aes\n\n    @param {int[]} data          16-Byte cipher\n    @param {int[]} expanded_key  176/208/240-Byte expanded key\n    @returns {int[]}             16-Byte state\n    \"\"\"\n    rounds = len(expanded_key) // BLOCK_SIZE_BYTES - 1\n\n    for i in range(rounds, 0, -1):\n        data = xor(data, expanded_key[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES])\n        if i != rounds:\n            data = mix_columns_inv(data)\n        data = shift_rows_inv(data)\n        data = sub_bytes_inv(data)\n    data = xor(data, expanded_key[:BLOCK_SIZE_BYTES])\n\n    return data",
        "begin_line": 147,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.aes_decrypt_text#168",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.aes_decrypt_text(data, password, key_size_bytes)",
        "snippet": "def aes_decrypt_text(data, password, key_size_bytes):\n    \"\"\"\n    Decrypt text\n    - The first 8 Bytes of decoded 'data' are the 8 high Bytes of the counter\n    - The cipher key is retrieved by encrypting the first 16 Byte of 'password'\n      with the first 'key_size_bytes' Bytes from 'password' (if necessary filled with 0's)\n    - Mode of operation is 'counter'\n\n    @param {str} data                    Base64 encoded string\n    @param {str,unicode} password        Password (will be encoded with utf-8)\n    @param {int} key_size_bytes          Possible values: 16 for 128-Bit, 24 for 192-Bit or 32 for 256-Bit\n    @returns {str}                       Decrypted data\n    \"\"\"\n    NONCE_LENGTH_BYTES = 8\n\n    data = bytes_to_intlist(compat_b64decode(data))\n    password = bytes_to_intlist(password.encode('utf-8'))\n\n    key = password[:key_size_bytes] + [0] * (key_size_bytes - len(password))\n    key = aes_encrypt(key[:BLOCK_SIZE_BYTES], key_expansion(key)) * (key_size_bytes // BLOCK_SIZE_BYTES)\n\n    nonce = data[:NONCE_LENGTH_BYTES]\n    cipher = data[NONCE_LENGTH_BYTES:]\n\n    class Counter(object):\n        __value = nonce + [0] * (BLOCK_SIZE_BYTES - NONCE_LENGTH_BYTES)\n\n        def next_value(self):\n            temp = self.__value\n            self.__value = inc(self.__value)\n            return temp\n\n    decrypted_data = aes_ctr_decrypt(cipher, key, Counter())\n    plaintext = intlist_to_bytes(decrypted_data)\n\n    return plaintext",
        "begin_line": 168,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.Counter.aes_decrypt_text#168",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes.Counter",
        "signature": "youtube_dl.aes.Counter.aes_decrypt_text(data, password, key_size_bytes)",
        "snippet": "def aes_decrypt_text(data, password, key_size_bytes):\n    \"\"\"\n    Decrypt text\n    - The first 8 Bytes of decoded 'data' are the 8 high Bytes of the counter\n    - The cipher key is retrieved by encrypting the first 16 Byte of 'password'\n      with the first 'key_size_bytes' Bytes from 'password' (if necessary filled with 0's)\n    - Mode of operation is 'counter'\n\n    @param {str} data                    Base64 encoded string\n    @param {str,unicode} password        Password (will be encoded with utf-8)\n    @param {int} key_size_bytes          Possible values: 16 for 128-Bit, 24 for 192-Bit or 32 for 256-Bit\n    @returns {str}                       Decrypted data\n    \"\"\"\n    NONCE_LENGTH_BYTES = 8\n\n    data = bytes_to_intlist(compat_b64decode(data))\n    password = bytes_to_intlist(password.encode('utf-8'))\n\n    key = password[:key_size_bytes] + [0] * (key_size_bytes - len(password))\n    key = aes_encrypt(key[:BLOCK_SIZE_BYTES], key_expansion(key)) * (key_size_bytes // BLOCK_SIZE_BYTES)\n\n    nonce = data[:NONCE_LENGTH_BYTES]\n    cipher = data[NONCE_LENGTH_BYTES:]\n\n    class Counter(object):\n        __value = nonce + [0] * (BLOCK_SIZE_BYTES - NONCE_LENGTH_BYTES)\n\n        def next_value(self):\n            temp = self.__value\n            self.__value = inc(self.__value)\n            return temp\n\n    decrypted_data = aes_ctr_decrypt(cipher, key, Counter())\n    plaintext = intlist_to_bytes(decrypted_data)\n\n    return plaintext",
        "begin_line": 168,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.Counter.next_value#195",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes.Counter",
        "signature": "youtube_dl.aes.Counter.next_value(self)",
        "snippet": "        def next_value(self):\n            temp = self.__value\n            self.__value = inc(self.__value)\n            return temp",
        "begin_line": 195,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.sub_bytes#281",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.sub_bytes(data)",
        "snippet": "def sub_bytes(data):\n    return [SBOX[x] for x in data]",
        "begin_line": 281,
        "end_line": 282,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.sub_bytes_inv#285",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.sub_bytes_inv(data)",
        "snippet": "def sub_bytes_inv(data):\n    return [SBOX_INV[x] for x in data]",
        "begin_line": 285,
        "end_line": 286,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.rotate#289",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.rotate(data)",
        "snippet": "def rotate(data):\n    return data[1:] + [data[0]]",
        "begin_line": 289,
        "end_line": 290,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.key_schedule_core#293",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.key_schedule_core(data, rcon_iteration)",
        "snippet": "def key_schedule_core(data, rcon_iteration):\n    data = rotate(data)\n    data = sub_bytes(data)\n    data[0] = data[0] ^ RCON[rcon_iteration]\n\n    return data",
        "begin_line": 293,
        "end_line": 298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.xor#301",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.xor(data1, data2)",
        "snippet": "def xor(data1, data2):\n    return [x ^ y for x, y in zip(data1, data2)]",
        "begin_line": 301,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.rijndael_mul#305",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.rijndael_mul(a, b)",
        "snippet": "def rijndael_mul(a, b):\n    if(a == 0 or b == 0):\n        return 0\n    return RIJNDAEL_EXP_TABLE[(RIJNDAEL_LOG_TABLE[a] + RIJNDAEL_LOG_TABLE[b]) % 0xFF]",
        "begin_line": 305,
        "end_line": 308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.mix_column#311",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.mix_column(data, matrix)",
        "snippet": "def mix_column(data, matrix):\n    data_mixed = []\n    for row in range(4):\n        mixed = 0\n        for column in range(4):\n            # xor is (+) and (-)\n            mixed ^= rijndael_mul(data[column], matrix[row][column])\n        data_mixed.append(mixed)\n    return data_mixed",
        "begin_line": 311,
        "end_line": 319,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.mix_columns#322",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.mix_columns(data, matrix=MIX_COLUMN_MATRIX)",
        "snippet": "def mix_columns(data, matrix=MIX_COLUMN_MATRIX):\n    data_mixed = []\n    for i in range(4):\n        column = data[i * 4: (i + 1) * 4]\n        data_mixed += mix_column(column, matrix)\n    return data_mixed",
        "begin_line": 322,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.mix_columns_inv#330",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.mix_columns_inv(data)",
        "snippet": "def mix_columns_inv(data):\n    return mix_columns(data, MIX_COLUMN_MATRIX_INV)",
        "begin_line": 330,
        "end_line": 331,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.shift_rows#334",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.shift_rows(data)",
        "snippet": "def shift_rows(data):\n    data_shifted = []\n    for column in range(4):\n        for row in range(4):\n            data_shifted.append(data[((column + row) & 0b11) * 4 + row])\n    return data_shifted",
        "begin_line": 334,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.shift_rows_inv#342",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.shift_rows_inv(data)",
        "snippet": "def shift_rows_inv(data):\n    data_shifted = []\n    for column in range(4):\n        for row in range(4):\n            data_shifted.append(data[((column - row) & 0b11) * 4 + row])\n    return data_shifted",
        "begin_line": 342,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.aes.inc#350",
        "src_path": "youtube_dl/aes.py",
        "class_name": "youtube_dl.aes",
        "signature": "youtube_dl.aes.inc(data)",
        "snippet": "def inc(data):\n    data = data[:]  # copy\n    for i in range(len(data) - 1, -1, -1):\n        if data[i] == 255:\n            data[i] = 0\n        else:\n            data[i] = data[i] + 1\n            break\n    return data",
        "begin_line": 350,
        "end_line": 358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.lovehomeporn.LoveHomePornIE._real_extract#25",
        "src_path": "youtube_dl/extractor/lovehomeporn.py",
        "class_name": "youtube_dl.extractor.lovehomeporn.LoveHomePornIE",
        "signature": "youtube_dl.extractor.lovehomeporn.LoveHomePornIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        display_id = mobj.group('display_id')\n\n        info = self._extract_nuevo(\n            'http://lovehomeporn.com/media/nuevo/config.php?key=%s' % video_id,\n            video_id)\n        info.update({\n            'display_id': display_id,\n            'age_limit': 18\n        })\n        return info",
        "begin_line": 25,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011335298118340512,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.bandcamp.BandcampAlbumIE.suitable#291",
        "src_path": "youtube_dl/extractor/bandcamp.py",
        "class_name": "youtube_dl.extractor.bandcamp.BandcampAlbumIE",
        "signature": "youtube_dl.extractor.bandcamp.BandcampAlbumIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False\n                if BandcampWeeklyIE.suitable(url) or BandcampIE.suitable(url)\n                else super(BandcampAlbumIE, cls).suitable(url))",
        "begin_line": 291,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.bandcamp.BandcampWeeklyIE._real_extract#354",
        "src_path": "youtube_dl/extractor/bandcamp.py",
        "class_name": "youtube_dl.extractor.bandcamp.BandcampWeeklyIE",
        "signature": "youtube_dl.extractor.bandcamp.BandcampWeeklyIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        blob = self._parse_json(\n            self._search_regex(\n                r'data-blob=([\"\\'])(?P<blob>{.+?})\\1', webpage,\n                'blob', group='blob'),\n            video_id, transform_source=unescapeHTML)\n\n        show = blob['bcw_show']\n\n        # This is desired because any invalid show id redirects to `bandcamp.com`\n        # which happens to expose the latest Bandcamp Weekly episode.\n        show_id = int_or_none(show.get('show_id')) or int_or_none(video_id)\n\n        formats = []\n        for format_id, format_url in show['audio_stream'].items():\n            if not url_or_none(format_url):\n                continue\n            for known_ext in KNOWN_EXTENSIONS:\n                if known_ext in format_id:\n                    ext = known_ext\n                    break\n            else:\n                ext = None\n            formats.append({\n                'format_id': format_id,\n                'url': format_url,\n                'ext': ext,\n                'vcodec': 'none',\n            })\n        self._sort_formats(formats)\n\n        title = show.get('audio_title') or 'Bandcamp Weekly'\n        subtitle = show.get('subtitle')\n        if subtitle:\n            title += ' - %s' % subtitle\n\n        episode_number = None\n        seq = blob.get('bcw_seq')\n\n        if seq and isinstance(seq, list):\n            try:\n                episode_number = next(\n                    int_or_none(e.get('episode_number'))\n                    for e in seq\n                    if isinstance(e, dict) and int_or_none(e.get('id')) == show_id)\n            except StopIteration:\n                pass\n\n        return {\n            'id': video_id,\n            'title': title,\n            'description': show.get('desc') or show.get('short_desc'),\n            'duration': float_or_none(show.get('audio_duration')),\n            'is_live': False,\n            'release_date': unified_strdate(show.get('published_date')),\n            'series': 'Bandcamp Weekly',\n            'episode': show.get('subtitle'),\n            'episode_number': episode_number,\n            'episode_id': compat_str(video_id),\n            'formats': formats\n        }",
        "begin_line": 354,
        "end_line": 417,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.tvnow.TVNowIE.suitable#120",
        "src_path": "youtube_dl/extractor/tvnow.py",
        "class_name": "youtube_dl.extractor.tvnow.TVNowIE",
        "signature": "youtube_dl.extractor.tvnow.TVNowIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if TVNowNewIE.suitable(url) or TVNowSeasonIE.suitable(url) or TVNowAnnualIE.suitable(url) or TVNowShowIE.suitable(url)\n                else super(TVNowIE, cls).suitable(url))",
        "begin_line": 120,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.tvnow.TVNowListBaseIE.suitable#361",
        "src_path": "youtube_dl/extractor/tvnow.py",
        "class_name": "youtube_dl.extractor.tvnow.TVNowListBaseIE",
        "signature": "youtube_dl.extractor.tvnow.TVNowListBaseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if TVNowNewIE.suitable(url)\n                else super(TVNowListBaseIE, cls).suitable(url))",
        "begin_line": 361,
        "end_line": 363,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.tvnow.TVNowShowIE.suitable#440",
        "src_path": "youtube_dl/extractor/tvnow.py",
        "class_name": "youtube_dl.extractor.tvnow.TVNowShowIE",
        "signature": "youtube_dl.extractor.tvnow.TVNowShowIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if TVNowNewIE.suitable(url) or TVNowSeasonIE.suitable(url) or TVNowAnnualIE.suitable(url)\n                else super(TVNowShowIE, cls).suitable(url))",
        "begin_line": 440,
        "end_line": 442,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020833333333333333,
            "pseudo_dstar_susp": 0.0019455252918287938,
            "pseudo_tarantula_susp": 0.0008756567425569177,
            "pseudo_op2_susp": 0.0019455252918287938,
            "pseudo_barinel_susp": 0.0008756567425569177
        }
    },
    {
        "name": "youtube_dl.extractor.tvplay.ViafreeIE.suitable#377",
        "src_path": "youtube_dl/extractor/tvplay.py",
        "class_name": "youtube_dl.extractor.tvplay.ViafreeIE",
        "signature": "youtube_dl.extractor.tvplay.ViafreeIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if TVPlayIE.suitable(url) else super(ViafreeIE, cls).suitable(url)",
        "begin_line": 377,
        "end_line": 378,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.viewlift.ViewLiftIE.suitable#228",
        "src_path": "youtube_dl/extractor/viewlift.py",
        "class_name": "youtube_dl.extractor.viewlift.ViewLiftIE",
        "signature": "youtube_dl.extractor.viewlift.ViewLiftIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ViewLiftEmbedIE.suitable(url) else super(ViewLiftIE, cls).suitable(url)",
        "begin_line": 228,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudIE._extract_info_dict#318",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudIE._extract_info_dict(self, info, full_title=None, secret_token=None)",
        "snippet": "    def _extract_info_dict(self, info, full_title=None, secret_token=None):\n        track_id = compat_str(info['id'])\n        title = info['title']\n\n        format_urls = set()\n        formats = []\n        query = {'client_id': self._CLIENT_ID}\n        if secret_token:\n            query['secret_token'] = secret_token\n\n        if info.get('downloadable') and info.get('has_downloads_left'):\n            download_url = update_url_query(\n                self._API_V2_BASE + 'tracks/' + track_id + '/download', query)\n            redirect_url = (self._download_json(download_url, track_id, fatal=False) or {}).get('redirectUri')\n            if redirect_url:\n                urlh = self._request_webpage(\n                    HEADRequest(redirect_url), track_id, fatal=False)\n                if urlh:\n                    format_url = urlh.geturl()\n                    format_urls.add(format_url)\n                    formats.append({\n                        'format_id': 'download',\n                        'ext': urlhandle_detect_ext(urlh) or 'mp3',\n                        'filesize': int_or_none(urlh.headers.get('Content-Length')),\n                        'url': format_url,\n                        'preference': 10,\n                    })\n\n        def invalid_url(url):\n            return not url or url in format_urls\n\n        def add_format(f, protocol, is_preview=False):\n            mobj = re.search(r'\\.(?P<abr>\\d+)\\.(?P<ext>[0-9a-z]{3,4})(?=[/?])', stream_url)\n            if mobj:\n                for k, v in mobj.groupdict().items():\n                    if not f.get(k):\n                        f[k] = v\n            format_id_list = []\n            if protocol:\n                format_id_list.append(protocol)\n            ext = f.get('ext')\n            if ext == 'aac':\n                f['abr'] = '256'\n            for k in ('ext', 'abr'):\n                v = f.get(k)\n                if v:\n                    format_id_list.append(v)\n            preview = is_preview or re.search(r'/(?:preview|playlist)/0/30/', f['url'])\n            if preview:\n                format_id_list.append('preview')\n            abr = f.get('abr')\n            if abr:\n                f['abr'] = int(abr)\n            if protocol == 'hls':\n                protocol = 'm3u8' if ext == 'aac' else 'm3u8_native'\n            else:\n                protocol = 'http'\n            f.update({\n                'format_id': '_'.join(format_id_list),\n                'protocol': protocol,\n                'preference': -10 if preview else None,\n            })\n            formats.append(f)\n\n        # New API\n        transcodings = try_get(\n            info, lambda x: x['media']['transcodings'], list) or []\n        for t in transcodings:\n            if not isinstance(t, dict):\n                continue\n            format_url = url_or_none(t.get('url'))\n            if not format_url:\n                continue\n            stream = self._download_json(\n                format_url, track_id, query=query, fatal=False)\n            if not isinstance(stream, dict):\n                continue\n            stream_url = url_or_none(stream.get('url'))\n            if invalid_url(stream_url):\n                continue\n            format_urls.add(stream_url)\n            stream_format = t.get('format') or {}\n            protocol = stream_format.get('protocol')\n            if protocol != 'hls' and '/hls' in format_url:\n                protocol = 'hls'\n            ext = None\n            preset = str_or_none(t.get('preset'))\n            if preset:\n                ext = preset.split('_')[0]\n            if ext not in KNOWN_EXTENSIONS:\n                ext = mimetype2ext(stream_format.get('mime_type'))\n            add_format({\n                'url': stream_url,\n                'ext': ext,\n            }, 'http' if protocol == 'progressive' else protocol,\n                t.get('snipped') or '/preview/' in format_url)\n\n        for f in formats:\n            f['vcodec'] = 'none'\n\n        if not formats and info.get('policy') == 'BLOCK':\n            self.raise_geo_restricted()\n        self._sort_formats(formats)\n\n        user = info.get('user') or {}\n\n        thumbnails = []\n        artwork_url = info.get('artwork_url')\n        thumbnail = artwork_url or user.get('avatar_url')\n        if isinstance(thumbnail, compat_str):\n            if re.search(self._IMAGE_REPL_RE, thumbnail):\n                for image_id, size in self._ARTWORK_MAP.items():\n                    i = {\n                        'id': image_id,\n                        'url': re.sub(self._IMAGE_REPL_RE, '-%s.jpg' % image_id, thumbnail),\n                    }\n                    if image_id == 'tiny' and not artwork_url:\n                        size = 18\n                    elif image_id == 'original':\n                        i['preference'] = 10\n                    if size:\n                        i.update({\n                            'width': size,\n                            'height': size,\n                        })\n                    thumbnails.append(i)\n            else:\n                thumbnails = [{'url': thumbnail}]\n\n        def extract_count(key):\n            return int_or_none(info.get('%s_count' % key))\n\n        return {\n            'id': track_id,\n            'uploader': user.get('username'),\n            'uploader_id': str_or_none(user.get('id')) or user.get('permalink'),\n            'uploader_url': user.get('permalink_url'),\n            'timestamp': unified_timestamp(info.get('created_at')),\n            'title': title,\n            'description': info.get('description'),\n            'thumbnails': thumbnails,\n            'duration': float_or_none(info.get('duration'), 1000),\n            'webpage_url': info.get('permalink_url'),\n            'license': info.get('license'),\n            'view_count': extract_count('playback'),\n            'like_count': extract_count('favoritings') or extract_count('likes'),\n            'comment_count': extract_count('comment'),\n            'repost_count': extract_count('reposts'),\n            'genre': info.get('genre'),\n            'formats': formats\n        }",
        "begin_line": 318,
        "end_line": 468,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00033090668431502316,
            "pseudo_tarantula_susp": 0.0002752546105147261,
            "pseudo_op2_susp": 0.00033090668431502316,
            "pseudo_barinel_susp": 0.0002729257641921397
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudTrackStationIE._real_extract#719",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudTrackStationIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudTrackStationIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        track_name = self._match_id(url)\n\n        track = self._download_json(self._resolv_url(url), track_name)\n        track_id = self._search_regex(\n            r'soundcloud:track-stations:(\\d+)', track['id'], 'track id')\n\n        return self._extract_playlist(\n            self._API_V2_BASE + 'stations/%s/tracks' % track['id'],\n            track_id, 'Track station: %s' % track['title'])",
        "begin_line": 719,
        "end_line": 728,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE._real_extract#744",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        playlist_id = mobj.group('id')\n\n        query = {}\n        token = mobj.group('token')\n        if token:\n            query['secret_token'] = token\n\n        data = self._download_json(\n            self._API_V2_BASE + 'playlists/' + playlist_id,\n            playlist_id, 'Downloading playlist', query=query)\n\n        return self._extract_set(data, token)",
        "begin_line": 744,
        "end_line": 757,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudSearchIE._get_collection#776",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudSearchIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudSearchIE._get_collection(self, endpoint, collection_id, **query)",
        "snippet": "    def _get_collection(self, endpoint, collection_id, **query):\n        limit = min(\n            query.get('limit', self._DEFAULT_RESULTS_PER_PAGE),\n            self._MAX_RESULTS_PER_PAGE)\n        query.update({\n            'limit': limit,\n            'linked_partitioning': 1,\n            'offset': 0,\n        })\n        next_url = update_url_query(self._API_V2_BASE + endpoint, query)\n\n        collected_results = 0\n\n        for i in itertools.count(1):\n            response = self._download_json(\n                next_url, collection_id, 'Downloading page {0}'.format(i),\n                'Unable to download API page')\n\n            collection = response.get('collection', [])\n            if not collection:\n                break\n\n            collection = list(filter(bool, collection))\n            collected_results += len(collection)\n\n            for item in collection:\n                yield self.url_result(item['uri'], SoundcloudIE.ie_key())\n\n            if not collection or collected_results >= limit:\n                break\n\n            next_url = response.get('next_href')\n            if not next_url:\n                break",
        "begin_line": 776,
        "end_line": 809,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.adn.ADNIE._real_extract#120",
        "src_path": "youtube_dl/extractor/adn.py",
        "class_name": "youtube_dl.extractor.adn.ADNIE",
        "signature": "youtube_dl.extractor.adn.ADNIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n        player_config = self._parse_json(self._search_regex(\n            r'playerConfig\\s*=\\s*({.+});', webpage,\n            'player config', default='{}'), video_id, fatal=False)\n        if not player_config:\n            config_url = urljoin(self._BASE_URL, self._search_regex(\n                r'(?:id=\"player\"|class=\"[^\"]*adn-player-container[^\"]*\")[^>]+data-url=\"([^\"]+)\"',\n                webpage, 'config url'))\n            player_config = self._download_json(\n                config_url, video_id,\n                'Downloading player config JSON metadata')['player']\n\n        video_info = {}\n        video_info_str = self._search_regex(\n            r'videoInfo\\s*=\\s*({.+});', webpage,\n            'video info', fatal=False)\n        if video_info_str:\n            video_info = self._parse_json(\n                video_info_str, video_id, fatal=False) or {}\n\n        options = player_config.get('options') or {}\n        metas = options.get('metas') or {}\n        links = player_config.get('links') or {}\n        sub_path = player_config.get('subtitles')\n        error = None\n        if not links:\n            links_url = player_config.get('linksurl') or options['videoUrl']\n            token = options['token']\n            self._K = ''.join([random.choice('0123456789abcdef') for _ in range(16)])\n            message = bytes_to_intlist(json.dumps({\n                'k': self._K,\n                'e': 60,\n                't': token,\n            }))\n            padded_message = intlist_to_bytes(pkcs1pad(message, 128))\n            n, e = self._RSA_KEY\n            encrypted_message = long_to_bytes(pow(bytes_to_long(padded_message), e, n))\n            authorization = base64.b64encode(encrypted_message).decode()\n            links_data = self._download_json(\n                urljoin(self._BASE_URL, links_url), video_id,\n                'Downloading links JSON metadata', headers={\n                    'Authorization': 'Bearer ' + authorization,\n                })\n            links = links_data.get('links') or {}\n            metas = metas or links_data.get('meta') or {}\n            sub_path = sub_path or links_data.get('subtitles') or \\\n                'index.php?option=com_vodapi&task=subtitles.getJSON&format=json&id=' + video_id\n            sub_path += '&token=' + token\n            error = links_data.get('error')\n        title = metas.get('title') or video_info['title']\n\n        formats = []\n        for format_id, qualities in links.items():\n            if not isinstance(qualities, dict):\n                continue\n            for quality, load_balancer_url in qualities.items():\n                load_balancer_data = self._download_json(\n                    load_balancer_url, video_id,\n                    'Downloading %s %s JSON metadata' % (format_id, quality),\n                    fatal=False) or {}\n                m3u8_url = load_balancer_data.get('location')\n                if not m3u8_url:\n                    continue\n                m3u8_formats = self._extract_m3u8_formats(\n                    m3u8_url, video_id, 'mp4', 'm3u8_native',\n                    m3u8_id=format_id, fatal=False)\n                if format_id == 'vf':\n                    for f in m3u8_formats:\n                        f['language'] = 'fr'\n                formats.extend(m3u8_formats)\n        if not error:\n            error = options.get('error')\n        if not formats and error:\n            raise ExtractorError('%s said: %s' % (self.IE_NAME, error), expected=True)\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'description': strip_or_none(metas.get('summary') or video_info.get('resume')),\n            'thumbnail': video_info.get('image'),\n            'formats': formats,\n            'subtitles': self.extract_subtitles(sub_path, video_id),\n            'episode': metas.get('subtitle') or video_info.get('videoTitle'),\n            'series': video_info.get('playlistTitle'),\n        }",
        "begin_line": 120,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004608294930875576,
            "pseudo_dstar_susp": 0.021739130434782608,
            "pseudo_tarantula_susp": 0.0009090909090909091,
            "pseudo_op2_susp": 0.021739130434782608,
            "pseudo_barinel_susp": 0.0009090909090909091
        }
    },
    {
        "name": "youtube_dl.extractor.rai.RaiIE._extract_from_content_id#383",
        "src_path": "youtube_dl/extractor/rai.py",
        "class_name": "youtube_dl.extractor.rai.RaiIE",
        "signature": "youtube_dl.extractor.rai.RaiIE._extract_from_content_id(self, content_id, url)",
        "snippet": "    def _extract_from_content_id(self, content_id, url):\n        media = self._download_json(\n            'http://www.rai.tv/dl/RaiTV/programmi/media/ContentItem-%s.html?json' % content_id,\n            content_id, 'Downloading video JSON')\n\n        title = media['name'].strip()\n\n        media_type = media['type']\n        if 'Audio' in media_type:\n            relinker_info = {\n                'formats': [{\n                    'format_id': media.get('formatoAudio'),\n                    'url': media['audioUrl'],\n                    'ext': media.get('formatoAudio'),\n                }]\n            }\n        elif 'Video' in media_type:\n            relinker_info = self._extract_relinker_info(media['mediaUri'], content_id)\n        else:\n            raise ExtractorError('not a media file')\n\n        self._sort_formats(relinker_info['formats'])\n\n        thumbnails = []\n        for image_type in ('image', 'image_medium', 'image_300'):\n            thumbnail_url = media.get(image_type)\n            if thumbnail_url:\n                thumbnails.append({\n                    'url': compat_urlparse.urljoin(url, thumbnail_url),\n                })\n\n        subtitles = self._extract_subtitles(url, media.get('subtitlesUrl'))\n\n        info = {\n            'id': content_id,\n            'title': title,\n            'description': strip_or_none(media.get('desc')),\n            'thumbnails': thumbnails,\n            'uploader': media.get('author'),\n            'upload_date': unified_strdate(media.get('date')),\n            'duration': parse_duration(media.get('length')),\n            'subtitles': subtitles,\n        }\n\n        info.update(relinker_info)\n\n        return info",
        "begin_line": 383,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.zdf.ZDFChannelIE.suitable#261",
        "src_path": "youtube_dl/extractor/zdf.py",
        "class_name": "youtube_dl.extractor.zdf.ZDFChannelIE",
        "signature": "youtube_dl.extractor.zdf.ZDFChannelIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ZDFIE.suitable(url) else super(ZDFChannelIE, cls).suitable(url)",
        "begin_line": 261,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011788282447247436,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.viki.VikiBaseIE._prepare_call#41",
        "src_path": "youtube_dl/extractor/viki.py",
        "class_name": "youtube_dl.extractor.viki.VikiBaseIE",
        "signature": "youtube_dl.extractor.viki.VikiBaseIE._prepare_call(self, path, timestamp=None, post_data=None)",
        "snippet": "    def _prepare_call(self, path, timestamp=None, post_data=None):\n        path += '?' if '?' not in path else '&'\n        if not timestamp:\n            timestamp = int(time.time())\n        query = self._API_QUERY_TEMPLATE % (path, self._APP, timestamp)\n        if self._token:\n            query += '&token=%s' % self._token\n        sig = hmac.new(\n            self._APP_SECRET.encode('ascii'),\n            query.encode('ascii'),\n            hashlib.sha1\n        ).hexdigest()\n        url = self._API_URL_TEMPLATE % (query, sig)\n        return sanitized_Request(\n            url, json.dumps(post_data).encode('utf-8')) if post_data else url",
        "begin_line": 41,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003489183531053733,
            "pseudo_dstar_susp": 0.00034340659340659343,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00034340659340659343,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.viki.VikiBaseIE._call_api#57",
        "src_path": "youtube_dl/extractor/viki.py",
        "class_name": "youtube_dl.extractor.viki.VikiBaseIE",
        "signature": "youtube_dl.extractor.viki.VikiBaseIE._call_api(self, path, video_id, note, timestamp=None, post_data=None)",
        "snippet": "    def _call_api(self, path, video_id, note, timestamp=None, post_data=None):\n        resp = self._download_json(\n            self._prepare_call(path, timestamp, post_data), video_id, note)\n\n        error = resp.get('error')\n        if error:\n            if error == 'invalid timestamp':\n                resp = self._download_json(\n                    self._prepare_call(path, int(resp['current_timestamp']), post_data),\n                    video_id, '%s (retry)' % note)\n                error = resp.get('error')\n            if error:\n                self._raise_error(resp['error'])\n\n        return resp",
        "begin_line": 57,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.viki.VikiBaseIE._login#90",
        "src_path": "youtube_dl/extractor/viki.py",
        "class_name": "youtube_dl.extractor.viki.VikiBaseIE",
        "signature": "youtube_dl.extractor.viki.VikiBaseIE._login(self)",
        "snippet": "    def _login(self):\n        username, password = self._get_login_info()\n        if username is None:\n            return\n\n        login_form = {\n            'login_id': username,\n            'password': password,\n        }\n\n        login = self._call_api(\n            'sessions.json', None,\n            'Logging in', post_data=login_form)\n\n        self._token = login.get('token')\n        if not self._token:\n            self.report_warning('Unable to get session token, login has probably failed')",
        "begin_line": 90,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003489183531053733,
            "pseudo_dstar_susp": 0.00034340659340659343,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00034340659340659343,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.globo.GloboArticleIE.suitable#226",
        "src_path": "youtube_dl/extractor/globo.py",
        "class_name": "youtube_dl.extractor.globo.GloboArticleIE",
        "signature": "youtube_dl.extractor.globo.GloboArticleIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if GloboIE.suitable(url) else super(GloboArticleIE, cls).suitable(url)",
        "begin_line": 226,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.0003971405877680699,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.0003971405877680699,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.espn.ESPNArticleIE.suitable#198",
        "src_path": "youtube_dl/extractor/espn.py",
        "class_name": "youtube_dl.extractor.espn.ESPNArticleIE",
        "signature": "youtube_dl.extractor.espn.ESPNArticleIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ESPNIE.suitable(url) else super(ESPNArticleIE, cls).suitable(url)",
        "begin_line": 198,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.twitter.TwitterAmplifyIE._real_extract#536",
        "src_path": "youtube_dl/extractor/twitter.py",
        "class_name": "youtube_dl.extractor.twitter.TwitterAmplifyIE",
        "signature": "youtube_dl.extractor.twitter.TwitterAmplifyIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        vmap_url = self._html_search_meta(\n            'twitter:amplify:vmap', webpage, 'vmap url')\n        formats = self._extract_formats_from_vmap_url(vmap_url, video_id)\n\n        thumbnails = []\n        thumbnail = self._html_search_meta(\n            'twitter:image:src', webpage, 'thumbnail', fatal=False)\n\n        def _find_dimension(target):\n            w = int_or_none(self._html_search_meta(\n                'twitter:%s:width' % target, webpage, fatal=False))\n            h = int_or_none(self._html_search_meta(\n                'twitter:%s:height' % target, webpage, fatal=False))\n            return w, h\n\n        if thumbnail:\n            thumbnail_w, thumbnail_h = _find_dimension('image')\n            thumbnails.append({\n                'url': thumbnail,\n                'width': thumbnail_w,\n                'height': thumbnail_h,\n            })\n\n        video_w, video_h = _find_dimension('player')\n        formats[0].update({\n            'width': video_w,\n            'height': video_h,\n        })\n\n        return {\n            'id': video_id,\n            'title': 'Twitter Video',\n            'formats': formats,\n            'thumbnails': thumbnails,\n        }",
        "begin_line": 536,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._extract_info#134",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._extract_info(self, webpage)",
        "snippet": "    def _extract_info(self, webpage):\n        info_json = self._search_regex(\n            r'(?s)q\\(\\s*\"\\w+.init\"\\s*,\\s*({.+?})\\)\\s*</script>',\n            webpage, 'info json')\n        return json.loads(info_json)",
        "begin_line": 134,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042716787697565144,
            "pseudo_dstar_susp": 0.00040899795501022495,
            "pseudo_tarantula_susp": 0.0017953321364452424,
            "pseudo_op2_susp": 0.00040899795501022495,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._real_extract#140",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        m = re.match(self._VALID_URL, url, re.VERBOSE)\n        if m.group('type').startswith('embed'):\n            desktop_url = m.group('proto') + 'www' + m.group('urlmain')\n            return self.url_result(desktop_url, 'TED')\n        name = m.group('name')\n        if m.group('type_talk'):\n            return self._talk_info(url, name)\n        elif m.group('type_watch'):\n            return self._watch_info(url, name)\n        else:\n            return self._playlist_videos_info(url, name)",
        "begin_line": 140,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004344048653344917,
            "pseudo_dstar_susp": 0.0004098360655737705,
            "pseudo_tarantula_susp": 0.0029850746268656717,
            "pseudo_op2_susp": 0.0004098360655737705,
            "pseudo_barinel_susp": 0.0029850746268656717
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._talk_info#175",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._talk_info(self, url, video_name)",
        "snippet": "    def _talk_info(self, url, video_name):\n        webpage = self._download_webpage(url, video_name)\n\n        info = self._extract_info(webpage)\n\n        data = try_get(info, lambda x: x['__INITIAL_DATA__'], dict) or info\n        talk_info = data['talks'][0]\n\n        title = talk_info['title'].strip()\n\n        downloads = talk_info.get('downloads') or {}\n        native_downloads = downloads.get('nativeDownloads') or talk_info.get('nativeDownloads') or {}\n\n        formats = [{\n            'url': format_url,\n            'format_id': format_id,\n        } for (format_id, format_url) in native_downloads.items() if format_url is not None]\n\n        subtitled_downloads = downloads.get('subtitledDownloads') or {}\n        for lang, subtitled_download in subtitled_downloads.items():\n            for q in self._NATIVE_FORMATS:\n                q_url = subtitled_download.get(q)\n                if not q_url:\n                    continue\n                formats.append({\n                    'url': q_url,\n                    'format_id': '%s-%s' % (q, lang),\n                    'language': lang,\n                })\n\n        if formats:\n            for f in formats:\n                finfo = self._NATIVE_FORMATS.get(f['format_id'].split('-')[0])\n                if finfo:\n                    f.update(finfo)\n\n        player_talk = talk_info['player_talks'][0]\n\n        external = player_talk.get('external')\n        if isinstance(external, dict):\n            service = external.get('service')\n            if isinstance(service, compat_str):\n                ext_url = None\n                if service.lower() == 'youtube':\n                    ext_url = external.get('code')\n\n                return self.url_result(ext_url or external['uri'])\n\n        resources_ = player_talk.get('resources') or talk_info.get('resources')\n\n        http_url = None\n        for format_id, resources in resources_.items():\n            if format_id == 'hls':\n                if not isinstance(resources, dict):\n                    continue\n                stream_url = url_or_none(resources.get('stream'))\n                if not stream_url:\n                    continue\n                formats.extend(self._extract_m3u8_formats(\n                    stream_url, video_name, 'mp4', m3u8_id=format_id,\n                    fatal=False))\n            else:\n                if not isinstance(resources, list):\n                    continue\n                if format_id == 'h264':\n                    for resource in resources:\n                        h264_url = resource.get('file')\n                        if not h264_url:\n                            continue\n                        bitrate = int_or_none(resource.get('bitrate'))\n                        formats.append({\n                            'url': h264_url,\n                            'format_id': '%s-%sk' % (format_id, bitrate),\n                            'tbr': bitrate,\n                        })\n                        if re.search(r'\\d+k', h264_url):\n                            http_url = h264_url\n                elif format_id == 'rtmp':\n                    streamer = talk_info.get('streamer')\n                    if not streamer:\n                        continue\n                    for resource in resources:\n                        formats.append({\n                            'format_id': '%s-%s' % (format_id, resource.get('name')),\n                            'url': streamer,\n                            'play_path': resource['file'],\n                            'ext': 'flv',\n                            'width': int_or_none(resource.get('width')),\n                            'height': int_or_none(resource.get('height')),\n                            'tbr': int_or_none(resource.get('bitrate')),\n                        })\n\n        m3u8_formats = list(filter(\n            lambda f: f.get('protocol') == 'm3u8' and f.get('vcodec') != 'none',\n            formats))\n        if http_url:\n            for m3u8_format in m3u8_formats:\n                bitrate = self._search_regex(r'(\\d+k)', m3u8_format['url'], 'bitrate', default=None)\n                if not bitrate:\n                    continue\n                bitrate_url = re.sub(r'\\d+k', bitrate, http_url)\n                if not self._is_valid_url(\n                        bitrate_url, video_name, '%s bitrate' % bitrate):\n                    continue\n                f = m3u8_format.copy()\n                f.update({\n                    'url': bitrate_url,\n                    'format_id': m3u8_format['format_id'].replace('hls', 'http'),\n                    'protocol': 'http',\n                })\n                if f.get('acodec') == 'none':\n                    del f['acodec']\n                formats.append(f)\n\n        audio_download = talk_info.get('audioDownload')\n        if audio_download:\n            formats.append({\n                'url': audio_download,\n                'format_id': 'audio',\n                'vcodec': 'none',\n            })\n\n        self._sort_formats(formats)\n\n        video_id = compat_str(talk_info['id'])\n\n        return {\n            'id': video_id,\n            'title': title,\n            'uploader': player_talk.get('speaker') or talk_info.get('speaker'),\n            'thumbnail': player_talk.get('thumb') or talk_info.get('thumb'),\n            'description': self._og_search_description(webpage),\n            'subtitles': self._get_subtitles(video_id, talk_info),\n            'formats': formats,\n            'duration': float_or_none(talk_info.get('duration')),\n            'view_count': int_or_none(data.get('viewed_count')),\n            'comment_count': int_or_none(\n                try_get(data, lambda x: x['comments']['count'])),\n            'tags': try_get(talk_info, lambda x: x['tags'], list),\n        }",
        "begin_line": 175,
        "end_line": 314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004664179104477612,
            "pseudo_dstar_susp": 0.00042426813746287653,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00042426813746287653,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.twitch.TwitchStreamIE.suitable#583",
        "src_path": "youtube_dl/extractor/twitch.py",
        "class_name": "youtube_dl.extractor.twitch.TwitchStreamIE",
        "signature": "youtube_dl.extractor.twitch.TwitchStreamIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False\n                if any(ie.suitable(url) for ie in (\n                    TwitchVideoIE,\n                    TwitchChapterIE,\n                    TwitchVodIE,\n                    TwitchProfileIE,\n                    TwitchAllVideosIE,\n                    TwitchUploadsIE,\n                    TwitchPastBroadcastsIE,\n                    TwitchHighlightsIE,\n                    TwitchClipsIE))\n                else super(TwitchStreamIE, cls).suitable(url))",
        "begin_line": 583,
        "end_line": 595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.twitch.TwitchClipsIE._real_extract#716",
        "src_path": "youtube_dl/extractor/twitch.py",
        "class_name": "youtube_dl.extractor.twitch.TwitchClipsIE",
        "signature": "youtube_dl.extractor.twitch.TwitchClipsIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        clip = self._download_json(\n            'https://gql.twitch.tv/gql', video_id, data=json.dumps({\n                'query': '''{\n  clip(slug: \"%s\") {\n    broadcaster {\n      displayName\n    }\n    createdAt\n    curator {\n      displayName\n      id\n    }\n    durationSeconds\n    id\n    tiny: thumbnailURL(width: 86, height: 45)\n    small: thumbnailURL(width: 260, height: 147)\n    medium: thumbnailURL(width: 480, height: 272)\n    title\n    videoQualities {\n      frameRate\n      quality\n      sourceURL\n    }\n    viewCount\n  }\n}''' % video_id,\n            }).encode(), headers={\n                'Client-ID': self._CLIENT_ID,\n            })['data']['clip']\n\n        if not clip:\n            raise ExtractorError(\n                'This clip is no longer available', expected=True)\n\n        formats = []\n        for option in clip.get('videoQualities', []):\n            if not isinstance(option, dict):\n                continue\n            source = url_or_none(option.get('sourceURL'))\n            if not source:\n                continue\n            formats.append({\n                'url': source,\n                'format_id': option.get('quality'),\n                'height': int_or_none(option.get('quality')),\n                'fps': int_or_none(option.get('frameRate')),\n            })\n        self._sort_formats(formats)\n\n        thumbnails = []\n        for thumbnail_id in ('tiny', 'small', 'medium'):\n            thumbnail_url = clip.get(thumbnail_id)\n            if not thumbnail_url:\n                continue\n            thumb = {\n                'id': thumbnail_id,\n                'url': thumbnail_url,\n            }\n            mobj = re.search(r'-(\\d+)x(\\d+)\\.', thumbnail_url)\n            if mobj:\n                thumb.update({\n                    'height': int(mobj.group(2)),\n                    'width': int(mobj.group(1)),\n                })\n            thumbnails.append(thumb)\n\n        return {\n            'id': clip.get('id') or video_id,\n            'title': clip.get('title') or video_id,\n            'formats': formats,\n            'duration': int_or_none(clip.get('durationSeconds')),\n            'views': int_or_none(clip.get('viewCount')),\n            'timestamp': unified_timestamp(clip.get('createdAt')),\n            'thumbnails': thumbnails,\n            'creator': try_get(clip, lambda x: x['broadcaster']['displayName'], compat_str),\n            'uploader': try_get(clip, lambda x: x['curator']['displayName'], compat_str),\n            'uploader_id': try_get(clip, lambda x: x['curator']['id'], compat_str),\n        }",
        "begin_line": 716,
        "end_line": 796,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.tagesschau.TagesschauIE.suitable#210",
        "src_path": "youtube_dl/extractor/tagesschau.py",
        "class_name": "youtube_dl.extractor.tagesschau.TagesschauIE",
        "signature": "youtube_dl.extractor.tagesschau.TagesschauIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if TagesschauPlayerIE.suitable(url) else super(TagesschauIE, cls).suitable(url)",
        "begin_line": 210,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.rutv.RUTVIE._extract_url#113",
        "src_path": "youtube_dl/extractor/rutv.py",
        "class_name": "youtube_dl.extractor.rutv.RUTVIE",
        "signature": "youtube_dl.extractor.rutv.RUTVIE._extract_url(cls, webpage)",
        "snippet": "    def _extract_url(cls, webpage):\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://(?:test)?player\\.(?:rutv\\.ru|vgtrk\\.com)/(?:iframe/(?:swf|video|live)/id|index/iframe/cast_id)/.+?)\\1', webpage)\n        if mobj:\n            return mobj.group('url')\n\n        mobj = re.search(\n            r'<meta[^>]+?property=([\"\\'])og:video\\1[^>]+?content=([\"\\'])(?P<url>https?://(?:test)?player\\.(?:rutv\\.ru|vgtrk\\.com)/flash\\d+v/container\\.swf\\?id=.+?\\2)',\n            webpage)\n        if mobj:\n            return mobj.group('url')",
        "begin_line": 113,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00017914725904693657,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKBaseIE._real_extract#28",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKBaseIE",
        "signature": "youtube_dl.extractor.nrk.NRKBaseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        api_hosts = (self._api_host, ) if self._api_host else self._API_HOSTS\n\n        for api_host in api_hosts:\n            data = self._download_json(\n                'http://%s/mediaelement/%s' % (api_host, video_id),\n                video_id, 'Downloading mediaelement JSON',\n                fatal=api_host == api_hosts[-1])\n            if not data:\n                continue\n            self._api_host = api_host\n            break\n\n        title = data.get('fullTitle') or data.get('mainTitle') or data['title']\n        video_id = data.get('id') or video_id\n\n        entries = []\n\n        conviva = data.get('convivaStatistics') or {}\n        live = (data.get('mediaElementType') == 'Live'\n                or data.get('isLive') is True or conviva.get('isLive'))\n\n        def make_title(t):\n            return self._live_title(t) if live else t\n\n        media_assets = data.get('mediaAssets')\n        if media_assets and isinstance(media_assets, list):\n            def video_id_and_title(idx):\n                return ((video_id, title) if len(media_assets) == 1\n                        else ('%s-%d' % (video_id, idx), '%s (Part %d)' % (title, idx)))\n            for num, asset in enumerate(media_assets, 1):\n                asset_url = asset.get('url')\n                if not asset_url:\n                    continue\n                formats = self._extract_akamai_formats(asset_url, video_id)\n                if not formats:\n                    continue\n                self._sort_formats(formats)\n\n                # Some f4m streams may not work with hdcore in fragments' URLs\n                for f in formats:\n                    extra_param = f.get('extra_param_to_segment_url')\n                    if extra_param and 'hdcore' in extra_param:\n                        del f['extra_param_to_segment_url']\n\n                entry_id, entry_title = video_id_and_title(num)\n                duration = parse_duration(asset.get('duration'))\n                subtitles = {}\n                for subtitle in ('webVtt', 'timedText'):\n                    subtitle_url = asset.get('%sSubtitlesUrl' % subtitle)\n                    if subtitle_url:\n                        subtitles.setdefault('no', []).append({\n                            'url': compat_urllib_parse_unquote(subtitle_url)\n                        })\n                entries.append({\n                    'id': asset.get('carrierId') or entry_id,\n                    'title': make_title(entry_title),\n                    'duration': duration,\n                    'subtitles': subtitles,\n                    'formats': formats,\n                })\n\n        if not entries:\n            media_url = data.get('mediaUrl')\n            if media_url:\n                formats = self._extract_akamai_formats(media_url, video_id)\n                self._sort_formats(formats)\n                duration = parse_duration(data.get('duration'))\n                entries = [{\n                    'id': video_id,\n                    'title': make_title(title),\n                    'duration': duration,\n                    'formats': formats,\n                }]\n\n        if not entries:\n            MESSAGES = {\n                'ProgramRightsAreNotReady': 'Du kan dessverre ikke se eller h\u00f8re programmet',\n                'ProgramRightsHasExpired': 'Programmet har g\u00e5tt ut',\n                'NoProgramRights': 'Ikke tilgjengelig',\n                'ProgramIsGeoBlocked': 'NRK har ikke rettigheter til \u00e5 vise dette programmet utenfor Norge',\n            }\n            message_type = data.get('messageType', '')\n            # Can be ProgramIsGeoBlocked or ChannelIsGeoBlocked*\n            if 'IsGeoBlocked' in message_type:\n                self.raise_geo_restricted(\n                    msg=MESSAGES.get('ProgramIsGeoBlocked'),\n                    countries=self._GEO_COUNTRIES)\n            raise ExtractorError(\n                '%s said: %s' % (self.IE_NAME, MESSAGES.get(\n                    message_type, message_type)),\n                expected=True)\n\n        series = conviva.get('seriesName') or data.get('seriesTitle')\n        episode = conviva.get('episodeName') or data.get('episodeNumberOrDate')\n\n        season_number = None\n        episode_number = None\n        if data.get('mediaElementType') == 'Episode':\n            _season_episode = data.get('scoresStatistics', {}).get('springStreamStream') or \\\n                data.get('relativeOriginUrl', '')\n            EPISODENUM_RE = [\n                r'/s(?P<season>\\d{,2})e(?P<episode>\\d{,2})\\.',\n                r'/sesong-(?P<season>\\d{,2})/episode-(?P<episode>\\d{,2})',\n            ]\n            season_number = int_or_none(self._search_regex(\n                EPISODENUM_RE, _season_episode, 'season number',\n                default=None, group='season'))\n            episode_number = int_or_none(self._search_regex(\n                EPISODENUM_RE, _season_episode, 'episode number',\n                default=None, group='episode'))\n\n        thumbnails = None\n        images = data.get('images')\n        if images and isinstance(images, dict):\n            web_images = images.get('webImages')\n            if isinstance(web_images, list):\n                thumbnails = [{\n                    'url': image['imageUrl'],\n                    'width': int_or_none(image.get('width')),\n                    'height': int_or_none(image.get('height')),\n                } for image in web_images if image.get('imageUrl')]\n\n        description = data.get('description')\n        category = data.get('mediaAnalytics', {}).get('category')\n\n        common_info = {\n            'description': description,\n            'series': series,\n            'episode': episode,\n            'season_number': season_number,\n            'episode_number': episode_number,\n            'categories': [category] if category else None,\n            'age_limit': parse_age_limit(data.get('legalAge')),\n            'thumbnails': thumbnails,\n        }\n\n        vcodec = 'none' if data.get('mediaType') == 'Audio' else None\n\n        for entry in entries:\n            entry.update(common_info)\n            for f in entry['formats']:\n                f['vcodec'] = vcodec\n\n        points = data.get('shortIndexPoints')\n        if isinstance(points, list):\n            chapters = []\n            for next_num, point in enumerate(points, start=1):\n                if not isinstance(point, dict):\n                    continue\n                start_time = parse_duration(point.get('startPoint'))\n                if start_time is None:\n                    continue\n                end_time = parse_duration(\n                    data.get('duration')\n                    if next_num == len(points)\n                    else points[next_num].get('startPoint'))\n                if end_time is None:\n                    continue\n                chapters.append({\n                    'start_time': start_time,\n                    'end_time': end_time,\n                    'title': point.get('title'),\n                })\n            if chapters and len(entries) == 1:\n                entries[0]['chapters'] = chapters\n\n        return self.playlist_result(entries, video_id, title, description)",
        "begin_line": 28,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004033884630899556,
            "pseudo_dstar_susp": 0.0003763643206624012,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003763643206624012,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKBaseIE.video_id_and_title#57",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKBaseIE",
        "signature": "youtube_dl.extractor.nrk.NRKBaseIE.video_id_and_title(idx)",
        "snippet": "            def video_id_and_title(idx):\n                return ((video_id, title) if len(media_assets) == 1\n                        else ('%s-%d' % (video_id, idx), '%s (Part %d)' % (title, idx)))",
        "begin_line": 57,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043975373790677223,
            "pseudo_dstar_susp": 0.0003944773175542406,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003944773175542406,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVSerieBaseIE._extract_episodes#460",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVSerieBaseIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVSerieBaseIE._extract_episodes(self, season)",
        "snippet": "    def _extract_episodes(self, season):\n        if not isinstance(season, dict):\n            return []\n        return self._extract_entries(season.get('episodes'))",
        "begin_line": 460,
        "end_line": 463,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVSeasonIE.suitable#490",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVSeasonIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVSeasonIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if NRKTVIE.suitable(url) or NRKTVEpisodeIE.suitable(url)\n                else super(NRKTVSeasonIE, cls).suitable(url))",
        "begin_line": 490,
        "end_line": 492,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVSeasonIE._real_extract#494",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVSeasonIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVSeasonIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, display_id)\n\n        series = self._extract_series(webpage, display_id)\n\n        season = next(\n            s for s in series['seasons']\n            if int(display_id) == s.get('seasonNumber'))\n\n        title = try_get(season, lambda x: x['titles']['title'], compat_str)\n        return self.playlist_result(\n            self._extract_episodes(season), display_id, title)",
        "begin_line": 494,
        "end_line": 507,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVSeriesIE.suitable#560",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVSeriesIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVSeriesIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (\n            False if any(ie.suitable(url)\n                         for ie in (NRKTVIE, NRKTVEpisodeIE, NRKTVSeasonIE))\n            else super(NRKTVSeriesIE, cls).suitable(url))",
        "begin_line": 560,
        "end_line": 564,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVSeriesIE._real_extract#566",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVSeriesIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVSeriesIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        series_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, series_id)\n\n        # New layout (e.g. https://tv.nrk.no/serie/backstage)\n        series = self._extract_series(webpage, series_id, fatal=False)\n        if series:\n            title = try_get(series, lambda x: x['titles']['title'], compat_str)\n            description = try_get(\n                series, lambda x: x['titles']['subtitle'], compat_str)\n            entries = []\n            entries.extend(self._extract_seasons(series.get('seasons')))\n            entries.extend(self._extract_entries(series.get('instalments')))\n            entries.extend(self._extract_episodes(series.get('extraMaterial')))\n            return self.playlist_result(entries, series_id, title, description)\n\n        # Old layout (e.g. https://tv.nrksuper.no/serie/labyrint)\n        entries = [\n            self.url_result(\n                'https://tv.nrk.no/program/Episodes/{series}/{season}'.format(\n                    series=series_id, season=season_id))\n            for season_id in re.findall(self._ITEM_RE, webpage)\n        ]\n\n        title = self._html_search_meta(\n            'seriestitle', webpage,\n            'title', default=None) or self._og_search_title(\n            webpage, fatal=False)\n        if title:\n            title = self._search_regex(\n                r'NRK (?:Super )?TV\\s*[-\u2013]\\s*(.+)', title, 'title', default=title)\n\n        description = self._html_search_meta(\n            'series_description', webpage,\n            'description', default=None) or self._og_search_description(webpage)\n\n        return self.playlist_result(entries, series_id, title, description)",
        "begin_line": 566,
        "end_line": 603,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE._extract_description#620",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE",
        "signature": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE._extract_description(self, webpage)",
        "snippet": "    def _extract_description(self, webpage):\n        pass",
        "begin_line": 620,
        "end_line": 621,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE._real_extract#623",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE",
        "signature": "youtube_dl.extractor.nrk.NRKPlaylistBaseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        entries = [\n            self.url_result('nrk:%s' % video_id, NRKIE.ie_key())\n            for video_id in re.findall(self._ITEM_RE, webpage)\n        ]\n\n        playlist_title = self. _extract_title(webpage)\n        playlist_description = self._extract_description(webpage)\n\n        return self.playlist_result(\n            entries, playlist_id, playlist_title, playlist_description)",
        "begin_line": 623,
        "end_line": 637,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030873726458783575,
            "pseudo_dstar_susp": 0.00030826140567200987,
            "pseudo_tarantula_susp": 0.0005293806246691371,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0005293806246691371
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKPlaylistIE._extract_title#661",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKPlaylistIE",
        "signature": "youtube_dl.extractor.nrk.NRKPlaylistIE._extract_title(self, webpage)",
        "snippet": "    def _extract_title(self, webpage):\n        return self._og_search_title(webpage, fatal=False)",
        "begin_line": 661,
        "end_line": 662,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKPlaylistIE._extract_description#664",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKPlaylistIE",
        "signature": "youtube_dl.extractor.nrk.NRKPlaylistIE._extract_description(self, webpage)",
        "snippet": "    def _extract_description(self, webpage):\n        return self._og_search_description(webpage)",
        "begin_line": 664,
        "end_line": 665,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKTVEpisodesIE._extract_title#680",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKTVEpisodesIE",
        "signature": "youtube_dl.extractor.nrk.NRKTVEpisodesIE._extract_title(self, webpage)",
        "snippet": "    def _extract_title(self, webpage):\n        return self._html_search_regex(\n            r'<h1>([^<]+)</h1>', webpage, 'title', fatal=False)",
        "begin_line": 680,
        "end_line": 682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nrk.NRKSkoleIE._real_extract#704",
        "src_path": "youtube_dl/extractor/nrk.py",
        "class_name": "youtube_dl.extractor.nrk.NRKSkoleIE",
        "signature": "youtube_dl.extractor.nrk.NRKSkoleIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        webpage = self._download_webpage(\n            'https://mimir.nrk.no/plugin/1.0/static?mediaId=%s' % video_id,\n            video_id)\n\n        nrk_id = self._parse_json(\n            self._search_regex(\n                r'<script[^>]+type=[\"\\']application/json[\"\\'][^>]*>({.+?})</script>',\n                webpage, 'application json'),\n            video_id)['activeMedia']['psId']\n\n        return self.url_result('nrk:%s' % nrk_id)",
        "begin_line": 704,
        "end_line": 717,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._real_extract#71",
        "src_path": "youtube_dl/extractor/ceskatelevize.py",
        "class_name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE",
        "signature": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        NOT_AVAILABLE_STRING = 'This content is not available at your territory due to limited copyright.'\n        if '%s</p>' % NOT_AVAILABLE_STRING in webpage:\n            raise ExtractorError(NOT_AVAILABLE_STRING, expected=True)\n\n        type_ = None\n        episode_id = None\n\n        playlist = self._parse_json(\n            self._search_regex(\n                r'getPlaylistUrl\\(\\[({.+?})\\]', webpage, 'playlist',\n                default='{}'), playlist_id)\n        if playlist:\n            type_ = playlist.get('type')\n            episode_id = playlist.get('id')\n\n        if not type_:\n            type_ = self._html_search_regex(\n                r'getPlaylistUrl\\(\\[\\{\"type\":\"(.+?)\",\"id\":\".+?\"\\}\\],',\n                webpage, 'type')\n        if not episode_id:\n            episode_id = self._html_search_regex(\n                r'getPlaylistUrl\\(\\[\\{\"type\":\".+?\",\"id\":\"(.+?)\"\\}\\],',\n                webpage, 'episode_id')\n\n        data = {\n            'playlist[0][type]': type_,\n            'playlist[0][id]': episode_id,\n            'requestUrl': compat_urllib_parse_urlparse(url).path,\n            'requestSource': 'iVysilani',\n        }\n\n        entries = []\n\n        for user_agent in (None, USER_AGENTS['Safari']):\n            req = sanitized_Request(\n                'https://www.ceskatelevize.cz/ivysilani/ajax/get-client-playlist',\n                data=urlencode_postdata(data))\n\n            req.add_header('Content-type', 'application/x-www-form-urlencoded')\n            req.add_header('x-addr', '127.0.0.1')\n            req.add_header('X-Requested-With', 'XMLHttpRequest')\n            if user_agent:\n                req.add_header('User-Agent', user_agent)\n            req.add_header('Referer', url)\n\n            playlistpage = self._download_json(req, playlist_id, fatal=False)\n\n            if not playlistpage:\n                continue\n\n            playlist_url = playlistpage['url']\n            if playlist_url == 'error_region':\n                raise ExtractorError(NOT_AVAILABLE_STRING, expected=True)\n\n            req = sanitized_Request(compat_urllib_parse_unquote(playlist_url))\n            req.add_header('Referer', url)\n\n            playlist_title = self._og_search_title(webpage, default=None)\n            playlist_description = self._og_search_description(webpage, default=None)\n\n            playlist = self._download_json(req, playlist_id, fatal=False)\n            if not playlist:\n                continue\n\n            playlist = playlist.get('playlist')\n            if not isinstance(playlist, list):\n                continue\n\n            playlist_len = len(playlist)\n\n            for num, item in enumerate(playlist):\n                is_live = item.get('type') == 'LIVE'\n                formats = []\n                for format_id, stream_url in item.get('streamUrls', {}).items():\n                    if 'drmOnly=true' in stream_url:\n                        continue\n                    if 'playerType=flash' in stream_url:\n                        stream_formats = self._extract_m3u8_formats(\n                            stream_url, playlist_id, 'mp4', 'm3u8_native',\n                            m3u8_id='hls-%s' % format_id, fatal=False)\n                    else:\n                        stream_formats = self._extract_mpd_formats(\n                            stream_url, playlist_id,\n                            mpd_id='dash-%s' % format_id, fatal=False)\n                    # See https://github.com/ytdl-org/youtube-dl/issues/12119#issuecomment-280037031\n                    if format_id == 'audioDescription':\n                        for f in stream_formats:\n                            f['source_preference'] = -10\n                    formats.extend(stream_formats)\n\n                if user_agent and len(entries) == playlist_len:\n                    entries[num]['formats'].extend(formats)\n                    continue\n\n                item_id = item.get('id') or item['assetId']\n                title = item['title']\n\n                duration = float_or_none(item.get('duration'))\n                thumbnail = item.get('previewImageUrl')\n\n                subtitles = {}\n                if item.get('type') == 'VOD':\n                    subs = item.get('subtitles')\n                    if subs:\n                        subtitles = self.extract_subtitles(episode_id, subs)\n\n                if playlist_len == 1:\n                    final_title = playlist_title or title\n                    if is_live:\n                        final_title = self._live_title(final_title)\n                else:\n                    final_title = '%s (%s)' % (playlist_title, title)\n\n                entries.append({\n                    'id': item_id,\n                    'title': final_title,\n                    'description': playlist_description if playlist_len == 1 else None,\n                    'thumbnail': thumbnail,\n                    'duration': duration,\n                    'formats': formats,\n                    'subtitles': subtitles,\n                    'is_live': is_live,\n                })\n\n        for e in entries:\n            self._sort_formats(e['formats'])\n\n        return self.playlist_result(entries, playlist_id, playlist_title, playlist_description)",
        "begin_line": 71,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00044444444444444447,
            "pseudo_dstar_susp": 0.00041135335252982314,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00041135335252982314,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._get_subtitles#205",
        "src_path": "youtube_dl/extractor/ceskatelevize.py",
        "class_name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE",
        "signature": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._get_subtitles(self, episode_id, subs)",
        "snippet": "    def _get_subtitles(self, episode_id, subs):\n        original_subtitles = self._download_webpage(\n            subs[0]['url'], episode_id, 'Downloading subtitles')\n        srt_subs = self._fix_subtitles(original_subtitles)\n        return {\n            'cs': [{\n                'ext': 'srt',\n                'data': srt_subs,\n            }]\n        }",
        "begin_line": 205,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00031201248049921997,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00031201248049921997,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._fix_subtitle#228",
        "src_path": "youtube_dl/extractor/ceskatelevize.py",
        "class_name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE",
        "signature": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._fix_subtitle(subtitle)",
        "snippet": "        def _fix_subtitle(subtitle):\n            for line in subtitle.splitlines():\n                m = re.match(r'^\\s*([0-9]+);\\s*([0-9]+)\\s+([0-9]+)\\s*$', line)\n                if m:\n                    yield m.group(1)\n                    start, stop = (_msectotimecode(int(t)) for t in m.groups()[1:])\n                    yield '{0} --> {1}'.format(start, stop)\n                else:\n                    yield line",
        "begin_line": 228,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.younow.YouNowLiveIE.suitable#39",
        "src_path": "youtube_dl/extractor/younow.py",
        "class_name": "youtube_dl.extractor.younow.YouNowLiveIE",
        "signature": "youtube_dl.extractor.younow.YouNowLiveIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False\n                if YouNowChannelIE.suitable(url) or YouNowMomentIE.suitable(url)\n                else super(YouNowLiveIE, cls).suitable(url))",
        "begin_line": 39,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003104625892579944,
            "pseudo_dstar_susp": 0.00036036036036036037,
            "pseudo_tarantula_susp": 0.0002864508736751647,
            "pseudo_op2_susp": 0.00036036036036036037,
            "pseudo_barinel_susp": 0.0002864508736751647
        }
    },
    {
        "name": "youtube_dl.extractor.walla.WallaIE._real_extract#36",
        "src_path": "youtube_dl/extractor/walla.py",
        "class_name": "youtube_dl.extractor.walla.WallaIE",
        "signature": "youtube_dl.extractor.walla.WallaIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        display_id = mobj.group('display_id')\n\n        video = self._download_xml(\n            'http://video2.walla.co.il/?w=null/null/%s/@@/video/flv_pl' % video_id,\n            display_id)\n\n        item = video.find('./items/item')\n\n        title = xpath_text(item, './title', 'title')\n        description = xpath_text(item, './synopsis', 'description')\n        thumbnail = xpath_text(item, './preview_pic', 'thumbnail')\n        duration = int_or_none(xpath_text(item, './duration', 'duration'))\n\n        subtitles = {}\n        for subtitle in item.findall('./subtitles/subtitle'):\n            lang = xpath_text(subtitle, './title')\n            subtitles[self._SUBTITLE_LANGS.get(lang, lang)] = [{\n                'ext': 'srt',\n                'url': xpath_text(subtitle, './src'),\n            }]\n\n        formats = []\n        for quality in item.findall('./qualities/quality'):\n            format_id = xpath_text(quality, './title')\n            fmt = {\n                'url': 'rtmp://wafla.walla.co.il/vod',\n                'play_path': xpath_text(quality, './src'),\n                'player_url': 'http://isc.walla.co.il/w9/swf/video_swf/vod/WallaMediaPlayerAvod.swf',\n                'page_url': url,\n                'ext': 'flv',\n                'format_id': xpath_text(quality, './title'),\n            }\n            m = re.search(r'^(?P<height>\\d+)[Pp]', format_id)\n            if m:\n                fmt['height'] = int(m.group('height'))\n            formats.append(fmt)\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'display_id': display_id,\n            'title': title,\n            'description': description,\n            'thumbnail': thumbnail,\n            'duration': duration,\n            'formats': formats,\n            'subtitles': subtitles,\n        }",
        "begin_line": 36,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004344048653344917,
            "pseudo_dstar_susp": 0.0004098360655737705,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0004098360655737705,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.raywenderlich.RayWenderlichCourseIE.suitable#156",
        "src_path": "youtube_dl/extractor/raywenderlich.py",
        "class_name": "youtube_dl.extractor.raywenderlich.RayWenderlichCourseIE",
        "signature": "youtube_dl.extractor.raywenderlich.RayWenderlichCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if RayWenderlichIE.suitable(url) else super(\n            RayWenderlichCourseIE, cls).suitable(url)",
        "begin_line": 156,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.0003971405877680699,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.0003971405877680699,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.vimple.SprutoBaseIE._extract_spruto#8",
        "src_path": "youtube_dl/extractor/vimple.py",
        "class_name": "youtube_dl.extractor.vimple.SprutoBaseIE",
        "signature": "youtube_dl.extractor.vimple.SprutoBaseIE._extract_spruto(self, spruto, video_id)",
        "snippet": "    def _extract_spruto(self, spruto, video_id):\n        playlist = spruto['playlist'][0]\n        title = playlist['title']\n        video_id = playlist.get('videoId') or video_id\n        thumbnail = playlist.get('posterUrl') or playlist.get('thumbnailUrl')\n        duration = int_or_none(playlist.get('duration'))\n\n        formats = [{\n            'url': f['url'],\n        } for f in playlist['video']]\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'thumbnail': thumbnail,\n            'duration': duration,\n            'formats': formats,\n        }",
        "begin_line": 8,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.discoverygo.DiscoveryGoPlaylistIE.suitable#148",
        "src_path": "youtube_dl/extractor/discoverygo.py",
        "class_name": "youtube_dl.extractor.discoverygo.DiscoveryGoPlaylistIE",
        "signature": "youtube_dl.extractor.discoverygo.DiscoveryGoPlaylistIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if DiscoveryGoIE.suitable(url) else super(\n            DiscoveryGoPlaylistIE, cls).suitable(url)",
        "begin_line": 148,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.0003971405877680699,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.0003971405877680699,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.francetv.FranceTVInfoSportIE._real_extract#401",
        "src_path": "youtube_dl/extractor/francetv.py",
        "class_name": "youtube_dl.extractor.francetv.FranceTVInfoSportIE",
        "signature": "youtube_dl.extractor.francetv.FranceTVInfoSportIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url)\n        webpage = self._download_webpage(url, display_id)\n        video_id = self._search_regex(r'data-video=\"([^\"]+)\"', webpage, 'video_id')\n        return self._make_url_result(video_id, 'Sport-web')",
        "begin_line": 401,
        "end_line": 405,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.francetv.CultureboxIE._real_extract#463",
        "src_path": "youtube_dl/extractor/francetv.py",
        "class_name": "youtube_dl.extractor.francetv.CultureboxIE",
        "signature": "youtube_dl.extractor.francetv.CultureboxIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, display_id)\n\n        if \">Ce live n'est plus disponible en replay<\" in webpage:\n            raise ExtractorError(\n                'Video %s is not available' % display_id, expected=True)\n\n        video_id, catalogue = self._search_regex(\n            r'[\"\\'>]https?://videos\\.francetv\\.fr/video/([^@]+@.+?)[\"\\'<]',\n            webpage, 'video id').split('@')\n\n        return self._make_url_result(video_id, catalogue)",
        "begin_line": 463,
        "end_line": 476,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.francetv.FranceTVJeunesseIE._real_extract#499",
        "src_path": "youtube_dl/extractor/francetv.py",
        "class_name": "youtube_dl.extractor.francetv.FranceTVJeunesseIE",
        "signature": "youtube_dl.extractor.francetv.FranceTVJeunesseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        playlist_id = mobj.group('id')\n\n        playlist = self._download_json(\n            '%s/%s' % (mobj.group('url'), 'playlist'), playlist_id)\n\n        if not playlist.get('count'):\n            raise ExtractorError(\n                '%s is not available' % playlist_id, expected=True)\n\n        entries = []\n        for item in playlist['items']:\n            identity = item.get('identity')\n            if identity and isinstance(identity, compat_str):\n                entries.append(self._make_url_result(identity))\n\n        return self.playlist_result(entries, playlist_id)",
        "begin_line": 499,
        "end_line": 516,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mediasite.MediasiteIE._real_extract#125",
        "src_path": "youtube_dl/extractor/mediasite.py",
        "class_name": "youtube_dl.extractor.mediasite.MediasiteIE",
        "signature": "youtube_dl.extractor.mediasite.MediasiteIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, data = unsmuggle_url(url, {})\n        mobj = re.match(self._VALID_URL, url)\n        resource_id = mobj.group('id')\n        query = mobj.group('query')\n\n        webpage, urlh = self._download_webpage_handle(url, resource_id)  # XXX: add UrlReferrer?\n        redirect_url = urlh.geturl()\n\n        # XXX: might have also extracted UrlReferrer and QueryString from the html\n        service_path = compat_urlparse.urljoin(redirect_url, self._html_search_regex(\n            r'<div[^>]+\\bid=[\"\\']ServicePath[^>]+>(.+?)</div>', webpage, resource_id,\n            default='/Mediasite/PlayerService/PlayerService.svc/json'))\n\n        player_options = self._download_json(\n            '%s/GetPlayerOptions' % service_path, resource_id,\n            headers={\n                'Content-type': 'application/json; charset=utf-8',\n                'X-Requested-With': 'XMLHttpRequest',\n            },\n            data=json.dumps({\n                'getPlayerOptionsRequest': {\n                    'ResourceId': resource_id,\n                    'QueryString': query,\n                    'UrlReferrer': data.get('UrlReferrer', ''),\n                    'UseScreenReader': False,\n                }\n            }).encode('utf-8'))['d']\n\n        presentation = player_options['Presentation']\n        title = presentation['Title']\n\n        if presentation is None:\n            raise ExtractorError(\n                'Mediasite says: %s' % player_options['PlayerPresentationStatusMessage'],\n                expected=True)\n\n        thumbnails = []\n        formats = []\n        for snum, Stream in enumerate(presentation['Streams']):\n            stream_type = Stream.get('StreamType')\n            if stream_type is None:\n                continue\n\n            video_urls = Stream.get('VideoUrls')\n            if not isinstance(video_urls, list):\n                video_urls = []\n\n            stream_id = self._STREAM_TYPES.get(\n                stream_type, 'type%u' % stream_type)\n\n            stream_formats = []\n            for unum, VideoUrl in enumerate(video_urls):\n                video_url = url_or_none(VideoUrl.get('Location'))\n                if not video_url:\n                    continue\n                # XXX: if Stream.get('CanChangeScheme', False), switch scheme to HTTP/HTTPS\n\n                media_type = VideoUrl.get('MediaType')\n                if media_type == 'SS':\n                    stream_formats.extend(self._extract_ism_formats(\n                        video_url, resource_id,\n                        ism_id='%s-%u.%u' % (stream_id, snum, unum),\n                        fatal=False))\n                elif media_type == 'Dash':\n                    stream_formats.extend(self._extract_mpd_formats(\n                        video_url, resource_id,\n                        mpd_id='%s-%u.%u' % (stream_id, snum, unum),\n                        fatal=False))\n                else:\n                    stream_formats.append({\n                        'format_id': '%s-%u.%u' % (stream_id, snum, unum),\n                        'url': video_url,\n                        'ext': mimetype2ext(VideoUrl.get('MimeType')),\n                    })\n\n            # TODO: if Stream['HasSlideContent']:\n            # synthesise an MJPEG video stream '%s-%u.slides' % (stream_type, snum)\n            # from Stream['Slides']\n            # this will require writing a custom downloader...\n\n            # disprefer 'secondary' streams\n            if stream_type != 0:\n                for fmt in stream_formats:\n                    fmt['preference'] = -1\n\n            thumbnail_url = Stream.get('ThumbnailUrl')\n            if thumbnail_url:\n                thumbnails.append({\n                    'id': '%s-%u' % (stream_id, snum),\n                    'url': urljoin(redirect_url, thumbnail_url),\n                    'preference': -1 if stream_type != 0 else 0,\n                })\n            formats.extend(stream_formats)\n\n        self._sort_formats(formats)\n\n        # XXX: Presentation['Presenters']\n        # XXX: Presentation['Transcript']\n\n        return {\n            'id': resource_id,\n            'title': title,\n            'description': presentation.get('Description'),\n            'duration': float_or_none(presentation.get('Duration'), 1000),\n            'timestamp': float_or_none(presentation.get('UnixTime'), 1000),\n            'formats': formats,\n            'thumbnails': thumbnails,\n        }",
        "begin_line": 125,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011467889908256881,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mediasite.MediasiteCatalogIE._real_extract#278",
        "src_path": "youtube_dl/extractor/mediasite.py",
        "class_name": "youtube_dl.extractor.mediasite.MediasiteCatalogIE",
        "signature": "youtube_dl.extractor.mediasite.MediasiteCatalogIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        mediasite_url = mobj.group('url')\n        catalog_id = mobj.group('catalog_id')\n        current_folder_id = mobj.group('current_folder_id') or catalog_id\n        root_dynamic_folder_id = mobj.group('root_dynamic_folder_id')\n\n        webpage = self._download_webpage(url, catalog_id)\n\n        # AntiForgeryToken is optional (e.g. [1])\n        # 1. https://live.libraries.psu.edu/Mediasite/Catalog/Full/8376d4b24dd1457ea3bfe4cf9163feda21\n        anti_forgery_token = self._search_regex(\n            r'AntiForgeryToken\\s*:\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1',\n            webpage, 'anti forgery token', default=None, group='value')\n        if anti_forgery_token:\n            anti_forgery_header = self._search_regex(\n                r'AntiForgeryHeaderName\\s*:\\s*([\"\\'])(?P<value>(?:(?!\\1).)+)\\1',\n                webpage, 'anti forgery header name',\n                default='X-SOFO-AntiForgeryHeader', group='value')\n\n        data = {\n            'IsViewPage': True,\n            'IsNewFolder': True,\n            'AuthTicket': None,\n            'CatalogId': catalog_id,\n            'CurrentFolderId': current_folder_id,\n            'RootDynamicFolderId': root_dynamic_folder_id,\n            'ItemsPerPage': 1000,\n            'PageIndex': 0,\n            'PermissionMask': 'Execute',\n            'CatalogSearchType': 'SearchInFolder',\n            'SortBy': 'Date',\n            'SortDirection': 'Descending',\n            'StartDate': None,\n            'EndDate': None,\n            'StatusFilterList': None,\n            'PreviewKey': None,\n            'Tags': [],\n        }\n\n        headers = {\n            'Content-Type': 'application/json; charset=UTF-8',\n            'Referer': url,\n            'X-Requested-With': 'XMLHttpRequest',\n        }\n        if anti_forgery_token:\n            headers[anti_forgery_header] = anti_forgery_token\n\n        catalog = self._download_json(\n            '%s/Catalog/Data/GetPresentationsForFolder' % mediasite_url,\n            catalog_id, data=json.dumps(data).encode(), headers=headers)\n\n        entries = []\n        for video in catalog['PresentationDetailsList']:\n            if not isinstance(video, dict):\n                continue\n            video_id = str_or_none(video.get('Id'))\n            if not video_id:\n                continue\n            entries.append(self.url_result(\n                '%s/Play/%s' % (mediasite_url, video_id),\n                ie=MediasiteIE.ie_key(), video_id=video_id))\n\n        title = try_get(\n            catalog, lambda x: x['CurrentFolder']['Name'], compat_str)\n\n        return self.playlist_result(entries, catalog_id, title,)",
        "begin_line": 278,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mediasite.MediasiteNamedCatalogIE._real_extract#354",
        "src_path": "youtube_dl/extractor/mediasite.py",
        "class_name": "youtube_dl.extractor.mediasite.MediasiteNamedCatalogIE",
        "signature": "youtube_dl.extractor.mediasite.MediasiteNamedCatalogIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        mediasite_url = mobj.group('url')\n        catalog_name = mobj.group('catalog_name')\n\n        webpage = self._download_webpage(url, catalog_name)\n\n        catalog_id = self._search_regex(\n            r'CatalogId\\s*:\\s*[\"\\'](%s)' % _ID_RE, webpage, 'catalog id')\n\n        return self.url_result(\n            '%s/Catalog/Full/%s' % (mediasite_url, catalog_id),\n            ie=MediasiteCatalogIE.ie_key(), video_id=catalog_id)",
        "begin_line": 354,
        "end_line": 366,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.frontendmasters.FrontendMastersLessonIE._real_extract#209",
        "src_path": "youtube_dl/extractor/frontendmasters.py",
        "class_name": "youtube_dl.extractor.frontendmasters.FrontendMastersLessonIE",
        "signature": "youtube_dl.extractor.frontendmasters.FrontendMastersLessonIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        course_name, lesson_name = mobj.group('course_name', 'lesson_name')\n\n        course = self._download_course(course_name, url)\n\n        lesson_id, lesson = next(\n            (video_id, data)\n            for video_id, data in course['lessonData'].items()\n            if data.get('slug') == lesson_name)\n\n        chapters = self._extract_chapters(course)\n        return self._extract_lesson(chapters, lesson_id, lesson)",
        "begin_line": 209,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011563367252543941,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.frontendmasters.FrontendMastersCourseIE.suitable#238",
        "src_path": "youtube_dl/extractor/frontendmasters.py",
        "class_name": "youtube_dl.extractor.frontendmasters.FrontendMastersCourseIE",
        "signature": "youtube_dl.extractor.frontendmasters.FrontendMastersCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if FrontendMastersLessonIE.suitable(url) else super(\n            FrontendMastersBaseIE, cls).suitable(url)",
        "begin_line": 238,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000333000333000333,
            "pseudo_dstar_susp": 0.000395882818685669,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.000395882818685669,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.downloader.fragment.FragmentFD._append_fragment#112",
        "src_path": "youtube_dl/downloader/fragment.py",
        "class_name": "youtube_dl.downloader.fragment.FragmentFD",
        "signature": "youtube_dl.downloader.fragment.FragmentFD._append_fragment(self, ctx, frag_content)",
        "snippet": "    def _append_fragment(self, ctx, frag_content):\n        try:\n            ctx['dest_stream'].write(frag_content)\n            ctx['dest_stream'].flush()\n        finally:\n            if self.__do_ytdl_file(ctx):\n                self._write_ytdl_file(ctx)\n            if not self.params.get('keep_fragments', False):\n                os.remove(encodeFilename(ctx['fragment_filename_sanitized']))\n            del ctx['fragment_filename_sanitized']",
        "begin_line": 112,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002457002457002457,
            "pseudo_dstar_susp": 0.0024937655860349127,
            "pseudo_tarantula_susp": 0.0006285355122564425,
            "pseudo_op2_susp": 0.0024937655860349127,
            "pseudo_barinel_susp": 0.0006285355122564425
        }
    },
    {
        "name": "youtube_dl.downloader.fragment.FragmentFD.frag_progress_hook#214",
        "src_path": "youtube_dl/downloader/fragment.py",
        "class_name": "youtube_dl.downloader.fragment.FragmentFD",
        "signature": "youtube_dl.downloader.fragment.FragmentFD.frag_progress_hook(s)",
        "snippet": "        def frag_progress_hook(s):\n            if s['status'] not in ('downloading', 'finished'):\n                return\n\n            time_now = time.time()\n            state['elapsed'] = time_now - start\n            frag_total_bytes = s.get('total_bytes') or 0\n            if not ctx['live']:\n                estimated_size = (\n                    (ctx['complete_frags_downloaded_bytes'] + frag_total_bytes)\n                    / (state['fragment_index'] + 1) * total_frags)\n                state['total_bytes_estimate'] = estimated_size\n\n            if s['status'] == 'finished':\n                state['fragment_index'] += 1\n                ctx['fragment_index'] = state['fragment_index']\n                state['downloaded_bytes'] += frag_total_bytes - ctx['prev_frag_downloaded_bytes']\n                ctx['complete_frags_downloaded_bytes'] = state['downloaded_bytes']\n                ctx['prev_frag_downloaded_bytes'] = 0\n            else:\n                frag_downloaded_bytes = s['downloaded_bytes']\n                state['downloaded_bytes'] += frag_downloaded_bytes - ctx['prev_frag_downloaded_bytes']\n                if not ctx['live']:\n                    state['eta'] = self.calc_eta(\n                        start, time_now, estimated_size - resume_len,\n                        state['downloaded_bytes'] - resume_len)\n                state['speed'] = s.get('speed') or ctx.get('speed')\n                ctx['speed'] = state['speed']\n                ctx['prev_frag_downloaded_bytes'] = frag_downloaded_bytes\n            self._hook_progress(state)",
        "begin_line": 214,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007293946024799417,
            "pseudo_dstar_susp": 0.0009541984732824427,
            "pseudo_tarantula_susp": 0.0002844141069397042,
            "pseudo_op2_susp": 0.0009541984732824427,
            "pseudo_barinel_susp": 0.0002844141069397042
        }
    },
    {
        "name": "youtube_dl.downloader.fragment.FragmentFD._finish_frag_download#249",
        "src_path": "youtube_dl/downloader/fragment.py",
        "class_name": "youtube_dl.downloader.fragment.FragmentFD",
        "signature": "youtube_dl.downloader.fragment.FragmentFD._finish_frag_download(self, ctx)",
        "snippet": "    def _finish_frag_download(self, ctx):\n        ctx['dest_stream'].close()\n        if self.__do_ytdl_file(ctx):\n            ytdl_filename = encodeFilename(self.ytdl_filename(ctx['filename']))\n            if os.path.isfile(ytdl_filename):\n                os.remove(ytdl_filename)\n        elapsed = time.time() - ctx['started']\n\n        if ctx['tmpfilename'] == '-':\n            downloaded_bytes = ctx['complete_frags_downloaded_bytes']\n        else:\n            self.try_rename(ctx['tmpfilename'], ctx['filename'])\n            downloaded_bytes = os.path.getsize(encodeFilename(ctx['filename']))\n\n        self._hook_progress({\n            'downloaded_bytes': downloaded_bytes,\n            'total_bytes': downloaded_bytes,\n            'filename': ctx['filename'],\n            'status': 'finished',\n            'elapsed': elapsed,\n        })",
        "begin_line": 249,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006333122229259025,
            "pseudo_dstar_susp": 0.0005988023952095808,
            "pseudo_tarantula_susp": 0.0006031363088057901,
            "pseudo_op2_susp": 0.0005988023952095808,
            "pseudo_barinel_susp": 0.0006031363088057901
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.real_download#28",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n\n        class DownloadContext(dict):\n            __getattr__ = dict.get\n            __setattr__ = dict.__setitem__\n            __delattr__ = dict.__delitem__\n\n        ctx = DownloadContext()\n        ctx.filename = filename\n        ctx.tmpfilename = self.temp_name(filename)\n        ctx.stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n\n        is_test = self.params.get('test', False)\n        chunk_size = self._TEST_FILE_SIZE if is_test else (\n            info_dict.get('downloader_options', {}).get('http_chunk_size')\n            or self.params.get('http_chunk_size') or 0)\n\n        ctx.open_mode = 'wb'\n        ctx.resume_len = 0\n        ctx.data_len = None\n        ctx.block_size = self.params.get('buffersize', 1024)\n        ctx.start_time = time.time()\n        ctx.chunk_size = None\n\n        if self.params.get('continuedl', True):\n            # Establish possible resume length\n            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n                ctx.resume_len = os.path.getsize(\n                    encodeFilename(ctx.tmpfilename))\n\n        ctx.is_resume = ctx.resume_len > 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n\n        class SucceedDownload(Exception):\n            pass\n\n        class RetryDownload(Exception):\n            def __init__(self, source_error):\n                self.source_error = source_error\n\n        class NextFragment(Exception):\n            pass\n\n        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)\n\n        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)\n\n        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True\n\n        while count <= retries:\n            try:\n                establish_connection()\n                return download()\n            except RetryDownload as e:\n                count += 1\n                if count <= retries:\n                    self.report_retry(e.source_error, count, retries)\n                continue\n            except NextFragment:\n                continue\n            except SucceedDownload:\n                return True\n\n        self.report_error('giving up after %s retries' % retries)\n        return False",
        "begin_line": 28,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002840909090909091,
            "pseudo_dstar_susp": 0.0045045045045045045,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.0045045045045045045,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.downloader.http.DownloadContext.real_download#28",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.DownloadContext",
        "signature": "youtube_dl.downloader.http.DownloadContext.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n\n        class DownloadContext(dict):\n            __getattr__ = dict.get\n            __setattr__ = dict.__setitem__\n            __delattr__ = dict.__delitem__\n\n        ctx = DownloadContext()\n        ctx.filename = filename\n        ctx.tmpfilename = self.temp_name(filename)\n        ctx.stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n\n        is_test = self.params.get('test', False)\n        chunk_size = self._TEST_FILE_SIZE if is_test else (\n            info_dict.get('downloader_options', {}).get('http_chunk_size')\n            or self.params.get('http_chunk_size') or 0)\n\n        ctx.open_mode = 'wb'\n        ctx.resume_len = 0\n        ctx.data_len = None\n        ctx.block_size = self.params.get('buffersize', 1024)\n        ctx.start_time = time.time()\n        ctx.chunk_size = None\n\n        if self.params.get('continuedl', True):\n            # Establish possible resume length\n            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n                ctx.resume_len = os.path.getsize(\n                    encodeFilename(ctx.tmpfilename))\n\n        ctx.is_resume = ctx.resume_len > 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n\n        class SucceedDownload(Exception):\n            pass\n\n        class RetryDownload(Exception):\n            def __init__(self, source_error):\n                self.source_error = source_error\n\n        class NextFragment(Exception):\n            pass\n\n        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)\n\n        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)\n\n        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True\n\n        while count <= retries:\n            try:\n                establish_connection()\n                return download()\n            except RetryDownload as e:\n                count += 1\n                if count <= retries:\n                    self.report_retry(e.source_error, count, retries)\n                continue\n            except NextFragment:\n                continue\n            except SucceedDownload:\n                return True\n\n        self.report_error('giving up after %s retries' % retries)\n        return False",
        "begin_line": 28,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00202020202020202,
            "pseudo_dstar_susp": 0.002173913043478261,
            "pseudo_tarantula_susp": 0.0004873294346978557,
            "pseudo_op2_susp": 0.002173913043478261,
            "pseudo_barinel_susp": 0.0004873294346978557
        }
    },
    {
        "name": "youtube_dl.downloader.http.SucceedDownload.real_download#28",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.SucceedDownload",
        "signature": "youtube_dl.downloader.http.SucceedDownload.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n\n        class DownloadContext(dict):\n            __getattr__ = dict.get\n            __setattr__ = dict.__setitem__\n            __delattr__ = dict.__delitem__\n\n        ctx = DownloadContext()\n        ctx.filename = filename\n        ctx.tmpfilename = self.temp_name(filename)\n        ctx.stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n\n        is_test = self.params.get('test', False)\n        chunk_size = self._TEST_FILE_SIZE if is_test else (\n            info_dict.get('downloader_options', {}).get('http_chunk_size')\n            or self.params.get('http_chunk_size') or 0)\n\n        ctx.open_mode = 'wb'\n        ctx.resume_len = 0\n        ctx.data_len = None\n        ctx.block_size = self.params.get('buffersize', 1024)\n        ctx.start_time = time.time()\n        ctx.chunk_size = None\n\n        if self.params.get('continuedl', True):\n            # Establish possible resume length\n            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n                ctx.resume_len = os.path.getsize(\n                    encodeFilename(ctx.tmpfilename))\n\n        ctx.is_resume = ctx.resume_len > 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n\n        class SucceedDownload(Exception):\n            pass\n\n        class RetryDownload(Exception):\n            def __init__(self, source_error):\n                self.source_error = source_error\n\n        class NextFragment(Exception):\n            pass\n\n        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)\n\n        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)\n\n        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True\n\n        while count <= retries:\n            try:\n                establish_connection()\n                return download()\n            except RetryDownload as e:\n                count += 1\n                if count <= retries:\n                    self.report_retry(e.source_error, count, retries)\n                continue\n            except NextFragment:\n                continue\n            except SucceedDownload:\n                return True\n\n        self.report_error('giving up after %s retries' % retries)\n        return False",
        "begin_line": 28,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011739845034045551,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.http.RetryDownload.real_download#28",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.RetryDownload",
        "signature": "youtube_dl.downloader.http.RetryDownload.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n\n        class DownloadContext(dict):\n            __getattr__ = dict.get\n            __setattr__ = dict.__setitem__\n            __delattr__ = dict.__delitem__\n\n        ctx = DownloadContext()\n        ctx.filename = filename\n        ctx.tmpfilename = self.temp_name(filename)\n        ctx.stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n\n        is_test = self.params.get('test', False)\n        chunk_size = self._TEST_FILE_SIZE if is_test else (\n            info_dict.get('downloader_options', {}).get('http_chunk_size')\n            or self.params.get('http_chunk_size') or 0)\n\n        ctx.open_mode = 'wb'\n        ctx.resume_len = 0\n        ctx.data_len = None\n        ctx.block_size = self.params.get('buffersize', 1024)\n        ctx.start_time = time.time()\n        ctx.chunk_size = None\n\n        if self.params.get('continuedl', True):\n            # Establish possible resume length\n            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n                ctx.resume_len = os.path.getsize(\n                    encodeFilename(ctx.tmpfilename))\n\n        ctx.is_resume = ctx.resume_len > 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n\n        class SucceedDownload(Exception):\n            pass\n\n        class RetryDownload(Exception):\n            def __init__(self, source_error):\n                self.source_error = source_error\n\n        class NextFragment(Exception):\n            pass\n\n        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)\n\n        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)\n\n        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True\n\n        while count <= retries:\n            try:\n                establish_connection()\n                return download()\n            except RetryDownload as e:\n                count += 1\n                if count <= retries:\n                    self.report_retry(e.source_error, count, retries)\n                continue\n            except NextFragment:\n                continue\n            except SucceedDownload:\n                return True\n\n        self.report_error('giving up after %s retries' % retries)\n        return False",
        "begin_line": 28,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022522522522522522,
            "pseudo_dstar_susp": 0.0023923444976076554,
            "pseudo_tarantula_susp": 0.0005181347150259067,
            "pseudo_op2_susp": 0.0023923444976076554,
            "pseudo_barinel_susp": 0.0005181347150259067
        }
    },
    {
        "name": "youtube_dl.downloader.http.RetryDownload.__init__#74",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.RetryDownload",
        "signature": "youtube_dl.downloader.http.RetryDownload.__init__(self, source_error)",
        "snippet": "            def __init__(self, source_error):\n                self.source_error = source_error",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011824524062906468,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.http.NextFragment.real_download#28",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.NextFragment",
        "signature": "youtube_dl.downloader.http.NextFragment.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n\n        class DownloadContext(dict):\n            __getattr__ = dict.get\n            __setattr__ = dict.__setitem__\n            __delattr__ = dict.__delitem__\n\n        ctx = DownloadContext()\n        ctx.filename = filename\n        ctx.tmpfilename = self.temp_name(filename)\n        ctx.stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n\n        is_test = self.params.get('test', False)\n        chunk_size = self._TEST_FILE_SIZE if is_test else (\n            info_dict.get('downloader_options', {}).get('http_chunk_size')\n            or self.params.get('http_chunk_size') or 0)\n\n        ctx.open_mode = 'wb'\n        ctx.resume_len = 0\n        ctx.data_len = None\n        ctx.block_size = self.params.get('buffersize', 1024)\n        ctx.start_time = time.time()\n        ctx.chunk_size = None\n\n        if self.params.get('continuedl', True):\n            # Establish possible resume length\n            if os.path.isfile(encodeFilename(ctx.tmpfilename)):\n                ctx.resume_len = os.path.getsize(\n                    encodeFilename(ctx.tmpfilename))\n\n        ctx.is_resume = ctx.resume_len > 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n\n        class SucceedDownload(Exception):\n            pass\n\n        class RetryDownload(Exception):\n            def __init__(self, source_error):\n                self.source_error = source_error\n\n        class NextFragment(Exception):\n            pass\n\n        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)\n\n        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)\n\n        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True\n\n        while count <= retries:\n            try:\n                establish_connection()\n                return download()\n            except RetryDownload as e:\n                count += 1\n                if count <= retries:\n                    self.report_retry(e.source_error, count, retries)\n                continue\n            except NextFragment:\n                continue\n            except SucceedDownload:\n                return True\n\n        self.report_error('giving up after %s retries' % retries)\n        return False",
        "begin_line": 28,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002232142857142857,
            "pseudo_dstar_susp": 0.002325581395348837,
            "pseudo_tarantula_susp": 0.0005341880341880342,
            "pseudo_op2_susp": 0.002325581395348837,
            "pseudo_barinel_susp": 0.0005341880341880342
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.set_range#80",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.set_range(req, start, end)",
        "snippet": "        def set_range(req, start, end):\n            range_header = 'bytes=%d-' % start\n            if end:\n                range_header += compat_str(end)\n            req.add_header('Range', range_header)",
        "begin_line": 80,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022222222222222222,
            "pseudo_dstar_susp": 0.0022988505747126436,
            "pseudo_tarantula_susp": 0.0005452562704471102,
            "pseudo_op2_susp": 0.0022988505747126436,
            "pseudo_barinel_susp": 0.0005452562704471102
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.establish_connection#86",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.establish_connection()",
        "snippet": "        def establish_connection():\n            ctx.chunk_size = (random.randint(int(chunk_size * 0.95), chunk_size)\n                              if not is_test and chunk_size else chunk_size)\n            if ctx.resume_len > 0:\n                range_start = ctx.resume_len\n                if ctx.is_resume:\n                    self.report_resuming_byte(ctx.resume_len)\n                ctx.open_mode = 'ab'\n            elif ctx.chunk_size > 0:\n                range_start = 0\n            else:\n                range_start = None\n            ctx.is_resume = False\n            range_end = range_start + ctx.chunk_size - 1 if ctx.chunk_size else None\n            if range_end and ctx.data_len is not None and range_end >= ctx.data_len:\n                range_end = ctx.data_len - 1\n            has_range = range_start is not None\n            ctx.has_range = has_range\n            request = sanitized_Request(url, None, headers)\n            if has_range:\n                set_range(request, range_start, range_end)\n            # Establish connection\n            try:\n                ctx.data = self.ydl.urlopen(request)\n                # When trying to resume, Content-Range HTTP header of response has to be checked\n                # to match the value of requested Range HTTP header. This is due to a webservers\n                # that don't support resuming and serve a whole file with no Content-Range\n                # set in response despite of requested Range (see\n                # https://github.com/ytdl-org/youtube-dl/issues/6057#issuecomment-126129799)\n                if has_range:\n                    content_range = ctx.data.headers.get('Content-Range')\n                    if content_range:\n                        content_range_m = re.search(r'bytes (\\d+)-(\\d+)?(?:/(\\d+))?', content_range)\n                        # Content-Range is present and matches requested Range, resume is possible\n                        if content_range_m:\n                            if range_start == int(content_range_m.group(1)):\n                                content_range_end = int_or_none(content_range_m.group(2))\n                                content_len = int_or_none(content_range_m.group(3))\n                                accept_content_len = (\n                                    # Non-chunked download\n                                    not ctx.chunk_size\n                                    # Chunked download and requested piece or\n                                    # its part is promised to be served\n                                    or content_range_end == range_end\n                                    or content_len < range_end)\n                                if accept_content_len:\n                                    ctx.data_len = content_len\n                                    return\n                    # Content-Range is either not present or invalid. Assuming remote webserver is\n                    # trying to send the whole file, resume is not possible, so wiping the local file\n                    # and performing entire redownload\n                    self.report_unable_to_resume()\n                    ctx.resume_len = 0\n                    ctx.open_mode = 'wb'\n                ctx.data_len = int_or_none(ctx.data.info().get('Content-length', None))\n                return\n            except (compat_urllib_error.HTTPError, ) as err:\n                if err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        ctx.data = self.ydl.urlopen(\n                            sanitized_Request(url, None, headers))\n                        content_length = ctx.data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None\n                                and (ctx.resume_len - 100 < int(content_length) < ctx.resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(ctx.filename)\n                            self.try_rename(ctx.tmpfilename, ctx.filename)\n                            self._hook_progress({\n                                'filename': ctx.filename,\n                                'status': 'finished',\n                                'downloaded_bytes': ctx.resume_len,\n                                'total_bytes': ctx.resume_len,\n                            })\n                            raise SucceedDownload()\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            ctx.resume_len = 0\n                            ctx.open_mode = 'wb'\n                            return\n                elif err.code < 500 or err.code >= 600:\n                    # Unexpected HTTP error\n                    raise\n                raise RetryDownload(err)\n            except socket.error as err:\n                if err.errno != errno.ECONNRESET:\n                    # Connection reset is no problem, just retry\n                    raise\n                raise RetryDownload(err)",
        "begin_line": 86,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.2,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.0008688097306689834,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.0008688097306689834
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.download#189",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.download()",
        "snippet": "        def download():\n            data_len = ctx.data.info().get('Content-length', None)\n\n            # Range HTTP header may be ignored/unsupported by a webserver\n            # (e.g. extractor/scivee.py, extractor/bambuser.py).\n            # However, for a test we still would like to download just a piece of a file.\n            # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n            # block size when downloading a file.\n            if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n                data_len = self._TEST_FILE_SIZE\n\n            if data_len is not None:\n                data_len = int(data_len) + ctx.resume_len\n                min_data_len = self.params.get('min_filesize')\n                max_data_len = self.params.get('max_filesize')\n                if min_data_len is not None and data_len < min_data_len:\n                    self.to_screen('\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                    return False\n                if max_data_len is not None and data_len > max_data_len:\n                    self.to_screen('\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                    return False\n\n            byte_counter = 0 + ctx.resume_len\n            block_size = ctx.block_size\n            start = time.time()\n\n            # measure time over whole while-loop, so slow_down() and best_block_size() work together properly\n            now = None  # needed for slow_down() in the first loop run\n            before = start  # start measuring\n\n            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)\n\n            while True:\n                try:\n                    # Download and write\n                    data_block = ctx.data.read(block_size if data_len is None else min(block_size, data_len - byte_counter))\n                # socket.timeout is a subclass of socket.error but may not have\n                # errno set\n                except socket.timeout as e:\n                    retry(e)\n                except socket.error as e:\n                    if e.errno not in (errno.ECONNRESET, errno.ETIMEDOUT):\n                        raise\n                    retry(e)\n\n                byte_counter += len(data_block)\n\n                # exit loop when download is finished\n                if len(data_block) == 0:\n                    break\n\n                # Open destination file just in time\n                if ctx.stream is None:\n                    try:\n                        ctx.stream, ctx.tmpfilename = sanitize_open(\n                            ctx.tmpfilename, ctx.open_mode)\n                        assert ctx.stream is not None\n                        ctx.filename = self.undo_temp_name(ctx.tmpfilename)\n                        self.report_destination(ctx.filename)\n                    except (OSError, IOError) as err:\n                        self.report_error('unable to open for writing: %s' % str(err))\n                        return False\n\n                    if self.params.get('xattr_set_filesize', False) and data_len is not None:\n                        try:\n                            write_xattr(ctx.tmpfilename, 'user.ytdl.filesize', str(data_len).encode('utf-8'))\n                        except (XAttrUnavailableError, XAttrMetadataError) as err:\n                            self.report_error('unable to set filesize xattr: %s' % str(err))\n\n                try:\n                    ctx.stream.write(data_block)\n                except (IOError, OSError) as err:\n                    self.to_stderr('\\n')\n                    self.report_error('unable to write data: %s' % str(err))\n                    return False\n\n                # Apply rate limit\n                self.slow_down(start, now, byte_counter - ctx.resume_len)\n\n                # end measuring of one loop run\n                now = time.time()\n                after = now\n\n                # Adjust block size\n                if not self.params.get('noresizebuffer', False):\n                    block_size = self.best_block_size(after - before, len(data_block))\n\n                before = after\n\n                # Progress message\n                speed = self.calc_speed(start, now, byte_counter - ctx.resume_len)\n                if ctx.data_len is None:\n                    eta = None\n                else:\n                    eta = self.calc_eta(start, time.time(), ctx.data_len - ctx.resume_len, byte_counter - ctx.resume_len)\n\n                self._hook_progress({\n                    'status': 'downloading',\n                    'downloaded_bytes': byte_counter,\n                    'total_bytes': ctx.data_len,\n                    'tmpfilename': ctx.tmpfilename,\n                    'filename': ctx.filename,\n                    'eta': eta,\n                    'speed': speed,\n                    'elapsed': now - ctx.start_time,\n                })\n\n                if data_len is not None and byte_counter == data_len:\n                    break\n\n            if not is_test and ctx.chunk_size and ctx.data_len is not None and byte_counter < ctx.data_len:\n                ctx.resume_len = byte_counter\n                # ctx.block_size = block_size\n                raise NextFragment()\n\n            if ctx.stream is None:\n                self.to_stderr('\\n')\n                self.report_error('Did not get any data blocks')\n                return False\n            if ctx.tmpfilename != '-':\n                ctx.stream.close()\n\n            if data_len is not None and byte_counter != data_len:\n                err = ContentTooShortError(byte_counter, int(data_len))\n                if count <= retries:\n                    retry(err)\n                raise err\n\n            self.try_rename(ctx.tmpfilename, ctx.filename)\n\n            # Update file modification time\n            if self.params.get('updatetime', True):\n                info_dict['filetime'] = self.try_utime(ctx.filename, ctx.data.info().get('last-modified', None))\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': byte_counter,\n                'filename': ctx.filename,\n                'status': 'finished',\n                'elapsed': time.time() - ctx.start_time,\n            })\n\n            return True",
        "begin_line": 189,
        "end_line": 337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.041666666666666664,
            "pseudo_dstar_susp": 0.06666666666666667,
            "pseudo_tarantula_susp": 0.0006510416666666666,
            "pseudo_op2_susp": 0.06666666666666667,
            "pseudo_barinel_susp": 0.0006510416666666666
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.retry#219",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.retry(e)",
        "snippet": "            def retry(e):\n                to_stdout = ctx.tmpfilename == '-'\n                if not to_stdout:\n                    ctx.stream.close()\n                ctx.stream = None\n                ctx.resume_len = byte_counter if to_stdout else os.path.getsize(encodeFilename(ctx.tmpfilename))\n                raise RetryDownload(e)",
        "begin_line": 219,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017123287671232876,
            "pseudo_dstar_susp": 0.0018975332068311196,
            "pseudo_tarantula_susp": 0.00029994001199760045,
            "pseudo_op2_susp": 0.0018975332068311196,
            "pseudo_barinel_susp": 0.00029994001199760045
        }
    },
    {
        "name": "youtube_dl.extractor.cbc.CBCWatchIE._real_extract#434",
        "src_path": "youtube_dl/extractor/cbc.py",
        "class_name": "youtube_dl.extractor.cbc.CBCWatchIE",
        "signature": "youtube_dl.extractor.cbc.CBCWatchIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        rss = self._call_api('web/browse/' + video_id, video_id)\n        return self._parse_rss_feed(rss)",
        "begin_line": 434,
        "end_line": 437,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00031735956839098697,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE._real_extract#664",
        "src_path": "youtube_dl/extractor/crunchyroll.py",
        "class_name": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE",
        "signature": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        show_id = self._match_id(url)\n\n        webpage = self._download_webpage(\n            self._add_skip_wall(url), show_id,\n            headers=self.geo_verification_headers())\n        title = self._html_search_meta('name', webpage, default=None)\n\n        episode_paths = re.findall(\n            r'(?s)<li id=\"showview_videos_media_(\\d+)\"[^>]+>.*?<a href=\"([^\"]+)\"',\n            webpage)\n        entries = [\n            self.url_result('http://www.crunchyroll.com' + ep, 'Crunchyroll', ep_id)\n            for ep_id, ep in episode_paths\n        ]\n        entries.reverse()\n\n        return {\n            '_type': 'playlist',\n            'id': show_id,\n            'title': title,\n            'entries': entries,\n        }",
        "begin_line": 664,
        "end_line": 686,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.dreisat.DreiSatIE._real_extract#190",
        "src_path": "youtube_dl/extractor/dreisat.py",
        "class_name": "youtube_dl.extractor.dreisat.DreiSatIE",
        "signature": "youtube_dl.extractor.dreisat.DreiSatIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        details_url = 'http://www.3sat.de/mediathek/xmlservice/web/beitragsDetails?id=%s' % video_id\n        return self.extract_from_xml_url(video_id, details_url)",
        "begin_line": 190,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.0001155001155001155,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._extract_vimeo_config#106",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor",
        "signature": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._extract_vimeo_config(self, webpage, video_id, *args, **kwargs)",
        "snippet": "    def _extract_vimeo_config(self, webpage, video_id, *args, **kwargs):\n        vimeo_config = self._search_regex(\n            r'vimeo\\.config\\s*=\\s*(?:({.+?})|_extend\\([^,]+,\\s+({.+?})\\));',\n            webpage, 'vimeo config', *args, **compat_kwargs(kwargs))\n        if vimeo_config:\n            return self._parse_json(vimeo_config, video_id)",
        "begin_line": 106,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003850596842510589,
            "pseudo_dstar_susp": 0.0003546099290780142,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0003546099290780142,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._vimeo_sort_formats#116",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor",
        "signature": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._vimeo_sort_formats(self, formats)",
        "snippet": "    def _vimeo_sort_formats(self, formats):\n        # Bitrates are completely broken. Single m3u8 may contain entries in kbps and bps\n        # at the same time without actual units specified. This lead to wrong sorting.\n        self._sort_formats(formats, field_preference=('preference', 'height', 'width', 'fps', 'tbr', 'format_id'))",
        "begin_line": 116,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033875338753387534,
            "pseudo_dstar_susp": 0.00034153005464480874,
            "pseudo_tarantula_susp": 0.0005640157924421884,
            "pseudo_op2_susp": 0.00034153005464480874,
            "pseudo_barinel_susp": 0.0005636978579481398
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._parse_config#121",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor",
        "signature": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._parse_config(self, config, video_id)",
        "snippet": "    def _parse_config(self, config, video_id):\n        video_data = config['video']\n        video_title = video_data['title']\n        live_event = video_data.get('live_event') or {}\n        is_live = live_event.get('status') == 'started'\n\n        formats = []\n        config_files = video_data.get('files') or config['request'].get('files', {})\n        for f in config_files.get('progressive', []):\n            video_url = f.get('url')\n            if not video_url:\n                continue\n            formats.append({\n                'url': video_url,\n                'format_id': 'http-%s' % f.get('quality'),\n                'width': int_or_none(f.get('width')),\n                'height': int_or_none(f.get('height')),\n                'fps': int_or_none(f.get('fps')),\n                'tbr': int_or_none(f.get('bitrate')),\n            })\n\n        # TODO: fix handling of 308 status code returned for live archive manifest requests\n        sep_pattern = r'/sep/video/'\n        for files_type in ('hls', 'dash'):\n            for cdn_name, cdn_data in config_files.get(files_type, {}).get('cdns', {}).items():\n                manifest_url = cdn_data.get('url')\n                if not manifest_url:\n                    continue\n                format_id = '%s-%s' % (files_type, cdn_name)\n                sep_manifest_urls = []\n                if re.search(sep_pattern, manifest_url):\n                    for suffix, repl in (('', 'video'), ('_sep', 'sep/video')):\n                        sep_manifest_urls.append((format_id + suffix, re.sub(\n                            sep_pattern, '/%s/' % repl, manifest_url)))\n                else:\n                    sep_manifest_urls = [(format_id, manifest_url)]\n                for f_id, m_url in sep_manifest_urls:\n                    if files_type == 'hls':\n                        formats.extend(self._extract_m3u8_formats(\n                            m_url, video_id, 'mp4',\n                            'm3u8' if is_live else 'm3u8_native', m3u8_id=f_id,\n                            note='Downloading %s m3u8 information' % cdn_name,\n                            fatal=False))\n                    elif files_type == 'dash':\n                        if 'json=1' in m_url:\n                            real_m_url = (self._download_json(m_url, video_id, fatal=False) or {}).get('url')\n                            if real_m_url:\n                                m_url = real_m_url\n                        mpd_formats = self._extract_mpd_formats(\n                            m_url.replace('/master.json', '/master.mpd'), video_id, f_id,\n                            'Downloading %s MPD information' % cdn_name,\n                            fatal=False)\n                        formats.extend(mpd_formats)\n\n        live_archive = live_event.get('archive') or {}\n        live_archive_source_url = live_archive.get('source_url')\n        if live_archive_source_url and live_archive.get('status') == 'done':\n            formats.append({\n                'format_id': 'live-archive-source',\n                'url': live_archive_source_url,\n                'preference': 1,\n            })\n\n        for f in formats:\n            if f.get('vcodec') == 'none':\n                f['preference'] = -50\n            elif f.get('acodec') == 'none':\n                f['preference'] = -40\n\n        subtitles = {}\n        text_tracks = config['request'].get('text_tracks')\n        if text_tracks:\n            for tt in text_tracks:\n                subtitles[tt['lang']] = [{\n                    'ext': 'vtt',\n                    'url': urljoin('https://vimeo.com', tt['url']),\n                }]\n\n        thumbnails = []\n        if not is_live:\n            for key, thumb in video_data.get('thumbs', {}).items():\n                thumbnails.append({\n                    'id': key,\n                    'width': int_or_none(key),\n                    'url': thumb,\n                })\n            thumbnail = video_data.get('thumbnail')\n            if thumbnail:\n                thumbnails.append({\n                    'url': thumbnail,\n                })\n\n        owner = video_data.get('owner') or {}\n        video_uploader_url = owner.get('url')\n\n        return {\n            'id': str_or_none(video_data.get('id')) or video_id,\n            'title': self._live_title(video_title) if is_live else video_title,\n            'uploader': owner.get('name'),\n            'uploader_id': video_uploader_url.split('/')[-1] if video_uploader_url else None,\n            'uploader_url': video_uploader_url,\n            'thumbnails': thumbnails,\n            'duration': int_or_none(video_data.get('duration')),\n            'formats': formats,\n            'subtitles': subtitles,\n            'is_live': is_live,\n        }",
        "begin_line": 121,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00044444444444444447,
            "pseudo_dstar_susp": 0.00041135335252982314,
            "pseudo_tarantula_susp": 0.003205128205128205,
            "pseudo_op2_susp": 0.00041135335252982314,
            "pseudo_barinel_susp": 0.003205128205128205
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._extract_original_format#229",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor",
        "signature": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._extract_original_format(self, url, video_id)",
        "snippet": "    def _extract_original_format(self, url, video_id):\n        download_data = self._download_json(\n            url, video_id, fatal=False,\n            query={'action': 'load_download_config'},\n            headers={'X-Requested-With': 'XMLHttpRequest'})\n        if download_data:\n            source_file = download_data.get('source_file')\n            if isinstance(source_file, dict):\n                download_url = source_file.get('download_url')\n                if download_url and not source_file.get('is_cold') and not source_file.get('is_defrosting'):\n                    source_name = source_file.get('public_name', 'Original')\n                    if self._is_valid_url(download_url, video_id, '%s video' % source_name):\n                        ext = (try_get(\n                            source_file, lambda x: x['extension'],\n                            compat_str) or determine_ext(\n                            download_url, None) or 'mp4').lower()\n                        return {\n                            'url': download_url,\n                            'ext': ext,\n                            'width': int_or_none(source_file.get('width')),\n                            'height': int_or_none(source_file.get('height')),\n                            'filesize': parse_filesize(source_file.get('size')),\n                            'format_id': source_name,\n                            'preference': 1,\n                        }",
        "begin_line": 229,
        "end_line": 253,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00031735956839098697,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE._extract_urls#522",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE._extract_urls(url, webpage)",
        "snippet": "    def _extract_urls(url, webpage):\n        urls = []\n        # Look for embedded (iframe) Vimeo player\n        for mobj in re.finditer(\n                r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//player\\.vimeo\\.com/video/\\d+.*?)\\1',\n                webpage):\n            urls.append(VimeoIE._smuggle_referrer(unescapeHTML(mobj.group('url')), url))\n        PLAIN_EMBED_RE = (\n            # Look for embedded (swf embed) Vimeo player\n            r'<embed[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?vimeo\\.com/moogaloop\\.swf.+?)\\1',\n            # Look more for non-standard embedded Vimeo player\n            r'<video[^>]+src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?vimeo\\.com/[0-9]+)\\1',\n        )\n        for embed_re in PLAIN_EMBED_RE:\n            for mobj in re.finditer(embed_re, webpage):\n                urls.append(mobj.group('url'))\n        return urls",
        "begin_line": 522,
        "end_line": 538,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000774593338497289,
            "pseudo_dstar_susp": 0.0007867820613690008,
            "pseudo_tarantula_susp": 0.00047938638542665386,
            "pseudo_op2_susp": 0.0007867820613690008,
            "pseudo_barinel_susp": 0.00047938638542665386
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE._real_initialize#562",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 562,
        "end_line": 563,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE._real_extract#565",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, data = unsmuggle_url(url, {})\n        headers = std_headers.copy()\n        if 'http_headers' in data:\n            headers.update(data['http_headers'])\n        if 'Referer' not in headers:\n            headers['Referer'] = url\n\n        channel_id = self._search_regex(\n            r'vimeo\\.com/channels/([^/]+)', url, 'channel id', default=None)\n\n        # Extract ID from URL\n        video_id = self._match_id(url)\n        orig_url = url\n        is_pro = 'vimeopro.com/' in url\n        is_player = '://player.vimeo.com/video/' in url\n        if is_pro:\n            # some videos require portfolio_id to be present in player url\n            # https://github.com/ytdl-org/youtube-dl/issues/20070\n            url = self._extract_url(url, self._download_webpage(url, video_id))\n            if not url:\n                url = 'https://vimeo.com/' + video_id\n        elif is_player:\n            url = 'https://player.vimeo.com/video/' + video_id\n        elif any(p in url for p in ('play_redirect_hls', 'moogaloop.swf')):\n            url = 'https://vimeo.com/' + video_id\n\n        try:\n            # Retrieve video webpage to extract further information\n            webpage, urlh = self._download_webpage_handle(\n                url, video_id, headers=headers)\n            redirect_url = urlh.geturl()\n        except ExtractorError as ee:\n            if isinstance(ee.cause, compat_HTTPError) and ee.cause.code == 403:\n                errmsg = ee.cause.read()\n                if b'Because of its privacy settings, this video cannot be played here' in errmsg:\n                    raise ExtractorError(\n                        'Cannot download embed-only video without embedding '\n                        'URL. Please call youtube-dl with the URL of the page '\n                        'that embeds this video.',\n                        expected=True)\n            raise\n\n        # Now we begin extracting as much information as we can from what we\n        # retrieved. First we extract the information common to all extractors,\n        # and latter we extract those that are Vimeo specific.\n        self.report_extraction(video_id)\n\n        vimeo_config = self._extract_vimeo_config(webpage, video_id, default=None)\n        if vimeo_config:\n            seed_status = vimeo_config.get('seed_status', {})\n            if seed_status.get('state') == 'failed':\n                raise ExtractorError(\n                    '%s said: %s' % (self.IE_NAME, seed_status['title']),\n                    expected=True)\n\n        cc_license = None\n        timestamp = None\n        video_description = None\n\n        # Extract the config JSON\n        try:\n            try:\n                config_url = self._html_search_regex(\n                    r' data-config-url=\"(.+?)\"', webpage,\n                    'config URL', default=None)\n                if not config_url:\n                    # Sometimes new react-based page is served instead of old one that require\n                    # different config URL extraction approach (see\n                    # https://github.com/ytdl-org/youtube-dl/pull/7209)\n                    page_config = self._parse_json(self._search_regex(\n                        r'vimeo\\.(?:clip|vod_title)_page_config\\s*=\\s*({.+?});',\n                        webpage, 'page config'), video_id)\n                    config_url = page_config['player']['config_url']\n                    cc_license = page_config.get('cc_license')\n                    timestamp = try_get(\n                        page_config, lambda x: x['clip']['uploaded_on'],\n                        compat_str)\n                    video_description = clean_html(dict_get(\n                        page_config, ('description', 'description_html_escaped')))\n                config = self._download_json(config_url, video_id)\n            except RegexNotFoundError:\n                # For pro videos or player.vimeo.com urls\n                # We try to find out to which variable is assigned the config dic\n                m_variable_name = re.search(r'(\\w)\\.video\\.id', webpage)\n                if m_variable_name is not None:\n                    config_re = [r'%s=({[^}].+?});' % re.escape(m_variable_name.group(1))]\n                else:\n                    config_re = [r' = {config:({.+?}),assets:', r'(?:[abc])=({.+?});']\n                config_re.append(r'\\bvar\\s+r\\s*=\\s*({.+?})\\s*;')\n                config_re.append(r'\\bconfig\\s*=\\s*({.+?})\\s*;')\n                config = self._search_regex(config_re, webpage, 'info section',\n                                            flags=re.DOTALL)\n                config = json.loads(config)\n        except Exception as e:\n            if re.search('The creator of this video has not given you permission to embed it on this domain.', webpage):\n                raise ExtractorError('The author has restricted the access to this video, try with the \"--referer\" option')\n\n            if re.search(r'<form[^>]+?id=\"pw_form\"', webpage) is not None:\n                if '_video_password_verified' in data:\n                    raise ExtractorError('video password verification failed!')\n                self._verify_video_password(redirect_url, video_id, webpage)\n                return self._real_extract(\n                    smuggle_url(redirect_url, {'_video_password_verified': 'verified'}))\n            else:\n                raise ExtractorError('Unable to extract info section',\n                                     cause=e)\n        else:\n            if config.get('view') == 4:\n                config = self._verify_player_video_password(redirect_url, video_id, headers)\n\n        vod = config.get('video', {}).get('vod', {})\n\n        def is_rented():\n            if '>You rented this title.<' in webpage:\n                return True\n            if config.get('user', {}).get('purchased'):\n                return True\n            for purchase_option in vod.get('purchase_options', []):\n                if purchase_option.get('purchased'):\n                    return True\n                label = purchase_option.get('label_string')\n                if label and (label.startswith('You rented this') or label.endswith(' remaining')):\n                    return True\n            return False\n\n        if is_rented() and vod.get('is_trailer'):\n            feature_id = vod.get('feature_id')\n            if feature_id and not data.get('force_feature_id', False):\n                return self.url_result(smuggle_url(\n                    'https://player.vimeo.com/player/%s' % feature_id,\n                    {'force_feature_id': True}), 'Vimeo')\n\n        # Extract video description\n        if not video_description:\n            video_description = self._html_search_regex(\n                r'(?s)<div\\s+class=\"[^\"]*description[^\"]*\"[^>]*>(.*?)</div>',\n                webpage, 'description', default=None)\n        if not video_description:\n            video_description = self._html_search_meta(\n                'description', webpage, default=None)\n        if not video_description and is_pro:\n            orig_webpage = self._download_webpage(\n                orig_url, video_id,\n                note='Downloading webpage for description',\n                fatal=False)\n            if orig_webpage:\n                video_description = self._html_search_meta(\n                    'description', orig_webpage, default=None)\n        if not video_description and not is_player:\n            self._downloader.report_warning('Cannot find video description')\n\n        # Extract upload date\n        if not timestamp:\n            timestamp = self._search_regex(\n                r'<time[^>]+datetime=\"([^\"]+)\"', webpage,\n                'timestamp', default=None)\n\n        try:\n            view_count = int(self._search_regex(r'UserPlays:(\\d+)', webpage, 'view count'))\n            like_count = int(self._search_regex(r'UserLikes:(\\d+)', webpage, 'like count'))\n            comment_count = int(self._search_regex(r'UserComments:(\\d+)', webpage, 'comment count'))\n        except RegexNotFoundError:\n            # This info is only available in vimeo.com/{id} urls\n            view_count = None\n            like_count = None\n            comment_count = None\n\n        formats = []\n\n        source_format = self._extract_original_format(\n            'https://vimeo.com/' + video_id, video_id)\n        if source_format:\n            formats.append(source_format)\n\n        info_dict_config = self._parse_config(config, video_id)\n        formats.extend(info_dict_config['formats'])\n        self._vimeo_sort_formats(formats)\n\n        json_ld = self._search_json_ld(webpage, video_id, default={})\n\n        if not cc_license:\n            cc_license = self._search_regex(\n                r'<link[^>]+rel=[\"\\']license[\"\\'][^>]+href=([\"\\'])(?P<license>(?:(?!\\1).)+)\\1',\n                webpage, 'license', default=None, group='license')\n\n        channel_url = 'https://vimeo.com/channels/%s' % channel_id if channel_id else None\n\n        info_dict = {\n            'formats': formats,\n            'timestamp': unified_timestamp(timestamp),\n            'description': video_description,\n            'webpage_url': url,\n            'view_count': view_count,\n            'like_count': like_count,\n            'comment_count': comment_count,\n            'license': cc_license,\n            'channel_id': channel_id,\n            'channel_url': channel_url,\n        }\n\n        info_dict = merge_dicts(info_dict, info_dict_config, json_ld)\n\n        return info_dict",
        "begin_line": 565,
        "end_line": 768,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00039541320680110717,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00039541320680110717,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE.is_rented#678",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE.is_rented()",
        "snippet": "        def is_rented():\n            if '>You rented this title.<' in webpage:\n                return True\n            if config.get('user', {}).get('purchased'):\n                return True\n            for purchase_option in vod.get('purchase_options', []):\n                if purchase_option.get('purchased'):\n                    return True\n                label = purchase_option.get('label_string')\n                if label and (label.startswith('You rented this') or label.endswith(' remaining')):\n                    return True\n            return False",
        "begin_line": 678,
        "end_line": 689,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.00038095238095238096,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00038095238095238096,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_list_title#842",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_list_title(self, webpage)",
        "snippet": "    def _extract_list_title(self, webpage):\n        return self._TITLE or self._html_search_regex(\n            self._TITLE_RE, webpage, 'list title', fatal=False)",
        "begin_line": 842,
        "end_line": 844,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._title_and_entries#846",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._title_and_entries(self, list_id, base_url)",
        "snippet": "    def _title_and_entries(self, list_id, base_url):\n        for pagenum in itertools.count(1):\n            page_url = self._page_url(base_url, pagenum)\n            webpage = self._download_webpage(\n                page_url, list_id,\n                'Downloading page %s' % pagenum)\n\n            if pagenum == 1:\n                yield self._extract_list_title(webpage)\n\n            # Try extracting href first since not all videos are available via\n            # short https://vimeo.com/id URL (e.g. https://vimeo.com/channels/tributes/6213729)\n            clips = re.findall(\n                r'id=\"clip_(\\d+)\"[^>]*>\\s*<a[^>]+href=\"(/(?:[^/]+/)*\\1)(?:[^>]+\\btitle=\"([^\"]+)\")?', webpage)\n            if clips:\n                for video_id, video_url, video_title in clips:\n                    yield self.url_result(\n                        compat_urlparse.urljoin(base_url, video_url),\n                        VimeoIE.ie_key(), video_id=video_id, video_title=video_title)\n            # More relaxed fallback\n            else:\n                for video_id in re.findall(r'id=[\"\\']clip_(\\d+)', webpage):\n                    yield self.url_result(\n                        'https://vimeo.com/%s' % video_id,\n                        VimeoIE.ie_key(), video_id=video_id)\n\n            if re.search(self._MORE_PAGES_INDICATOR, webpage, re.DOTALL) is None:\n                break",
        "begin_line": 846,
        "end_line": 873,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_videos#875",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_videos(self, list_id, base_url)",
        "snippet": "    def _extract_videos(self, list_id, base_url):\n        title_and_entries = self._title_and_entries(list_id, base_url)\n        list_title = next(title_and_entries)\n        return self.playlist_result(title_and_entries, list_id, list_title)",
        "begin_line": 875,
        "end_line": 878,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._real_extract#880",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        channel_id = self._match_id(url)\n        return self._extract_videos(channel_id, self._BASE_URL_TEMPL % channel_id)",
        "begin_line": 880,
        "end_line": 882,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00017914725904693657,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoAlbumIE._fetch_page#925",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoAlbumIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoAlbumIE._fetch_page(self, album_id, authorizaion, hashed_pass, page)",
        "snippet": "    def _fetch_page(self, album_id, authorizaion, hashed_pass, page):\n        api_page = page + 1\n        query = {\n            'fields': 'link,uri',\n            'page': api_page,\n            'per_page': self._PAGE_SIZE,\n        }\n        if hashed_pass:\n            query['_hashed_pass'] = hashed_pass\n        videos = self._download_json(\n            'https://api.vimeo.com/albums/%s/videos' % album_id,\n            album_id, 'Downloading page %d' % api_page, query=query, headers={\n                'Authorization': 'jwt ' + authorizaion,\n            })['data']\n        for video in videos:\n            link = video.get('link')\n            if not link:\n                continue\n            uri = video.get('uri')\n            video_id = self._search_regex(r'/videos/(\\d+)', uri, 'video_id', default=None) if uri else None\n            yield self.url_result(link, VimeoIE.ie_key(), video_id)",
        "begin_line": 925,
        "end_line": 945,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoReviewIE._real_initialize#1045",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoReviewIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoReviewIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 1045,
        "end_line": 1046,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoReviewIE._real_extract#1048",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoReviewIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoReviewIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        page_url, video_id = re.match(self._VALID_URL, url).groups()\n        clip_data = self._download_json(\n            page_url.replace('/review/', '/review/data/'),\n            video_id)['clipData']\n        config_url = clip_data['configUrl']\n        config = self._download_json(config_url, video_id)\n        info_dict = self._parse_config(config, video_id)\n        source_format = self._extract_original_format(\n            page_url + '/action', video_id)\n        if source_format:\n            info_dict['formats'].append(source_format)\n        self._vimeo_sort_formats(info_dict['formats'])\n        info_dict['description'] = clean_html(clip_data.get('description'))\n        return info_dict",
        "begin_line": 1048,
        "end_line": 1062,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoWatchLaterIE._real_extract#1087",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoWatchLaterIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoWatchLaterIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        return self._extract_videos('watchlater', 'https://vimeo.com/watchlater')",
        "begin_line": 1087,
        "end_line": 1088,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoLikesIE._real_extract#1110",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoLikesIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoLikesIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        user_id = self._match_id(url)\n        return self._extract_videos(user_id, 'https://vimeo.com/%s/likes' % user_id)",
        "begin_line": 1110,
        "end_line": 1112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.__init__.get_suitable_downloader#32",
        "src_path": "youtube_dl/downloader/__init__.py",
        "class_name": "youtube_dl.downloader.__init__",
        "signature": "youtube_dl.downloader.__init__.get_suitable_downloader(info_dict, params={})",
        "snippet": "def get_suitable_downloader(info_dict, params={}):\n    \"\"\"Get the downloader class that can handle the info dict.\"\"\"\n    protocol = determine_protocol(info_dict)\n    info_dict['protocol'] = protocol\n\n    # if (info_dict.get('start_time') or info_dict.get('end_time')) and not info_dict.get('requested_formats') and FFmpegFD.can_download(info_dict):\n    #     return FFmpegFD\n\n    external_downloader = params.get('external_downloader')\n    if external_downloader is not None:\n        ed = get_external_downloader(external_downloader)\n        if ed.can_download(info_dict):\n            return ed\n\n    if protocol.startswith('m3u8') and info_dict.get('is_live'):\n        return FFmpegFD\n\n    if protocol == 'm3u8' and params.get('hls_prefer_native') is True:\n        return HlsFD\n\n    if protocol == 'm3u8_native' and params.get('hls_prefer_native') is False:\n        return FFmpegFD\n\n    return PROTOCOL_MAP.get(protocol, HttpFD)",
        "begin_line": 32,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033112582781456954,
            "pseudo_dstar_susp": 0.023255813953488372,
            "pseudo_tarantula_susp": 0.000574052812858783,
            "pseudo_op2_susp": 0.023255813953488372,
            "pseudo_barinel_susp": 0.000574052812858783
        }
    },
    {
        "name": "youtube_dl.extractor.packtpub.PacktPubCourseIE.suitable#121",
        "src_path": "youtube_dl/extractor/packtpub.py",
        "class_name": "youtube_dl.extractor.packtpub.PacktPubCourseIE",
        "signature": "youtube_dl.extractor.packtpub.PacktPubCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if PacktPubIE.suitable(url) else super(\n            PacktPubCourseIE, cls).suitable(url)",
        "begin_line": 121,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.0003971405877680699,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.0003971405877680699,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.smotri.SmotriIE._extract_url#144",
        "src_path": "youtube_dl/extractor/smotri.py",
        "class_name": "youtube_dl.extractor.smotri.SmotriIE",
        "signature": "youtube_dl.extractor.smotri.SmotriIE._extract_url(cls, webpage)",
        "snippet": "    def _extract_url(cls, webpage):\n        mobj = re.search(\n            r'<embed[^>]src=([\"\\'])(?P<url>http://pics\\.smotri\\.com/(?:player|scrubber_custom8)\\.swf\\?file=v.+?\\1)',\n            webpage)\n        if mobj is not None:\n            return mobj.group('url')\n\n        mobj = re.search(\n            r'''(?x)<div\\s+class=\"video_file\">http://smotri\\.com/video/download/file/[^<]+</div>\\s*\n                    <div\\s+class=\"video_image\">[^<]+</div>\\s*\n                    <div\\s+class=\"video_id\">(?P<id>[^<]+)</div>''', webpage)\n        if mobj is not None:\n            return 'http://smotri.com/video/view/?id=%s' % mobj.group('id')",
        "begin_line": 144,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youporn.YouPornIE._real_extract#73",
        "src_path": "youtube_dl/extractor/youporn.py",
        "class_name": "youtube_dl.extractor.youporn.YouPornIE",
        "signature": "youtube_dl.extractor.youporn.YouPornIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        display_id = mobj.group('display_id') or video_id\n\n        webpage = self._download_webpage(\n            'http://www.youporn.com/watch/%s' % video_id, display_id,\n            headers={'Cookie': 'age_verified=1'})\n\n        title = self._html_search_regex(\n            r'(?s)<div[^>]+class=[\"\\']watchVideoTitle[^>]+>(.+?)</div>',\n            webpage, 'title', default=None) or self._og_search_title(\n            webpage, default=None) or self._html_search_meta(\n            'title', webpage, fatal=True)\n\n        links = []\n\n        # Main source\n        definitions = self._parse_json(\n            self._search_regex(\n                r'mediaDefinition\\s*=\\s*(\\[.+?\\]);', webpage,\n                'media definitions', default='[]'),\n            video_id, fatal=False)\n        if definitions:\n            for definition in definitions:\n                if not isinstance(definition, dict):\n                    continue\n                video_url = url_or_none(definition.get('videoUrl'))\n                if video_url:\n                    links.append(video_url)\n\n        # Fallback #1, this also contains extra low quality 180p format\n        for _, link in re.findall(r'<a[^>]+href=([\"\\'])(http.+?)\\1[^>]+title=[\"\\']Download [Vv]ideo', webpage):\n            links.append(link)\n\n        # Fallback #2 (unavailable as at 22.06.2017)\n        sources = self._search_regex(\n            r'(?s)sources\\s*:\\s*({.+?})', webpage, 'sources', default=None)\n        if sources:\n            for _, link in re.findall(r'[^:]+\\s*:\\s*([\"\\'])(http.+?)\\1', sources):\n                links.append(link)\n\n        # Fallback #3 (unavailable as at 22.06.2017)\n        for _, link in re.findall(\n                r'(?:videoSrc|videoIpadUrl|html5PlayerSrc)\\s*[:=]\\s*([\"\\'])(http.+?)\\1', webpage):\n            links.append(link)\n\n        # Fallback #4, encrypted links (unavailable as at 22.06.2017)\n        for _, encrypted_link in re.findall(\n                r'encryptedQuality\\d{3,4}URL\\s*=\\s*([\"\\'])([\\da-zA-Z+/=]+)\\1', webpage):\n            links.append(aes_decrypt_text(encrypted_link, title, 32).decode('utf-8'))\n\n        formats = []\n        for video_url in set(unescapeHTML(link) for link in links):\n            f = {\n                'url': video_url,\n            }\n            # Video URL's path looks like this:\n            #  /201012/17/505835/720p_1500k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4\n            #  /201012/17/505835/vl_240p_240k_505835/YouPorn%20-%20Sex%20Ed%20Is%20It%20Safe%20To%20Masturbate%20Daily.mp4\n            # We will benefit from it by extracting some metadata\n            mobj = re.search(r'(?P<height>\\d{3,4})[pP]_(?P<bitrate>\\d+)[kK]_\\d+/', video_url)\n            if mobj:\n                height = int(mobj.group('height'))\n                bitrate = int(mobj.group('bitrate'))\n                f.update({\n                    'format_id': '%dp-%dk' % (height, bitrate),\n                    'height': height,\n                    'tbr': bitrate,\n                })\n            formats.append(f)\n        self._sort_formats(formats)\n\n        description = self._html_search_regex(\n            r'(?s)<div[^>]+\\bid=[\"\\']description[\"\\'][^>]*>(.+?)</div>',\n            webpage, 'description',\n            default=None) or self._og_search_description(\n            webpage, default=None)\n        thumbnail = self._search_regex(\n            r'(?:imageurl\\s*=|poster\\s*:)\\s*([\"\\'])(?P<thumbnail>.+?)\\1',\n            webpage, 'thumbnail', fatal=False, group='thumbnail')\n\n        uploader = self._html_search_regex(\n            r'(?s)<div[^>]+class=[\"\\']submitByLink[\"\\'][^>]*>(.+?)</div>',\n            webpage, 'uploader', fatal=False)\n        upload_date = unified_strdate(self._html_search_regex(\n            [r'Date\\s+[Aa]dded:\\s*<span>([^<]+)',\n             r'(?s)<div[^>]+class=[\"\\']videoInfo(?:Date|Time)[\"\\'][^>]*>(.+?)</div>'],\n            webpage, 'upload date', fatal=False))\n\n        age_limit = self._rta_search(webpage)\n\n        average_rating = int_or_none(self._search_regex(\n            r'<div[^>]+class=[\"\\']videoRatingPercentage[\"\\'][^>]*>(\\d+)%</div>',\n            webpage, 'average rating', fatal=False))\n\n        view_count = str_to_int(self._search_regex(\n            r'(?s)<div[^>]+class=([\"\\']).*?\\bvideoInfoViews\\b.*?\\1[^>]*>.*?(?P<count>[\\d,.]+)<',\n            webpage, 'view count', fatal=False, group='count'))\n        comment_count = str_to_int(self._search_regex(\n            r'>All [Cc]omments? \\(([\\d,.]+)\\)',\n            webpage, 'comment count', fatal=False))\n\n        def extract_tag_box(regex, title):\n            tag_box = self._search_regex(regex, webpage, title, default=None)\n            if not tag_box:\n                return []\n            return re.findall(r'<a[^>]+href=[^>]+>([^<]+)', tag_box)\n\n        categories = extract_tag_box(\n            r'(?s)Categories:.*?</[^>]+>(.+?)</div>', 'categories')\n        tags = extract_tag_box(\n            r'(?s)Tags:.*?</div>\\s*<div[^>]+class=[\"\\']tagBoxContent[\"\\'][^>]*>(.+?)</div>',\n            'tags')\n\n        return {\n            'id': video_id,\n            'display_id': display_id,\n            'title': title,\n            'description': description,\n            'thumbnail': thumbnail,\n            'uploader': uploader,\n            'upload_date': upload_date,\n            'average_rating': average_rating,\n            'view_count': view_count,\n            'comment_count': comment_count,\n            'categories': categories,\n            'tags': tags,\n            'age_limit': age_limit,\n            'formats': formats,\n        }",
        "begin_line": 73,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.downloader.hls.HlsFD.real_download#58",
        "src_path": "youtube_dl/downloader/hls.py",
        "class_name": "youtube_dl.downloader.hls.HlsFD",
        "signature": "youtube_dl.downloader.hls.HlsFD.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        man_url = info_dict['url']\n        self.to_screen('[%s] Downloading m3u8 manifest' % self.FD_NAME)\n\n        urlh = self.ydl.urlopen(self._prepare_url(info_dict, man_url))\n        man_url = urlh.geturl()\n        s = urlh.read().decode('utf-8', 'ignore')\n\n        if not self.can_download(s, info_dict):\n            if info_dict.get('extra_param_to_segment_url') or info_dict.get('_decryption_key_url'):\n                self.report_error('pycrypto not found. Please install it.')\n                return False\n            self.report_warning(\n                'hlsnative has detected features it does not support, '\n                'extraction will be delegated to ffmpeg')\n            fd = FFmpegFD(self.ydl, self.params)\n            for ph in self._progress_hooks:\n                fd.add_progress_hook(ph)\n            return fd.real_download(filename, info_dict)\n\n        def is_ad_fragment_start(s):\n            return (s.startswith('#ANVATO-SEGMENT-INFO') and 'type=ad' in s\n                    or s.startswith('#UPLYNK-SEGMENT') and s.endswith(',ad'))\n\n        def is_ad_fragment_end(s):\n            return (s.startswith('#ANVATO-SEGMENT-INFO') and 'type=master' in s\n                    or s.startswith('#UPLYNK-SEGMENT') and s.endswith(',segment'))\n\n        media_frags = 0\n        ad_frags = 0\n        ad_frag_next = False\n        for line in s.splitlines():\n            line = line.strip()\n            if not line:\n                continue\n            if line.startswith('#'):\n                if is_ad_fragment_start(line):\n                    ad_frag_next = True\n                elif is_ad_fragment_end(line):\n                    ad_frag_next = False\n                continue\n            if ad_frag_next:\n                ad_frags += 1\n                continue\n            media_frags += 1\n\n        ctx = {\n            'filename': filename,\n            'total_frags': media_frags,\n            'ad_frags': ad_frags,\n        }\n\n        self._prepare_and_start_frag_download(ctx)\n\n        fragment_retries = self.params.get('fragment_retries', 0)\n        skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)\n        test = self.params.get('test', False)\n\n        extra_query = None\n        extra_param_to_segment_url = info_dict.get('extra_param_to_segment_url')\n        if extra_param_to_segment_url:\n            extra_query = compat_urlparse.parse_qs(extra_param_to_segment_url)\n        i = 0\n        media_sequence = 0\n        decrypt_info = {'METHOD': 'NONE'}\n        byte_range = {}\n        frag_index = 0\n        ad_frag_next = False\n        for line in s.splitlines():\n            line = line.strip()\n            if line:\n                if not line.startswith('#'):\n                    if ad_frag_next:\n                        continue\n                    frag_index += 1\n                    if frag_index <= ctx['fragment_index']:\n                        continue\n                    frag_url = (\n                        line\n                        if re.match(r'^https?://', line)\n                        else compat_urlparse.urljoin(man_url, line))\n                    if extra_query:\n                        frag_url = update_url_query(frag_url, extra_query)\n                    count = 0\n                    headers = info_dict.get('http_headers', {})\n                    if byte_range:\n                        headers['Range'] = 'bytes=%d-%d' % (byte_range['start'], byte_range['end'])\n                    while count <= fragment_retries:\n                        try:\n                            success, frag_content = self._download_fragment(\n                                ctx, frag_url, info_dict, headers)\n                            if not success:\n                                return False\n                            break\n                        except compat_urllib_error.HTTPError as err:\n                            # Unavailable (possibly temporary) fragments may be served.\n                            # First we try to retry then either skip or abort.\n                            # See https://github.com/ytdl-org/youtube-dl/issues/10165,\n                            # https://github.com/ytdl-org/youtube-dl/issues/10448).\n                            count += 1\n                            if count <= fragment_retries:\n                                self.report_retry_fragment(err, frag_index, count, fragment_retries)\n                    if count > fragment_retries:\n                        if skip_unavailable_fragments:\n                            i += 1\n                            media_sequence += 1\n                            self.report_skip_fragment(frag_index)\n                            continue\n                        self.report_error(\n                            'giving up after %s fragment retries' % fragment_retries)\n                        return False\n                    if decrypt_info['METHOD'] == 'AES-128':\n                        iv = decrypt_info.get('IV') or compat_struct_pack('>8xq', media_sequence)\n                        decrypt_info['KEY'] = decrypt_info.get('KEY') or self.ydl.urlopen(\n                            self._prepare_url(info_dict, info_dict.get('_decryption_key_url') or decrypt_info['URI'])).read()\n                        frag_content = AES.new(\n                            decrypt_info['KEY'], AES.MODE_CBC, iv).decrypt(frag_content)\n                    self._append_fragment(ctx, frag_content)\n                    # We only download the first fragment during the test\n                    if test:\n                        break\n                    i += 1\n                    media_sequence += 1\n                elif line.startswith('#EXT-X-KEY'):\n                    decrypt_url = decrypt_info.get('URI')\n                    decrypt_info = parse_m3u8_attributes(line[11:])\n                    if decrypt_info['METHOD'] == 'AES-128':\n                        if 'IV' in decrypt_info:\n                            decrypt_info['IV'] = binascii.unhexlify(decrypt_info['IV'][2:].zfill(32))\n                        if not re.match(r'^https?://', decrypt_info['URI']):\n                            decrypt_info['URI'] = compat_urlparse.urljoin(\n                                man_url, decrypt_info['URI'])\n                        if extra_query:\n                            decrypt_info['URI'] = update_url_query(decrypt_info['URI'], extra_query)\n                        if decrypt_url != decrypt_info['URI']:\n                            decrypt_info['KEY'] = None\n                elif line.startswith('#EXT-X-MEDIA-SEQUENCE'):\n                    media_sequence = int(line[22:])\n                elif line.startswith('#EXT-X-BYTERANGE'):\n                    splitted_byte_range = line[17:].split('@')\n                    sub_range_start = int(splitted_byte_range[1]) if len(splitted_byte_range) == 2 else byte_range['end']\n                    byte_range = {\n                        'start': sub_range_start,\n                        'end': sub_range_start + int(splitted_byte_range[0]),\n                    }\n                elif is_ad_fragment_start(line):\n                    ad_frag_next = True\n                elif is_ad_fragment_end(line):\n                    ad_frag_next = False\n\n        self._finish_frag_download(ctx)\n\n        return True",
        "begin_line": 58,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014285714285714286,
            "pseudo_dstar_susp": 0.0010905125408942203,
            "pseudo_tarantula_susp": 0.0005555555555555556,
            "pseudo_op2_susp": 0.0010905125408942203,
            "pseudo_barinel_susp": 0.0005555555555555556
        }
    },
    {
        "name": "youtube_dl.extractor.pornhub.PornHubIE._extract_urls#158",
        "src_path": "youtube_dl/extractor/pornhub.py",
        "class_name": "youtube_dl.extractor.pornhub.PornHubIE",
        "signature": "youtube_dl.extractor.pornhub.PornHubIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        return re.findall(\n            r'<iframe[^>]+?src=[\"\\'](?P<url>(?:https?:)?//(?:www\\.)?pornhub\\.(?:com|net)/embed/[\\da-z]+)',\n            webpage)",
        "begin_line": 158,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.0001858736059479554,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.pornhub.PornHubPagedVideoListIE.suitable#594",
        "src_path": "youtube_dl/extractor/pornhub.py",
        "class_name": "youtube_dl.extractor.pornhub.PornHubPagedVideoListIE",
        "signature": "youtube_dl.extractor.pornhub.PornHubPagedVideoListIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False\n                if PornHubIE.suitable(url) or PornHubUserIE.suitable(url) or PornHubUserVideosUploadIE.suitable(url)\n                else super(PornHubPagedVideoListIE, cls).suitable(url))",
        "begin_line": 594,
        "end_line": 597,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.__init__#61",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.__init__(self, ydl, params)",
        "snippet": "    def __init__(self, ydl, params):\n        \"\"\"Create a FileDownloader object with the given options.\"\"\"\n        self.ydl = ydl\n        self._progress_hooks = []\n        self.params = params\n        self.add_progress_hook(self.report_progress)",
        "begin_line": 61,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002183406113537118,
            "pseudo_dstar_susp": 0.0023148148148148147,
            "pseudo_tarantula_susp": 0.0004923682914820286,
            "pseudo_op2_susp": 0.0023148148148148147,
            "pseudo_barinel_susp": 0.0004923682914820286
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.format_seconds#69",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.format_seconds(seconds)",
        "snippet": "    def format_seconds(seconds):\n        (mins, secs) = divmod(seconds, 60)\n        (hours, mins) = divmod(mins, 60)\n        if hours > 99:\n            return '--:--:--'\n        if hours == 0:\n            return '%02d:%02d' % (mins, secs)\n        else:\n            return '%02d:%02d:%02d' % (hours, mins, secs)",
        "begin_line": 69,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011904761904761905,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.calc_eta#92",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.calc_eta(start, now, total, current)",
        "snippet": "    def calc_eta(start, now, total, current):\n        if total is None:\n            return None\n        if now is None:\n            now = time.time()\n        dif = now - start\n        if current == 0 or dif < 0.001:  # One millisecond\n            return None\n        rate = float(current) / dif\n        return int((float(total) - float(current)) / rate)",
        "begin_line": 92,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002053388090349076,
            "pseudo_dstar_susp": 0.0022675736961451248,
            "pseudo_tarantula_susp": 0.00048496605237633366,
            "pseudo_op2_susp": 0.0022675736961451248,
            "pseudo_barinel_susp": 0.00048496605237633366
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.format_eta#104",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.format_eta(eta)",
        "snippet": "    def format_eta(eta):\n        if eta is None:\n            return '--:--'\n        return FileDownloader.format_seconds(eta)",
        "begin_line": 104,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018083182640144665,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.calc_speed#110",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.calc_speed(start, now, bytes)",
        "snippet": "    def calc_speed(start, now, bytes):\n        dif = now - start\n        if bytes == 0 or dif < 0.001:  # One millisecond\n            return None\n        return float(bytes) / dif",
        "begin_line": 110,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.03125,
            "pseudo_dstar_susp": 0.045454545454545456,
            "pseudo_tarantula_susp": 0.0006277463904582549,
            "pseudo_op2_susp": 0.045454545454545456,
            "pseudo_barinel_susp": 0.0006277463904582549
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.best_block_size#127",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.best_block_size(elapsed_time, bytes)",
        "snippet": "    def best_block_size(elapsed_time, bytes):\n        new_min = max(bytes / 2.0, 1.0)\n        new_max = min(max(bytes * 2.0, 1.0), 4194304)  # Do not surpass 4 MB\n        if elapsed_time < 0.001:\n            return int(new_max)\n        rate = bytes / elapsed_time\n        if rate > new_max:\n            return int(new_max)\n        if rate < new_min:\n            return int(new_min)\n        return int(rate)",
        "begin_line": 127,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00017812611328820805,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.to_screen#149",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.to_screen(self, *args, **kargs)",
        "snippet": "    def to_screen(self, *args, **kargs):\n        self.ydl.to_screen(*args, **kargs)",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.to_console_title#155",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.to_console_title(self, message)",
        "snippet": "    def to_console_title(self, message):\n        self.ydl.to_console_title(message)",
        "begin_line": 155,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.slow_down#167",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.slow_down(self, start_time, now, byte_counter)",
        "snippet": "    def slow_down(self, start_time, now, byte_counter):\n        \"\"\"Sleep if the download speed is over the rate limit.\"\"\"\n        rate_limit = self.params.get('ratelimit')\n        if rate_limit is None or byte_counter == 0:\n            return\n        if now is None:\n            now = time.time()\n        elapsed = now - start_time\n        if elapsed <= 0.0:\n            return\n        speed = float(byte_counter) / elapsed\n        if speed > rate_limit:\n            sleep_time = float(byte_counter) / rate_limit - elapsed\n            if sleep_time > 0:\n                time.sleep(sleep_time)",
        "begin_line": 167,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001652892561983471,
            "pseudo_dstar_susp": 0.001718213058419244,
            "pseudo_tarantula_susp": 0.000444247001332741,
            "pseudo_op2_susp": 0.001718213058419244,
            "pseudo_barinel_susp": 0.000444247001332741
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.temp_name#183",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.temp_name(self, filename)",
        "snippet": "    def temp_name(self, filename):\n        \"\"\"Returns a temporary filename for the given filename.\"\"\"\n        if self.params.get('nopart', False) or filename == '-' or \\\n                (os.path.exists(encodeFilename(filename)) and not os.path.isfile(encodeFilename(filename))):\n            return filename\n        return filename + '.part'",
        "begin_line": 183,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001869158878504673,
            "pseudo_dstar_susp": 0.00205761316872428,
            "pseudo_tarantula_susp": 0.0003720238095238095,
            "pseudo_op2_susp": 0.00205761316872428,
            "pseudo_barinel_susp": 0.0003720238095238095
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.undo_temp_name#190",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.undo_temp_name(self, filename)",
        "snippet": "    def undo_temp_name(self, filename):\n        if filename.endswith('.part'):\n            return filename[:-len('.part')]\n        return filename",
        "begin_line": 190,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011668611435239206,
            "pseudo_dstar_susp": 0.00117096018735363,
            "pseudo_tarantula_susp": 0.0004008016032064128,
            "pseudo_op2_susp": 0.00117096018735363,
            "pseudo_barinel_susp": 0.0004008016032064128
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.try_rename#198",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.try_rename(self, old_filename, new_filename)",
        "snippet": "    def try_rename(self, old_filename, new_filename):\n        try:\n            if old_filename == new_filename:\n                return\n            os.rename(encodeFilename(old_filename), encodeFilename(new_filename))\n        except (IOError, OSError) as err:\n            self.report_error('unable to rename file: %s' % error_to_compat_str(err))",
        "begin_line": 198,
        "end_line": 204,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0047169811320754715,
            "pseudo_dstar_susp": 0.02857142857142857,
            "pseudo_tarantula_susp": 0.000864304235090752,
            "pseudo_op2_susp": 0.02857142857142857,
            "pseudo_barinel_susp": 0.000864304235090752
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.try_utime#206",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.try_utime(self, filename, last_modified_hdr)",
        "snippet": "    def try_utime(self, filename, last_modified_hdr):\n        \"\"\"Try to set the last-modified time of the given file.\"\"\"\n        if last_modified_hdr is None:\n            return\n        if not os.path.isfile(encodeFilename(filename)):\n            return\n        timestr = last_modified_hdr\n        if timestr is None:\n            return\n        filetime = timeconvert(timestr)\n        if filetime is None:\n            return filetime\n        # Ignore obviously invalid dates\n        if filetime == 0:\n            return\n        try:\n            os.utime(filename, (time.time(), filetime))\n        except Exception:\n            pass\n        return filetime",
        "begin_line": 206,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024691358024691358,
            "pseudo_dstar_susp": 0.002320185614849188,
            "pseudo_tarantula_susp": 0.0009132420091324201,
            "pseudo_op2_susp": 0.002320185614849188,
            "pseudo_barinel_susp": 0.0009132420091324201
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.report_destination#227",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.report_destination(self, filename)",
        "snippet": "    def report_destination(self, filename):\n        \"\"\"Report destination filename.\"\"\"\n        self.to_screen('[download] Destination: ' + filename)",
        "begin_line": 227,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022172949002217295,
            "pseudo_dstar_susp": 0.0022935779816513763,
            "pseudo_tarantula_susp": 0.0005443658138268917,
            "pseudo_op2_susp": 0.0022935779816513763,
            "pseudo_barinel_susp": 0.0005443658138268917
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader._report_progress_status#231",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader._report_progress_status(self, msg, is_last_line=False)",
        "snippet": "    def _report_progress_status(self, msg, is_last_line=False):\n        fullmsg = '[download] ' + msg\n        if self.params.get('progress_with_newline', False):\n            self.to_screen(fullmsg)\n        else:\n            if compat_os_name == 'nt':\n                prev_len = getattr(self, '_report_progress_prev_line_length',\n                                   0)\n                if prev_len > len(fullmsg):\n                    fullmsg += ' ' * (prev_len - len(fullmsg))\n                self._report_progress_prev_line_length = len(fullmsg)\n                clear_line = '\\r'\n            else:\n                clear_line = ('\\r\\x1b[K' if sys.stderr.isatty() else '\\r')\n            self.to_screen(clear_line + fullmsg, skip_eol=not is_last_line)\n        self.to_console_title('youtube-dl ' + msg)",
        "begin_line": 231,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002551020408163265,
            "pseudo_dstar_susp": 0.0028089887640449437,
            "pseudo_tarantula_susp": 0.0006134969325153375,
            "pseudo_op2_susp": 0.0028089887640449437,
            "pseudo_barinel_susp": 0.0006134969325153375
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.report_progress#248",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.report_progress(self, s)",
        "snippet": "    def report_progress(self, s):\n        if s['status'] == 'finished':\n            if self.params.get('noprogress', False):\n                self.to_screen('[download] Download completed')\n            else:\n                msg_template = '100%%'\n                if s.get('total_bytes') is not None:\n                    s['_total_bytes_str'] = format_bytes(s['total_bytes'])\n                    msg_template += ' of %(_total_bytes_str)s'\n                if s.get('elapsed') is not None:\n                    s['_elapsed_str'] = self.format_seconds(s['elapsed'])\n                    msg_template += ' in %(_elapsed_str)s'\n                self._report_progress_status(\n                    msg_template % s, is_last_line=True)\n\n        if self.params.get('noprogress'):\n            return\n\n        if s['status'] != 'downloading':\n            return\n\n        if s.get('eta') is not None:\n            s['_eta_str'] = self.format_eta(s['eta'])\n        else:\n            s['_eta_str'] = 'Unknown ETA'\n\n        if s.get('total_bytes') and s.get('downloaded_bytes') is not None:\n            s['_percent_str'] = self.format_percent(100 * s['downloaded_bytes'] / s['total_bytes'])\n        elif s.get('total_bytes_estimate') and s.get('downloaded_bytes') is not None:\n            s['_percent_str'] = self.format_percent(100 * s['downloaded_bytes'] / s['total_bytes_estimate'])\n        else:\n            if s.get('downloaded_bytes') == 0:\n                s['_percent_str'] = self.format_percent(0)\n            else:\n                s['_percent_str'] = 'Unknown %'\n\n        if s.get('speed') is not None:\n            s['_speed_str'] = self.format_speed(s['speed'])\n        else:\n            s['_speed_str'] = 'Unknown speed'\n\n        if s.get('total_bytes') is not None:\n            s['_total_bytes_str'] = format_bytes(s['total_bytes'])\n            msg_template = '%(_percent_str)s of %(_total_bytes_str)s at %(_speed_str)s ETA %(_eta_str)s'\n        elif s.get('total_bytes_estimate') is not None:\n            s['_total_bytes_estimate_str'] = format_bytes(s['total_bytes_estimate'])\n            msg_template = '%(_percent_str)s of ~%(_total_bytes_estimate_str)s at %(_speed_str)s ETA %(_eta_str)s'\n        else:\n            if s.get('downloaded_bytes') is not None:\n                s['_downloaded_bytes_str'] = format_bytes(s['downloaded_bytes'])\n                if s.get('elapsed'):\n                    s['_elapsed_str'] = self.format_seconds(s['elapsed'])\n                    msg_template = '%(_downloaded_bytes_str)s at %(_speed_str)s (%(_elapsed_str)s)'\n                else:\n                    msg_template = '%(_downloaded_bytes_str)s at %(_speed_str)s'\n            else:\n                msg_template = '%(_percent_str)s % at %(_speed_str)s ETA %(_eta_str)s'\n\n        self._report_progress_status(msg_template % s)",
        "begin_line": 248,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002512562814070352,
            "pseudo_dstar_susp": 0.0028011204481792717,
            "pseudo_tarantula_susp": 0.0006305170239596469,
            "pseudo_op2_susp": 0.0028011204481792717,
            "pseudo_barinel_susp": 0.0006305170239596469
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.report_unable_to_resume#325",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.report_unable_to_resume(self)",
        "snippet": "    def report_unable_to_resume(self):\n        \"\"\"Report it was impossible to resume download.\"\"\"\n        self.to_screen('[download] Unable to resume')",
        "begin_line": 325,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.download#329",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.download(self, filename, info_dict)",
        "snippet": "    def download(self, filename, info_dict):\n        \"\"\"Download to a filename using the info from info_dict\n        Return True on success and False otherwise\n        \"\"\"\n\n        nooverwrites_and_exists = (\n            self.params.get('nooverwrites', False)\n            and os.path.exists(encodeFilename(filename))\n        )\n\n        if not hasattr(filename, 'write'):\n            continuedl_and_exists = (\n                self.params.get('continuedl', True)\n                and os.path.isfile(encodeFilename(filename))\n                and not self.params.get('nopart', False)\n            )\n\n            # Check file already present\n            if filename != '-' and (nooverwrites_and_exists or continuedl_and_exists):\n                self.report_file_already_downloaded(filename)\n                self._hook_progress({\n                    'filename': filename,\n                    'status': 'finished',\n                    'total_bytes': os.path.getsize(encodeFilename(filename)),\n                })\n                return True\n\n        min_sleep_interval = self.params.get('sleep_interval')\n        if min_sleep_interval:\n            max_sleep_interval = self.params.get('max_sleep_interval', min_sleep_interval)\n            sleep_interval = random.uniform(min_sleep_interval, max_sleep_interval)\n            self.to_screen(\n                '[download] Sleeping %s seconds...' % (\n                    int(sleep_interval) if sleep_interval.is_integer()\n                    else '%.2f' % sleep_interval))\n            time.sleep(sleep_interval)\n\n        return self.real_download(filename, info_dict)",
        "begin_line": 329,
        "end_line": 366,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009407337723424271,
            "pseudo_dstar_susp": 0.0009900990099009901,
            "pseudo_tarantula_susp": 0.0003508771929824561,
            "pseudo_op2_susp": 0.0009900990099009901,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader._hook_progress#372",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader._hook_progress(self, status)",
        "snippet": "    def _hook_progress(self, status):\n        for ph in self._progress_hooks:\n            ph(status)",
        "begin_line": 372,
        "end_line": 374,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002760905577029266,
            "pseudo_dstar_susp": 0.000271370420624152,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.000271370420624152,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.add_progress_hook#376",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.add_progress_hook(self, ph)",
        "snippet": "    def add_progress_hook(self, ph):\n        # See YoutubeDl.py (search for progress_hooks) for a description of\n        # this interface\n        self._progress_hooks.append(ph)",
        "begin_line": 376,
        "end_line": 379,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009345794392523365,
            "pseudo_dstar_susp": 0.0009832841691248771,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.0009832841691248771,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE.suitable#113",
        "src_path": "youtube_dl/extractor/ciscolive.py",
        "class_name": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE",
        "signature": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if CiscoLiveSessionIE.suitable(url) else super(CiscoLiveSearchIE, cls).suitable(url)",
        "begin_line": 113,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE._entries#120",
        "src_path": "youtube_dl/extractor/ciscolive.py",
        "class_name": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE",
        "signature": "youtube_dl.extractor.ciscolive.CiscoLiveSearchIE._entries(self, query, url)",
        "snippet": "    def _entries(self, query, url):\n        query['size'] = 50\n        query['from'] = 0\n        for page_num in itertools.count(1):\n            results = self._call_api(\n                'search', None, query, url,\n                'Downloading search JSON page %d' % page_num)\n            sl = try_get(results, lambda x: x['sectionList'][0], dict)\n            if sl:\n                results = sl\n            items = results.get('items')\n            if not items or not isinstance(items, list):\n                break\n            for item in items:\n                if not isinstance(item, dict):\n                    continue\n                if not self._check_bc_id_exists(item):\n                    continue\n                yield self._parse_rf_item(item)\n            size = int_or_none(results.get('size'))\n            if size is not None:\n                query['size'] = size\n            total = int_or_none(results.get('total'))\n            if total is not None and query['from'] + query['size'] > total:\n                break\n            query['from'] += query['size']",
        "begin_line": 120,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011466574934067194,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.__init__#343",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.__init__(self, params=None, auto_init=True)",
        "snippet": "    def __init__(self, params=None, auto_init=True):\n        \"\"\"Create a FileDownloader object with the given options.\"\"\"\n        if params is None:\n            params = {}\n        self._ies = []\n        self._ies_instances = {}\n        self._pps = []\n        self._progress_hooks = []\n        self._download_retcode = 0\n        self._num_downloads = 0\n        self._screen_file = [sys.stdout, sys.stderr][params.get('logtostderr', False)]\n        self._err_file = sys.stderr\n        self.params = {\n            # Default parameters\n            'nocheckcertificate': False,\n        }\n        self.params.update(params)\n        self.cache = Cache(self)\n\n        def check_deprecated(param, option, suggestion):\n            if self.params.get(param) is not None:\n                self.report_warning(\n                    '%s is deprecated. Use %s instead.' % (option, suggestion))\n                return True\n            return False\n\n        if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):\n            if self.params.get('geo_verification_proxy') is None:\n                self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']\n\n        check_deprecated('autonumber_size', '--autonumber-size', 'output template with %(autonumber)0Nd, where N in the number of digits')\n        check_deprecated('autonumber', '--auto-number', '-o \"%(autonumber)s-%(title)s.%(ext)s\"')\n        check_deprecated('usetitle', '--title', '-o \"%(title)s-%(id)s.%(ext)s\"')\n\n        if params.get('bidi_workaround', False):\n            try:\n                import pty\n                master, slave = pty.openpty()\n                width = compat_get_terminal_size().columns\n                if width is None:\n                    width_args = []\n                else:\n                    width_args = ['-w', str(width)]\n                sp_kwargs = dict(\n                    stdin=subprocess.PIPE,\n                    stdout=slave,\n                    stderr=self._err_file)\n                try:\n                    self._output_process = subprocess.Popen(\n                        ['bidiv'] + width_args, **sp_kwargs\n                    )\n                except OSError:\n                    self._output_process = subprocess.Popen(\n                        ['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)\n                self._output_channel = os.fdopen(master, 'rb')\n            except OSError as ose:\n                if ose.errno == errno.ENOENT:\n                    self.report_warning('Could not find fribidi executable, ignoring --bidi-workaround . Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')\n                else:\n                    raise\n\n        if (sys.platform != 'win32'\n                and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']\n                and not params.get('restrictfilenames', False)):\n            # Unicode filesystem API will throw errors (#1474, #13027)\n            self.report_warning(\n                'Assuming --restrict-filenames since file system encoding '\n                'cannot encode all characters. '\n                'Set the LC_ALL environment variable to fix this.')\n            self.params['restrictfilenames'] = True\n\n        if isinstance(params.get('outtmpl'), bytes):\n            self.report_warning(\n                'Parameter outtmpl is bytes, but should be a unicode string. '\n                'Put  from __future__ import unicode_literals  at the top of your code file or consider switching to Python 3.x.')\n\n        self._setup_opener()\n\n        if auto_init:\n            self.print_debug_header()\n            self.add_default_info_extractors()\n\n        for pp_def_raw in self.params.get('postprocessors', []):\n            pp_class = get_postprocessor(pp_def_raw['key'])\n            pp_def = dict(pp_def_raw)\n            del pp_def['key']\n            pp = pp_class(self, **compat_kwargs(pp_def))\n            self.add_post_processor(pp)\n\n        for ph in self.params.get('progress_hooks', []):\n            self.add_progress_hook(ph)\n\n        register_socks_protocols()",
        "begin_line": 343,
        "end_line": 435,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007246376811594203,
            "pseudo_dstar_susp": 0.020833333333333332,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.020833333333333332,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.check_deprecated#362",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.check_deprecated(param, option, suggestion)",
        "snippet": "        def check_deprecated(param, option, suggestion):\n            if self.params.get(param) is not None:\n                self.report_warning(\n                    '%s is deprecated. Use %s instead.' % (option, suggestion))\n                return True\n            return False",
        "begin_line": 362,
        "end_line": 367,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005917159763313609,
            "pseudo_dstar_susp": 0.016666666666666666,
            "pseudo_tarantula_susp": 0.001072961373390558,
            "pseudo_op2_susp": 0.016666666666666666,
            "pseudo_barinel_susp": 0.001072961373390558
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_info_extractor#453",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_info_extractor(self, ie)",
        "snippet": "    def add_info_extractor(self, ie):\n        \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\n        self._ies.append(ie)\n        if not isinstance(ie, type):\n            self._ies_instances[ie.ie_key()] = ie\n            ie.set_downloader(self)",
        "begin_line": 453,
        "end_line": 458,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014492753623188406,
            "pseudo_dstar_susp": 0.008064516129032258,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.008064516129032258,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.get_info_extractor#460",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.get_info_extractor(self, ie_key)",
        "snippet": "    def get_info_extractor(self, ie_key):\n        \"\"\"\n        Get an instance of an IE with name ie_key, it will try to get one from\n        the _ies list, if there's no instance it will create a new one and add\n        it to the extractor list.\n        \"\"\"\n        ie = self._ies_instances.get(ie_key)\n        if ie is None:\n            ie = get_info_extractor(ie_key)()\n            self.add_info_extractor(ie)\n        return ie",
        "begin_line": 460,
        "end_line": 470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.016666666666666666,
            "pseudo_dstar_susp": 0.008695652173913044,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.008695652173913044,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_default_info_extractors#472",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_default_info_extractors(self)",
        "snippet": "    def add_default_info_extractors(self):\n        \"\"\"\n        Add the InfoExtractors returned by gen_extractors to the end of the list\n        \"\"\"\n        for ie in gen_extractor_classes():\n            self.add_info_extractor(ie)",
        "begin_line": 472,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006329113924050633,
            "pseudo_dstar_susp": 0.00510204081632653,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.00510204081632653,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_post_processor#479",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_post_processor(self, pp)",
        "snippet": "    def add_post_processor(self, pp):\n        \"\"\"Add a PostProcessor object to the end of the chain.\"\"\"\n        self._pps.append(pp)\n        pp.set_downloader(self)",
        "begin_line": 479,
        "end_line": 482,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_progress_hook#484",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_progress_hook(self, ph)",
        "snippet": "    def add_progress_hook(self, ph):\n        \"\"\"Add the progress hook (currently only for the file downloader)\"\"\"\n        self._progress_hooks.append(ph)",
        "begin_line": 484,
        "end_line": 486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0064516129032258064,
            "pseudo_dstar_susp": 0.004975124378109453,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.004975124378109453,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._bidi_workaround#488",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._bidi_workaround(self, message)",
        "snippet": "    def _bidi_workaround(self, message):\n        if not hasattr(self, '_output_channel'):\n            return message\n\n        assert hasattr(self, '_output_process')\n        assert isinstance(message, compat_str)\n        line_count = message.count('\\n') + 1\n        self._output_process.stdin.write((message + '\\n').encode('utf-8'))\n        self._output_process.stdin.flush()\n        res = ''.join(self._output_channel.readline().decode('utf-8')\n                      for _ in range(line_count))\n        return res[:-len('\\n')]",
        "begin_line": 488,
        "end_line": 499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007575757575757576,
            "pseudo_dstar_susp": 0.0055248618784530384,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.0055248618784530384,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_screen#501",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_screen(self, message, skip_eol=False)",
        "snippet": "    def to_screen(self, message, skip_eol=False):\n        \"\"\"Print message to stdout if not in quiet mode.\"\"\"\n        return self.to_stdout(message, skip_eol, check_quiet=True)",
        "begin_line": 501,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0070921985815602835,
            "pseudo_dstar_susp": 0.005154639175257732,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.005154639175257732,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._write_string#505",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._write_string(self, s, out=None)",
        "snippet": "    def _write_string(self, s, out=None):\n        write_string(s, out=out, encoding=self.params.get('encoding'))",
        "begin_line": 505,
        "end_line": 506,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007575757575757576,
            "pseudo_dstar_susp": 0.0055248618784530384,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.0055248618784530384,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_stdout#508",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_stdout(self, message, skip_eol=False, check_quiet=False)",
        "snippet": "    def to_stdout(self, message, skip_eol=False, check_quiet=False):\n        \"\"\"Print message to stdout if not in quiet mode.\"\"\"\n        if self.params.get('logger'):\n            self.params['logger'].debug(message)\n        elif not check_quiet or not self.params.get('quiet', False):\n            message = self._bidi_workaround(message)\n            terminator = ['\\n', ''][skip_eol]\n            output = message + terminator\n\n            self._write_string(output, self._screen_file)",
        "begin_line": 508,
        "end_line": 517,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007462686567164179,
            "pseudo_dstar_susp": 0.0051813471502590676,
            "pseudo_tarantula_susp": 0.0011890606420927466,
            "pseudo_op2_susp": 0.0051813471502590676,
            "pseudo_barinel_susp": 0.0011890606420927466
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_stderr#519",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_stderr(self, message)",
        "snippet": "    def to_stderr(self, message):\n        \"\"\"Print message to stderr.\"\"\"\n        assert isinstance(message, compat_str)\n        if self.params.get('logger'):\n            self.params['logger'].error(message)\n        else:\n            message = self._bidi_workaround(message)\n            output = message + '\\n'\n            self._write_string(output, self._err_file)",
        "begin_line": 519,
        "end_line": 527,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003850596842510589,
            "pseudo_dstar_susp": 0.0003546099290780142,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0003546099290780142,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_console_title#529",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_console_title(self, message)",
        "snippet": "    def to_console_title(self, message):\n        if not self.params.get('consoletitle', False):\n            return\n        if compat_os_name == 'nt':\n            if ctypes.windll.kernel32.GetConsoleWindow():\n                # c_wchar_p() might not be necessary if `message` is\n                # already of type unicode()\n                ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))\n        elif 'TERM' in os.environ:\n            self._write_string('\\033]0;%s\\007' % message, self._screen_file)",
        "begin_line": 529,
        "end_line": 538,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.trouble#568",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.trouble(self, message=None, tb=None)",
        "snippet": "    def trouble(self, message=None, tb=None):\n        \"\"\"Determine action to take when a download problem appears.\n\n        Depending on if the downloader has been configured to ignore\n        download errors or not, this method may throw an exception or\n        not when errors are found, after printing the message.\n\n        tb, if given, is additional traceback information.\n        \"\"\"\n        if message is not None:\n            self.to_stderr(message)\n        if self.params.get('verbose'):\n            if tb is None:\n                if sys.exc_info()[0]:  # if .trouble has been called from an except block\n                    tb = ''\n                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))\n                    tb += encode_compat_str(traceback.format_exc())\n                else:\n                    tb_data = traceback.format_list(traceback.extract_stack())\n                    tb = ''.join(tb_data)\n            self.to_stderr(tb)\n        if not self.params.get('ignoreerrors', False):\n            if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                exc_info = sys.exc_info()[1].exc_info\n            else:\n                exc_info = sys.exc_info()\n            raise DownloadError(message, exc_info)\n        self._download_retcode = 1",
        "begin_line": 568,
        "end_line": 596,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0044444444444444444,
            "pseudo_dstar_susp": 0.003134796238244514,
            "pseudo_tarantula_susp": 0.002403846153846154,
            "pseudo_op2_susp": 0.003134796238244514,
            "pseudo_barinel_susp": 0.002403846153846154
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.report_warning#598",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.report_warning(self, message)",
        "snippet": "    def report_warning(self, message):\n        '''\n        Print the message to stderr, it will be prefixed with 'WARNING:'\n        If stderr is a tty file the 'WARNING:' will be colored\n        '''\n        if self.params.get('logger') is not None:\n            self.params['logger'].warning(message)\n        else:\n            if self.params.get('no_warnings'):\n                return\n            if not self.params.get('no_color') and self._err_file.isatty() and compat_os_name != 'nt':\n                _msg_header = '\\033[0;33mWARNING:\\033[0m'\n            else:\n                _msg_header = 'WARNING:'\n            warning_message = '%s %s' % (_msg_header, message)\n            self.to_stderr(warning_message)",
        "begin_line": 598,
        "end_line": 613,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.report_error#615",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.report_error(self, message, tb=None)",
        "snippet": "    def report_error(self, message, tb=None):\n        '''\n        Do the same as trouble, but prefixes the message with 'ERROR:', colored\n        in red if stderr is a tty file.\n        '''\n        if not self.params.get('no_color') and self._err_file.isatty() and compat_os_name != 'nt':\n            _msg_header = '\\033[0;31mERROR:\\033[0m'\n        else:\n            _msg_header = 'ERROR:'\n        error_message = '%s %s' % (_msg_header, message)\n        self.trouble(error_message, tb)",
        "begin_line": 615,
        "end_line": 625,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0045045045045045045,
            "pseudo_dstar_susp": 0.0031545741324921135,
            "pseudo_tarantula_susp": 0.003424657534246575,
            "pseudo_op2_susp": 0.0031545741324921135,
            "pseudo_barinel_susp": 0.003424657534246575
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.prepare_filename#634",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.prepare_filename(self, info_dict)",
        "snippet": "    def prepare_filename(self, info_dict):\n        \"\"\"Generate the output filename.\"\"\"\n        try:\n            template_dict = dict(info_dict)\n\n            template_dict['epoch'] = int(time.time())\n            autonumber_size = self.params.get('autonumber_size')\n            if autonumber_size is None:\n                autonumber_size = 5\n            template_dict['autonumber'] = self.params.get('autonumber_start', 1) - 1 + self._num_downloads\n            if template_dict.get('resolution') is None:\n                if template_dict.get('width') and template_dict.get('height'):\n                    template_dict['resolution'] = '%dx%d' % (template_dict['width'], template_dict['height'])\n                elif template_dict.get('height'):\n                    template_dict['resolution'] = '%sp' % template_dict['height']\n                elif template_dict.get('width'):\n                    template_dict['resolution'] = '%dx?' % template_dict['width']\n\n            sanitize = lambda k, v: sanitize_filename(\n                compat_str(v),\n                restricted=self.params.get('restrictfilenames'),\n                is_id=(k == 'id' or k.endswith('_id')))\n            template_dict = dict((k, v if isinstance(v, compat_numeric_types) else sanitize(k, v))\n                                 for k, v in template_dict.items()\n                                 if v is not None and not isinstance(v, (list, tuple, dict)))\n            template_dict = collections.defaultdict(lambda: 'NA', template_dict)\n\n            outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n\n            # For fields playlist_index and autonumber convert all occurrences\n            # of %(field)s to %(field)0Nd for backward compatibility\n            field_size_compat_map = {\n                'playlist_index': len(str(template_dict['n_entries'])),\n                'autonumber': autonumber_size,\n            }\n            FIELD_SIZE_COMPAT_RE = r'(?<!%)%\\((?P<field>autonumber|playlist_index)\\)s'\n            mobj = re.search(FIELD_SIZE_COMPAT_RE, outtmpl)\n            if mobj:\n                outtmpl = re.sub(\n                    FIELD_SIZE_COMPAT_RE,\n                    r'%%(\\1)0%dd' % field_size_compat_map[mobj.group('field')],\n                    outtmpl)\n\n            # Missing numeric fields used together with integer presentation types\n            # in format specification will break the argument substitution since\n            # string 'NA' is returned for missing fields. We will patch output\n            # template for missing fields to meet string presentation type.\n            for numeric_field in self._NUMERIC_FIELDS:\n                if numeric_field not in template_dict:\n                    # As of [1] format syntax is:\n                    #  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n                    # 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\n                    FORMAT_RE = r'''(?x)\n                        (?<!%)\n                        %\n                        \\({0}\\)  # mapping key\n                        (?:[#0\\-+ ]+)?  # conversion flags (optional)\n                        (?:\\d+)?  # minimum field width (optional)\n                        (?:\\.\\d+)?  # precision (optional)\n                        [hlL]?  # length modifier (optional)\n                        [diouxXeEfFgGcrs%]  # conversion type\n                    '''\n                    outtmpl = re.sub(\n                        FORMAT_RE.format(numeric_field),\n                        r'%({0})s'.format(numeric_field), outtmpl)\n\n            # expand_path translates '%%' into '%' and '$$' into '$'\n            # correspondingly that is not what we want since we need to keep\n            # '%%' intact for template dict substitution step. Working around\n            # with boundary-alike separator hack.\n            sep = ''.join([random.choice(ascii_letters) for _ in range(32)])\n            outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))\n\n            # outtmpl should be expand_path'ed before template dict substitution\n            # because meta fields may contain env variables we don't want to\n            # be expanded. For example, for outtmpl \"%(title)s.%(ext)s\" and\n            # title \"Hello $PATH\", we don't want `$PATH` to be expanded.\n            filename = expand_path(outtmpl).replace(sep, '') % template_dict\n\n            # Temporary fix for #4787\n            # 'Treat' all problem characters by passing filename through preferredencoding\n            # to workaround encoding issues with subprocess on python2 @ Windows\n            if sys.version_info < (3, 0) and sys.platform == 'win32':\n                filename = encodeFilename(filename, True).decode(preferredencoding())\n            return sanitize_path(filename)\n        except ValueError as err:\n            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n            return None",
        "begin_line": 634,
        "end_line": 721,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0038314176245210726,
            "pseudo_dstar_susp": 0.004347826086956522,
            "pseudo_tarantula_susp": 0.0009541984732824427,
            "pseudo_op2_susp": 0.004347826086956522,
            "pseudo_barinel_susp": 0.0009541984732824427
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._match_entry#723",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._match_entry(self, info_dict, incomplete)",
        "snippet": "    def _match_entry(self, info_dict, incomplete):\n        \"\"\" Returns None iff the file should be downloaded \"\"\"\n\n        video_title = info_dict.get('title', info_dict.get('id', 'video'))\n        if 'title' in info_dict:\n            # This can happen when we're just evaluating the playlist\n            title = info_dict['title']\n            matchtitle = self.params.get('matchtitle', False)\n            if matchtitle:\n                if not re.search(matchtitle, title, re.IGNORECASE):\n                    return '\"' + title + '\" title did not match pattern \"' + matchtitle + '\"'\n            rejecttitle = self.params.get('rejecttitle', False)\n            if rejecttitle:\n                if re.search(rejecttitle, title, re.IGNORECASE):\n                    return '\"' + title + '\" title matched reject pattern \"' + rejecttitle + '\"'\n        date = info_dict.get('upload_date')\n        if date is not None:\n            dateRange = self.params.get('daterange', DateRange())\n            if date not in dateRange:\n                return '%s upload date is not in range %s' % (date_from_str(date).isoformat(), dateRange)\n        view_count = info_dict.get('view_count')\n        if view_count is not None:\n            min_views = self.params.get('min_views')\n            if min_views is not None and view_count < min_views:\n                return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)\n            max_views = self.params.get('max_views')\n            if max_views is not None and view_count > max_views:\n                return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)\n        if age_restricted(info_dict.get('age_limit'), self.params.get('age_limit')):\n            return 'Skipping \"%s\" because it is age restricted' % video_title\n        if self.in_download_archive(info_dict):\n            return '%s has already been recorded in archive' % video_title\n\n        if not incomplete:\n            match_filter = self.params.get('match_filter')\n            if match_filter is not None:\n                ret = match_filter(info_dict)\n                if ret is not None:\n                    return ret\n\n        return None",
        "begin_line": 723,
        "end_line": 763,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001692047377326565,
            "pseudo_dstar_susp": 0.0017985611510791368,
            "pseudo_tarantula_susp": 0.00035435861091424523,
            "pseudo_op2_susp": 0.0017985611510791368,
            "pseudo_barinel_susp": 0.00035435861091424523
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_extra_info#766",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_extra_info(info_dict, extra_info)",
        "snippet": "    def add_extra_info(info_dict, extra_info):\n        '''Set the keys from extra_info in info dict if they are missing'''\n        for key, value in extra_info.items():\n            info_dict.setdefault(key, value)",
        "begin_line": 766,
        "end_line": 769,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020876826722338203,
            "pseudo_dstar_susp": 0.0020833333333333333,
            "pseudo_tarantula_susp": 0.0005482456140350877,
            "pseudo_op2_susp": 0.0020833333333333333,
            "pseudo_barinel_susp": 0.0005482456140350877
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.extract_info#771",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.extract_info(self, url, download=True, ie_key=None, extra_info={}, process=True, force_generic_extractor=False)",
        "snippet": "    def extract_info(self, url, download=True, ie_key=None, extra_info={},\n                     process=True, force_generic_extractor=False):\n        '''\n        Returns a list with a dictionary for each video we find.\n        If 'download', also downloads the videos.\n        extra_info is a dict containing the extra values to add to each result\n        '''\n\n        if not ie_key and force_generic_extractor:\n            ie_key = 'Generic'\n\n        if ie_key:\n            ies = [self.get_info_extractor(ie_key)]\n        else:\n            ies = self._ies\n\n        for ie in ies:\n            if not ie.suitable(url):\n                continue\n\n            ie = self.get_info_extractor(ie.ie_key())\n            if not ie.working():\n                self.report_warning('The program functionality for this site has been marked as broken, '\n                                    'and will probably not work.')\n\n            try:\n                ie_result = ie.extract(url)\n                if ie_result is None:  # Finished already (backwards compatibility; listformats and friends should be moved here)\n                    break\n                if isinstance(ie_result, list):\n                    # Backwards compatibility: old IE result format\n                    ie_result = {\n                        '_type': 'compat_list',\n                        'entries': ie_result,\n                    }\n                self.add_default_extra_info(ie_result, ie, url)\n                if process:\n                    return self.process_ie_result(ie_result, download, extra_info)\n                else:\n                    return ie_result\n            except GeoRestrictedError as e:\n                msg = e.msg\n                if e.countries:\n                    msg += '\\nThis video is available in %s.' % ', '.join(\n                        map(ISO3166Utils.short2full, e.countries))\n                msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'\n                self.report_error(msg)\n                break\n            except ExtractorError as e:  # An error we somewhat expected\n                self.report_error(compat_str(e), e.format_traceback())\n                break\n            except MaxDownloadsReached:\n                raise\n            except Exception as e:\n                if self.params.get('ignoreerrors', False):\n                    self.report_error(error_to_compat_str(e), tb=encode_compat_str(traceback.format_exc()))\n                    break\n                else:\n                    raise\n        else:\n            self.report_error('no suitable InfoExtractor for URL %s' % url)",
        "begin_line": 771,
        "end_line": 831,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017857142857142856,
            "pseudo_dstar_susp": 0.009009009009009009,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.009009009009009009,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_default_extra_info#833",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_default_extra_info(self, ie_result, ie, url)",
        "snippet": "    def add_default_extra_info(self, ie_result, ie, url):\n        self.add_extra_info(ie_result, {\n            'extractor': ie.IE_NAME,\n            'webpage_url': url,\n            'webpage_url_basename': url_basename(url),\n            'extractor_key': ie.ie_key(),\n        })",
        "begin_line": 833,
        "end_line": 839,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002109704641350211,
            "pseudo_dstar_susp": 0.002105263157894737,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.002105263157894737,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_ie_result#841",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_ie_result(self, ie_result, download=True, extra_info={})",
        "snippet": "    def process_ie_result(self, ie_result, download=True, extra_info={}):\n        \"\"\"\n        Take the result of the ie(may be modified) and resolve all unresolved\n        references (URLs, playlist items).\n\n        It will also download the videos if 'download'.\n        Returns the resolved ie_result.\n        \"\"\"\n        result_type = ie_result.get('_type', 'video')\n\n        if result_type in ('url', 'url_transparent'):\n            ie_result['url'] = sanitize_url(ie_result['url'])\n            extract_flat = self.params.get('extract_flat', False)\n            if ((extract_flat == 'in_playlist' and 'playlist' in extra_info)\n                    or extract_flat is True):\n                self.__forced_printings(\n                    ie_result, self.prepare_filename(ie_result),\n                    incomplete=True)\n                return ie_result\n\n        if result_type == 'video':\n            self.add_extra_info(ie_result, extra_info)\n            return self.process_video_result(ie_result, download=download)\n        elif result_type == 'url':\n            # We have to add extra_info to the results because it may be\n            # contained in a playlist\n            return self.extract_info(ie_result['url'],\n                                     download,\n                                     ie_key=ie_result.get('ie_key'),\n                                     extra_info=extra_info)\n        elif result_type == 'url_transparent':\n            # Use the information from the embedding page\n            info = self.extract_info(\n                ie_result['url'], ie_key=ie_result.get('ie_key'),\n                extra_info=extra_info, download=False, process=False)\n\n            # extract_info may return None when ignoreerrors is enabled and\n            # extraction failed with an error, don't crash and return early\n            # in this case\n            if not info:\n                return info\n\n            force_properties = dict(\n                (k, v) for k, v in ie_result.items() if v is not None)\n            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):\n                if f in force_properties:\n                    del force_properties[f]\n            new_result = info.copy()\n            new_result.update(force_properties)\n\n            # Extracted info may not be a video result (i.e.\n            # info.get('_type', 'video') != video) but rather an url or\n            # url_transparent. In such cases outer metadata (from ie_result)\n            # should be propagated to inner one (info). For this to happen\n            # _type of info should be overridden with url_transparent. This\n            # fixes issue from https://github.com/ytdl-org/youtube-dl/pull/11163.\n            if new_result.get('_type') == 'url':\n                new_result['_type'] = 'url_transparent'\n\n            return self.process_ie_result(\n                new_result, download=download, extra_info=extra_info)\n        elif result_type in ('playlist', 'multi_video'):\n            # We process each entry in the playlist\n            playlist = ie_result.get('title') or ie_result.get('id')\n            self.to_screen('[download] Downloading playlist: %s' % playlist)\n\n            playlist_results = []\n\n            playliststart = self.params.get('playliststart', 1) - 1\n            playlistend = self.params.get('playlistend')\n            # For backwards compatibility, interpret -1 as whole list\n            if playlistend == -1:\n                playlistend = None\n\n            playlistitems_str = self.params.get('playlist_items')\n            playlistitems = None\n            if playlistitems_str is not None:\n                def iter_playlistitems(format):\n                    for string_segment in format.split(','):\n                        if '-' in string_segment:\n                            start, end = string_segment.split('-')\n                            for item in range(int(start), int(end) + 1):\n                                yield int(item)\n                        else:\n                            yield int(string_segment)\n                playlistitems = orderedSet(iter_playlistitems(playlistitems_str))\n\n            ie_entries = ie_result['entries']\n\n            def make_playlistitems_entries(list_ie_entries):\n                num_entries = len(list_ie_entries)\n                return [\n                    list_ie_entries[i - 1] for i in playlistitems\n                    if -num_entries <= i - 1 < num_entries]\n\n            def report_download(num_entries):\n                self.to_screen(\n                    '[%s] playlist %s: Downloading %d videos' %\n                    (ie_result['extractor'], playlist, num_entries))\n\n            if isinstance(ie_entries, list):\n                n_all_entries = len(ie_entries)\n                if playlistitems:\n                    entries = make_playlistitems_entries(ie_entries)\n                else:\n                    entries = ie_entries[playliststart:playlistend]\n                n_entries = len(entries)\n                self.to_screen(\n                    '[%s] playlist %s: Collected %d video ids (downloading %d of them)' %\n                    (ie_result['extractor'], playlist, n_all_entries, n_entries))\n            elif isinstance(ie_entries, PagedList):\n                if playlistitems:\n                    entries = []\n                    for item in playlistitems:\n                        entries.extend(ie_entries.getslice(\n                            item - 1, item\n                        ))\n                else:\n                    entries = ie_entries.getslice(\n                        playliststart, playlistend)\n                n_entries = len(entries)\n                report_download(n_entries)\n            else:  # iterable\n                if playlistitems:\n                    entries = make_playlistitems_entries(list(itertools.islice(\n                        ie_entries, 0, max(playlistitems))))\n                else:\n                    entries = list(itertools.islice(\n                        ie_entries, playliststart, playlistend))\n                n_entries = len(entries)\n                report_download(n_entries)\n\n            if self.params.get('playlistreverse', False):\n                entries = entries[::-1]\n\n            if self.params.get('playlistrandom', False):\n                random.shuffle(entries)\n\n            x_forwarded_for = ie_result.get('__x_forwarded_for_ip')\n\n            for i, entry in enumerate(entries, 1):\n                self.to_screen('[download] Downloading video %s of %s' % (i, n_entries))\n                # This __x_forwarded_for_ip thing is a bit ugly but requires\n                # minimal changes\n                if x_forwarded_for:\n                    entry['__x_forwarded_for_ip'] = x_forwarded_for\n                extra = {\n                    'n_entries': n_entries,\n                    'playlist': playlist,\n                    'playlist_id': ie_result.get('id'),\n                    'playlist_title': ie_result.get('title'),\n                    'playlist_uploader': ie_result.get('uploader'),\n                    'playlist_uploader_id': ie_result.get('uploader_id'),\n                    'playlist_index': playlistitems[i - 1] if playlistitems else i + playliststart,\n                    'extractor': ie_result['extractor'],\n                    'webpage_url': ie_result['webpage_url'],\n                    'webpage_url_basename': url_basename(ie_result['webpage_url']),\n                    'extractor_key': ie_result['extractor_key'],\n                }\n\n                reason = self._match_entry(entry, incomplete=True)\n                if reason is not None:\n                    self.to_screen('[download] ' + reason)\n                    continue\n\n                entry_result = self.process_ie_result(entry,\n                                                      download=download,\n                                                      extra_info=extra)\n                playlist_results.append(entry_result)\n            ie_result['entries'] = playlist_results\n            self.to_screen('[download] Finished downloading playlist: %s' % playlist)\n            return ie_result\n        elif result_type == 'compat_list':\n            self.report_warning(\n                'Extractor %s returned a compat_list result. '\n                'It needs to be updated.' % ie_result.get('extractor'))\n\n            def _fixup(r):\n                self.add_extra_info(\n                    r,\n                    {\n                        'extractor': ie_result['extractor'],\n                        'webpage_url': ie_result['webpage_url'],\n                        'webpage_url_basename': url_basename(ie_result['webpage_url']),\n                        'extractor_key': ie_result['extractor_key'],\n                    }\n                )\n                return r\n            ie_result['entries'] = [\n                self.process_ie_result(_fixup(r), download, extra_info)\n                for r in ie_result['entries']\n            ]\n            return ie_result\n        else:\n            raise Exception('Invalid result type: %s' % result_type)",
        "begin_line": 841,
        "end_line": 1035,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002079002079002079,
            "pseudo_dstar_susp": 0.002079002079002079,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.002079002079002079,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.iter_playlistitems#918",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.iter_playlistitems(format)",
        "snippet": "                def iter_playlistitems(format):\n                    for string_segment in format.split(','):\n                        if '-' in string_segment:\n                            start, end = string_segment.split('-')\n                            for item in range(int(start), int(end) + 1):\n                                yield int(item)\n                        else:\n                            yield int(string_segment)",
        "begin_line": 918,
        "end_line": 925,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.make_playlistitems_entries#930",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.make_playlistitems_entries(list_ie_entries)",
        "snippet": "            def make_playlistitems_entries(list_ie_entries):\n                num_entries = len(list_ie_entries)\n                return [\n                    list_ie_entries[i - 1] for i in playlistitems\n                    if -num_entries <= i - 1 < num_entries]",
        "begin_line": 930,
        "end_line": 934,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015503875968992248,
            "pseudo_dstar_susp": 0.0010799136069114472,
            "pseudo_tarantula_susp": 0.0005787037037037037,
            "pseudo_op2_susp": 0.0010799136069114472,
            "pseudo_barinel_susp": 0.0005787037037037037
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.report_download#936",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.report_download(num_entries)",
        "snippet": "            def report_download(num_entries):\n                self.to_screen(\n                    '[%s] playlist %s: Downloading %d videos' %\n                    (ie_result['extractor'], playlist, num_entries))",
        "begin_line": 936,
        "end_line": 939,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015503875968992248,
            "pseudo_dstar_susp": 0.0010799136069114472,
            "pseudo_tarantula_susp": 0.001451378809869376,
            "pseudo_op2_susp": 0.0010799136069114472,
            "pseudo_barinel_susp": 0.001451378809869376
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._build_format_filter#1037",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._build_format_filter(self, filter_spec)",
        "snippet": "    def _build_format_filter(self, filter_spec):\n        \" Returns a function to filter the formats according to the filter_spec \"\n\n        OPERATORS = {\n            '<': operator.lt,\n            '<=': operator.le,\n            '>': operator.gt,\n            '>=': operator.ge,\n            '=': operator.eq,\n            '!=': operator.ne,\n        }\n        operator_rex = re.compile(r'''(?x)\\s*\n            (?P<key>width|height|tbr|abr|vbr|asr|filesize|filesize_approx|fps)\n            \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n            (?P<value>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)\n            $\n            ''' % '|'.join(map(re.escape, OPERATORS.keys())))\n        m = operator_rex.search(filter_spec)\n        if m:\n            try:\n                comparison_value = int(m.group('value'))\n            except ValueError:\n                comparison_value = parse_filesize(m.group('value'))\n                if comparison_value is None:\n                    comparison_value = parse_filesize(m.group('value') + 'B')\n                if comparison_value is None:\n                    raise ValueError(\n                        'Invalid value %r in format specification %r' % (\n                            m.group('value'), filter_spec))\n            op = OPERATORS[m.group('op')]\n\n        if not m:\n            STR_OPERATORS = {\n                '=': operator.eq,\n                '^=': lambda attr, value: attr.startswith(value),\n                '$=': lambda attr, value: attr.endswith(value),\n                '*=': lambda attr, value: value in attr,\n            }\n            str_operator_rex = re.compile(r'''(?x)\n                \\s*(?P<key>ext|acodec|vcodec|container|protocol|format_id)\n                \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\n                \\s*(?P<value>[a-zA-Z0-9._-]+)\n                \\s*$\n                ''' % '|'.join(map(re.escape, STR_OPERATORS.keys())))\n            m = str_operator_rex.search(filter_spec)\n            if m:\n                comparison_value = m.group('value')\n                str_op = STR_OPERATORS[m.group('op')]\n                if m.group('negation'):\n                    op = lambda attr, value: not str_op(attr, value)\n                else:\n                    op = str_op\n\n        if not m:\n            raise ValueError('Invalid filter specification %r' % filter_spec)\n\n        def _filter(f):\n            actual_value = f.get(m.group('key'))\n            if actual_value is None:\n                return m.group('none_inclusive')\n            return op(actual_value, comparison_value)\n        return _filter",
        "begin_line": 1037,
        "end_line": 1098,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030441400304414006,
            "pseudo_dstar_susp": 0.00033909799932180403,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00033909799932180403,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._filter#1093",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._filter(f)",
        "snippet": "        def _filter(f):\n            actual_value = f.get(m.group('key'))\n            if actual_value is None:\n                return m.group('none_inclusive')\n            return op(actual_value, comparison_value)",
        "begin_line": 1093,
        "end_line": 1097,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00032829940906106366,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00032829940906106366,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._default_format_spec#1100",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._default_format_spec(self, info_dict, download=True)",
        "snippet": "    def _default_format_spec(self, info_dict, download=True):\n\n        def can_merge():\n            merger = FFmpegMergerPP(self)\n            return merger.available and merger.can_merge()\n\n        def prefer_best():\n            if self.params.get('simulate', False):\n                return False\n            if not download:\n                return False\n            if self.params.get('outtmpl', DEFAULT_OUTTMPL) == '-':\n                return True\n            if info_dict.get('is_live'):\n                return True\n            if not can_merge():\n                return True\n            return False\n\n        req_format_list = ['bestvideo+bestaudio', 'best']\n        if prefer_best():\n            req_format_list.reverse()\n        return '/'.join(req_format_list)",
        "begin_line": 1100,
        "end_line": 1122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.can_merge#1102",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.can_merge()",
        "snippet": "        def can_merge():\n            merger = FFmpegMergerPP(self)\n            return merger.available and merger.can_merge()",
        "begin_line": 1102,
        "end_line": 1104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.prefer_best#1106",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.prefer_best()",
        "snippet": "        def prefer_best():\n            if self.params.get('simulate', False):\n                return False\n            if not download:\n                return False\n            if self.params.get('outtmpl', DEFAULT_OUTTMPL) == '-':\n                return True\n            if info_dict.get('is_live'):\n                return True\n            if not can_merge():\n                return True\n            return False",
        "begin_line": 1106,
        "end_line": 1117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.build_format_selector#1124",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.build_format_selector(self, format_spec)",
        "snippet": "    def build_format_selector(self, format_spec):\n        def syntax_error(note, start):\n            message = (\n                'Invalid format specification: '\n                '{0}\\n\\t{1}\\n\\t{2}^'.format(note, format_spec, ' ' * start[1]))\n            return SyntaxError(message)\n\n        PICKFIRST = 'PICKFIRST'\n        MERGE = 'MERGE'\n        SINGLE = 'SINGLE'\n        GROUP = 'GROUP'\n        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])\n\n        def _parse_filter(tokens):\n            filter_parts = []\n            for type, string, start, _, _ in tokens:\n                if type == tokenize.OP and string == ']':\n                    return ''.join(filter_parts)\n                else:\n                    filter_parts.append(string)\n\n        def _remove_unused_ops(tokens):\n            # Remove operators that we don't use and join them with the surrounding strings\n            # for example: 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'\n            ALLOWED_OPS = ('/', '+', ',', '(', ')')\n            last_string, last_start, last_end, last_line = None, None, None, None\n            for type, string, start, end, line in tokens:\n                if type == tokenize.OP and string == '[':\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                    # everything inside brackets will be handled by _parse_filter\n                    for type, string, start, end, line in tokens:\n                        yield type, string, start, end, line\n                        if type == tokenize.OP and string == ']':\n                            break\n                elif type == tokenize.OP and string in ALLOWED_OPS:\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:\n                    if not last_string:\n                        last_string = string\n                        last_start = start\n                        last_end = end\n                    else:\n                        last_string += string\n            if last_string:\n                yield tokenize.NAME, last_string, last_start, last_end, last_line\n\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n            selectors = []\n            current_selector = None\n            for type, string, start, _, _ in tokens:\n                # ENCODING is only defined in python 3.x\n                if type == getattr(tokenize, 'ENCODING', None):\n                    continue\n                elif type in [tokenize.NAME, tokenize.NUMBER]:\n                    current_selector = FormatSelector(SINGLE, string, [])\n                elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            # ')' will be handled by the parentheses group\n                            tokens.restore_last_token()\n                        break\n                    elif inside_merge and string in ['/', ',']:\n                        tokens.restore_last_token()\n                        break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                        if not current_selector:\n                            raise syntax_error('\",\" must follow a format selector', start)\n                        selectors.append(current_selector)\n                        current_selector = None\n                    elif string == '/':\n                        if not current_selector:\n                            raise syntax_error('\"/\" must follow a format selector', start)\n                        first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                    elif string == '[':\n                        if not current_selector:\n                            current_selector = FormatSelector(SINGLE, 'best', [])\n                        format_filter = _parse_filter(tokens)\n                        current_selector.filters.append(format_filter)\n                    elif string == '(':\n                        if current_selector:\n                            raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                    elif string == '+':\n                        video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        if not video_selector or not audio_selector:\n                            raise syntax_error('\"+\" must be between two format selectors', start)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                    else:\n                        raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                elif type == tokenize.ENDMARKER:\n                    break\n            if current_selector:\n                selectors.append(current_selector)\n            return selectors\n\n        def _build_selector_function(selector):\n            if isinstance(selector, list):\n                fs = [_build_selector_function(s) for s in selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        for format in f(ctx):\n                            yield format\n                return selector_function\n            elif selector.type == GROUP:\n                selector_function = _build_selector_function(selector.selector)\n            elif selector.type == PICKFIRST:\n                fs = [_build_selector_function(s) for s in selector.selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        picked_formats = list(f(ctx))\n                        if picked_formats:\n                            return picked_formats\n                    return []\n            elif selector.type == SINGLE:\n                format_spec = selector.selector\n\n                def selector_function(ctx):\n                    formats = list(ctx['formats'])\n                    if not formats:\n                        return\n                    if format_spec == 'all':\n                        for f in formats:\n                            yield f\n                    elif format_spec in ['best', 'worst', None]:\n                        format_idx = 0 if format_spec == 'worst' else -1\n                        audiovideo_formats = [\n                            f for f in formats\n                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']\n                        if audiovideo_formats:\n                            yield audiovideo_formats[format_idx]\n                        # for extractors with incomplete formats (audio only (soundcloud)\n                        # or video only (imgur)) we will fallback to best/worst\n                        # {video,audio}-only format\n                        elif ctx['incomplete_formats']:\n                            yield formats[format_idx]\n                    elif format_spec == 'bestaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[-1]\n                    elif format_spec == 'worstaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[0]\n                    elif format_spec == 'bestvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[-1]\n                    elif format_spec == 'worstvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[0]\n                    else:\n                        extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a', 'mp3', 'ogg', 'aac', 'wav']\n                        if format_spec in extensions:\n                            filter_f = lambda f: f['ext'] == format_spec\n                        else:\n                            filter_f = lambda f: f['format_id'] == format_spec\n                        matches = list(filter(filter_f, formats))\n                        if matches:\n                            yield matches[-1]\n            elif selector.type == MERGE:\n                def _merge(formats_info):\n                    format_1, format_2 = [f['format_id'] for f in formats_info]\n                    # The first format must contain the video and the\n                    # second the audio\n                    if formats_info[0].get('vcodec') == 'none':\n                        self.report_error('The first format must '\n                                          'contain the video, try using '\n                                          '\"-f %s+%s\"' % (format_2, format_1))\n                        return\n                    # Formats must be opposite (video+audio)\n                    if formats_info[0].get('acodec') == 'none' and formats_info[1].get('acodec') == 'none':\n                        self.report_error(\n                            'Both formats %s and %s are video-only, you must specify \"-f video+audio\"'\n                            % (format_1, format_2))\n                        return\n                    output_ext = (\n                        formats_info[0]['ext']\n                        if self.params.get('merge_output_format') is None\n                        else self.params['merge_output_format'])\n                    return {\n                        'requested_formats': formats_info,\n                        'format': '%s+%s' % (formats_info[0].get('format'),\n                                             formats_info[1].get('format')),\n                        'format_id': '%s+%s' % (formats_info[0].get('format_id'),\n                                                formats_info[1].get('format_id')),\n                        'width': formats_info[0].get('width'),\n                        'height': formats_info[0].get('height'),\n                        'resolution': formats_info[0].get('resolution'),\n                        'fps': formats_info[0].get('fps'),\n                        'vcodec': formats_info[0].get('vcodec'),\n                        'vbr': formats_info[0].get('vbr'),\n                        'stretched_ratio': formats_info[0].get('stretched_ratio'),\n                        'acodec': formats_info[1].get('acodec'),\n                        'abr': formats_info[1].get('abr'),\n                        'ext': output_ext,\n                    }\n                video_selector, audio_selector = map(_build_selector_function, selector.selector)\n\n                def selector_function(ctx):\n                    for pair in itertools.product(\n                            video_selector(copy.deepcopy(ctx)), audio_selector(copy.deepcopy(ctx))):\n                        yield _merge(pair)\n\n            filters = [self._build_format_filter(f) for f in selector.filters]\n\n            def final_selector(ctx):\n                ctx_copy = copy.deepcopy(ctx)\n                for _filter in filters:\n                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n                return selector_function(ctx_copy)\n            return final_selector\n\n        stream = io.BytesIO(format_spec.encode('utf-8'))\n        try:\n            tokens = list(_remove_unused_ops(compat_tokenize_tokenize(stream.readline)))\n        except tokenize.TokenError:\n            raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))\n\n        class TokenIterator(object):\n            def __init__(self, tokens):\n                self.tokens = tokens\n                self.counter = 0\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self.counter >= len(self.tokens):\n                    raise StopIteration()\n                value = self.tokens[self.counter]\n                self.counter += 1\n                return value\n\n            next = __next__\n\n            def restore_last_token(self):\n                self.counter -= 1\n\n        parsed_selector = _parse_format_selection(iter(TokenIterator(tokens)))\n        return _build_selector_function(parsed_selector)",
        "begin_line": 1124,
        "end_line": 1387,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.00037383177570093456,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.00037383177570093456
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.syntax_error#1125",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.syntax_error(note, start)",
        "snippet": "        def syntax_error(note, start):\n            message = (\n                'Invalid format specification: '\n                '{0}\\n\\t{1}\\n\\t{2}^'.format(note, format_spec, ' ' * start[1]))\n            return SyntaxError(message)",
        "begin_line": 1125,
        "end_line": 1129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.000365764447695684,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000365764447695684
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._parse_filter#1137",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._parse_filter(tokens)",
        "snippet": "        def _parse_filter(tokens):\n            filter_parts = []\n            for type, string, start, _, _ in tokens:\n                if type == tokenize.OP and string == ']':\n                    return ''.join(filter_parts)\n                else:\n                    filter_parts.append(string)",
        "begin_line": 1137,
        "end_line": 1143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.000365764447695684,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000365764447695684
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._remove_unused_ops#1145",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._remove_unused_ops(tokens)",
        "snippet": "        def _remove_unused_ops(tokens):\n            # Remove operators that we don't use and join them with the surrounding strings\n            # for example: 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'\n            ALLOWED_OPS = ('/', '+', ',', '(', ')')\n            last_string, last_start, last_end, last_line = None, None, None, None\n            for type, string, start, end, line in tokens:\n                if type == tokenize.OP and string == '[':\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                    # everything inside brackets will be handled by _parse_filter\n                    for type, string, start, end, line in tokens:\n                        yield type, string, start, end, line\n                        if type == tokenize.OP and string == ']':\n                            break\n                elif type == tokenize.OP and string in ALLOWED_OPS:\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:\n                    if not last_string:\n                        last_string = string\n                        last_start = start\n                        last_end = end\n                    else:\n                        last_string += string\n            if last_string:\n                yield tokenize.NAME, last_string, last_start, last_end, last_line",
        "begin_line": 1145,
        "end_line": 1174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014124293785310734,
            "pseudo_dstar_susp": 0.001557632398753894,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001557632398753894,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._parse_format_selection#1176",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False)",
        "snippet": "        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n            selectors = []\n            current_selector = None\n            for type, string, start, _, _ in tokens:\n                # ENCODING is only defined in python 3.x\n                if type == getattr(tokenize, 'ENCODING', None):\n                    continue\n                elif type in [tokenize.NAME, tokenize.NUMBER]:\n                    current_selector = FormatSelector(SINGLE, string, [])\n                elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            # ')' will be handled by the parentheses group\n                            tokens.restore_last_token()\n                        break\n                    elif inside_merge and string in ['/', ',']:\n                        tokens.restore_last_token()\n                        break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                        if not current_selector:\n                            raise syntax_error('\",\" must follow a format selector', start)\n                        selectors.append(current_selector)\n                        current_selector = None\n                    elif string == '/':\n                        if not current_selector:\n                            raise syntax_error('\"/\" must follow a format selector', start)\n                        first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                    elif string == '[':\n                        if not current_selector:\n                            current_selector = FormatSelector(SINGLE, 'best', [])\n                        format_filter = _parse_filter(tokens)\n                        current_selector.filters.append(format_filter)\n                    elif string == '(':\n                        if current_selector:\n                            raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                    elif string == '+':\n                        video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        if not video_selector or not audio_selector:\n                            raise syntax_error('\"+\" must be between two format selectors', start)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                    else:\n                        raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                elif type == tokenize.ENDMARKER:\n                    break\n            if current_selector:\n                selectors.append(current_selector)\n            return selectors",
        "begin_line": 1176,
        "end_line": 1230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._build_selector_function#1232",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._build_selector_function(selector)",
        "snippet": "        def _build_selector_function(selector):\n            if isinstance(selector, list):\n                fs = [_build_selector_function(s) for s in selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        for format in f(ctx):\n                            yield format\n                return selector_function\n            elif selector.type == GROUP:\n                selector_function = _build_selector_function(selector.selector)\n            elif selector.type == PICKFIRST:\n                fs = [_build_selector_function(s) for s in selector.selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        picked_formats = list(f(ctx))\n                        if picked_formats:\n                            return picked_formats\n                    return []\n            elif selector.type == SINGLE:\n                format_spec = selector.selector\n\n                def selector_function(ctx):\n                    formats = list(ctx['formats'])\n                    if not formats:\n                        return\n                    if format_spec == 'all':\n                        for f in formats:\n                            yield f\n                    elif format_spec in ['best', 'worst', None]:\n                        format_idx = 0 if format_spec == 'worst' else -1\n                        audiovideo_formats = [\n                            f for f in formats\n                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']\n                        if audiovideo_formats:\n                            yield audiovideo_formats[format_idx]\n                        # for extractors with incomplete formats (audio only (soundcloud)\n                        # or video only (imgur)) we will fallback to best/worst\n                        # {video,audio}-only format\n                        elif ctx['incomplete_formats']:\n                            yield formats[format_idx]\n                    elif format_spec == 'bestaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[-1]\n                    elif format_spec == 'worstaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[0]\n                    elif format_spec == 'bestvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[-1]\n                    elif format_spec == 'worstvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[0]\n                    else:\n                        extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a', 'mp3', 'ogg', 'aac', 'wav']\n                        if format_spec in extensions:\n                            filter_f = lambda f: f['ext'] == format_spec\n                        else:\n                            filter_f = lambda f: f['format_id'] == format_spec\n                        matches = list(filter(filter_f, formats))\n                        if matches:\n                            yield matches[-1]\n            elif selector.type == MERGE:\n                def _merge(formats_info):\n                    format_1, format_2 = [f['format_id'] for f in formats_info]\n                    # The first format must contain the video and the\n                    # second the audio\n                    if formats_info[0].get('vcodec') == 'none':\n                        self.report_error('The first format must '\n                                          'contain the video, try using '\n                                          '\"-f %s+%s\"' % (format_2, format_1))\n                        return\n                    # Formats must be opposite (video+audio)\n                    if formats_info[0].get('acodec') == 'none' and formats_info[1].get('acodec') == 'none':\n                        self.report_error(\n                            'Both formats %s and %s are video-only, you must specify \"-f video+audio\"'\n                            % (format_1, format_2))\n                        return\n                    output_ext = (\n                        formats_info[0]['ext']\n                        if self.params.get('merge_output_format') is None\n                        else self.params['merge_output_format'])\n                    return {\n                        'requested_formats': formats_info,\n                        'format': '%s+%s' % (formats_info[0].get('format'),\n                                             formats_info[1].get('format')),\n                        'format_id': '%s+%s' % (formats_info[0].get('format_id'),\n                                                formats_info[1].get('format_id')),\n                        'width': formats_info[0].get('width'),\n                        'height': formats_info[0].get('height'),\n                        'resolution': formats_info[0].get('resolution'),\n                        'fps': formats_info[0].get('fps'),\n                        'vcodec': formats_info[0].get('vcodec'),\n                        'vbr': formats_info[0].get('vbr'),\n                        'stretched_ratio': formats_info[0].get('stretched_ratio'),\n                        'acodec': formats_info[1].get('acodec'),\n                        'abr': formats_info[1].get('abr'),\n                        'ext': output_ext,\n                    }\n                video_selector, audio_selector = map(_build_selector_function, selector.selector)\n\n                def selector_function(ctx):\n                    for pair in itertools.product(\n                            video_selector(copy.deepcopy(ctx)), audio_selector(copy.deepcopy(ctx))):\n                        yield _merge(pair)\n\n            filters = [self._build_format_filter(f) for f in selector.filters]\n\n            def final_selector(ctx):\n                ctx_copy = copy.deepcopy(ctx)\n                for _filter in filters:\n                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n                return selector_function(ctx_copy)\n            return final_selector",
        "begin_line": 1232,
        "end_line": 1358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.selector_function#1236",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.selector_function(ctx)",
        "snippet": "                def selector_function(ctx):\n                    for f in fs:\n                        for format in f(ctx):\n                            yield format",
        "begin_line": 1236,
        "end_line": 1239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.selector_function#1246",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.selector_function(ctx)",
        "snippet": "                def selector_function(ctx):\n                    for f in fs:\n                        picked_formats = list(f(ctx))\n                        if picked_formats:\n                            return picked_formats\n                    return []",
        "begin_line": 1246,
        "end_line": 1251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.selector_function#1255",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.selector_function(ctx)",
        "snippet": "                def selector_function(ctx):\n                    formats = list(ctx['formats'])\n                    if not formats:\n                        return\n                    if format_spec == 'all':\n                        for f in formats:\n                            yield f\n                    elif format_spec in ['best', 'worst', None]:\n                        format_idx = 0 if format_spec == 'worst' else -1\n                        audiovideo_formats = [\n                            f for f in formats\n                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']\n                        if audiovideo_formats:\n                            yield audiovideo_formats[format_idx]\n                        # for extractors with incomplete formats (audio only (soundcloud)\n                        # or video only (imgur)) we will fallback to best/worst\n                        # {video,audio}-only format\n                        elif ctx['incomplete_formats']:\n                            yield formats[format_idx]\n                    elif format_spec == 'bestaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[-1]\n                    elif format_spec == 'worstaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[0]\n                    elif format_spec == 'bestvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[-1]\n                    elif format_spec == 'worstvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[0]\n                    else:\n                        extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a', 'mp3', 'ogg', 'aac', 'wav']\n                        if format_spec in extensions:\n                            filter_f = lambda f: f['ext'] == format_spec\n                        else:\n                            filter_f = lambda f: f['format_id'] == format_spec\n                        matches = list(filter(filter_f, formats))\n                        if matches:\n                            yield matches[-1]",
        "begin_line": 1255,
        "end_line": 1306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._merge#1308",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._merge(formats_info)",
        "snippet": "                def _merge(formats_info):\n                    format_1, format_2 = [f['format_id'] for f in formats_info]\n                    # The first format must contain the video and the\n                    # second the audio\n                    if formats_info[0].get('vcodec') == 'none':\n                        self.report_error('The first format must '\n                                          'contain the video, try using '\n                                          '\"-f %s+%s\"' % (format_2, format_1))\n                        return\n                    # Formats must be opposite (video+audio)\n                    if formats_info[0].get('acodec') == 'none' and formats_info[1].get('acodec') == 'none':\n                        self.report_error(\n                            'Both formats %s and %s are video-only, you must specify \"-f video+audio\"'\n                            % (format_1, format_2))\n                        return\n                    output_ext = (\n                        formats_info[0]['ext']\n                        if self.params.get('merge_output_format') is None\n                        else self.params['merge_output_format'])\n                    return {\n                        'requested_formats': formats_info,\n                        'format': '%s+%s' % (formats_info[0].get('format'),\n                                             formats_info[1].get('format')),\n                        'format_id': '%s+%s' % (formats_info[0].get('format_id'),\n                                                formats_info[1].get('format_id')),\n                        'width': formats_info[0].get('width'),\n                        'height': formats_info[0].get('height'),\n                        'resolution': formats_info[0].get('resolution'),\n                        'fps': formats_info[0].get('fps'),\n                        'vcodec': formats_info[0].get('vcodec'),\n                        'vbr': formats_info[0].get('vbr'),\n                        'stretched_ratio': formats_info[0].get('stretched_ratio'),\n                        'acodec': formats_info[1].get('acodec'),\n                        'abr': formats_info[1].get('abr'),\n                        'ext': output_ext,\n                    }",
        "begin_line": 1308,
        "end_line": 1343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.selector_function#1346",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.selector_function(ctx)",
        "snippet": "                def selector_function(ctx):\n                    for pair in itertools.product(\n                            video_selector(copy.deepcopy(ctx)), audio_selector(copy.deepcopy(ctx))):\n                        yield _merge(pair)",
        "begin_line": 1346,
        "end_line": 1349,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.final_selector#1353",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.final_selector(ctx)",
        "snippet": "            def final_selector(ctx):\n                ctx_copy = copy.deepcopy(ctx)\n                for _filter in filters:\n                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n                return selector_function(ctx_copy)",
        "begin_line": 1353,
        "end_line": 1357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.TokenIterator.build_format_selector#1124",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.TokenIterator",
        "signature": "youtube_dl.YoutubeDL.TokenIterator.build_format_selector(self, format_spec)",
        "snippet": "    def build_format_selector(self, format_spec):\n        def syntax_error(note, start):\n            message = (\n                'Invalid format specification: '\n                '{0}\\n\\t{1}\\n\\t{2}^'.format(note, format_spec, ' ' * start[1]))\n            return SyntaxError(message)\n\n        PICKFIRST = 'PICKFIRST'\n        MERGE = 'MERGE'\n        SINGLE = 'SINGLE'\n        GROUP = 'GROUP'\n        FormatSelector = collections.namedtuple('FormatSelector', ['type', 'selector', 'filters'])\n\n        def _parse_filter(tokens):\n            filter_parts = []\n            for type, string, start, _, _ in tokens:\n                if type == tokenize.OP and string == ']':\n                    return ''.join(filter_parts)\n                else:\n                    filter_parts.append(string)\n\n        def _remove_unused_ops(tokens):\n            # Remove operators that we don't use and join them with the surrounding strings\n            # for example: 'mp4' '-' 'baseline' '-' '16x9' is converted to 'mp4-baseline-16x9'\n            ALLOWED_OPS = ('/', '+', ',', '(', ')')\n            last_string, last_start, last_end, last_line = None, None, None, None\n            for type, string, start, end, line in tokens:\n                if type == tokenize.OP and string == '[':\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                    # everything inside brackets will be handled by _parse_filter\n                    for type, string, start, end, line in tokens:\n                        yield type, string, start, end, line\n                        if type == tokenize.OP and string == ']':\n                            break\n                elif type == tokenize.OP and string in ALLOWED_OPS:\n                    if last_string:\n                        yield tokenize.NAME, last_string, last_start, last_end, last_line\n                        last_string = None\n                    yield type, string, start, end, line\n                elif type in [tokenize.NAME, tokenize.NUMBER, tokenize.OP]:\n                    if not last_string:\n                        last_string = string\n                        last_start = start\n                        last_end = end\n                    else:\n                        last_string += string\n            if last_string:\n                yield tokenize.NAME, last_string, last_start, last_end, last_line\n\n        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):\n            selectors = []\n            current_selector = None\n            for type, string, start, _, _ in tokens:\n                # ENCODING is only defined in python 3.x\n                if type == getattr(tokenize, 'ENCODING', None):\n                    continue\n                elif type in [tokenize.NAME, tokenize.NUMBER]:\n                    current_selector = FormatSelector(SINGLE, string, [])\n                elif type == tokenize.OP:\n                    if string == ')':\n                        if not inside_group:\n                            # ')' will be handled by the parentheses group\n                            tokens.restore_last_token()\n                        break\n                    elif inside_merge and string in ['/', ',']:\n                        tokens.restore_last_token()\n                        break\n                    elif inside_choice and string == ',':\n                        tokens.restore_last_token()\n                        break\n                    elif string == ',':\n                        if not current_selector:\n                            raise syntax_error('\",\" must follow a format selector', start)\n                        selectors.append(current_selector)\n                        current_selector = None\n                    elif string == '/':\n                        if not current_selector:\n                            raise syntax_error('\"/\" must follow a format selector', start)\n                        first_choice = current_selector\n                        second_choice = _parse_format_selection(tokens, inside_choice=True)\n                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])\n                    elif string == '[':\n                        if not current_selector:\n                            current_selector = FormatSelector(SINGLE, 'best', [])\n                        format_filter = _parse_filter(tokens)\n                        current_selector.filters.append(format_filter)\n                    elif string == '(':\n                        if current_selector:\n                            raise syntax_error('Unexpected \"(\"', start)\n                        group = _parse_format_selection(tokens, inside_group=True)\n                        current_selector = FormatSelector(GROUP, group, [])\n                    elif string == '+':\n                        video_selector = current_selector\n                        audio_selector = _parse_format_selection(tokens, inside_merge=True)\n                        if not video_selector or not audio_selector:\n                            raise syntax_error('\"+\" must be between two format selectors', start)\n                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])\n                    else:\n                        raise syntax_error('Operator not recognized: \"{0}\"'.format(string), start)\n                elif type == tokenize.ENDMARKER:\n                    break\n            if current_selector:\n                selectors.append(current_selector)\n            return selectors\n\n        def _build_selector_function(selector):\n            if isinstance(selector, list):\n                fs = [_build_selector_function(s) for s in selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        for format in f(ctx):\n                            yield format\n                return selector_function\n            elif selector.type == GROUP:\n                selector_function = _build_selector_function(selector.selector)\n            elif selector.type == PICKFIRST:\n                fs = [_build_selector_function(s) for s in selector.selector]\n\n                def selector_function(ctx):\n                    for f in fs:\n                        picked_formats = list(f(ctx))\n                        if picked_formats:\n                            return picked_formats\n                    return []\n            elif selector.type == SINGLE:\n                format_spec = selector.selector\n\n                def selector_function(ctx):\n                    formats = list(ctx['formats'])\n                    if not formats:\n                        return\n                    if format_spec == 'all':\n                        for f in formats:\n                            yield f\n                    elif format_spec in ['best', 'worst', None]:\n                        format_idx = 0 if format_spec == 'worst' else -1\n                        audiovideo_formats = [\n                            f for f in formats\n                            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']\n                        if audiovideo_formats:\n                            yield audiovideo_formats[format_idx]\n                        # for extractors with incomplete formats (audio only (soundcloud)\n                        # or video only (imgur)) we will fallback to best/worst\n                        # {video,audio}-only format\n                        elif ctx['incomplete_formats']:\n                            yield formats[format_idx]\n                    elif format_spec == 'bestaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[-1]\n                    elif format_spec == 'worstaudio':\n                        audio_formats = [\n                            f for f in formats\n                            if f.get('vcodec') == 'none']\n                        if audio_formats:\n                            yield audio_formats[0]\n                    elif format_spec == 'bestvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[-1]\n                    elif format_spec == 'worstvideo':\n                        video_formats = [\n                            f for f in formats\n                            if f.get('acodec') == 'none']\n                        if video_formats:\n                            yield video_formats[0]\n                    else:\n                        extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a', 'mp3', 'ogg', 'aac', 'wav']\n                        if format_spec in extensions:\n                            filter_f = lambda f: f['ext'] == format_spec\n                        else:\n                            filter_f = lambda f: f['format_id'] == format_spec\n                        matches = list(filter(filter_f, formats))\n                        if matches:\n                            yield matches[-1]\n            elif selector.type == MERGE:\n                def _merge(formats_info):\n                    format_1, format_2 = [f['format_id'] for f in formats_info]\n                    # The first format must contain the video and the\n                    # second the audio\n                    if formats_info[0].get('vcodec') == 'none':\n                        self.report_error('The first format must '\n                                          'contain the video, try using '\n                                          '\"-f %s+%s\"' % (format_2, format_1))\n                        return\n                    # Formats must be opposite (video+audio)\n                    if formats_info[0].get('acodec') == 'none' and formats_info[1].get('acodec') == 'none':\n                        self.report_error(\n                            'Both formats %s and %s are video-only, you must specify \"-f video+audio\"'\n                            % (format_1, format_2))\n                        return\n                    output_ext = (\n                        formats_info[0]['ext']\n                        if self.params.get('merge_output_format') is None\n                        else self.params['merge_output_format'])\n                    return {\n                        'requested_formats': formats_info,\n                        'format': '%s+%s' % (formats_info[0].get('format'),\n                                             formats_info[1].get('format')),\n                        'format_id': '%s+%s' % (formats_info[0].get('format_id'),\n                                                formats_info[1].get('format_id')),\n                        'width': formats_info[0].get('width'),\n                        'height': formats_info[0].get('height'),\n                        'resolution': formats_info[0].get('resolution'),\n                        'fps': formats_info[0].get('fps'),\n                        'vcodec': formats_info[0].get('vcodec'),\n                        'vbr': formats_info[0].get('vbr'),\n                        'stretched_ratio': formats_info[0].get('stretched_ratio'),\n                        'acodec': formats_info[1].get('acodec'),\n                        'abr': formats_info[1].get('abr'),\n                        'ext': output_ext,\n                    }\n                video_selector, audio_selector = map(_build_selector_function, selector.selector)\n\n                def selector_function(ctx):\n                    for pair in itertools.product(\n                            video_selector(copy.deepcopy(ctx)), audio_selector(copy.deepcopy(ctx))):\n                        yield _merge(pair)\n\n            filters = [self._build_format_filter(f) for f in selector.filters]\n\n            def final_selector(ctx):\n                ctx_copy = copy.deepcopy(ctx)\n                for _filter in filters:\n                    ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n                return selector_function(ctx_copy)\n            return final_selector\n\n        stream = io.BytesIO(format_spec.encode('utf-8'))\n        try:\n            tokens = list(_remove_unused_ops(compat_tokenize_tokenize(stream.readline)))\n        except tokenize.TokenError:\n            raise syntax_error('Missing closing/opening brackets or parenthesis', (0, len(format_spec)))\n\n        class TokenIterator(object):\n            def __init__(self, tokens):\n                self.tokens = tokens\n                self.counter = 0\n\n            def __iter__(self):\n                return self\n\n            def __next__(self):\n                if self.counter >= len(self.tokens):\n                    raise StopIteration()\n                value = self.tokens[self.counter]\n                self.counter += 1\n                return value\n\n            next = __next__\n\n            def restore_last_token(self):\n                self.counter -= 1\n\n        parsed_selector = _parse_format_selection(iter(TokenIterator(tokens)))\n        return _build_selector_function(parsed_selector)",
        "begin_line": 1124,
        "end_line": 1387,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.000365764447695684,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000365764447695684
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.TokenIterator.__init__#1367",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.TokenIterator",
        "signature": "youtube_dl.YoutubeDL.TokenIterator.__init__(self, tokens)",
        "snippet": "            def __init__(self, tokens):\n                self.tokens = tokens\n                self.counter = 0",
        "begin_line": 1367,
        "end_line": 1369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.TokenIterator.__iter__#1371",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.TokenIterator",
        "signature": "youtube_dl.YoutubeDL.TokenIterator.__iter__(self)",
        "snippet": "            def __iter__(self):\n                return self",
        "begin_line": 1371,
        "end_line": 1372,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.TokenIterator.__next__#1374",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.TokenIterator",
        "signature": "youtube_dl.YoutubeDL.TokenIterator.__next__(self)",
        "snippet": "            def __next__(self):\n                if self.counter >= len(self.tokens):\n                    raise StopIteration()\n                value = self.tokens[self.counter]\n                self.counter += 1\n                return value",
        "begin_line": 1374,
        "end_line": 1379,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.TokenIterator.restore_last_token#1383",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.TokenIterator",
        "signature": "youtube_dl.YoutubeDL.TokenIterator.restore_last_token(self)",
        "snippet": "            def restore_last_token(self):\n                self.counter -= 1",
        "begin_line": 1383,
        "end_line": 1384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.000365764447695684,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000365764447695684
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._calc_headers#1389",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._calc_headers(self, info_dict)",
        "snippet": "    def _calc_headers(self, info_dict):\n        res = std_headers.copy()\n\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            res.update(add_headers)\n\n        cookies = self._calc_cookies(info_dict)\n        if cookies:\n            res['Cookie'] = cookies\n\n        if 'X-Forwarded-For' not in res:\n            x_forwarded_for_ip = info_dict.get('__x_forwarded_for_ip')\n            if x_forwarded_for_ip:\n                res['X-Forwarded-For'] = x_forwarded_for_ip\n\n        return res",
        "begin_line": 1389,
        "end_line": 1405,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._calc_cookies#1407",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._calc_cookies(self, info_dict)",
        "snippet": "    def _calc_cookies(self, info_dict):\n        pr = sanitized_Request(info_dict['url'])\n        self.cookiejar.add_cookie_header(pr)\n        return pr.get_header('Cookie')",
        "begin_line": 1407,
        "end_line": 1410,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013850415512465374,
            "pseudo_dstar_susp": 0.001524390243902439,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.001524390243902439,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_video_result#1412",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_video_result(self, info_dict, download=True)",
        "snippet": "    def process_video_result(self, info_dict, download=True):\n        assert info_dict.get('_type', 'video') == 'video'\n\n        if 'id' not in info_dict:\n            raise ExtractorError('Missing \"id\" field in extractor result')\n        if 'title' not in info_dict:\n            raise ExtractorError('Missing \"title\" field in extractor result')\n\n        def report_force_conversion(field, field_not, conversion):\n            self.report_warning(\n                '\"%s\" field is not %s - forcing %s conversion, there is an error in extractor'\n                % (field, field_not, conversion))\n\n        def sanitize_string_field(info, string_field):\n            field = info.get(string_field)\n            if field is None or isinstance(field, compat_str):\n                return\n            report_force_conversion(string_field, 'a string', 'string')\n            info[string_field] = compat_str(field)\n\n        def sanitize_numeric_fields(info):\n            for numeric_field in self._NUMERIC_FIELDS:\n                field = info.get(numeric_field)\n                if field is None or isinstance(field, compat_numeric_types):\n                    continue\n                report_force_conversion(numeric_field, 'numeric', 'int')\n                info[numeric_field] = int_or_none(field)\n\n        sanitize_string_field(info_dict, 'id')\n        sanitize_numeric_fields(info_dict)\n\n        if 'playlist' not in info_dict:\n            # It isn't part of a playlist\n            info_dict['playlist'] = None\n            info_dict['playlist_index'] = None\n\n        thumbnails = info_dict.get('thumbnails')\n        if thumbnails is None:\n            thumbnail = info_dict.get('thumbnail')\n            if thumbnail:\n                info_dict['thumbnails'] = thumbnails = [{'url': thumbnail}]\n        if thumbnails:\n            thumbnails.sort(key=lambda t: (\n                t.get('preference') if t.get('preference') is not None else -1,\n                t.get('width') if t.get('width') is not None else -1,\n                t.get('height') if t.get('height') is not None else -1,\n                t.get('id') if t.get('id') is not None else '', t.get('url')))\n            for i, t in enumerate(thumbnails):\n                t['url'] = sanitize_url(t['url'])\n                if t.get('width') and t.get('height'):\n                    t['resolution'] = '%dx%d' % (t['width'], t['height'])\n                if t.get('id') is None:\n                    t['id'] = '%d' % i\n\n        if self.params.get('list_thumbnails'):\n            self.list_thumbnails(info_dict)\n            return\n\n        thumbnail = info_dict.get('thumbnail')\n        if thumbnail:\n            info_dict['thumbnail'] = sanitize_url(thumbnail)\n        elif thumbnails:\n            info_dict['thumbnail'] = thumbnails[-1]['url']\n\n        if 'display_id' not in info_dict and 'id' in info_dict:\n            info_dict['display_id'] = info_dict['id']\n\n        if info_dict.get('upload_date') is None and info_dict.get('timestamp') is not None:\n            # Working around out-of-range timestamp values (e.g. negative ones on Windows,\n            # see http://bugs.python.org/issue1646728)\n            try:\n                upload_date = datetime.datetime.utcfromtimestamp(info_dict['timestamp'])\n                info_dict['upload_date'] = upload_date.strftime('%Y%m%d')\n            except (ValueError, OverflowError, OSError):\n                pass\n\n        # Auto generate title fields corresponding to the *_number fields when missing\n        # in order to always have clean titles. This is very common for TV series.\n        for field in ('chapter', 'season', 'episode'):\n            if info_dict.get('%s_number' % field) is not None and not info_dict.get(field):\n                info_dict[field] = '%s %d' % (field.capitalize(), info_dict['%s_number' % field])\n\n        for cc_kind in ('subtitles', 'automatic_captions'):\n            cc = info_dict.get(cc_kind)\n            if cc:\n                for _, subtitle in cc.items():\n                    for subtitle_format in subtitle:\n                        if subtitle_format.get('url'):\n                            subtitle_format['url'] = sanitize_url(subtitle_format['url'])\n                        if subtitle_format.get('ext') is None:\n                            subtitle_format['ext'] = determine_ext(subtitle_format['url']).lower()\n\n        automatic_captions = info_dict.get('automatic_captions')\n        subtitles = info_dict.get('subtitles')\n\n        if self.params.get('listsubtitles', False):\n            if 'automatic_captions' in info_dict:\n                self.list_subtitles(\n                    info_dict['id'], automatic_captions, 'automatic captions')\n            self.list_subtitles(info_dict['id'], subtitles, 'subtitles')\n            return\n\n        info_dict['requested_subtitles'] = self.process_subtitles(\n            info_dict['id'], subtitles, automatic_captions)\n\n        # We now pick which formats have to be downloaded\n        if info_dict.get('formats') is None:\n            # There's only one format available\n            formats = [info_dict]\n        else:\n            formats = info_dict['formats']\n\n        if not formats:\n            raise ExtractorError('No video formats found!')\n\n        def is_wellformed(f):\n            url = f.get('url')\n            if not url:\n                self.report_warning(\n                    '\"url\" field is missing or empty - skipping format, '\n                    'there is an error in extractor')\n                return False\n            if isinstance(url, bytes):\n                sanitize_string_field(f, 'url')\n            return True\n\n        # Filter out malformed formats for better extraction robustness\n        formats = list(filter(is_wellformed, formats))\n\n        formats_dict = {}\n\n        # We check that all the formats have the format and format_id fields\n        for i, format in enumerate(formats):\n            sanitize_string_field(format, 'format_id')\n            sanitize_numeric_fields(format)\n            format['url'] = sanitize_url(format['url'])\n            if not format.get('format_id'):\n                format['format_id'] = compat_str(i)\n            else:\n                # Sanitize format_id from characters used in format selector expression\n                format['format_id'] = re.sub(r'[\\s,/+\\[\\]()]', '_', format['format_id'])\n            format_id = format['format_id']\n            if format_id not in formats_dict:\n                formats_dict[format_id] = []\n            formats_dict[format_id].append(format)\n\n        # Make sure all formats have unique format_id\n        for format_id, ambiguous_formats in formats_dict.items():\n            if len(ambiguous_formats) > 1:\n                for i, format in enumerate(ambiguous_formats):\n                    format['format_id'] = '%s-%d' % (format_id, i)\n\n        for i, format in enumerate(formats):\n            if format.get('format') is None:\n                format['format'] = '{id} - {res}{note}'.format(\n                    id=format['format_id'],\n                    res=self.format_resolution(format),\n                    note=' ({0})'.format(format['format_note']) if format.get('format_note') is not None else '',\n                )\n            # Automatically determine file extension if missing\n            if format.get('ext') is None:\n                format['ext'] = determine_ext(format['url']).lower()\n            # Automatically determine protocol if missing (useful for format\n            # selection purposes)\n            if format.get('protocol') is None:\n                format['protocol'] = determine_protocol(format)\n            # Add HTTP headers, so that external programs can use them from the\n            # json output\n            full_format_info = info_dict.copy()\n            full_format_info.update(format)\n            format['http_headers'] = self._calc_headers(full_format_info)\n        # Remove private housekeeping stuff\n        if '__x_forwarded_for_ip' in info_dict:\n            del info_dict['__x_forwarded_for_ip']\n\n        # TODO Central sorting goes here\n\n        if formats[0] is not info_dict:\n            # only set the 'formats' fields if the original info_dict list them\n            # otherwise we end up with a circular reference, the first (and unique)\n            # element in the 'formats' field in info_dict is info_dict itself,\n            # which can't be exported to json\n            info_dict['formats'] = formats\n        if self.params.get('listformats'):\n            self.list_formats(info_dict)\n            return\n\n        req_format = self.params.get('format')\n        if req_format is None:\n            req_format = self._default_format_spec(info_dict, download=download)\n            if self.params.get('verbose'):\n                self.to_stdout('[debug] Default format spec: %s' % req_format)\n\n        format_selector = self.build_format_selector(req_format)\n\n        # While in format selection we may need to have an access to the original\n        # format set in order to calculate some metrics or do some processing.\n        # For now we need to be able to guess whether original formats provided\n        # by extractor are incomplete or not (i.e. whether extractor provides only\n        # video-only or audio-only formats) for proper formats selection for\n        # extractors with such incomplete formats (see\n        # https://github.com/ytdl-org/youtube-dl/pull/5556).\n        # Since formats may be filtered during format selection and may not match\n        # the original formats the results may be incorrect. Thus original formats\n        # or pre-calculated metrics should be passed to format selection routines\n        # as well.\n        # We will pass a context object containing all necessary additional data\n        # instead of just formats.\n        # This fixes incorrect format selection issue (see\n        # https://github.com/ytdl-org/youtube-dl/issues/10083).\n        incomplete_formats = (\n            # All formats are video-only or\n            all(f.get('vcodec') != 'none' and f.get('acodec') == 'none' for f in formats)\n            # all formats are audio-only\n            or all(f.get('vcodec') == 'none' and f.get('acodec') != 'none' for f in formats))\n\n        ctx = {\n            'formats': formats,\n            'incomplete_formats': incomplete_formats,\n        }\n\n        formats_to_download = list(format_selector(ctx))\n        if not formats_to_download:\n            raise ExtractorError('requested format not available',\n                                 expected=True)\n\n        if download:\n            if len(formats_to_download) > 1:\n                self.to_screen('[info] %s: downloading video in %s formats' % (info_dict['id'], len(formats_to_download)))\n            for format in formats_to_download:\n                new_info = dict(info_dict)\n                new_info.update(format)\n                self.process_info(new_info)\n        # We update the info dict with the best quality format (backwards compatibility)\n        info_dict.update(formats_to_download[-1])\n        return info_dict",
        "begin_line": 1412,
        "end_line": 1647,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014947683109118087,
            "pseudo_dstar_susp": 0.0016474464579901153,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0016474464579901153,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.report_force_conversion#1420",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.report_force_conversion(field, field_not, conversion)",
        "snippet": "        def report_force_conversion(field, field_not, conversion):\n            self.report_warning(\n                '\"%s\" field is not %s - forcing %s conversion, there is an error in extractor'\n                % (field, field_not, conversion))",
        "begin_line": 1420,
        "end_line": 1423,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014947683109118087,
            "pseudo_dstar_susp": 0.0016474464579901153,
            "pseudo_tarantula_susp": 0.00038595137012736397,
            "pseudo_op2_susp": 0.0016474464579901153,
            "pseudo_barinel_susp": 0.00038595137012736397
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.sanitize_string_field#1425",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.sanitize_string_field(info, string_field)",
        "snippet": "        def sanitize_string_field(info, string_field):\n            field = info.get(string_field)\n            if field is None or isinstance(field, compat_str):\n                return\n            report_force_conversion(string_field, 'a string', 'string')\n            info[string_field] = compat_str(field)",
        "begin_line": 1425,
        "end_line": 1430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014577259475218659,
            "pseudo_dstar_susp": 0.0016025641025641025,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0016025641025641025,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.sanitize_numeric_fields#1432",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.sanitize_numeric_fields(info)",
        "snippet": "        def sanitize_numeric_fields(info):\n            for numeric_field in self._NUMERIC_FIELDS:\n                field = info.get(numeric_field)\n                if field is None or isinstance(field, compat_numeric_types):\n                    continue\n                report_force_conversion(numeric_field, 'numeric', 'int')\n                info[numeric_field] = int_or_none(field)",
        "begin_line": 1432,
        "end_line": 1438,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014577259475218659,
            "pseudo_dstar_susp": 0.0016025641025641025,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0016025641025641025,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.is_wellformed#1527",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.is_wellformed(f)",
        "snippet": "        def is_wellformed(f):\n            url = f.get('url')\n            if not url:\n                self.report_warning(\n                    '\"url\" field is missing or empty - skipping format, '\n                    'there is an error in extractor')\n                return False\n            if isinstance(url, bytes):\n                sanitize_string_field(f, 'url')\n            return True",
        "begin_line": 1527,
        "end_line": 1536,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_subtitles#1649",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_subtitles(self, video_id, normal_subtitles, automatic_captions)",
        "snippet": "    def process_subtitles(self, video_id, normal_subtitles, automatic_captions):\n        \"\"\"Select the requested subtitles and their format\"\"\"\n        available_subs = {}\n        if normal_subtitles and self.params.get('writesubtitles'):\n            available_subs.update(normal_subtitles)\n        if automatic_captions and self.params.get('writeautomaticsub'):\n            for lang, cap_info in automatic_captions.items():\n                if lang not in available_subs:\n                    available_subs[lang] = cap_info\n\n        if (not self.params.get('writesubtitles') and not\n                self.params.get('writeautomaticsub') or not\n                available_subs):\n            return None\n\n        if self.params.get('allsubtitles', False):\n            requested_langs = available_subs.keys()\n        else:\n            if self.params.get('subtitleslangs', False):\n                requested_langs = self.params.get('subtitleslangs')\n            elif 'en' in available_subs:\n                requested_langs = ['en']\n            else:\n                requested_langs = [list(available_subs.keys())[0]]\n\n        formats_query = self.params.get('subtitlesformat', 'best')\n        formats_preference = formats_query.split('/') if formats_query else []\n        subs = {}\n        for lang in requested_langs:\n            formats = available_subs.get(lang)\n            if formats is None:\n                self.report_warning('%s subtitles not available for %s' % (lang, video_id))\n                continue\n            for ext in formats_preference:\n                if ext == 'best':\n                    f = formats[-1]\n                    break\n                matches = list(filter(lambda f: f['ext'] == ext, formats))\n                if matches:\n                    f = matches[-1]\n                    break\n            else:\n                f = formats[-1]\n                self.report_warning(\n                    'No subtitle format found matching \"%s\" for language %s, '\n                    'using %s' % (formats_query, lang, f['ext']))\n            subs[lang] = f\n        return subs",
        "begin_line": 1649,
        "end_line": 1696,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014577259475218659,
            "pseudo_dstar_susp": 0.0016025641025641025,
            "pseudo_tarantula_susp": 0.0009174311926605505,
            "pseudo_op2_susp": 0.0016025641025641025,
            "pseudo_barinel_susp": 0.0009165902841429881
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.__forced_printings#1698",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.__forced_printings(self, info_dict, filename, incomplete)",
        "snippet": "    def __forced_printings(self, info_dict, filename, incomplete):\n        def print_mandatory(field):\n            if (self.params.get('force%s' % field, False)\n                    and (not incomplete or info_dict.get(field) is not None)):\n                self.to_stdout(info_dict[field])\n\n        def print_optional(field):\n            if (self.params.get('force%s' % field, False)\n                    and info_dict.get(field) is not None):\n                self.to_stdout(info_dict[field])\n\n        print_mandatory('title')\n        print_mandatory('id')\n        if self.params.get('forceurl', False) and not incomplete:\n            if info_dict.get('requested_formats') is not None:\n                for f in info_dict['requested_formats']:\n                    self.to_stdout(f['url'] + f.get('play_path', ''))\n            else:\n                # For RTMP URLs, also include the playpath\n                self.to_stdout(info_dict['url'] + info_dict.get('play_path', ''))\n        print_optional('thumbnail')\n        print_optional('description')\n        if self.params.get('forcefilename', False) and filename is not None:\n            self.to_stdout(filename)\n        if self.params.get('forceduration', False) and info_dict.get('duration') is not None:\n            self.to_stdout(formatSeconds(info_dict['duration']))\n        print_mandatory('format')\n        if self.params.get('forcejson', False):\n            self.to_stdout(json.dumps(info_dict))",
        "begin_line": 1698,
        "end_line": 1726,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016155088852988692,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.print_mandatory#1699",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.print_mandatory(field)",
        "snippet": "        def print_mandatory(field):\n            if (self.params.get('force%s' % field, False)\n                    and (not incomplete or info_dict.get(field) is not None)):\n                self.to_stdout(info_dict[field])",
        "begin_line": 1699,
        "end_line": 1702,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016155088852988692,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.print_optional#1704",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.print_optional(field)",
        "snippet": "        def print_optional(field):\n            if (self.params.get('force%s' % field, False)\n                    and info_dict.get(field) is not None):\n                self.to_stdout(info_dict[field])",
        "begin_line": 1704,
        "end_line": 1707,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016155088852988692,
            "pseudo_dstar_susp": 0.0017452006980802793,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.0017452006980802793,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_info#1728",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_info(self, info_dict)",
        "snippet": "    def process_info(self, info_dict):\n        \"\"\"Process a single resolved IE result.\"\"\"\n\n        assert info_dict.get('_type', 'video') == 'video'\n\n        max_downloads = self.params.get('max_downloads')\n        if max_downloads is not None:\n            if self._num_downloads >= int(max_downloads):\n                raise MaxDownloadsReached()\n\n        # TODO: backward compatibility, to be removed\n        info_dict['fulltitle'] = info_dict['title']\n\n        if 'format' not in info_dict:\n            info_dict['format'] = info_dict['ext']\n\n        reason = self._match_entry(info_dict, incomplete=False)\n        if reason is not None:\n            self.to_screen('[download] ' + reason)\n            return\n\n        self._num_downloads += 1\n\n        info_dict['_filename'] = filename = self.prepare_filename(info_dict)\n\n        # Forced printings\n        self.__forced_printings(info_dict, filename, incomplete=False)\n\n        # Do nothing else if in simulate mode\n        if self.params.get('simulate', False):\n            return\n\n        if filename is None:\n            return\n\n        def ensure_dir_exists(path):\n            try:\n                dn = os.path.dirname(path)\n                if dn and not os.path.exists(dn):\n                    os.makedirs(dn)\n                return True\n            except (OSError, IOError) as err:\n                self.report_error('unable to create directory ' + error_to_compat_str(err))\n                return False\n\n        if not ensure_dir_exists(sanitize_path(encodeFilename(filename))):\n            return\n\n        if self.params.get('writedescription', False):\n            descfn = replace_extension(filename, 'description', info_dict.get('ext'))\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(descfn)):\n                self.to_screen('[info] Video description is already present')\n            elif info_dict.get('description') is None:\n                self.report_warning('There\\'s no description to write.')\n            else:\n                try:\n                    self.to_screen('[info] Writing video description to: ' + descfn)\n                    with io.open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:\n                        descfile.write(info_dict['description'])\n                except (OSError, IOError):\n                    self.report_error('Cannot write description file ' + descfn)\n                    return\n\n        if self.params.get('writeannotations', False):\n            annofn = replace_extension(filename, 'annotations.xml', info_dict.get('ext'))\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(annofn)):\n                self.to_screen('[info] Video annotations are already present')\n            elif not info_dict.get('annotations'):\n                self.report_warning('There are no annotations to write.')\n            else:\n                try:\n                    self.to_screen('[info] Writing video annotations to: ' + annofn)\n                    with io.open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:\n                        annofile.write(info_dict['annotations'])\n                except (KeyError, TypeError):\n                    self.report_warning('There are no annotations to write.')\n                except (OSError, IOError):\n                    self.report_error('Cannot write annotations file: ' + annofn)\n                    return\n\n        subtitles_are_requested = any([self.params.get('writesubtitles', False),\n                                       self.params.get('writeautomaticsub')])\n\n        if subtitles_are_requested and info_dict.get('requested_subtitles'):\n            # subtitles download errors are already managed as troubles in relevant IE\n            # that way it will silently go on when used with unsupporting IE\n            subtitles = info_dict['requested_subtitles']\n            ie = self.get_info_extractor(info_dict['extractor_key'])\n            for sub_lang, sub_info in subtitles.items():\n                sub_format = sub_info['ext']\n                sub_filename = subtitles_filename(filename, sub_lang, sub_format, info_dict.get('ext'))\n                if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(sub_filename)):\n                    self.to_screen('[info] Video subtitle %s.%s is already present' % (sub_lang, sub_format))\n                else:\n                    self.to_screen('[info] Writing video subtitles to: ' + sub_filename)\n                    if sub_info.get('data') is not None:\n                        try:\n                            # Use newline='' to prevent conversion of newline characters\n                            # See https://github.com/ytdl-org/youtube-dl/issues/10268\n                            with io.open(encodeFilename(sub_filename), 'w', encoding='utf-8', newline='') as subfile:\n                                subfile.write(sub_info['data'])\n                        except (OSError, IOError):\n                            self.report_error('Cannot write subtitles file ' + sub_filename)\n                            return\n                    else:\n                        try:\n                            sub_data = ie._request_webpage(\n                                sub_info['url'], info_dict['id'], note=False).read()\n                            with io.open(encodeFilename(sub_filename), 'wb') as subfile:\n                                subfile.write(sub_data)\n                        except (ExtractorError, IOError, OSError, ValueError) as err:\n                            self.report_warning('Unable to download subtitle for \"%s\": %s' %\n                                                (sub_lang, error_to_compat_str(err)))\n                            continue\n\n        if self.params.get('writeinfojson', False):\n            infofn = replace_extension(filename, 'info.json', info_dict.get('ext'))\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(infofn)):\n                self.to_screen('[info] Video description metadata is already present')\n            else:\n                self.to_screen('[info] Writing video description metadata as JSON to: ' + infofn)\n                try:\n                    write_json_file(self.filter_requested_info(info_dict), infofn)\n                except (OSError, IOError):\n                    self.report_error('Cannot write metadata to JSON file ' + infofn)\n                    return\n\n        self._write_thumbnails(info_dict, filename)\n\n        if not self.params.get('skip_download', False):\n            try:\n                def dl(name, info):\n                    fd = get_suitable_downloader(info, self.params)(self, self.params)\n                    for ph in self._progress_hooks:\n                        fd.add_progress_hook(ph)\n                    if self.params.get('verbose'):\n                        self.to_stdout('[debug] Invoking downloader on %r' % info.get('url'))\n                    return fd.download(name, info)\n\n                if info_dict.get('requested_formats') is not None:\n                    downloaded = []\n                    success = True\n                    merger = FFmpegMergerPP(self)\n                    if not merger.available:\n                        postprocessors = []\n                        self.report_warning('You have requested multiple '\n                                            'formats but ffmpeg or avconv are not installed.'\n                                            ' The formats won\\'t be merged.')\n                    else:\n                        postprocessors = [merger]\n\n                    def compatible_formats(formats):\n                        video, audio = formats\n                        # Check extension\n                        video_ext, audio_ext = video.get('ext'), audio.get('ext')\n                        if video_ext and audio_ext:\n                            COMPATIBLE_EXTS = (\n                                ('mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma'),\n                                ('webm')\n                            )\n                            for exts in COMPATIBLE_EXTS:\n                                if video_ext in exts and audio_ext in exts:\n                                    return True\n                        # TODO: Check acodec/vcodec\n                        return False\n\n                    filename_real_ext = os.path.splitext(filename)[1][1:]\n                    filename_wo_ext = (\n                        os.path.splitext(filename)[0]\n                        if filename_real_ext == info_dict['ext']\n                        else filename)\n                    requested_formats = info_dict['requested_formats']\n                    if self.params.get('merge_output_format') is None and not compatible_formats(requested_formats):\n                        info_dict['ext'] = 'mkv'\n                        self.report_warning(\n                            'Requested formats are incompatible for merge and will be merged into mkv.')\n                    # Ensure filename always has a correct extension for successful merge\n                    filename = '%s.%s' % (filename_wo_ext, info_dict['ext'])\n                    if os.path.exists(encodeFilename(filename)):\n                        self.to_screen(\n                            '[download] %s has already been downloaded and '\n                            'merged' % filename)\n                    else:\n                        for f in requested_formats:\n                            new_info = dict(info_dict)\n                            new_info.update(f)\n                            fname = prepend_extension(\n                                self.prepare_filename(new_info),\n                                'f%s' % f['format_id'], new_info['ext'])\n                            if not ensure_dir_exists(fname):\n                                return\n                            downloaded.append(fname)\n                            partial_success = dl(fname, new_info)\n                            success = success and partial_success\n                        info_dict['__postprocessors'] = postprocessors\n                        info_dict['__files_to_merge'] = downloaded\n                else:\n                    # Just a single file\n                    success = dl(filename, info_dict)\n            except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n                self.report_error('unable to download video data: %s' % error_to_compat_str(err))\n                return\n            except (OSError, IOError) as err:\n                raise UnavailableVideoError(err)\n            except (ContentTooShortError, ) as err:\n                self.report_error('content too short (expected %s bytes and served %s)' % (err.expected, err.downloaded))\n                return\n\n            if success and filename != '-':\n                # Fixup content\n                fixup_policy = self.params.get('fixup')\n                if fixup_policy is None:\n                    fixup_policy = 'detect_or_warn'\n\n                INSTALL_FFMPEG_MESSAGE = 'Install ffmpeg or avconv to fix this automatically.'\n\n                stretched_ratio = info_dict.get('stretched_ratio')\n                if stretched_ratio is not None and stretched_ratio != 1:\n                    if fixup_policy == 'warn':\n                        self.report_warning('%s: Non-uniform pixel ratio (%s)' % (\n                            info_dict['id'], stretched_ratio))\n                    elif fixup_policy == 'detect_or_warn':\n                        stretched_pp = FFmpegFixupStretchedPP(self)\n                        if stretched_pp.available:\n                            info_dict.setdefault('__postprocessors', [])\n                            info_dict['__postprocessors'].append(stretched_pp)\n                        else:\n                            self.report_warning(\n                                '%s: Non-uniform pixel ratio (%s). %s'\n                                % (info_dict['id'], stretched_ratio, INSTALL_FFMPEG_MESSAGE))\n                    else:\n                        assert fixup_policy in ('ignore', 'never')\n\n                if (info_dict.get('requested_formats') is None\n                        and info_dict.get('container') == 'm4a_dash'):\n                    if fixup_policy == 'warn':\n                        self.report_warning(\n                            '%s: writing DASH m4a. '\n                            'Only some players support this container.'\n                            % info_dict['id'])\n                    elif fixup_policy == 'detect_or_warn':\n                        fixup_pp = FFmpegFixupM4aPP(self)\n                        if fixup_pp.available:\n                            info_dict.setdefault('__postprocessors', [])\n                            info_dict['__postprocessors'].append(fixup_pp)\n                        else:\n                            self.report_warning(\n                                '%s: writing DASH m4a. '\n                                'Only some players support this container. %s'\n                                % (info_dict['id'], INSTALL_FFMPEG_MESSAGE))\n                    else:\n                        assert fixup_policy in ('ignore', 'never')\n\n                if (info_dict.get('protocol') == 'm3u8_native'\n                        or info_dict.get('protocol') == 'm3u8'\n                        and self.params.get('hls_prefer_native')):\n                    if fixup_policy == 'warn':\n                        self.report_warning('%s: malformed AAC bitstream detected.' % (\n                            info_dict['id']))\n                    elif fixup_policy == 'detect_or_warn':\n                        fixup_pp = FFmpegFixupM3u8PP(self)\n                        if fixup_pp.available:\n                            info_dict.setdefault('__postprocessors', [])\n                            info_dict['__postprocessors'].append(fixup_pp)\n                        else:\n                            self.report_warning(\n                                '%s: malformed AAC bitstream detected. %s'\n                                % (info_dict['id'], INSTALL_FFMPEG_MESSAGE))\n                    else:\n                        assert fixup_policy in ('ignore', 'never')\n\n                try:\n                    self.post_process(filename, info_dict)\n                except (PostProcessingError) as err:\n                    self.report_error('postprocessing: %s' % str(err))\n                    return\n                self.record_download_archive(info_dict)",
        "begin_line": 1728,
        "end_line": 2004,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011494252873563218,
            "pseudo_dstar_susp": 0.0012285012285012285,
            "pseudo_tarantula_susp": 0.0017953321364452424,
            "pseudo_op2_susp": 0.0012285012285012285,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.ensure_dir_exists#1763",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.ensure_dir_exists(path)",
        "snippet": "        def ensure_dir_exists(path):\n            try:\n                dn = os.path.dirname(path)\n                if dn and not os.path.exists(dn):\n                    os.makedirs(dn)\n                return True\n            except (OSError, IOError) as err:\n                self.report_error('unable to create directory ' + error_to_compat_str(err))\n                return False",
        "begin_line": 1763,
        "end_line": 1771,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011350737797956867,
            "pseudo_dstar_susp": 0.0012135922330097086,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0012135922330097086,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.dl#1859",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.dl(name, info)",
        "snippet": "                def dl(name, info):\n                    fd = get_suitable_downloader(info, self.params)(self, self.params)\n                    for ph in self._progress_hooks:\n                        fd.add_progress_hook(ph)\n                    if self.params.get('verbose'):\n                        self.to_stdout('[debug] Invoking downloader on %r' % info.get('url'))\n                    return fd.download(name, info)",
        "begin_line": 1859,
        "end_line": 1865,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000946073793755913,
            "pseudo_dstar_susp": 0.00099601593625498,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00099601593625498,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.download#2006",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.download(self, url_list)",
        "snippet": "    def download(self, url_list):\n        \"\"\"Download a given list of URLs.\"\"\"\n        outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n        if (len(url_list) > 1\n                and outtmpl != '-'\n                and '%' not in outtmpl\n                and self.params.get('max_downloads') != 1):\n            raise SameFileError(outtmpl)\n\n        for url in url_list:\n            try:\n                # It also downloads the videos\n                res = self.extract_info(\n                    url, force_generic_extractor=self.params.get('force_generic_extractor', False))\n            except UnavailableVideoError:\n                self.report_error('unable to download video')\n            except MaxDownloadsReached:\n                self.to_screen('[info] Maximum number of downloaded files reached.')\n                raise\n            else:\n                if self.params.get('dump_single_json', False):\n                    self.to_stdout(json.dumps(res))\n\n        return self._download_retcode",
        "begin_line": 2006,
        "end_line": 2029,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004033884630899556,
            "pseudo_dstar_susp": 0.00035829451809387314,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00035829451809387314,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.filter_requested_info#2049",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.filter_requested_info(info_dict)",
        "snippet": "    def filter_requested_info(info_dict):\n        return dict(\n            (k, v) for k, v in info_dict.items()\n            if k not in ['requested_formats', 'requested_subtitles'])",
        "begin_line": 2049,
        "end_line": 2052,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011494252873563218,
            "pseudo_dstar_susp": 0.0012285012285012285,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0012285012285012285,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.post_process#2054",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.post_process(self, filename, ie_info)",
        "snippet": "    def post_process(self, filename, ie_info):\n        \"\"\"Run all the postprocessors on the given file.\"\"\"\n        info = dict(ie_info)\n        info['filepath'] = filename\n        pps_chain = []\n        if ie_info.get('__postprocessors') is not None:\n            pps_chain.extend(ie_info['__postprocessors'])\n        pps_chain.extend(self._pps)\n        for pp in pps_chain:\n            files_to_delete = []\n            try:\n                files_to_delete, info = pp.run(info)\n            except PostProcessingError as e:\n                self.report_error(e.msg)\n            if files_to_delete and not self.params.get('keepvideo', False):\n                for old_filename in files_to_delete:\n                    self.to_screen('Deleting original file %s (pass -k to keep)' % old_filename)\n                    try:\n                        os.remove(encodeFilename(old_filename))\n                    except (IOError, OSError):\n                        self.report_warning('Unable to remove downloaded original file')",
        "begin_line": 2054,
        "end_line": 2074,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008340283569641367,
            "pseudo_dstar_susp": 0.0008944543828264759,
            "pseudo_tarantula_susp": 0.00030998140111593303,
            "pseudo_op2_susp": 0.0008944543828264759,
            "pseudo_barinel_susp": 0.00030998140111593303
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.in_download_archive#2096",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.in_download_archive(self, info_dict)",
        "snippet": "    def in_download_archive(self, info_dict):\n        fn = self.params.get('download_archive')\n        if fn is None:\n            return False\n\n        vid_id = self._make_archive_id(info_dict)\n        if not vid_id:\n            return False  # Incomplete video information\n\n        try:\n            with locked_file(fn, 'r', encoding='utf-8') as archive_file:\n                for line in archive_file:\n                    if line.strip() == vid_id:\n                        return True\n        except IOError as ioe:\n            if ioe.errno != errno.ENOENT:\n                raise\n        return False",
        "begin_line": 2096,
        "end_line": 2113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016666666666666668,
            "pseudo_dstar_susp": 0.0017761989342806395,
            "pseudo_tarantula_susp": 0.00033534540576794097,
            "pseudo_op2_susp": 0.0017761989342806395,
            "pseudo_barinel_susp": 0.00033534540576794097
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.record_download_archive#2115",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.record_download_archive(self, info_dict)",
        "snippet": "    def record_download_archive(self, info_dict):\n        fn = self.params.get('download_archive')\n        if fn is None:\n            return\n        vid_id = self._make_archive_id(info_dict)\n        assert vid_id\n        with locked_file(fn, 'a', encoding='utf-8') as archive_file:\n            archive_file.write(vid_id + '\\n')",
        "begin_line": 2115,
        "end_line": 2122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008410428931875525,
            "pseudo_dstar_susp": 0.0009025270758122744,
            "pseudo_tarantula_susp": 0.00031104199066874026,
            "pseudo_op2_susp": 0.0009025270758122744,
            "pseudo_barinel_susp": 0.00031104199066874026
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.format_resolution#2125",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.format_resolution(format, default='unknown')",
        "snippet": "    def format_resolution(format, default='unknown'):\n        if format.get('vcodec') == 'none':\n            return 'audio only'\n        if format.get('resolution') is not None:\n            return format['resolution']\n        if format.get('height') is not None:\n            if format.get('width') is not None:\n                res = '%sx%s' % (format['width'], format['height'])\n            else:\n                res = '%sp' % format['height']\n        elif format.get('width') is not None:\n            res = '%dx?' % format['width']\n        else:\n            res = default\n        return res",
        "begin_line": 2125,
        "end_line": 2139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001303780964797914,
            "pseudo_dstar_susp": 0.0014285714285714286,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0014285714285714286,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._format_note#2141",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._format_note(self, fdict)",
        "snippet": "    def _format_note(self, fdict):\n        res = ''\n        if fdict.get('ext') in ['f4f', 'f4m']:\n            res += '(unsupported) '\n        if fdict.get('language'):\n            if res:\n                res += ' '\n            res += '[%s] ' % fdict['language']\n        if fdict.get('format_note') is not None:\n            res += fdict['format_note'] + ' '\n        if fdict.get('tbr') is not None:\n            res += '%4dk ' % fdict['tbr']\n        if fdict.get('container') is not None:\n            if res:\n                res += ', '\n            res += '%s container' % fdict['container']\n        if (fdict.get('vcodec') is not None\n                and fdict.get('vcodec') != 'none'):\n            if res:\n                res += ', '\n            res += fdict['vcodec']\n            if fdict.get('vbr') is not None:\n                res += '@'\n        elif fdict.get('vbr') is not None and fdict.get('abr') is not None:\n            res += 'video@'\n        if fdict.get('vbr') is not None:\n            res += '%4dk' % fdict['vbr']\n        if fdict.get('fps') is not None:\n            if res:\n                res += ', '\n            res += '%sfps' % fdict['fps']\n        if fdict.get('acodec') is not None:\n            if res:\n                res += ', '\n            if fdict['acodec'] == 'none':\n                res += 'video only'\n            else:\n                res += '%-5s' % fdict['acodec']\n        elif fdict.get('abr') is not None:\n            if res:\n                res += ', '\n            res += 'audio'\n        if fdict.get('abr') is not None:\n            res += '@%3dk' % fdict['abr']\n        if fdict.get('asr') is not None:\n            res += ' (%5dHz)' % fdict['asr']\n        if fdict.get('filesize') is not None:\n            if res:\n                res += ', '\n            res += format_bytes(fdict['filesize'])\n        elif fdict.get('filesize_approx') is not None:\n            if res:\n                res += ', '\n            res += '~' + format_bytes(fdict['filesize_approx'])\n        return res",
        "begin_line": 2141,
        "end_line": 2195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.list_formats#2197",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.list_formats(self, info_dict)",
        "snippet": "    def list_formats(self, info_dict):\n        formats = info_dict.get('formats', [info_dict])\n        table = [\n            [f['format_id'], f['ext'], self.format_resolution(f), self._format_note(f)]\n            for f in formats\n            if f.get('preference') is None or f['preference'] >= -1000]\n        if len(formats) > 1:\n            table[-1][-1] += (' ' if table[-1][-1] else '') + '(best)'\n\n        header_line = ['format code', 'extension', 'resolution', 'note']\n        self.to_screen(\n            '[info] Available formats for %s:\\n%s' %\n            (info_dict['id'], render_table(header_line, table)))",
        "begin_line": 2197,
        "end_line": 2209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.list_subtitles#2223",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.list_subtitles(self, video_id, subtitles, name='subtitles')",
        "snippet": "    def list_subtitles(self, video_id, subtitles, name='subtitles'):\n        if not subtitles:\n            self.to_screen('%s has no %s' % (video_id, name))\n            return\n        self.to_screen(\n            'Available %s for %s:' % (name, video_id))\n        self.to_screen(render_table(\n            ['Language', 'formats'],\n            [[lang, ', '.join(f['ext'] for f in reversed(formats))]\n                for lang, formats in subtitles.items()]))",
        "begin_line": 2223,
        "end_line": 2232,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.urlopen#2234",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.urlopen(self, req)",
        "snippet": "    def urlopen(self, req):\n        \"\"\" Start an HTTP download \"\"\"\n        if isinstance(req, compat_basestring):\n            req = sanitized_Request(req)\n        return self._opener.open(req, timeout=self._socket_timeout)",
        "begin_line": 2234,
        "end_line": 2238,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008547008547008548,
            "pseudo_dstar_susp": 0.006097560975609756,
            "pseudo_tarantula_susp": 0.0029850746268656717,
            "pseudo_op2_susp": 0.006097560975609756,
            "pseudo_barinel_susp": 0.0029850746268656717
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.print_debug_header#2240",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.print_debug_header(self)",
        "snippet": "    def print_debug_header(self):\n        if not self.params.get('verbose'):\n            return\n\n        if type('') is not compat_str:\n            # Python 2.6 on SLES11 SP1 (https://github.com/ytdl-org/youtube-dl/issues/3326)\n            self.report_warning(\n                'Your Python is broken! Update to a newer and supported version')\n\n        stdout_encoding = getattr(\n            sys.stdout, 'encoding', 'missing (%s)' % type(sys.stdout).__name__)\n        encoding_str = (\n            '[debug] Encodings: locale %s, fs %s, out %s, pref %s\\n' % (\n                locale.getpreferredencoding(),\n                sys.getfilesystemencoding(),\n                stdout_encoding,\n                self.get_encoding()))\n        write_string(encoding_str, encoding=None)\n\n        self._write_string('[debug] youtube-dl version ' + __version__ + '\\n')\n        if _LAZY_LOADER:\n            self._write_string('[debug] Lazy loading extractors enabled' + '\\n')\n        try:\n            sp = subprocess.Popen(\n                ['git', 'rev-parse', '--short', 'HEAD'],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                cwd=os.path.dirname(os.path.abspath(__file__)))\n            out, err = sp.communicate()\n            out = out.decode().strip()\n            if re.match('[0-9a-f]+', out):\n                self._write_string('[debug] Git HEAD: ' + out + '\\n')\n        except Exception:\n            try:\n                sys.exc_clear()\n            except Exception:\n                pass\n\n        def python_implementation():\n            impl_name = platform.python_implementation()\n            if impl_name == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n                return impl_name + ' version %d.%d.%d' % sys.pypy_version_info[:3]\n            return impl_name\n\n        self._write_string('[debug] Python version %s (%s) - %s\\n' % (\n            platform.python_version(), python_implementation(),\n            platform_name()))\n\n        exe_versions = FFmpegPostProcessor.get_versions(self)\n        exe_versions['rtmpdump'] = rtmpdump_version()\n        exe_versions['phantomjs'] = PhantomJSwrapper._version()\n        exe_str = ', '.join(\n            '%s %s' % (exe, v)\n            for exe, v in sorted(exe_versions.items())\n            if v\n        )\n        if not exe_str:\n            exe_str = 'none'\n        self._write_string('[debug] exe versions: %s\\n' % exe_str)\n\n        proxy_map = {}\n        for handler in self._opener.handlers:\n            if hasattr(handler, 'proxies'):\n                proxy_map.update(handler.proxies)\n        self._write_string('[debug] Proxy map: ' + compat_str(proxy_map) + '\\n')\n\n        if self.params.get('call_home', False):\n            ipaddr = self.urlopen('https://yt-dl.org/ip').read().decode('utf-8')\n            self._write_string('[debug] Public IP address: %s\\n' % ipaddr)\n            latest_version = self.urlopen(\n                'https://yt-dl.org/latest/version').read().decode('utf-8')\n            if version_tuple(latest_version) > version_tuple(__version__):\n                self.report_warning(\n                    'You are using an outdated version (newest version: %s)! '\n                    'See https://yt-dl.org/update if you need help updating.' %\n                    latest_version)",
        "begin_line": 2240,
        "end_line": 2314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00033025099075297226,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00033025099075297226,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.python_implementation#2277",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.python_implementation()",
        "snippet": "        def python_implementation():\n            impl_name = platform.python_implementation()\n            if impl_name == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n                return impl_name + ' version %d.%d.%d' % sys.pypy_version_info[:3]\n            return impl_name",
        "begin_line": 2277,
        "end_line": 2281,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._setup_opener#2316",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._setup_opener(self)",
        "snippet": "    def _setup_opener(self):\n        timeout_val = self.params.get('socket_timeout')\n        self._socket_timeout = 600 if timeout_val is None else float(timeout_val)\n\n        opts_cookiefile = self.params.get('cookiefile')\n        opts_proxy = self.params.get('proxy')\n\n        if opts_cookiefile is None:\n            self.cookiejar = compat_cookiejar.CookieJar()\n        else:\n            opts_cookiefile = expand_path(opts_cookiefile)\n            self.cookiejar = YoutubeDLCookieJar(opts_cookiefile)\n            if os.access(opts_cookiefile, os.R_OK):\n                self.cookiejar.load(ignore_discard=True, ignore_expires=True)\n\n        cookie_processor = YoutubeDLCookieProcessor(self.cookiejar)\n        if opts_proxy is not None:\n            if opts_proxy == '':\n                proxies = {}\n            else:\n                proxies = {'http': opts_proxy, 'https': opts_proxy}\n        else:\n            proxies = compat_urllib_request.getproxies()\n            # Set HTTPS proxy to HTTP one if given (https://github.com/ytdl-org/youtube-dl/issues/805)\n            if 'http' in proxies and 'https' not in proxies:\n                proxies['https'] = proxies['http']\n        proxy_handler = PerRequestProxyHandler(proxies)\n\n        debuglevel = 1 if self.params.get('debug_printtraffic') else 0\n        https_handler = make_HTTPS_handler(self.params, debuglevel=debuglevel)\n        ydlh = YoutubeDLHandler(self.params, debuglevel=debuglevel)\n        redirect_handler = YoutubeDLRedirectHandler()\n        data_handler = compat_urllib_request_DataHandler()\n\n        # When passing our own FileHandler instance, build_opener won't add the\n        # default FileHandler and allows us to disable the file protocol, which\n        # can be used for malicious purposes (see\n        # https://github.com/ytdl-org/youtube-dl/issues/8227)\n        file_handler = compat_urllib_request.FileHandler()\n\n        def file_open(*args, **kwargs):\n            raise compat_urllib_error.URLError('file:// scheme is explicitly disabled in youtube-dl for security reasons')\n        file_handler.file_open = file_open\n\n        opener = compat_urllib_request.build_opener(\n            proxy_handler, https_handler, cookie_processor, ydlh, redirect_handler, data_handler, file_handler)\n\n        # Delete the default user-agent header, which would otherwise apply in\n        # cases where our custom HTTP handler doesn't come into play\n        # (See https://github.com/ytdl-org/youtube-dl/issues/1309 for details)\n        opener.addheaders = []\n        self._opener = opener",
        "begin_line": 2316,
        "end_line": 2367,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005917159763313609,
            "pseudo_dstar_susp": 0.016666666666666666,
            "pseudo_tarantula_susp": 0.0010857763300760044,
            "pseudo_op2_susp": 0.016666666666666666,
            "pseudo_barinel_susp": 0.0010857763300760044
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.file_open#2356",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.file_open(*args, **kwargs)",
        "snippet": "        def file_open(*args, **kwargs):\n            raise compat_urllib_error.URLError('file:// scheme is explicitly disabled in youtube-dl for security reasons')",
        "begin_line": 2356,
        "end_line": 2357,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005208333333333333,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.encode#2369",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.encode(self, s)",
        "snippet": "    def encode(self, s):\n        if isinstance(s, bytes):\n            return s  # Already encoded\n\n        try:\n            return s.encode(self.get_encoding())\n        except UnicodeEncodeError as err:\n            err.reason = err.reason + '. Check your system encoding configuration or use the --encoding option.'\n            raise",
        "begin_line": 2369,
        "end_line": 2377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.get_encoding#2379",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.get_encoding(self)",
        "snippet": "    def get_encoding(self):\n        encoding = self.params.get('encoding')\n        if encoding is None:\n            encoding = preferredencoding()\n        return encoding",
        "begin_line": 2379,
        "end_line": 2383,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._write_thumbnails#2385",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._write_thumbnails(self, info_dict, filename)",
        "snippet": "    def _write_thumbnails(self, info_dict, filename):\n        if self.params.get('writethumbnail', False):\n            thumbnails = info_dict.get('thumbnails')\n            if thumbnails:\n                thumbnails = [thumbnails[-1]]\n        elif self.params.get('write_all_thumbnails', False):\n            thumbnails = info_dict.get('thumbnails')\n        else:\n            return\n\n        if not thumbnails:\n            # No thumbnails present, so return immediately\n            return\n\n        for t in thumbnails:\n            thumb_ext = determine_ext(t['url'], 'jpg')\n            suffix = '_%s' % t['id'] if len(thumbnails) > 1 else ''\n            thumb_display_id = '%s ' % t['id'] if len(thumbnails) > 1 else ''\n            t['filename'] = thumb_filename = os.path.splitext(filename)[0] + suffix + '.' + thumb_ext\n\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(thumb_filename)):\n                self.to_screen('[%s] %s: Thumbnail %sis already present' %\n                               (info_dict['extractor'], info_dict['id'], thumb_display_id))\n            else:\n                self.to_screen('[%s] %s: Downloading thumbnail %s...' %\n                               (info_dict['extractor'], info_dict['id'], thumb_display_id))\n                try:\n                    uf = self.urlopen(t['url'])\n                    with open(encodeFilename(thumb_filename), 'wb') as thumbf:\n                        shutil.copyfileobj(uf, thumbf)\n                    self.to_screen('[%s] %s: Writing thumbnail %sto: %s' %\n                                   (info_dict['extractor'], info_dict['id'], thumb_display_id, thumb_filename))\n                except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n                    self.report_warning('Unable to download thumbnail \"%s\": %s' %\n                                        (t['url'], error_to_compat_str(err)))",
        "begin_line": 2385,
        "end_line": 2419,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011494252873563218,
            "pseudo_dstar_susp": 0.0012285012285012285,
            "pseudo_tarantula_susp": 0.0003588087549336204,
            "pseudo_op2_susp": 0.0012285012285012285,
            "pseudo_barinel_susp": 0.0003588087549336204
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.__init__#396",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.__init__(self, downloader=None)",
        "snippet": "    def __init__(self, downloader=None):\n        \"\"\"Constructor. Receives an optional downloader.\"\"\"\n        self._ready = False\n        self._x_forwarded_for_ip = None\n        self.set_downloader(downloader)",
        "begin_line": 396,
        "end_line": 400,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004672897196261682,
            "pseudo_dstar_susp": 0.02564102564102564,
            "pseudo_tarantula_susp": 0.0009066183136899365,
            "pseudo_op2_susp": 0.02564102564102564,
            "pseudo_barinel_susp": 0.0009066183136899365
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.suitable#403",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n\n        # This does not use has/getattr intentionally - we want to know whether\n        # we have cached the regexp for *this* class, whereas getattr would also\n        # match the superclass\n        if '_VALID_URL_RE' not in cls.__dict__:\n            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n        return cls._VALID_URL_RE.match(url) is not None",
        "begin_line": 403,
        "end_line": 411,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.005952380952380952,
            "pseudo_tarantula_susp": 0.001128668171557562,
            "pseudo_op2_susp": 0.005952380952380952,
            "pseudo_barinel_susp": 0.001128668171557562
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._match_id#414",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._match_id(cls, url)",
        "snippet": "    def _match_id(cls, url):\n        if '_VALID_URL_RE' not in cls.__dict__:\n            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n        m = cls._VALID_URL_RE.match(url)\n        assert m\n        return compat_str(m.group('id'))",
        "begin_line": 414,
        "end_line": 419,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00273224043715847,
            "pseudo_dstar_susp": 0.002717391304347826,
            "pseudo_tarantula_susp": 0.00145985401459854,
            "pseudo_op2_susp": 0.002717391304347826,
            "pseudo_barinel_susp": 0.00145985401459854
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.working#422",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.working(cls)",
        "snippet": "    def working(cls):\n        \"\"\"Getter method for _WORKING.\"\"\"\n        return cls._WORKING",
        "begin_line": 422,
        "end_line": 424,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004739336492890996,
            "pseudo_dstar_susp": 0.023809523809523808,
            "pseudo_tarantula_susp": 0.0008787346221441124,
            "pseudo_op2_susp": 0.023809523809523808,
            "pseudo_barinel_susp": 0.0008787346221441124
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.initialize#426",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.initialize(self)",
        "snippet": "    def initialize(self):\n        \"\"\"Initializes an instance (authentication, etc).\"\"\"\n        self._initialize_geo_bypass({\n            'countries': self._GEO_COUNTRIES,\n            'ip_blocks': self._GEO_IP_BLOCKS,\n        })\n        if not self._ready:\n            self._real_initialize()\n            self._ready = True",
        "begin_line": 426,
        "end_line": 434,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.010416666666666666,
            "pseudo_tarantula_susp": 0.0029850746268656717,
            "pseudo_op2_susp": 0.010416666666666666,
            "pseudo_barinel_susp": 0.0029850746268656717
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._initialize_geo_bypass#436",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._initialize_geo_bypass(self, geo_bypass_context)",
        "snippet": "    def _initialize_geo_bypass(self, geo_bypass_context):\n        \"\"\"\n        Initialize geo restriction bypass mechanism.\n\n        This method is used to initialize geo bypass mechanism based on faking\n        X-Forwarded-For HTTP header. A random country from provided country list\n        is selected and a random IP belonging to this country is generated. This\n        IP will be passed as X-Forwarded-For HTTP header in all subsequent\n        HTTP requests.\n\n        This method will be used for initial geo bypass mechanism initialization\n        during the instance initialization with _GEO_COUNTRIES and\n        _GEO_IP_BLOCKS.\n\n        You may also manually call it from extractor's code if geo bypass\n        information is not available beforehand (e.g. obtained during\n        extraction) or due to some other reason. In this case you should pass\n        this information in geo bypass context passed as first argument. It may\n        contain following fields:\n\n        countries:  List of geo unrestricted countries (similar\n                    to _GEO_COUNTRIES)\n        ip_blocks:  List of geo unrestricted IP blocks in CIDR notation\n                    (similar to _GEO_IP_BLOCKS)\n\n        \"\"\"\n        if not self._x_forwarded_for_ip:\n\n            # Geo bypass mechanism is explicitly disabled by user\n            if not self._downloader.params.get('geo_bypass', True):\n                return\n\n            if not geo_bypass_context:\n                geo_bypass_context = {}\n\n            # Backward compatibility: previously _initialize_geo_bypass\n            # expected a list of countries, some 3rd party code may still use\n            # it this way\n            if isinstance(geo_bypass_context, (list, tuple)):\n                geo_bypass_context = {\n                    'countries': geo_bypass_context,\n                }\n\n            # The whole point of geo bypass mechanism is to fake IP\n            # as X-Forwarded-For HTTP header based on some IP block or\n            # country code.\n\n            # Path 1: bypassing based on IP block in CIDR notation\n\n            # Explicit IP block specified by user, use it right away\n            # regardless of whether extractor is geo bypassable or not\n            ip_block = self._downloader.params.get('geo_bypass_ip_block', None)\n\n            # Otherwise use random IP block from geo bypass context but only\n            # if extractor is known as geo bypassable\n            if not ip_block:\n                ip_blocks = geo_bypass_context.get('ip_blocks')\n                if self._GEO_BYPASS and ip_blocks:\n                    ip_block = random.choice(ip_blocks)\n\n            if ip_block:\n                self._x_forwarded_for_ip = GeoUtils.random_ipv4(ip_block)\n                if self._downloader.params.get('verbose', False):\n                    self._downloader.to_screen(\n                        '[debug] Using fake IP %s as X-Forwarded-For.'\n                        % self._x_forwarded_for_ip)\n                return\n\n            # Path 2: bypassing based on country code\n\n            # Explicit country code specified by user, use it right away\n            # regardless of whether extractor is geo bypassable or not\n            country = self._downloader.params.get('geo_bypass_country', None)\n\n            # Otherwise use random country code from geo bypass context but\n            # only if extractor is known as geo bypassable\n            if not country:\n                countries = geo_bypass_context.get('countries')\n                if self._GEO_BYPASS and countries:\n                    country = random.choice(countries)\n\n            if country:\n                self._x_forwarded_for_ip = GeoUtils.random_ipv4(country)\n                if self._downloader.params.get('verbose', False):\n                    self._downloader.to_screen(\n                        '[debug] Using fake IP %s (%s) as X-Forwarded-For.'\n                        % (self._x_forwarded_for_ip, country.upper()))",
        "begin_line": 436,
        "end_line": 522,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.010416666666666666,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.010416666666666666,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract#524",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract(self, url)",
        "snippet": "    def extract(self, url):\n        \"\"\"Extracts URL information and returns it in list of dicts.\"\"\"\n        try:\n            for _ in range(2):\n                try:\n                    self.initialize()\n                    ie_result = self._real_extract(url)\n                    if self._x_forwarded_for_ip:\n                        ie_result['__x_forwarded_for_ip'] = self._x_forwarded_for_ip\n                    return ie_result\n                except GeoRestrictedError as e:\n                    if self.__maybe_fake_ip_and_retry(e.countries):\n                        continue\n                    raise\n        except ExtractorError:\n            raise\n        except compat_http_client.IncompleteRead as e:\n            raise ExtractorError('A network error has occurred.', cause=e, expected=True)\n        except (KeyError, StopIteration) as e:\n            raise ExtractorError('An extractor error has occurred.', cause=e)",
        "begin_line": 524,
        "end_line": 543,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.010416666666666666,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.010416666666666666,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.__maybe_fake_ip_and_retry#545",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.__maybe_fake_ip_and_retry(self, countries)",
        "snippet": "    def __maybe_fake_ip_and_retry(self, countries):\n        if (not self._downloader.params.get('geo_bypass_country', None)\n                and self._GEO_BYPASS\n                and self._downloader.params.get('geo_bypass', True)\n                and not self._x_forwarded_for_ip\n                and countries):\n            country_code = random.choice(countries)\n            self._x_forwarded_for_ip = GeoUtils.random_ipv4(country_code)\n            if self._x_forwarded_for_ip:\n                self.report_warning(\n                    'Video is geo restricted. Retrying extraction with fake IP %s (%s) as X-Forwarded-For.'\n                    % (self._x_forwarded_for_ip, country_code.upper()))\n                return True\n        return False",
        "begin_line": 545,
        "end_line": 558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005497526113249038,
            "pseudo_dstar_susp": 0.0004930966469428008,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004930966469428008,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.set_downloader#560",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.set_downloader(self, downloader)",
        "snippet": "    def set_downloader(self, downloader):\n        \"\"\"Sets the downloader for this IE.\"\"\"\n        self._downloader = downloader",
        "begin_line": 560,
        "end_line": 562,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004629629629629629,
            "pseudo_dstar_susp": 0.024390243902439025,
            "pseudo_tarantula_susp": 0.0009066183136899365,
            "pseudo_op2_susp": 0.024390243902439025,
            "pseudo_barinel_susp": 0.0009066183136899365
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._real_initialize#564",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        \"\"\"Real initialization process. Redefine in subclasses.\"\"\"\n        pass",
        "begin_line": 564,
        "end_line": 566,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004048582995951417,
            "pseudo_dstar_susp": 0.004424778761061947,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.004424778761061947,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.ie_key#573",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.ie_key(cls)",
        "snippet": "    def ie_key(cls):\n        \"\"\"A string for getting the InfoExtractor with get_info_extractor\"\"\"\n        return compat_str(cls.__name__[:-2])",
        "begin_line": 573,
        "end_line": 575,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01282051282051282,
            "pseudo_dstar_susp": 0.007518796992481203,
            "pseudo_tarantula_susp": 0.0011933174224343676,
            "pseudo_op2_susp": 0.007518796992481203,
            "pseudo_barinel_susp": 0.0011933174224343676
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.IE_NAME#578",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.IE_NAME(self)",
        "snippet": "    def IE_NAME(self):\n        return compat_str(type(self).__name__[:-2])",
        "begin_line": 578,
        "end_line": 579,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026226068712300026,
            "pseudo_dstar_susp": 0.00026171159382360636,
            "pseudo_tarantula_susp": 0.0002676659528907923,
            "pseudo_op2_susp": 0.00026171159382360636,
            "pseudo_barinel_susp": 0.0002676659528907923
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.__can_accept_status_code#582",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.__can_accept_status_code(err, expected_status)",
        "snippet": "    def __can_accept_status_code(err, expected_status):\n        assert isinstance(err, compat_urllib_error.HTTPError)\n        if expected_status is None:\n            return False\n        if isinstance(expected_status, compat_integer_types):\n            return err.code == expected_status\n        elif isinstance(expected_status, (list, tuple)):\n            return err.code in expected_status\n        elif callable(expected_status):\n            return expected_status(err.code) is True\n        else:\n            assert False",
        "begin_line": 582,
        "end_line": 593,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024271844660194173,
            "pseudo_dstar_susp": 0.0019342359767891683,
            "pseudo_tarantula_susp": 0.003289473684210526,
            "pseudo_op2_susp": 0.0019342359767891683,
            "pseudo_barinel_susp": 0.003289473684210526
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._request_webpage#595",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._request_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _request_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True, data=None, headers={}, query={}, expected_status=None):\n        \"\"\"\n        Return the response handle.\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        if note is None:\n            self.report_download_webpage(video_id)\n        elif note is not False:\n            if video_id is None:\n                self.to_screen('%s' % (note,))\n            else:\n                self.to_screen('%s: %s' % (video_id, note))\n\n        # Some sites check X-Forwarded-For HTTP header in order to figure out\n        # the origin of the client behind proxy. This allows bypassing geo\n        # restriction by faking this header's value to IP that belongs to some\n        # geo unrestricted country. We will do so once we encounter any\n        # geo restriction error.\n        if self._x_forwarded_for_ip:\n            if 'X-Forwarded-For' not in headers:\n                headers['X-Forwarded-For'] = self._x_forwarded_for_ip\n\n        if isinstance(url_or_request, compat_urllib_request.Request):\n            url_or_request = update_Request(\n                url_or_request, data=data, headers=headers, query=query)\n        else:\n            if query:\n                url_or_request = update_url_query(url_or_request, query)\n            if data is not None or headers:\n                url_or_request = sanitized_Request(url_or_request, data, headers)\n        try:\n            return self._downloader.urlopen(url_or_request)\n        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n            if isinstance(err, compat_urllib_error.HTTPError):\n                if self.__can_accept_status_code(err, expected_status):\n                    # Retain reference to error to prevent file object from\n                    # being closed before it can be read. Works around the\n                    # effects of <https://bugs.python.org/issue15002>\n                    # introduced in Python 3.4.1.\n                    err.fp._error = err\n                    return err.fp\n\n            if errnote is False:\n                return False\n            if errnote is None:\n                errnote = 'Unable to download webpage'\n\n            errmsg = '%s: %s' % (errnote, error_to_compat_str(err))\n            if fatal:\n                raise ExtractorError(errmsg, sys.exc_info()[2], cause=err)\n            else:\n                self._downloader.report_warning(errmsg)\n                return False",
        "begin_line": 595,
        "end_line": 648,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014492753623188406,
            "pseudo_dstar_susp": 0.008064516129032258,
            "pseudo_tarantula_susp": 0.003205128205128205,
            "pseudo_op2_susp": 0.008064516129032258,
            "pseudo_barinel_susp": 0.003205128205128205
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_webpage_handle#650",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_webpage_handle(self, url_or_request, video_id, note=None, errnote=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_webpage_handle(self, url_or_request, video_id, note=None, errnote=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None):\n        \"\"\"\n        Return a tuple (page content as string, URL handle).\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        # Strip hashes from the URL (#1038)\n        if isinstance(url_or_request, (compat_str, str)):\n            url_or_request = url_or_request.partition('#')[0]\n\n        urlh = self._request_webpage(url_or_request, video_id, note, errnote, fatal, data=data, headers=headers, query=query, expected_status=expected_status)\n        if urlh is False:\n            assert not fatal\n            return False\n        content = self._webpage_read_content(urlh, url_or_request, video_id, note, errnote, fatal, encoding=encoding)\n        return (content, urlh)",
        "begin_line": 650,
        "end_line": 665,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009523809523809525,
            "pseudo_dstar_susp": 0.004629629629629629,
            "pseudo_tarantula_susp": 0.003205128205128205,
            "pseudo_op2_susp": 0.004629629629629629,
            "pseudo_barinel_susp": 0.003205128205128205
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._guess_encoding_from_content#668",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._guess_encoding_from_content(content_type, webpage_bytes)",
        "snippet": "    def _guess_encoding_from_content(content_type, webpage_bytes):\n        m = re.match(r'[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+\\s*;\\s*charset=(.+)', content_type)\n        if m:\n            encoding = m.group(1)\n        else:\n            m = re.search(br'<meta[^>]+charset=[\\'\"]?([^\\'\")]+)[ /\\'\">]',\n                          webpage_bytes[:1024])\n            if m:\n                encoding = m.group(1).decode('ascii')\n            elif webpage_bytes.startswith(b'\\xff\\xfe'):\n                encoding = 'utf-16'\n            else:\n                encoding = 'utf-8'\n\n        return encoding",
        "begin_line": 668,
        "end_line": 682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029585798816568047,
            "pseudo_dstar_susp": 0.0032679738562091504,
            "pseudo_tarantula_susp": 0.001303780964797914,
            "pseudo_op2_susp": 0.0032679738562091504,
            "pseudo_barinel_susp": 0.001303780964797914
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.__check_blocked#684",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.__check_blocked(self, content)",
        "snippet": "    def __check_blocked(self, content):\n        first_block = content[:512]\n        if ('<title>Access to this site is blocked</title>' in content\n                and 'Websense' in first_block):\n            msg = 'Access to this webpage has been blocked by Websense filtering software in your network.'\n            blocked_iframe = self._html_search_regex(\n                r'<iframe src=\"([^\"]+)\"', content,\n                'Websense information URL', default=None)\n            if blocked_iframe:\n                msg += ' Visit %s for more details' % blocked_iframe\n            raise ExtractorError(msg, expected=True)\n        if '<title>The URL you requested has been blocked</title>' in first_block:\n            msg = (\n                'Access to this webpage has been blocked by Indian censorship. '\n                'Use a VPN or proxy server (with --proxy) to route around it.')\n            block_msg = self._html_search_regex(\n                r'</h1><p>(.*?)</p>',\n                content, 'block message', default=None)\n            if block_msg:\n                msg += ' (Message: \"%s\")' % block_msg.replace('\\n', ' ')\n            raise ExtractorError(msg, expected=True)\n        if ('<title>TTK :: \u0414\u043e\u0441\u0442\u0443\u043f \u043a \u0440\u0435\u0441\u0443\u0440\u0441\u0443 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d</title>' in content\n                and 'blocklist.rkn.gov.ru' in content):\n            raise ExtractorError(\n                'Access to this webpage has been blocked by decision of the Russian government. '\n                'Visit http://blocklist.rkn.gov.ru/ for a block reason.',\n                expected=True)",
        "begin_line": 684,
        "end_line": 710,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003048780487804878,
            "pseudo_dstar_susp": 0.003367003367003367,
            "pseudo_tarantula_susp": 0.001893939393939394,
            "pseudo_op2_susp": 0.003367003367003367,
            "pseudo_barinel_susp": 0.001893939393939394
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._webpage_read_content#712",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._webpage_read_content(self, urlh, url_or_request, video_id, note=None, errnote=None, fatal=True, prefix=None, encoding=None)",
        "snippet": "    def _webpage_read_content(self, urlh, url_or_request, video_id, note=None, errnote=None, fatal=True, prefix=None, encoding=None):\n        content_type = urlh.headers.get('Content-Type', '')\n        webpage_bytes = urlh.read()\n        if prefix is not None:\n            webpage_bytes = prefix + webpage_bytes\n        if not encoding:\n            encoding = self._guess_encoding_from_content(content_type, webpage_bytes)\n        if self._downloader.params.get('dump_intermediate_pages', False):\n            self.to_screen('Dumping request to ' + urlh.geturl())\n            dump = base64.b64encode(webpage_bytes).decode('ascii')\n            self._downloader.to_screen(dump)\n        if self._downloader.params.get('write_pages', False):\n            basen = '%s_%s' % (video_id, urlh.geturl())\n            if len(basen) > 240:\n                h = '___' + hashlib.md5(basen.encode('utf-8')).hexdigest()\n                basen = basen[:240 - len(h)] + h\n            raw_filename = basen + '.dump'\n            filename = sanitize_filename(raw_filename, restricted=True)\n            self.to_screen('Saving request to ' + filename)\n            # Working around MAX_PATH limitation on Windows (see\n            # http://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx)\n            if compat_os_name == 'nt':\n                absfilepath = os.path.abspath(filename)\n                if len(absfilepath) > 259:\n                    filename = '\\\\\\\\?\\\\' + absfilepath\n            with open(filename, 'wb') as outf:\n                outf.write(webpage_bytes)\n\n        try:\n            content = webpage_bytes.decode(encoding, 'replace')\n        except LookupError:\n            content = webpage_bytes.decode('utf-8', 'replace')\n\n        self.__check_blocked(content)\n\n        return content",
        "begin_line": 712,
        "end_line": 747,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003048780487804878,
            "pseudo_dstar_susp": 0.003367003367003367,
            "pseudo_tarantula_susp": 0.001893939393939394,
            "pseudo_op2_susp": 0.003367003367003367,
            "pseudo_barinel_susp": 0.001893939393939394
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_webpage#749",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True, tries=1, timeout=5, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_webpage(\n            self, url_or_request, video_id, note=None, errnote=None,\n            fatal=True, tries=1, timeout=5, encoding=None, data=None,\n            headers={}, query={}, expected_status=None):\n        \"\"\"\n        Return the data of the page as a string.\n\n        Arguments:\n        url_or_request -- plain text URL as a string or\n            a compat_urllib_request.Requestobject\n        video_id -- Video/playlist/item identifier (string)\n\n        Keyword arguments:\n        note -- note printed before downloading (string)\n        errnote -- note printed in case of an error (string)\n        fatal -- flag denoting whether error should be considered fatal,\n            i.e. whether it should cause ExtractionError to be raised,\n            otherwise a warning will be reported and extraction continued\n        tries -- number of tries\n        timeout -- sleep interval between tries\n        encoding -- encoding for a page content decoding, guessed automatically\n            when not explicitly specified\n        data -- POST data (bytes)\n        headers -- HTTP headers (dict)\n        query -- URL query (dict)\n        expected_status -- allows to accept failed HTTP requests (non 2xx\n            status code) by explicitly specifying a set of accepted status\n            codes. Can be any of the following entities:\n                - an integer type specifying an exact failed status code to\n                  accept\n                - a list or a tuple of integer types specifying a list of\n                  failed status codes to accept\n                - a callable accepting an actual failed status code and\n                  returning True if it should be accepted\n            Note that this argument does not affect success status codes (2xx)\n            which are always accepted.\n        \"\"\"\n\n        success = False\n        try_count = 0\n        while success is False:\n            try:\n                res = self._download_webpage_handle(\n                    url_or_request, video_id, note, errnote, fatal,\n                    encoding=encoding, data=data, headers=headers, query=query,\n                    expected_status=expected_status)\n                success = True\n            except compat_http_client.IncompleteRead as e:\n                try_count += 1\n                if try_count >= tries:\n                    raise e\n                self._sleep(timeout, video_id)\n        if res is False:\n            return res\n        else:\n            content, _ = res\n            return content",
        "begin_line": 749,
        "end_line": 805,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002824858757062147,
            "pseudo_dstar_susp": 0.002785515320334262,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.002785515320334262,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_xml_handle#807",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_xml_handle(self, url_or_request, video_id, note='Downloading XML', errnote='Unable to download XML', transform_source=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_xml_handle(\n            self, url_or_request, video_id, note='Downloading XML',\n            errnote='Unable to download XML', transform_source=None,\n            fatal=True, encoding=None, data=None, headers={}, query={},\n            expected_status=None):\n        \"\"\"\n        Return a tuple (xml as an compat_etree_Element, URL handle).\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        res = self._download_webpage_handle(\n            url_or_request, video_id, note, errnote, fatal=fatal,\n            encoding=encoding, data=data, headers=headers, query=query,\n            expected_status=expected_status)\n        if res is False:\n            return res\n        xml_string, urlh = res\n        return self._parse_xml(\n            xml_string, video_id, transform_source=transform_source,\n            fatal=fatal), urlh",
        "begin_line": 807,
        "end_line": 826,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009199632014719411,
            "pseudo_dstar_susp": 0.0008453085376162299,
            "pseudo_tarantula_susp": 0.0005396654074473826,
            "pseudo_op2_susp": 0.0008453085376162299,
            "pseudo_barinel_susp": 0.0005396654074473826
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_xml#828",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_xml(self, url_or_request, video_id, note='Downloading XML', errnote='Unable to download XML', transform_source=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_xml(\n            self, url_or_request, video_id,\n            note='Downloading XML', errnote='Unable to download XML',\n            transform_source=None, fatal=True, encoding=None,\n            data=None, headers={}, query={}, expected_status=None):\n        \"\"\"\n        Return the xml as an compat_etree_Element.\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        res = self._download_xml_handle(\n            url_or_request, video_id, note=note, errnote=errnote,\n            transform_source=transform_source, fatal=fatal, encoding=encoding,\n            data=data, headers=headers, query=query,\n            expected_status=expected_status)\n        return res if res is False else res[0]",
        "begin_line": 828,
        "end_line": 843,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00088261253309797,
            "pseudo_dstar_susp": 0.0008143322475570033,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_xml#845",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_xml(self, xml_string, video_id, transform_source=None, fatal=True)",
        "snippet": "    def _parse_xml(self, xml_string, video_id, transform_source=None, fatal=True):\n        if transform_source:\n            xml_string = transform_source(xml_string)\n        try:\n            return compat_etree_fromstring(xml_string.encode('utf-8'))\n        except compat_xml_parse_error as ve:\n            errmsg = '%s: Failed to parse XML ' % video_id\n            if fatal:\n                raise ExtractorError(errmsg, cause=ve)\n            else:\n                self.report_warning(errmsg + str(ve))",
        "begin_line": 845,
        "end_line": 855,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007763975155279503,
            "pseudo_dstar_susp": 0.0007905138339920949,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0007905138339920949,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_json_handle#857",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_json_handle(self, url_or_request, video_id, note='Downloading JSON metadata', errnote='Unable to download JSON metadata', transform_source=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_json_handle(\n            self, url_or_request, video_id, note='Downloading JSON metadata',\n            errnote='Unable to download JSON metadata', transform_source=None,\n            fatal=True, encoding=None, data=None, headers={}, query={},\n            expected_status=None):\n        \"\"\"\n        Return a tuple (JSON object, URL handle).\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        res = self._download_webpage_handle(\n            url_or_request, video_id, note, errnote, fatal=fatal,\n            encoding=encoding, data=data, headers=headers, query=query,\n            expected_status=expected_status)\n        if res is False:\n            return res\n        json_string, urlh = res\n        return self._parse_json(\n            json_string, video_id, transform_source=transform_source,\n            fatal=fatal), urlh",
        "begin_line": 857,
        "end_line": 876,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002352941176470588,
            "pseudo_dstar_susp": 0.002188183807439825,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.002188183807439825,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_json#878",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_json(self, url_or_request, video_id, note='Downloading JSON metadata', errnote='Unable to download JSON metadata', transform_source=None, fatal=True, encoding=None, data=None, headers={}, query={}, expected_status=None)",
        "snippet": "    def _download_json(\n            self, url_or_request, video_id, note='Downloading JSON metadata',\n            errnote='Unable to download JSON metadata', transform_source=None,\n            fatal=True, encoding=None, data=None, headers={}, query={},\n            expected_status=None):\n        \"\"\"\n        Return the JSON object as a dict.\n\n        See _download_webpage docstring for arguments specification.\n        \"\"\"\n        res = self._download_json_handle(\n            url_or_request, video_id, note=note, errnote=errnote,\n            transform_source=transform_source, fatal=fatal, encoding=encoding,\n            data=data, headers=headers, query=query,\n            expected_status=expected_status)\n        return res if res is False else res[0]",
        "begin_line": 878,
        "end_line": 893,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0023752969121140144,
            "pseudo_dstar_susp": 0.0022271714922048997,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0022271714922048997,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_json#895",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_json(self, json_string, video_id, transform_source=None, fatal=True)",
        "snippet": "    def _parse_json(self, json_string, video_id, transform_source=None, fatal=True):\n        if transform_source:\n            json_string = transform_source(json_string)\n        try:\n            return json.loads(json_string)\n        except ValueError as ve:\n            errmsg = '%s: Failed to parse JSON ' % video_id\n            if fatal:\n                raise ExtractorError(errmsg, cause=ve)\n            else:\n                self.report_warning(errmsg + str(ve))",
        "begin_line": 895,
        "end_line": 905,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022624434389140274,
            "pseudo_dstar_susp": 0.0021645021645021645,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_warning#907",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_warning(self, msg, video_id=None)",
        "snippet": "    def report_warning(self, msg, video_id=None):\n        idstr = '' if video_id is None else '%s: ' % video_id\n        self._downloader.report_warning(\n            '[%s] %s%s' % (self.IE_NAME, idstr, msg))",
        "begin_line": 907,
        "end_line": 910,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006414368184733803,
            "pseudo_dstar_susp": 0.0005361930294906167,
            "pseudo_tarantula_susp": 0.0024330900243309003,
            "pseudo_op2_susp": 0.0005361930294906167,
            "pseudo_barinel_susp": 0.0024330900243309003
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.to_screen#912",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.to_screen(self, msg)",
        "snippet": "    def to_screen(self, msg):\n        \"\"\"Print msg to screen, prefixing it with '[ie_name]'\"\"\"\n        self._downloader.to_screen('[%s] %s' % (self.IE_NAME, msg))",
        "begin_line": 912,
        "end_line": 914,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013157894736842105,
            "pseudo_dstar_susp": 0.007633587786259542,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.007633587786259542,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_extraction#916",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_extraction(self, id_or_name)",
        "snippet": "    def report_extraction(self, id_or_name):\n        \"\"\"Report information extraction.\"\"\"\n        self.to_screen('%s: Extracting information' % id_or_name)",
        "begin_line": 916,
        "end_line": 918,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.0002678810608090008,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0002678810608090008,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_download_webpage#920",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_download_webpage(self, video_id)",
        "snippet": "    def report_download_webpage(self, video_id):\n        \"\"\"Report webpage download.\"\"\"\n        self.to_screen('%s: Downloading webpage' % video_id)",
        "begin_line": 920,
        "end_line": 922,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029069767441860465,
            "pseudo_dstar_susp": 0.002898550724637681,
            "pseudo_tarantula_susp": 0.0029850746268656717,
            "pseudo_op2_susp": 0.002898550724637681,
            "pseudo_barinel_susp": 0.0029850746268656717
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.raise_login_required#933",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.raise_login_required(msg='This video is only available for registered users')",
        "snippet": "    def raise_login_required(msg='This video is only available for registered users'):\n        raise ExtractorError(\n            '%s. Use --username and --password or --netrc to provide account credentials.' % msg,\n            expected=True)",
        "begin_line": 933,
        "end_line": 936,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043975373790677223,
            "pseudo_dstar_susp": 0.0003944773175542406,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003944773175542406,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.raise_geo_restricted#939",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.raise_geo_restricted(msg='This video is not available from your location due to geo restriction', countries=None)",
        "snippet": "    def raise_geo_restricted(msg='This video is not available from your location due to geo restriction', countries=None):\n        raise GeoRestrictedError(msg, countries=countries)",
        "begin_line": 939,
        "end_line": 940,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005497526113249038,
            "pseudo_dstar_susp": 0.0004930966469428008,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.0004930966469428008,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.url_result#944",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.url_result(url, ie=None, video_id=None, video_title=None)",
        "snippet": "    def url_result(url, ie=None, video_id=None, video_title=None):\n        \"\"\"Returns a URL that points to a page that should be processed\"\"\"\n        # TODO: ie should be the class used for getting the info\n        video_info = {'_type': 'url',\n                      'url': url,\n                      'ie_key': ie}\n        if video_id is not None:\n            video_info['id'] = video_id\n        if video_title is not None:\n            video_info['title'] = video_title\n        return video_info",
        "begin_line": 944,
        "end_line": 954,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001584786053882726,
            "pseudo_dstar_susp": 0.001145475372279496,
            "pseudo_tarantula_susp": 0.0005431830526887561,
            "pseudo_op2_susp": 0.001145475372279496,
            "pseudo_barinel_susp": 0.0005431830526887561
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.playlist_from_matches#956",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.playlist_from_matches(self, matches, playlist_id=None, playlist_title=None, getter=None, ie=None)",
        "snippet": "    def playlist_from_matches(self, matches, playlist_id=None, playlist_title=None, getter=None, ie=None):\n        urls = orderedSet(\n            self.url_result(self._proto_relative_url(getter(m) if getter else m), ie)\n            for m in matches)\n        return self.playlist_result(\n            urls, playlist_id=playlist_id, playlist_title=playlist_title)",
        "begin_line": 956,
        "end_line": 961,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005385029617662897,
            "pseudo_dstar_susp": 0.0005373455131649651,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005373455131649651,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.playlist_result#964",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.playlist_result(entries, playlist_id=None, playlist_title=None, playlist_description=None)",
        "snippet": "    def playlist_result(entries, playlist_id=None, playlist_title=None, playlist_description=None):\n        \"\"\"Returns a playlist\"\"\"\n        video_info = {'_type': 'playlist',\n                      'entries': entries}\n        if playlist_id:\n            video_info['id'] = playlist_id\n        if playlist_title:\n            video_info['title'] = playlist_title\n        if playlist_description:\n            video_info['description'] = playlist_description\n        return video_info",
        "begin_line": 964,
        "end_line": 974,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015360983102918587,
            "pseudo_dstar_susp": 0.0010604453870625664,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0010604453870625664,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._search_regex#976",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None)",
        "snippet": "    def _search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None):\n        \"\"\"\n        Perform a regex search on the given string, using a single or a list of\n        patterns returning the first matching group.\n        In case of failure return a default value or raise a WARNING or a\n        RegexNotFoundError, depending on fatal, specifying the field name.\n        \"\"\"\n        if isinstance(pattern, (str, compat_str, compiled_regex_type)):\n            mobj = re.search(pattern, string, flags)\n        else:\n            for p in pattern:\n                mobj = re.search(p, string, flags)\n                if mobj:\n                    break\n\n        if not self._downloader.params.get('no_color') and compat_os_name != 'nt' and sys.stderr.isatty():\n            _name = '\\033[0;34m%s\\033[0m' % name\n        else:\n            _name = name\n\n        if mobj:\n            if group is None:\n                # return the first matching group\n                return next(g for g in mobj.groups() if g is not None)\n            else:\n                return mobj.group(group)\n        elif default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            raise RegexNotFoundError('Unable to extract %s' % _name)\n        else:\n            self._downloader.report_warning('unable to extract %s' % _name + bug_reports_message())\n            return None",
        "begin_line": 976,
        "end_line": 1008,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026666666666666666,
            "pseudo_dstar_susp": 0.00267379679144385,
            "pseudo_tarantula_susp": 0.0027397260273972603,
            "pseudo_op2_susp": 0.00267379679144385,
            "pseudo_barinel_susp": 0.0027397260273972603
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._html_search_regex#1010",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._html_search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None)",
        "snippet": "    def _html_search_regex(self, pattern, string, name, default=NO_DEFAULT, fatal=True, flags=0, group=None):\n        \"\"\"\n        Like _search_regex, but strips HTML tags and unescapes entities.\n        \"\"\"\n        res = self._search_regex(pattern, string, name, default, fatal, flags, group)\n        if res:\n            return clean_html(res).strip()\n        else:\n            return res",
        "begin_line": 1010,
        "end_line": 1018,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021598272138228943,
            "pseudo_dstar_susp": 0.0019267822736030828,
            "pseudo_tarantula_susp": 0.0010604453870625664,
            "pseudo_op2_susp": 0.0019267822736030828,
            "pseudo_barinel_susp": 0.0010604453870625664
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_netrc_login_info#1020",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_netrc_login_info(self, netrc_machine=None)",
        "snippet": "    def _get_netrc_login_info(self, netrc_machine=None):\n        username = None\n        password = None\n        netrc_machine = netrc_machine or self._NETRC_MACHINE\n\n        if self._downloader.params.get('usenetrc', False):\n            try:\n                info = netrc.netrc().authenticators(netrc_machine)\n                if info is not None:\n                    username = info[0]\n                    password = info[2]\n                else:\n                    raise netrc.NetrcParseError(\n                        'No authenticators for %s' % netrc_machine)\n            except (IOError, netrc.NetrcParseError) as err:\n                self._downloader.report_warning(\n                    'parsing .netrc: %s' % error_to_compat_str(err))\n\n        return username, password",
        "begin_line": 1020,
        "end_line": 1038,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001893939393939394,
            "pseudo_dstar_susp": 0.0012453300124533001,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0012453300124533001,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_login_info#1040",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_login_info(self, username_option='username', password_option='password', netrc_machine=None)",
        "snippet": "    def _get_login_info(self, username_option='username', password_option='password', netrc_machine=None):\n        \"\"\"\n        Get the login info as (username, password)\n        First look for the manually specified credentials using username_option\n        and password_option as keys in params dictionary. If no such credentials\n        available look in the netrc file using the netrc_machine or _NETRC_MACHINE\n        value.\n        If there's no info available, return (None, None)\n        \"\"\"\n        if self._downloader is None:\n            return (None, None)\n\n        downloader_params = self._downloader.params\n\n        # Attempt to use provided username and password or .netrc data\n        if downloader_params.get(username_option) is not None:\n            username = downloader_params[username_option]\n            password = downloader_params[password_option]\n        else:\n            username, password = self._get_netrc_login_info(netrc_machine)\n\n        return username, password",
        "begin_line": 1040,
        "end_line": 1061,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001893939393939394,
            "pseudo_dstar_susp": 0.0012453300124533001,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0012453300124533001,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_regexes#1081",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_regexes(prop)",
        "snippet": "    def _og_regexes(prop):\n        content_re = r'content=(?:\"([^\"]+?)\"|\\'([^\\']+?)\\'|\\s*([^\\s\"\\'=<>`]+?))'\n        property_re = (r'(?:name|property)=(?:\\'og[:-]%(prop)s\\'|\"og[:-]%(prop)s\"|\\s*og[:-]%(prop)s\\b)'\n                       % {'prop': re.escape(prop)})\n        template = r'<meta[^>]+?%s[^>]+?%s'\n        return [\n            template % (property_re, content_re),\n            template % (content_re, property_re),\n        ]",
        "begin_line": 1081,
        "end_line": 1089,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017921146953405018,
            "pseudo_dstar_susp": 0.0016891891891891893,
            "pseudo_tarantula_susp": 0.0006527415143603133,
            "pseudo_op2_susp": 0.0016891891891891893,
            "pseudo_barinel_susp": 0.0006527415143603133
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._meta_regex#1092",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._meta_regex(prop)",
        "snippet": "    def _meta_regex(prop):\n        return r'''(?isx)<meta\n                    (?=[^>]+(?:itemprop|name|property|id|http-equiv)=([\"\\']?)%s\\1)\n                    [^>]+?content=([\"\\'])(?P<content>.*?)\\2''' % re.escape(prop)",
        "begin_line": 1092,
        "end_line": 1095,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010482180293501049,
            "pseudo_dstar_susp": 0.0009596928982725527,
            "pseudo_tarantula_susp": 0.0008539709649871904,
            "pseudo_op2_susp": 0.0009596928982725527,
            "pseudo_barinel_susp": 0.0008539709649871904
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_property#1097",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_property(self, prop, html, name=None, **kargs)",
        "snippet": "    def _og_search_property(self, prop, html, name=None, **kargs):\n        if not isinstance(prop, (list, tuple)):\n            prop = [prop]\n        if name is None:\n            name = 'OpenGraph %s' % prop[0]\n        og_regexes = []\n        for p in prop:\n            og_regexes.extend(self._og_regexes(p))\n        escaped = self._search_regex(og_regexes, html, name, flags=re.DOTALL, **kargs)\n        if escaped is None:\n            return None\n        return unescapeHTML(escaped)",
        "begin_line": 1097,
        "end_line": 1108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017699115044247787,
            "pseudo_dstar_susp": 0.0016722408026755853,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0016722408026755853,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_thumbnail#1110",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_thumbnail(self, html, **kargs)",
        "snippet": "    def _og_search_thumbnail(self, html, **kargs):\n        return self._og_search_property('image', html, 'thumbnail URL', fatal=False, **kargs)",
        "begin_line": 1110,
        "end_line": 1111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009099181073703367,
            "pseudo_dstar_susp": 0.0008431703204047217,
            "pseudo_tarantula_susp": 0.0005216484089723526,
            "pseudo_op2_susp": 0.0008431703204047217,
            "pseudo_barinel_susp": 0.0005216484089723526
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_description#1113",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_description(self, html, **kargs)",
        "snippet": "    def _og_search_description(self, html, **kargs):\n        return self._og_search_property('description', html, fatal=False, **kargs)",
        "begin_line": 1113,
        "end_line": 1114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011655011655011655,
            "pseudo_dstar_susp": 0.0010438413361169101,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0010438413361169101,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_title#1116",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_title(self, html, **kargs)",
        "snippet": "    def _og_search_title(self, html, **kargs):\n        return self._og_search_property('title', html, **kargs)",
        "begin_line": 1116,
        "end_line": 1117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016313213703099511,
            "pseudo_dstar_susp": 0.0011001100110011,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0011001100110011,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_video_url#1119",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_video_url(self, html, name='video url', secure=True, **kargs)",
        "snippet": "    def _og_search_video_url(self, html, name='video url', secure=True, **kargs):\n        regexes = self._og_regexes('video') + self._og_regexes('video:url')\n        if secure:\n            regexes = self._og_regexes('video:secure_url') + regexes\n        return self._html_search_regex(regexes, html, name, **kargs)",
        "begin_line": 1119,
        "end_line": 1123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007251631617113851,
            "pseudo_dstar_susp": 0.0007072135785007072,
            "pseudo_tarantula_susp": 0.0005115089514066496,
            "pseudo_op2_susp": 0.0007072135785007072,
            "pseudo_barinel_susp": 0.0005115089514066496
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_url#1125",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_url(self, html, **kargs)",
        "snippet": "    def _og_search_url(self, html, **kargs):\n        return self._og_search_property('url', html, **kargs)",
        "begin_line": 1125,
        "end_line": 1126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004065040650406504,
            "pseudo_dstar_susp": 0.00038819875776397513,
            "pseudo_tarantula_susp": 0.0014749262536873156,
            "pseudo_op2_susp": 0.00038819875776397513,
            "pseudo_barinel_susp": 0.0014749262536873156
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._html_search_meta#1128",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._html_search_meta(self, name, html, display_name=None, fatal=False, **kwargs)",
        "snippet": "    def _html_search_meta(self, name, html, display_name=None, fatal=False, **kwargs):\n        if not isinstance(name, (list, tuple)):\n            name = [name]\n        if display_name is None:\n            display_name = name[0]\n        return self._html_search_regex(\n            [self._meta_regex(n) for n in name],\n            html, display_name, fatal=fatal, group='content', **kwargs)",
        "begin_line": 1128,
        "end_line": 1135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010482180293501049,
            "pseudo_dstar_susp": 0.0009596928982725527,
            "pseudo_tarantula_susp": 0.0008539709649871904,
            "pseudo_op2_susp": 0.0009596928982725527,
            "pseudo_barinel_susp": 0.0008539709649871904
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._rta_search#1140",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._rta_search(self, html)",
        "snippet": "    def _rta_search(self, html):\n        # See http://www.rtalabel.org/index.php?content=howtofaq#single\n        if re.search(r'(?ix)<meta\\s+name=\"rating\"\\s+'\n                     r'     content=\"RTA-5042-1996-1400-1577-RTA\"',\n                     html):\n            return 18\n        return 0",
        "begin_line": 1140,
        "end_line": 1146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00078064012490242,
            "pseudo_dstar_susp": 0.0007936507936507937,
            "pseudo_tarantula_susp": 0.0004918839153959665,
            "pseudo_op2_susp": 0.0007936507936507937,
            "pseudo_barinel_susp": 0.0004918839153959665
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._family_friendly_search#1164",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._family_friendly_search(self, html)",
        "snippet": "    def _family_friendly_search(self, html):\n        # See http://schema.org/VideoObject\n        family_friendly = self._html_search_meta(\n            'isFamilyFriendly', html, default=None)\n\n        if not family_friendly:\n            return None\n\n        RATING_TABLE = {\n            '1': 0,\n            'true': 0,\n            '0': 18,\n            'false': 18,\n        }\n        return RATING_TABLE.get(family_friendly.lower())",
        "begin_line": 1164,
        "end_line": 1178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._search_json_ld#1184",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._search_json_ld(self, html, video_id, expected_type=None, **kwargs)",
        "snippet": "    def _search_json_ld(self, html, video_id, expected_type=None, **kwargs):\n        json_ld_list = list(re.finditer(JSON_LD_RE, html))\n        default = kwargs.get('default', NO_DEFAULT)\n        # JSON-LD may be malformed and thus `fatal` should be respected.\n        # At the same time `default` may be passed that assumes `fatal=False`\n        # for _search_regex. Let's simulate the same behavior here as well.\n        fatal = kwargs.get('fatal', True) if default == NO_DEFAULT else False\n        json_ld = []\n        for mobj in json_ld_list:\n            json_ld_item = self._parse_json(\n                mobj.group('json_ld'), video_id, fatal=fatal)\n            if not json_ld_item:\n                continue\n            if isinstance(json_ld_item, dict):\n                json_ld.append(json_ld_item)\n            elif isinstance(json_ld_item, (list, tuple)):\n                json_ld.extend(json_ld_item)\n        if json_ld:\n            json_ld = self._json_ld(json_ld, video_id, fatal=fatal, expected_type=expected_type)\n        if json_ld:\n            return json_ld\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            raise RegexNotFoundError('Unable to extract JSON-LD')\n        else:\n            self._downloader.report_warning('unable to extract JSON-LD %s' % bug_reports_message())\n            return {}",
        "begin_line": 1184,
        "end_line": 1211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006613756613756613,
            "pseudo_dstar_susp": 0.00066711140760507,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00066711140760507,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._json_ld#1213",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._json_ld(self, json_ld, video_id, fatal=True, expected_type=None)",
        "snippet": "    def _json_ld(self, json_ld, video_id, fatal=True, expected_type=None):\n        if isinstance(json_ld, compat_str):\n            json_ld = self._parse_json(json_ld, video_id, fatal=fatal)\n        if not json_ld:\n            return {}\n        info = {}\n        if not isinstance(json_ld, (list, tuple, dict)):\n            return info\n        if isinstance(json_ld, dict):\n            json_ld = [json_ld]\n\n        INTERACTION_TYPE_MAP = {\n            'CommentAction': 'comment',\n            'AgreeAction': 'like',\n            'DisagreeAction': 'dislike',\n            'LikeAction': 'like',\n            'DislikeAction': 'dislike',\n            'ListenAction': 'view',\n            'WatchAction': 'view',\n            'ViewAction': 'view',\n        }\n\n        def extract_interaction_statistic(e):\n            interaction_statistic = e.get('interactionStatistic')\n            if not isinstance(interaction_statistic, list):\n                return\n            for is_e in interaction_statistic:\n                if not isinstance(is_e, dict):\n                    continue\n                if is_e.get('@type') != 'InteractionCounter':\n                    continue\n                interaction_type = is_e.get('interactionType')\n                if not isinstance(interaction_type, compat_str):\n                    continue\n                interaction_count = int_or_none(is_e.get('userInteractionCount'))\n                if interaction_count is None:\n                    continue\n                count_kind = INTERACTION_TYPE_MAP.get(interaction_type.split('/')[-1])\n                if not count_kind:\n                    continue\n                count_key = '%s_count' % count_kind\n                if info.get(count_key) is not None:\n                    continue\n                info[count_key] = interaction_count\n\n        def extract_video_object(e):\n            assert e['@type'] == 'VideoObject'\n            info.update({\n                'url': url_or_none(e.get('contentUrl')),\n                'title': unescapeHTML(e.get('name')),\n                'description': unescapeHTML(e.get('description')),\n                'thumbnail': url_or_none(e.get('thumbnailUrl') or e.get('thumbnailURL')),\n                'duration': parse_duration(e.get('duration')),\n                'timestamp': unified_timestamp(e.get('uploadDate')),\n                'filesize': float_or_none(e.get('contentSize')),\n                'tbr': int_or_none(e.get('bitrate')),\n                'width': int_or_none(e.get('width')),\n                'height': int_or_none(e.get('height')),\n                'view_count': int_or_none(e.get('interactionCount')),\n            })\n            extract_interaction_statistic(e)\n\n        for e in json_ld:\n            if '@context' in e:\n                item_type = e.get('@type')\n                if expected_type is not None and expected_type != item_type:\n                    continue\n                if item_type in ('TVEpisode', 'Episode'):\n                    episode_name = unescapeHTML(e.get('name'))\n                    info.update({\n                        'episode': episode_name,\n                        'episode_number': int_or_none(e.get('episodeNumber')),\n                        'description': unescapeHTML(e.get('description')),\n                    })\n                    if not info.get('title') and episode_name:\n                        info['title'] = episode_name\n                    part_of_season = e.get('partOfSeason')\n                    if isinstance(part_of_season, dict) and part_of_season.get('@type') in ('TVSeason', 'Season', 'CreativeWorkSeason'):\n                        info.update({\n                            'season': unescapeHTML(part_of_season.get('name')),\n                            'season_number': int_or_none(part_of_season.get('seasonNumber')),\n                        })\n                    part_of_series = e.get('partOfSeries') or e.get('partOfTVSeries')\n                    if isinstance(part_of_series, dict) and part_of_series.get('@type') in ('TVSeries', 'Series', 'CreativeWorkSeries'):\n                        info['series'] = unescapeHTML(part_of_series.get('name'))\n                elif item_type == 'Movie':\n                    info.update({\n                        'title': unescapeHTML(e.get('name')),\n                        'description': unescapeHTML(e.get('description')),\n                        'duration': parse_duration(e.get('duration')),\n                        'timestamp': unified_timestamp(e.get('dateCreated')),\n                    })\n                elif item_type in ('Article', 'NewsArticle'):\n                    info.update({\n                        'timestamp': parse_iso8601(e.get('datePublished')),\n                        'title': unescapeHTML(e.get('headline')),\n                        'description': unescapeHTML(e.get('articleBody')),\n                    })\n                elif item_type == 'VideoObject':\n                    extract_video_object(e)\n                    if expected_type is None:\n                        continue\n                    else:\n                        break\n                video = e.get('video')\n                if isinstance(video, dict) and video.get('@type') == 'VideoObject':\n                    extract_video_object(video)\n                if expected_type is None:\n                    continue\n                else:\n                    break\n        return dict((k, v) for k, v in info.items() if v is not None)",
        "begin_line": 1213,
        "end_line": 1324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005630630630630631,
            "pseudo_dstar_susp": 0.0005733944954128441,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005733944954128441,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_interaction_statistic#1235",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_interaction_statistic(e)",
        "snippet": "        def extract_interaction_statistic(e):\n            interaction_statistic = e.get('interactionStatistic')\n            if not isinstance(interaction_statistic, list):\n                return\n            for is_e in interaction_statistic:\n                if not isinstance(is_e, dict):\n                    continue\n                if is_e.get('@type') != 'InteractionCounter':\n                    continue\n                interaction_type = is_e.get('interactionType')\n                if not isinstance(interaction_type, compat_str):\n                    continue\n                interaction_count = int_or_none(is_e.get('userInteractionCount'))\n                if interaction_count is None:\n                    continue\n                count_kind = INTERACTION_TYPE_MAP.get(interaction_type.split('/')[-1])\n                if not count_kind:\n                    continue\n                count_key = '%s_count' % count_kind\n                if info.get(count_key) is not None:\n                    continue\n                info[count_key] = interaction_count",
        "begin_line": 1235,
        "end_line": 1256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005630630630630631,
            "pseudo_dstar_susp": 0.0005733944954128441,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005733944954128441,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_video_object#1258",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_video_object(e)",
        "snippet": "        def extract_video_object(e):\n            assert e['@type'] == 'VideoObject'\n            info.update({\n                'url': url_or_none(e.get('contentUrl')),\n                'title': unescapeHTML(e.get('name')),\n                'description': unescapeHTML(e.get('description')),\n                'thumbnail': url_or_none(e.get('thumbnailUrl') or e.get('thumbnailURL')),\n                'duration': parse_duration(e.get('duration')),\n                'timestamp': unified_timestamp(e.get('uploadDate')),\n                'filesize': float_or_none(e.get('contentSize')),\n                'tbr': int_or_none(e.get('bitrate')),\n                'width': int_or_none(e.get('width')),\n                'height': int_or_none(e.get('height')),\n                'view_count': int_or_none(e.get('interactionCount')),\n            })\n            extract_interaction_statistic(e)",
        "begin_line": 1258,
        "end_line": 1273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005630630630630631,
            "pseudo_dstar_susp": 0.0005733944954128441,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0005733944954128441,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._hidden_inputs#1327",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._hidden_inputs(html)",
        "snippet": "    def _hidden_inputs(html):\n        html = re.sub(r'<!--(?:(?!<!--).)*-->', '', html)\n        hidden_inputs = {}\n        for input in re.findall(r'(?i)(<input[^>]+>)', html):\n            attrs = extract_attributes(input)\n            if not input:\n                continue\n            if attrs.get('type') not in ('hidden', 'submit'):\n                continue\n            name = attrs.get('name') or attrs.get('id')\n            value = attrs.get('value')\n            if name and value is not None:\n                hidden_inputs[name] = value\n        return hidden_inputs",
        "begin_line": 1327,
        "end_line": 1340,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000544069640914037,
            "pseudo_dstar_susp": 0.000484027105517909,
            "pseudo_tarantula_susp": 0.003125,
            "pseudo_op2_susp": 0.000484027105517909,
            "pseudo_barinel_susp": 0.003125
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._sort_formats#1348",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._sort_formats(self, formats, field_preference=None)",
        "snippet": "    def _sort_formats(self, formats, field_preference=None):\n        if not formats:\n            raise ExtractorError('No video formats found')\n\n        for f in formats:\n            # Automatically determine tbr when missing based on abr and vbr (improves\n            # formats sorting in some cases)\n            if 'tbr' not in f and f.get('abr') is not None and f.get('vbr') is not None:\n                f['tbr'] = f['abr'] + f['vbr']\n\n        def _formats_key(f):\n            # TODO remove the following workaround\n            from ..utils import determine_ext\n            if not f.get('ext') and 'url' in f:\n                f['ext'] = determine_ext(f['url'])\n\n            if isinstance(field_preference, (list, tuple)):\n                return tuple(\n                    f.get(field)\n                    if f.get(field) is not None\n                    else ('' if field == 'format_id' else -1)\n                    for field in field_preference)\n\n            preference = f.get('preference')\n            if preference is None:\n                preference = 0\n                if f.get('ext') in ['f4f', 'f4m']:  # Not yet supported\n                    preference -= 0.5\n\n            protocol = f.get('protocol') or determine_protocol(f)\n            proto_preference = 0 if protocol in ['http', 'https'] else (-0.5 if protocol == 'rtsp' else -0.1)\n\n            if f.get('vcodec') == 'none':  # audio only\n                preference -= 50\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['aac', 'mp3', 'm4a', 'webm', 'ogg', 'opus']\n                else:\n                    ORDER = ['webm', 'opus', 'ogg', 'mp3', 'aac', 'm4a']\n                ext_preference = 0\n                try:\n                    audio_ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    audio_ext_preference = -1\n            else:\n                if f.get('acodec') == 'none':  # video only\n                    preference -= 40\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['flv', 'mp4', 'webm']\n                else:\n                    ORDER = ['webm', 'flv', 'mp4']\n                try:\n                    ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    ext_preference = -1\n                audio_ext_preference = 0\n\n            return (\n                preference,\n                f.get('language_preference') if f.get('language_preference') is not None else -1,\n                f.get('quality') if f.get('quality') is not None else -1,\n                f.get('tbr') if f.get('tbr') is not None else -1,\n                f.get('filesize') if f.get('filesize') is not None else -1,\n                f.get('vbr') if f.get('vbr') is not None else -1,\n                f.get('height') if f.get('height') is not None else -1,\n                f.get('width') if f.get('width') is not None else -1,\n                proto_preference,\n                ext_preference,\n                f.get('abr') if f.get('abr') is not None else -1,\n                audio_ext_preference,\n                f.get('fps') if f.get('fps') is not None else -1,\n                f.get('filesize_approx') if f.get('filesize_approx') is not None else -1,\n                f.get('source_preference') if f.get('source_preference') is not None else -1,\n                f.get('format_id') if f.get('format_id') is not None else '',\n            )\n        formats.sort(key=_formats_key)",
        "begin_line": 1348,
        "end_line": 1422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001201923076923077,
            "pseudo_dstar_susp": 0.0011695906432748538,
            "pseudo_tarantula_susp": 0.001834862385321101,
            "pseudo_op2_susp": 0.0011695906432748538,
            "pseudo_barinel_susp": 0.001834862385321101
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._formats_key#1358",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._formats_key(f)",
        "snippet": "        def _formats_key(f):\n            # TODO remove the following workaround\n            from ..utils import determine_ext\n            if not f.get('ext') and 'url' in f:\n                f['ext'] = determine_ext(f['url'])\n\n            if isinstance(field_preference, (list, tuple)):\n                return tuple(\n                    f.get(field)\n                    if f.get(field) is not None\n                    else ('' if field == 'format_id' else -1)\n                    for field in field_preference)\n\n            preference = f.get('preference')\n            if preference is None:\n                preference = 0\n                if f.get('ext') in ['f4f', 'f4m']:  # Not yet supported\n                    preference -= 0.5\n\n            protocol = f.get('protocol') or determine_protocol(f)\n            proto_preference = 0 if protocol in ['http', 'https'] else (-0.5 if protocol == 'rtsp' else -0.1)\n\n            if f.get('vcodec') == 'none':  # audio only\n                preference -= 50\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['aac', 'mp3', 'm4a', 'webm', 'ogg', 'opus']\n                else:\n                    ORDER = ['webm', 'opus', 'ogg', 'mp3', 'aac', 'm4a']\n                ext_preference = 0\n                try:\n                    audio_ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    audio_ext_preference = -1\n            else:\n                if f.get('acodec') == 'none':  # video only\n                    preference -= 40\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['flv', 'mp4', 'webm']\n                else:\n                    ORDER = ['webm', 'flv', 'mp4']\n                try:\n                    ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    ext_preference = -1\n                audio_ext_preference = 0\n\n            return (\n                preference,\n                f.get('language_preference') if f.get('language_preference') is not None else -1,\n                f.get('quality') if f.get('quality') is not None else -1,\n                f.get('tbr') if f.get('tbr') is not None else -1,\n                f.get('filesize') if f.get('filesize') is not None else -1,\n                f.get('vbr') if f.get('vbr') is not None else -1,\n                f.get('height') if f.get('height') is not None else -1,\n                f.get('width') if f.get('width') is not None else -1,\n                proto_preference,\n                ext_preference,\n                f.get('abr') if f.get('abr') is not None else -1,\n                audio_ext_preference,\n                f.get('fps') if f.get('fps') is not None else -1,\n                f.get('filesize_approx') if f.get('filesize_approx') is not None else -1,\n                f.get('source_preference') if f.get('source_preference') is not None else -1,\n                f.get('format_id') if f.get('format_id') is not None else '',\n            )",
        "begin_line": 1358,
        "end_line": 1421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010604453870625664,
            "pseudo_dstar_susp": 0.0011273957158962795,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0011273957158962795,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._check_formats#1424",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._check_formats(self, formats, video_id)",
        "snippet": "    def _check_formats(self, formats, video_id):\n        if formats:\n            formats[:] = filter(\n                lambda f: self._is_valid_url(\n                    f['url'], video_id,\n                    item='%s video format' % f.get('format_id') if f.get('format_id') else 'video'),\n                formats)",
        "begin_line": 1424,
        "end_line": 1430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00030039050765995795,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00030039050765995795,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._remove_duplicate_formats#1433",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._remove_duplicate_formats(formats)",
        "snippet": "    def _remove_duplicate_formats(formats):\n        format_urls = set()\n        unique_formats = []\n        for f in formats:\n            if f['url'] not in format_urls:\n                format_urls.add(f['url'])\n                unique_formats.append(f)\n        formats[:] = unique_formats",
        "begin_line": 1433,
        "end_line": 1440,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._is_valid_url#1442",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._is_valid_url(self, url, video_id, item='video', headers={})",
        "snippet": "    def _is_valid_url(self, url, video_id, item='video', headers={}):\n        url = self._proto_relative_url(url, scheme='http:')\n        # For now assume non HTTP(S) URLs always valid\n        if not (url.startswith('http://') or url.startswith('https://')):\n            return True\n        try:\n            self._request_webpage(url, video_id, 'Checking %s URL' % item, headers=headers)\n            return True\n        except ExtractorError:\n            self.to_screen(\n                '%s: %s URL is invalid, skipping' % (video_id, item))\n            return False",
        "begin_line": 1442,
        "end_line": 1453,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004675081813931744,
            "pseudo_dstar_susp": 0.00051440329218107,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00051440329218107,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._proto_relative_url#1462",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._proto_relative_url(self, url, scheme=None)",
        "snippet": "    def _proto_relative_url(self, url, scheme=None):\n        if url is None:\n            return url\n        if url.startswith('//'):\n            if scheme is None:\n                scheme = self.http_scheme()\n            return scheme + url\n        else:\n            return url",
        "begin_line": 1462,
        "end_line": 1470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006493506493506494,
            "pseudo_dstar_susp": 0.000691085003455425,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.000691085003455425,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._sleep#1472",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._sleep(self, timeout, video_id, msg_template=None)",
        "snippet": "    def _sleep(self, timeout, video_id, msg_template=None):\n        if msg_template is None:\n            msg_template = '%(video_id)s: Waiting for %(timeout)s seconds'\n        msg = msg_template % {'video_id': video_id, 'timeout': timeout}\n        self.to_screen(msg)\n        time.sleep(timeout)",
        "begin_line": 1472,
        "end_line": 1477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_f4m_formats#1479",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_f4m_formats(self, manifest_url, video_id, preference=None, f4m_id=None, transform_source=lambda s: fix_xml_ampersands(s).strip(), fatal=True, m3u8_id=None, data=None, headers={}, query={})",
        "snippet": "    def _extract_f4m_formats(self, manifest_url, video_id, preference=None, f4m_id=None,\n                             transform_source=lambda s: fix_xml_ampersands(s).strip(),\n                             fatal=True, m3u8_id=None, data=None, headers={}, query={}):\n        manifest = self._download_xml(\n            manifest_url, video_id, 'Downloading f4m manifest',\n            'Unable to download f4m manifest',\n            # Some manifests may be malformed, e.g. prosiebensat1 generated manifests\n            # (see https://github.com/ytdl-org/youtube-dl/issues/6215#issuecomment-121704244)\n            transform_source=transform_source,\n            fatal=fatal, data=data, headers=headers, query=query)\n\n        if manifest is False:\n            return []\n\n        return self._parse_f4m_formats(\n            manifest, manifest_url, video_id, preference=preference, f4m_id=f4m_id,\n            transform_source=transform_source, fatal=fatal, m3u8_id=m3u8_id)",
        "begin_line": 1479,
        "end_line": 1495,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006321112515802782,
            "pseudo_dstar_susp": 0.0005865102639296188,
            "pseudo_tarantula_susp": 0.000630119722747322,
            "pseudo_op2_susp": 0.0005865102639296188,
            "pseudo_barinel_susp": 0.000630119722747322
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_f4m_formats#1497",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_f4m_formats(self, manifest, manifest_url, video_id, preference=None, f4m_id=None, transform_source=lambda s: fix_xml_ampersands(s).strip(), fatal=True, m3u8_id=None)",
        "snippet": "    def _parse_f4m_formats(self, manifest, manifest_url, video_id, preference=None, f4m_id=None,\n                           transform_source=lambda s: fix_xml_ampersands(s).strip(),\n                           fatal=True, m3u8_id=None):\n        if not isinstance(manifest, compat_etree_Element) and not fatal:\n            return []\n\n        # currently youtube-dl cannot decode the playerVerificationChallenge as Akamai uses Adobe Alchemy\n        akamai_pv = manifest.find('{http://ns.adobe.com/f4m/1.0}pv-2.0')\n        if akamai_pv is not None and ';' in akamai_pv.text:\n            playerVerificationChallenge = akamai_pv.text.split(';')[0]\n            if playerVerificationChallenge.strip() != '':\n                return []\n\n        formats = []\n        manifest_version = '1.0'\n        media_nodes = manifest.findall('{http://ns.adobe.com/f4m/1.0}media')\n        if not media_nodes:\n            manifest_version = '2.0'\n            media_nodes = manifest.findall('{http://ns.adobe.com/f4m/2.0}media')\n        # Remove unsupported DRM protected media from final formats\n        # rendition (see https://github.com/ytdl-org/youtube-dl/issues/8573).\n        media_nodes = remove_encrypted_media(media_nodes)\n        if not media_nodes:\n            return formats\n\n        manifest_base_url = get_base_url(manifest)\n\n        bootstrap_info = xpath_element(\n            manifest, ['{http://ns.adobe.com/f4m/1.0}bootstrapInfo', '{http://ns.adobe.com/f4m/2.0}bootstrapInfo'],\n            'bootstrap info', default=None)\n\n        vcodec = None\n        mime_type = xpath_text(\n            manifest, ['{http://ns.adobe.com/f4m/1.0}mimeType', '{http://ns.adobe.com/f4m/2.0}mimeType'],\n            'base URL', default=None)\n        if mime_type and mime_type.startswith('audio/'):\n            vcodec = 'none'\n\n        for i, media_el in enumerate(media_nodes):\n            tbr = int_or_none(media_el.attrib.get('bitrate'))\n            width = int_or_none(media_el.attrib.get('width'))\n            height = int_or_none(media_el.attrib.get('height'))\n            format_id = '-'.join(filter(None, [f4m_id, compat_str(i if tbr is None else tbr)]))\n            # If <bootstrapInfo> is present, the specified f4m is a\n            # stream-level manifest, and only set-level manifests may refer to\n            # external resources.  See section 11.4 and section 4 of F4M spec\n            if bootstrap_info is None:\n                media_url = None\n                # @href is introduced in 2.0, see section 11.6 of F4M spec\n                if manifest_version == '2.0':\n                    media_url = media_el.attrib.get('href')\n                if media_url is None:\n                    media_url = media_el.attrib.get('url')\n                if not media_url:\n                    continue\n                manifest_url = (\n                    media_url if media_url.startswith('http://') or media_url.startswith('https://')\n                    else ((manifest_base_url or '/'.join(manifest_url.split('/')[:-1])) + '/' + media_url))\n                # If media_url is itself a f4m manifest do the recursive extraction\n                # since bitrates in parent manifest (this one) and media_url manifest\n                # may differ leading to inability to resolve the format by requested\n                # bitrate in f4m downloader\n                ext = determine_ext(manifest_url)\n                if ext == 'f4m':\n                    f4m_formats = self._extract_f4m_formats(\n                        manifest_url, video_id, preference=preference, f4m_id=f4m_id,\n                        transform_source=transform_source, fatal=fatal)\n                    # Sometimes stream-level manifest contains single media entry that\n                    # does not contain any quality metadata (e.g. http://matchtv.ru/#live-player).\n                    # At the same time parent's media entry in set-level manifest may\n                    # contain it. We will copy it from parent in such cases.\n                    if len(f4m_formats) == 1:\n                        f = f4m_formats[0]\n                        f.update({\n                            'tbr': f.get('tbr') or tbr,\n                            'width': f.get('width') or width,\n                            'height': f.get('height') or height,\n                            'format_id': f.get('format_id') if not tbr else format_id,\n                            'vcodec': vcodec,\n                        })\n                    formats.extend(f4m_formats)\n                    continue\n                elif ext == 'm3u8':\n                    formats.extend(self._extract_m3u8_formats(\n                        manifest_url, video_id, 'mp4', preference=preference,\n                        m3u8_id=m3u8_id, fatal=fatal))\n                    continue\n            formats.append({\n                'format_id': format_id,\n                'url': manifest_url,\n                'manifest_url': manifest_url,\n                'ext': 'flv' if bootstrap_info is not None else None,\n                'protocol': 'f4m',\n                'tbr': tbr,\n                'width': width,\n                'height': height,\n                'vcodec': vcodec,\n                'preference': preference,\n            })\n        return formats",
        "begin_line": 1497,
        "end_line": 1596,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005178663904712584,
            "pseudo_dstar_susp": 0.0005277044854881266,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005277044854881266,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_m3u8_formats#1609",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_m3u8_formats(self, m3u8_url, video_id, ext=None, entry_protocol='m3u8', preference=None, m3u8_id=None, note=None, errnote=None, fatal=True, live=False, data=None, headers={}, query={})",
        "snippet": "    def _extract_m3u8_formats(self, m3u8_url, video_id, ext=None,\n                              entry_protocol='m3u8', preference=None,\n                              m3u8_id=None, note=None, errnote=None,\n                              fatal=True, live=False, data=None, headers={},\n                              query={}):\n        res = self._download_webpage_handle(\n            m3u8_url, video_id,\n            note=note or 'Downloading m3u8 information',\n            errnote=errnote or 'Failed to download m3u8 information',\n            fatal=fatal, data=data, headers=headers, query=query)\n\n        if res is False:\n            return []\n\n        m3u8_doc, urlh = res\n        m3u8_url = urlh.geturl()\n\n        return self._parse_m3u8_formats(\n            m3u8_doc, m3u8_url, ext=ext, entry_protocol=entry_protocol,\n            preference=preference, m3u8_id=m3u8_id, live=live)",
        "begin_line": 1609,
        "end_line": 1628,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009624639076034649,
            "pseudo_dstar_susp": 0.001,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_m3u8_formats#1630",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_m3u8_formats(self, m3u8_doc, m3u8_url, ext=None, entry_protocol='m3u8', preference=None, m3u8_id=None, live=False)",
        "snippet": "    def _parse_m3u8_formats(self, m3u8_doc, m3u8_url, ext=None,\n                            entry_protocol='m3u8', preference=None,\n                            m3u8_id=None, live=False):\n        if '#EXT-X-FAXS-CM:' in m3u8_doc:  # Adobe Flash Access\n            return []\n\n        if re.search(r'#EXT-X-SESSION-KEY:.*?URI=\"skd://', m3u8_doc):  # Apple FairPlay\n            return []\n\n        formats = []\n\n        format_url = lambda u: (\n            u\n            if re.match(r'^https?://', u)\n            else compat_urlparse.urljoin(m3u8_url, u))\n\n        # References:\n        # 1. https://tools.ietf.org/html/draft-pantos-http-live-streaming-21\n        # 2. https://github.com/ytdl-org/youtube-dl/issues/12211\n        # 3. https://github.com/ytdl-org/youtube-dl/issues/18923\n\n        # We should try extracting formats only from master playlists [1, 4.3.4],\n        # i.e. playlists that describe available qualities. On the other hand\n        # media playlists [1, 4.3.3] should be returned as is since they contain\n        # just the media without qualities renditions.\n        # Fortunately, master playlist can be easily distinguished from media\n        # playlist based on particular tags availability. As of [1, 4.3.3, 4.3.4]\n        # master playlist tags MUST NOT appear in a media playist and vice versa.\n        # As of [1, 4.3.3.1] #EXT-X-TARGETDURATION tag is REQUIRED for every\n        # media playlist and MUST NOT appear in master playlist thus we can\n        # clearly detect media playlist with this criterion.\n\n        if '#EXT-X-TARGETDURATION' in m3u8_doc:  # media playlist, return as is\n            return [{\n                'url': m3u8_url,\n                'format_id': m3u8_id,\n                'ext': ext,\n                'protocol': entry_protocol,\n                'preference': preference,\n            }]\n\n        groups = {}\n        last_stream_inf = {}\n\n        def extract_media(x_media_line):\n            media = parse_m3u8_attributes(x_media_line)\n            # As per [1, 4.3.4.1] TYPE, GROUP-ID and NAME are REQUIRED\n            media_type, group_id, name = media.get('TYPE'), media.get('GROUP-ID'), media.get('NAME')\n            if not (media_type and group_id and name):\n                return\n            groups.setdefault(group_id, []).append(media)\n            if media_type not in ('VIDEO', 'AUDIO'):\n                return\n            media_url = media.get('URI')\n            if media_url:\n                format_id = []\n                for v in (m3u8_id, group_id, name):\n                    if v:\n                        format_id.append(v)\n                f = {\n                    'format_id': '-'.join(format_id),\n                    'url': format_url(media_url),\n                    'manifest_url': m3u8_url,\n                    'language': media.get('LANGUAGE'),\n                    'ext': ext,\n                    'protocol': entry_protocol,\n                    'preference': preference,\n                }\n                if media_type == 'AUDIO':\n                    f['vcodec'] = 'none'\n                formats.append(f)\n\n        def build_stream_name():\n            # Despite specification does not mention NAME attribute for\n            # EXT-X-STREAM-INF tag it still sometimes may be present (see [1]\n            # or vidio test in TestInfoExtractor.test_parse_m3u8_formats)\n            # 1. http://www.vidio.com/watch/165683-dj_ambred-booyah-live-2015\n            stream_name = last_stream_inf.get('NAME')\n            if stream_name:\n                return stream_name\n            # If there is no NAME in EXT-X-STREAM-INF it will be obtained\n            # from corresponding rendition group\n            stream_group_id = last_stream_inf.get('VIDEO')\n            if not stream_group_id:\n                return\n            stream_group = groups.get(stream_group_id)\n            if not stream_group:\n                return stream_group_id\n            rendition = stream_group[0]\n            return rendition.get('NAME') or stream_group_id\n\n        # parse EXT-X-MEDIA tags before EXT-X-STREAM-INF in order to have the\n        # chance to detect video only formats when EXT-X-STREAM-INF tags\n        # precede EXT-X-MEDIA tags in HLS manifest such as [3].\n        for line in m3u8_doc.splitlines():\n            if line.startswith('#EXT-X-MEDIA:'):\n                extract_media(line)\n\n        for line in m3u8_doc.splitlines():\n            if line.startswith('#EXT-X-STREAM-INF:'):\n                last_stream_inf = parse_m3u8_attributes(line)\n            elif line.startswith('#') or not line.strip():\n                continue\n            else:\n                tbr = float_or_none(\n                    last_stream_inf.get('AVERAGE-BANDWIDTH')\n                    or last_stream_inf.get('BANDWIDTH'), scale=1000)\n                format_id = []\n                if m3u8_id:\n                    format_id.append(m3u8_id)\n                stream_name = build_stream_name()\n                # Bandwidth of live streams may differ over time thus making\n                # format_id unpredictable. So it's better to keep provided\n                # format_id intact.\n                if not live:\n                    format_id.append(stream_name if stream_name else '%d' % (tbr if tbr else len(formats)))\n                manifest_url = format_url(line.strip())\n                f = {\n                    'format_id': '-'.join(format_id),\n                    'url': manifest_url,\n                    'manifest_url': m3u8_url,\n                    'tbr': tbr,\n                    'ext': ext,\n                    'fps': float_or_none(last_stream_inf.get('FRAME-RATE')),\n                    'protocol': entry_protocol,\n                    'preference': preference,\n                }\n                resolution = last_stream_inf.get('RESOLUTION')\n                if resolution:\n                    mobj = re.search(r'(?P<width>\\d+)[xX](?P<height>\\d+)', resolution)\n                    if mobj:\n                        f['width'] = int(mobj.group('width'))\n                        f['height'] = int(mobj.group('height'))\n                # Unified Streaming Platform\n                mobj = re.search(\n                    r'audio.*?(?:%3D|=)(\\d+)(?:-video.*?(?:%3D|=)(\\d+))?', f['url'])\n                if mobj:\n                    abr, vbr = mobj.groups()\n                    abr, vbr = float_or_none(abr, 1000), float_or_none(vbr, 1000)\n                    f.update({\n                        'vbr': vbr,\n                        'abr': abr,\n                    })\n                codecs = parse_codecs(last_stream_inf.get('CODECS'))\n                f.update(codecs)\n                audio_group_id = last_stream_inf.get('AUDIO')\n                # As per [1, 4.3.4.1.1] any EXT-X-STREAM-INF tag which\n                # references a rendition group MUST have a CODECS attribute.\n                # However, this is not always respected, for example, [2]\n                # contains EXT-X-STREAM-INF tag which references AUDIO\n                # rendition group but does not have CODECS and despite\n                # referencing an audio group it represents a complete\n                # (with audio and video) format. So, for such cases we will\n                # ignore references to rendition groups and treat them\n                # as complete formats.\n                if audio_group_id and codecs and f.get('vcodec') != 'none':\n                    audio_group = groups.get(audio_group_id)\n                    if audio_group and audio_group[0].get('URI'):\n                        # TODO: update acodec for audio only formats with\n                        # the same GROUP-ID\n                        f['acodec'] = 'none'\n                formats.append(f)\n\n                # for DailyMotion\n                progressive_uri = last_stream_inf.get('PROGRESSIVE-URI')\n                if progressive_uri:\n                    http_f = f.copy()\n                    del http_f['manifest_url']\n                    http_f.update({\n                        'format_id': f['format_id'].replace('hls-', 'http-'),\n                        'protocol': 'http',\n                        'url': progressive_uri,\n                    })\n                    formats.append(http_f)\n\n                last_stream_inf = {}\n        return formats",
        "begin_line": 1630,
        "end_line": 1806,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009033423667570009,
            "pseudo_dstar_susp": 0.0009442870632672333,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0009442870632672333,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_media#1674",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_media(x_media_line)",
        "snippet": "        def extract_media(x_media_line):\n            media = parse_m3u8_attributes(x_media_line)\n            # As per [1, 4.3.4.1] TYPE, GROUP-ID and NAME are REQUIRED\n            media_type, group_id, name = media.get('TYPE'), media.get('GROUP-ID'), media.get('NAME')\n            if not (media_type and group_id and name):\n                return\n            groups.setdefault(group_id, []).append(media)\n            if media_type not in ('VIDEO', 'AUDIO'):\n                return\n            media_url = media.get('URI')\n            if media_url:\n                format_id = []\n                for v in (m3u8_id, group_id, name):\n                    if v:\n                        format_id.append(v)\n                f = {\n                    'format_id': '-'.join(format_id),\n                    'url': format_url(media_url),\n                    'manifest_url': m3u8_url,\n                    'language': media.get('LANGUAGE'),\n                    'ext': ext,\n                    'protocol': entry_protocol,\n                    'preference': preference,\n                }\n                if media_type == 'AUDIO':\n                    f['vcodec'] = 'none'\n                formats.append(f)",
        "begin_line": 1674,
        "end_line": 1700,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008764241893076249,
            "pseudo_dstar_susp": 0.0009216589861751152,
            "pseudo_tarantula_susp": 0.00048520135856380397,
            "pseudo_op2_susp": 0.0009216589861751152,
            "pseudo_barinel_susp": 0.00048520135856380397
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.build_stream_name#1702",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.build_stream_name()",
        "snippet": "        def build_stream_name():\n            # Despite specification does not mention NAME attribute for\n            # EXT-X-STREAM-INF tag it still sometimes may be present (see [1]\n            # or vidio test in TestInfoExtractor.test_parse_m3u8_formats)\n            # 1. http://www.vidio.com/watch/165683-dj_ambred-booyah-live-2015\n            stream_name = last_stream_inf.get('NAME')\n            if stream_name:\n                return stream_name\n            # If there is no NAME in EXT-X-STREAM-INF it will be obtained\n            # from corresponding rendition group\n            stream_group_id = last_stream_inf.get('VIDEO')\n            if not stream_group_id:\n                return\n            stream_group = groups.get(stream_group_id)\n            if not stream_group:\n                return stream_group_id\n            rendition = stream_group[0]\n            return rendition.get('NAME') or stream_group_id",
        "begin_line": 1702,
        "end_line": 1719,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008936550491510277,
            "pseudo_dstar_susp": 0.0009372071227741331,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0009372071227741331,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._xpath_ns#1809",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._xpath_ns(path, namespace=None)",
        "snippet": "    def _xpath_ns(path, namespace=None):\n        if not namespace:\n            return path\n        out = []\n        for c in path.split('/'):\n            if not c or c == '.':\n                out.append(c)\n            else:\n                out.append('{%s}%s' % (namespace, c))\n        return '/'.join(out)",
        "begin_line": 1809,
        "end_line": 1818,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005399568034557236,
            "pseudo_dstar_susp": 0.0005672149744753262,
            "pseudo_tarantula_susp": 0.0002975304968759298,
            "pseudo_op2_susp": 0.0005672149744753262,
            "pseudo_barinel_susp": 0.0002975304968759298
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_smil_formats#1820",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_smil_formats(self, smil_url, video_id, fatal=True, f4m_params=None, transform_source=None)",
        "snippet": "    def _extract_smil_formats(self, smil_url, video_id, fatal=True, f4m_params=None, transform_source=None):\n        smil = self._download_smil(smil_url, video_id, fatal=fatal, transform_source=transform_source)\n\n        if smil is False:\n            assert not fatal\n            return []\n\n        namespace = self._parse_smil_namespace(smil)\n\n        return self._parse_smil_formats(\n            smil, smil_url, video_id, namespace=namespace, f4m_params=f4m_params)",
        "begin_line": 1820,
        "end_line": 1830,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003109452736318408,
            "pseudo_dstar_susp": 0.00036088054853843375,
            "pseudo_tarantula_susp": 0.0002886002886002886,
            "pseudo_op2_susp": 0.00036088054853843375,
            "pseudo_barinel_susp": 0.0002886002886002886
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_smil_info#1832",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_smil_info(self, smil_url, video_id, fatal=True, f4m_params=None)",
        "snippet": "    def _extract_smil_info(self, smil_url, video_id, fatal=True, f4m_params=None):\n        smil = self._download_smil(smil_url, video_id, fatal=fatal)\n        if smil is False:\n            return {}\n        return self._parse_smil(smil, smil_url, video_id, f4m_params=f4m_params)",
        "begin_line": 1832,
        "end_line": 1836,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_smil#1838",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_smil(self, smil_url, video_id, fatal=True, transform_source=None)",
        "snippet": "    def _download_smil(self, smil_url, video_id, fatal=True, transform_source=None):\n        return self._download_xml(\n            smil_url, video_id, 'Downloading SMIL file',\n            'Unable to download SMIL file', fatal=fatal, transform_source=transform_source)",
        "begin_line": 1838,
        "end_line": 1841,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003076923076923077,
            "pseudo_dstar_susp": 0.0003599712023038157,
            "pseudo_tarantula_susp": 0.0002794076557697681,
            "pseudo_op2_susp": 0.0003599712023038157,
            "pseudo_barinel_susp": 0.0002794076557697681
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_smil#1843",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_smil(self, smil, smil_url, video_id, f4m_params=None)",
        "snippet": "    def _parse_smil(self, smil, smil_url, video_id, f4m_params=None):\n        namespace = self._parse_smil_namespace(smil)\n\n        formats = self._parse_smil_formats(\n            smil, smil_url, video_id, namespace=namespace, f4m_params=f4m_params)\n        subtitles = self._parse_smil_subtitles(smil, namespace=namespace)\n\n        video_id = os.path.splitext(url_basename(smil_url))[0]\n        title = None\n        description = None\n        upload_date = None\n        for meta in smil.findall(self._xpath_ns('./head/meta', namespace)):\n            name = meta.attrib.get('name')\n            content = meta.attrib.get('content')\n            if not name or not content:\n                continue\n            if not title and name == 'title':\n                title = content\n            elif not description and name in ('description', 'abstract'):\n                description = content\n            elif not upload_date and name == 'date':\n                upload_date = unified_strdate(content)\n\n        thumbnails = [{\n            'id': image.get('type'),\n            'url': image.get('src'),\n            'width': int_or_none(image.get('width')),\n            'height': int_or_none(image.get('height')),\n        } for image in smil.findall(self._xpath_ns('.//image', namespace)) if image.get('src')]\n\n        return {\n            'id': video_id,\n            'title': title or video_id,\n            'description': description,\n            'upload_date': upload_date,\n            'thumbnails': thumbnails,\n            'formats': formats,\n            'subtitles': subtitles,\n        }",
        "begin_line": 1843,
        "end_line": 1881,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_smil_namespace#1883",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_smil_namespace(self, smil)",
        "snippet": "    def _parse_smil_namespace(self, smil):\n        return self._search_regex(\n            r'(?i)^{([^}]+)?}smil$', smil.tag, 'namespace', default=None)",
        "begin_line": 1883,
        "end_line": 1885,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00027901785714285713,
            "pseudo_dstar_susp": 0.0003266906239790918,
            "pseudo_tarantula_susp": 0.0002628811777076761,
            "pseudo_op2_susp": 0.0003266906239790918,
            "pseudo_barinel_susp": 0.0002628811777076761
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_smil_formats#1887",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_smil_formats(self, smil, smil_url, video_id, namespace=None, f4m_params=None, transform_rtmp_url=None)",
        "snippet": "    def _parse_smil_formats(self, smil, smil_url, video_id, namespace=None, f4m_params=None, transform_rtmp_url=None):\n        base = smil_url\n        for meta in smil.findall(self._xpath_ns('./head/meta', namespace)):\n            b = meta.get('base') or meta.get('httpBase')\n            if b:\n                base = b\n                break\n\n        formats = []\n        rtmp_count = 0\n        http_count = 0\n        m3u8_count = 0\n\n        srcs = []\n        media = smil.findall(self._xpath_ns('.//video', namespace)) + smil.findall(self._xpath_ns('.//audio', namespace))\n        for medium in media:\n            src = medium.get('src')\n            if not src or src in srcs:\n                continue\n            srcs.append(src)\n\n            bitrate = float_or_none(medium.get('system-bitrate') or medium.get('systemBitrate'), 1000)\n            filesize = int_or_none(medium.get('size') or medium.get('fileSize'))\n            width = int_or_none(medium.get('width'))\n            height = int_or_none(medium.get('height'))\n            proto = medium.get('proto')\n            ext = medium.get('ext')\n            src_ext = determine_ext(src)\n            streamer = medium.get('streamer') or base\n\n            if proto == 'rtmp' or streamer.startswith('rtmp'):\n                rtmp_count += 1\n                formats.append({\n                    'url': streamer,\n                    'play_path': src,\n                    'ext': 'flv',\n                    'format_id': 'rtmp-%d' % (rtmp_count if bitrate is None else bitrate),\n                    'tbr': bitrate,\n                    'filesize': filesize,\n                    'width': width,\n                    'height': height,\n                })\n                if transform_rtmp_url:\n                    streamer, src = transform_rtmp_url(streamer, src)\n                    formats[-1].update({\n                        'url': streamer,\n                        'play_path': src,\n                    })\n                continue\n\n            src_url = src if src.startswith('http') else compat_urlparse.urljoin(base, src)\n            src_url = src_url.strip()\n\n            if proto == 'm3u8' or src_ext == 'm3u8':\n                m3u8_formats = self._extract_m3u8_formats(\n                    src_url, video_id, ext or 'mp4', m3u8_id='hls', fatal=False)\n                if len(m3u8_formats) == 1:\n                    m3u8_count += 1\n                    m3u8_formats[0].update({\n                        'format_id': 'hls-%d' % (m3u8_count if bitrate is None else bitrate),\n                        'tbr': bitrate,\n                        'width': width,\n                        'height': height,\n                    })\n                formats.extend(m3u8_formats)\n            elif src_ext == 'f4m':\n                f4m_url = src_url\n                if not f4m_params:\n                    f4m_params = {\n                        'hdcore': '3.2.0',\n                        'plugin': 'flowplayer-3.2.0.1',\n                    }\n                f4m_url += '&' if '?' in f4m_url else '?'\n                f4m_url += compat_urllib_parse_urlencode(f4m_params)\n                formats.extend(self._extract_f4m_formats(f4m_url, video_id, f4m_id='hds', fatal=False))\n            elif src_ext == 'mpd':\n                formats.extend(self._extract_mpd_formats(\n                    src_url, video_id, mpd_id='dash', fatal=False))\n            elif re.search(r'\\.ism/[Mm]anifest', src_url):\n                formats.extend(self._extract_ism_formats(\n                    src_url, video_id, ism_id='mss', fatal=False))\n            elif src_url.startswith('http') and self._is_valid_url(src, video_id):\n                http_count += 1\n                formats.append({\n                    'url': src_url,\n                    'ext': ext or src_ext or 'flv',\n                    'format_id': 'http-%d' % (bitrate or http_count),\n                    'tbr': bitrate,\n                    'filesize': filesize,\n                    'width': width,\n                    'height': height,\n                })\n\n        return formats",
        "begin_line": 1887,
        "end_line": 1980,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004344048653344917,
            "pseudo_dstar_susp": 0.000471253534401508,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.000471253534401508,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_smil_subtitles#1982",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_smil_subtitles(self, smil, namespace=None, subtitles_lang='en')",
        "snippet": "    def _parse_smil_subtitles(self, smil, namespace=None, subtitles_lang='en'):\n        urls = []\n        subtitles = {}\n        for num, textstream in enumerate(smil.findall(self._xpath_ns('.//textstream', namespace))):\n            src = textstream.get('src')\n            if not src or src in urls:\n                continue\n            urls.append(src)\n            ext = textstream.get('ext') or mimetype2ext(textstream.get('type')) or determine_ext(src)\n            lang = textstream.get('systemLanguage') or textstream.get('systemLanguageName') or textstream.get('lang') or subtitles_lang\n            subtitles.setdefault(lang, []).append({\n                'url': src,\n                'ext': ext,\n            })\n        return subtitles",
        "begin_line": 1982,
        "end_line": 1996,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033772374197906115,
            "pseudo_dstar_susp": 0.0003979307600477517,
            "pseudo_tarantula_susp": 0.0002840909090909091,
            "pseudo_op2_susp": 0.0003979307600477517,
            "pseudo_barinel_susp": 0.0002840909090909091
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_xspf#2008",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_xspf(self, xspf_doc, playlist_id, xspf_url=None, xspf_base_url=None)",
        "snippet": "    def _parse_xspf(self, xspf_doc, playlist_id, xspf_url=None, xspf_base_url=None):\n        NS_MAP = {\n            'xspf': 'http://xspf.org/ns/0/',\n            's1': 'http://static.streamone.nl/player/ns/0',\n        }\n\n        entries = []\n        for track in xspf_doc.findall(xpath_with_ns('./xspf:trackList/xspf:track', NS_MAP)):\n            title = xpath_text(\n                track, xpath_with_ns('./xspf:title', NS_MAP), 'title', default=playlist_id)\n            description = xpath_text(\n                track, xpath_with_ns('./xspf:annotation', NS_MAP), 'description')\n            thumbnail = xpath_text(\n                track, xpath_with_ns('./xspf:image', NS_MAP), 'thumbnail')\n            duration = float_or_none(\n                xpath_text(track, xpath_with_ns('./xspf:duration', NS_MAP), 'duration'), 1000)\n\n            formats = []\n            for location in track.findall(xpath_with_ns('./xspf:location', NS_MAP)):\n                format_url = urljoin(xspf_base_url, location.text)\n                if not format_url:\n                    continue\n                formats.append({\n                    'url': format_url,\n                    'manifest_url': xspf_url,\n                    'format_id': location.get(xpath_with_ns('s1:label', NS_MAP)),\n                    'width': int_or_none(location.get(xpath_with_ns('s1:width', NS_MAP))),\n                    'height': int_or_none(location.get(xpath_with_ns('s1:height', NS_MAP))),\n                })\n            self._sort_formats(formats)\n\n            entries.append({\n                'id': playlist_id,\n                'title': title,\n                'description': description,\n                'thumbnail': thumbnail,\n                'duration': duration,\n                'formats': formats,\n            })\n        return entries",
        "begin_line": 2008,
        "end_line": 2047,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_mpd_formats#2049",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_mpd_formats(self, mpd_url, video_id, mpd_id=None, note=None, errnote=None, fatal=True, formats_dict={}, data=None, headers={}, query={})",
        "snippet": "    def _extract_mpd_formats(self, mpd_url, video_id, mpd_id=None, note=None, errnote=None, fatal=True, formats_dict={}, data=None, headers={}, query={}):\n        res = self._download_xml_handle(\n            mpd_url, video_id,\n            note=note or 'Downloading MPD manifest',\n            errnote=errnote or 'Failed to download MPD manifest',\n            fatal=fatal, data=data, headers=headers, query=query)\n        if res is False:\n            return []\n        mpd_doc, urlh = res\n        if mpd_doc is None:\n            return []\n        mpd_base_url = base_url(urlh.geturl())\n\n        return self._parse_mpd_formats(\n            mpd_doc, mpd_id=mpd_id, mpd_base_url=mpd_base_url,\n            formats_dict=formats_dict, mpd_url=mpd_url)",
        "begin_line": 2049,
        "end_line": 2064,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005141388174807198,
            "pseudo_dstar_susp": 0.0005356186395286556,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0005356186395286556,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_mpd_formats#2066",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', formats_dict={}, mpd_url=None)",
        "snippet": "    def _parse_mpd_formats(self, mpd_doc, mpd_id=None, mpd_base_url='', formats_dict={}, mpd_url=None):\n        \"\"\"\n        Parse formats from MPD manifest.\n        References:\n         1. MPEG-DASH Standard, ISO/IEC 23009-1:2014(E),\n            http://standards.iso.org/ittf/PubliclyAvailableStandards/c065274_ISO_IEC_23009-1_2014.zip\n         2. https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP\n        \"\"\"\n        if mpd_doc.get('type') == 'dynamic':\n            return []\n\n        namespace = self._search_regex(r'(?i)^{([^}]+)?}MPD$', mpd_doc.tag, 'namespace', default=None)\n\n        def _add_ns(path):\n            return self._xpath_ns(path, namespace)\n\n        def is_drm_protected(element):\n            return element.find(_add_ns('ContentProtection')) is not None\n\n        def extract_multisegment_info(element, ms_parent_info):\n            ms_info = ms_parent_info.copy()\n\n            # As per [1, 5.3.9.2.2] SegmentList and SegmentTemplate share some\n            # common attributes and elements.  We will only extract relevant\n            # for us.\n            def extract_common(source):\n                segment_timeline = source.find(_add_ns('SegmentTimeline'))\n                if segment_timeline is not None:\n                    s_e = segment_timeline.findall(_add_ns('S'))\n                    if s_e:\n                        ms_info['total_number'] = 0\n                        ms_info['s'] = []\n                        for s in s_e:\n                            r = int(s.get('r', 0))\n                            ms_info['total_number'] += 1 + r\n                            ms_info['s'].append({\n                                't': int(s.get('t', 0)),\n                                # @d is mandatory (see [1, 5.3.9.6.2, Table 17, page 60])\n                                'd': int(s.attrib['d']),\n                                'r': r,\n                            })\n                start_number = source.get('startNumber')\n                if start_number:\n                    ms_info['start_number'] = int(start_number)\n                timescale = source.get('timescale')\n                if timescale:\n                    ms_info['timescale'] = int(timescale)\n                segment_duration = source.get('duration')\n                if segment_duration:\n                    ms_info['segment_duration'] = float(segment_duration)\n\n            def extract_Initialization(source):\n                initialization = source.find(_add_ns('Initialization'))\n                if initialization is not None:\n                    ms_info['initialization_url'] = initialization.attrib['sourceURL']\n\n            segment_list = element.find(_add_ns('SegmentList'))\n            if segment_list is not None:\n                extract_common(segment_list)\n                extract_Initialization(segment_list)\n                segment_urls_e = segment_list.findall(_add_ns('SegmentURL'))\n                if segment_urls_e:\n                    ms_info['segment_urls'] = [segment.attrib['media'] for segment in segment_urls_e]\n            else:\n                segment_template = element.find(_add_ns('SegmentTemplate'))\n                if segment_template is not None:\n                    extract_common(segment_template)\n                    media = segment_template.get('media')\n                    if media:\n                        ms_info['media'] = media\n                    initialization = segment_template.get('initialization')\n                    if initialization:\n                        ms_info['initialization'] = initialization\n                    else:\n                        extract_Initialization(segment_template)\n            return ms_info\n\n        mpd_duration = parse_duration(mpd_doc.get('mediaPresentationDuration'))\n        formats = []\n        for period in mpd_doc.findall(_add_ns('Period')):\n            period_duration = parse_duration(period.get('duration')) or mpd_duration\n            period_ms_info = extract_multisegment_info(period, {\n                'start_number': 1,\n                'timescale': 1,\n            })\n            for adaptation_set in period.findall(_add_ns('AdaptationSet')):\n                if is_drm_protected(adaptation_set):\n                    continue\n                adaption_set_ms_info = extract_multisegment_info(adaptation_set, period_ms_info)\n                for representation in adaptation_set.findall(_add_ns('Representation')):\n                    if is_drm_protected(representation):\n                        continue\n                    representation_attrib = adaptation_set.attrib.copy()\n                    representation_attrib.update(representation.attrib)\n                    # According to [1, 5.3.7.2, Table 9, page 41], @mimeType is mandatory\n                    mime_type = representation_attrib['mimeType']\n                    content_type = mime_type.split('/')[0]\n                    if content_type == 'text':\n                        # TODO implement WebVTT downloading\n                        pass\n                    elif content_type in ('video', 'audio'):\n                        base_url = ''\n                        for element in (representation, adaptation_set, period, mpd_doc):\n                            base_url_e = element.find(_add_ns('BaseURL'))\n                            if base_url_e is not None:\n                                base_url = base_url_e.text + base_url\n                                if re.match(r'^https?://', base_url):\n                                    break\n                        if mpd_base_url and not re.match(r'^https?://', base_url):\n                            if not mpd_base_url.endswith('/') and not base_url.startswith('/'):\n                                mpd_base_url += '/'\n                            base_url = mpd_base_url + base_url\n                        representation_id = representation_attrib.get('id')\n                        lang = representation_attrib.get('lang')\n                        url_el = representation.find(_add_ns('BaseURL'))\n                        filesize = int_or_none(url_el.attrib.get('{http://youtube.com/yt/2012/10/10}contentLength') if url_el is not None else None)\n                        bandwidth = int_or_none(representation_attrib.get('bandwidth'))\n                        f = {\n                            'format_id': '%s-%s' % (mpd_id, representation_id) if mpd_id else representation_id,\n                            'manifest_url': mpd_url,\n                            'ext': mimetype2ext(mime_type),\n                            'width': int_or_none(representation_attrib.get('width')),\n                            'height': int_or_none(representation_attrib.get('height')),\n                            'tbr': float_or_none(bandwidth, 1000),\n                            'asr': int_or_none(representation_attrib.get('audioSamplingRate')),\n                            'fps': int_or_none(representation_attrib.get('frameRate')),\n                            'language': lang if lang not in ('mul', 'und', 'zxx', 'mis') else None,\n                            'format_note': 'DASH %s' % content_type,\n                            'filesize': filesize,\n                            'container': mimetype2ext(mime_type) + '_dash',\n                        }\n                        f.update(parse_codecs(representation_attrib.get('codecs')))\n                        representation_ms_info = extract_multisegment_info(representation, adaption_set_ms_info)\n\n                        def prepare_template(template_name, identifiers):\n                            tmpl = representation_ms_info[template_name]\n                            # First of, % characters outside $...$ templates\n                            # must be escaped by doubling for proper processing\n                            # by % operator string formatting used further (see\n                            # https://github.com/ytdl-org/youtube-dl/issues/16867).\n                            t = ''\n                            in_template = False\n                            for c in tmpl:\n                                t += c\n                                if c == '$':\n                                    in_template = not in_template\n                                elif c == '%' and not in_template:\n                                    t += c\n                            # Next, $...$ templates are translated to their\n                            # %(...) counterparts to be used with % operator\n                            t = t.replace('$RepresentationID$', representation_id)\n                            t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n                            t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n                            t.replace('$$', '$')\n                            return t\n\n                        # @initialization is a regular template like @media one\n                        # so it should be handled just the same way (see\n                        # https://github.com/ytdl-org/youtube-dl/issues/11605)\n                        if 'initialization' in representation_ms_info:\n                            initialization_template = prepare_template(\n                                'initialization',\n                                # As per [1, 5.3.9.4.2, Table 15, page 54] $Number$ and\n                                # $Time$ shall not be included for @initialization thus\n                                # only $Bandwidth$ remains\n                                ('Bandwidth', ))\n                            representation_ms_info['initialization_url'] = initialization_template % {\n                                'Bandwidth': bandwidth,\n                            }\n\n                        def location_key(location):\n                            return 'url' if re.match(r'^https?://', location) else 'path'\n\n                        if 'segment_urls' not in representation_ms_info and 'media' in representation_ms_info:\n\n                            media_template = prepare_template('media', ('Number', 'Bandwidth', 'Time'))\n                            media_location_key = location_key(media_template)\n\n                            # As per [1, 5.3.9.4.4, Table 16, page 55] $Number$ and $Time$\n                            # can't be used at the same time\n                            if '%(Number' in media_template and 's' not in representation_ms_info:\n                                segment_duration = None\n                                if 'total_number' not in representation_ms_info and 'segment_duration' in representation_ms_info:\n                                    segment_duration = float_or_none(representation_ms_info['segment_duration'], representation_ms_info['timescale'])\n                                    representation_ms_info['total_number'] = int(math.ceil(float(period_duration) / segment_duration))\n                                representation_ms_info['fragments'] = [{\n                                    media_location_key: media_template % {\n                                        'Number': segment_number,\n                                        'Bandwidth': bandwidth,\n                                    },\n                                    'duration': segment_duration,\n                                } for segment_number in range(\n                                    representation_ms_info['start_number'],\n                                    representation_ms_info['total_number'] + representation_ms_info['start_number'])]\n                            else:\n                                # $Number*$ or $Time$ in media template with S list available\n                                # Example $Number*$: http://www.svtplay.se/klipp/9023742/stopptid-om-bjorn-borg\n                                # Example $Time$: https://play.arkena.com/embed/avp/v2/player/media/b41dda37-d8e7-4d3f-b1b5-9a9db578bdfe/1/129411\n                                representation_ms_info['fragments'] = []\n                                segment_time = 0\n                                segment_d = None\n                                segment_number = representation_ms_info['start_number']\n\n                                def add_segment_url():\n                                    segment_url = media_template % {\n                                        'Time': segment_time,\n                                        'Bandwidth': bandwidth,\n                                        'Number': segment_number,\n                                    }\n                                    representation_ms_info['fragments'].append({\n                                        media_location_key: segment_url,\n                                        'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n                                    })\n\n                                for num, s in enumerate(representation_ms_info['s']):\n                                    segment_time = s.get('t') or segment_time\n                                    segment_d = s['d']\n                                    add_segment_url()\n                                    segment_number += 1\n                                    for r in range(s.get('r', 0)):\n                                        segment_time += segment_d\n                                        add_segment_url()\n                                        segment_number += 1\n                                    segment_time += segment_d\n                        elif 'segment_urls' in representation_ms_info and 's' in representation_ms_info:\n                            # No media template\n                            # Example: https://www.youtube.com/watch?v=iXZV5uAYMJI\n                            # or any YouTube dashsegments video\n                            fragments = []\n                            segment_index = 0\n                            timescale = representation_ms_info['timescale']\n                            for s in representation_ms_info['s']:\n                                duration = float_or_none(s['d'], timescale)\n                                for r in range(s.get('r', 0) + 1):\n                                    segment_uri = representation_ms_info['segment_urls'][segment_index]\n                                    fragments.append({\n                                        location_key(segment_uri): segment_uri,\n                                        'duration': duration,\n                                    })\n                                    segment_index += 1\n                            representation_ms_info['fragments'] = fragments\n                        elif 'segment_urls' in representation_ms_info:\n                            # Segment URLs with no SegmentTimeline\n                            # Example: https://www.seznam.cz/zpravy/clanek/cesko-zasahne-vitr-o-sile-vichrice-muze-byt-i-zivotu-nebezpecny-39091\n                            # https://github.com/ytdl-org/youtube-dl/pull/14844\n                            fragments = []\n                            segment_duration = float_or_none(\n                                representation_ms_info['segment_duration'],\n                                representation_ms_info['timescale']) if 'segment_duration' in representation_ms_info else None\n                            for segment_url in representation_ms_info['segment_urls']:\n                                fragment = {\n                                    location_key(segment_url): segment_url,\n                                }\n                                if segment_duration:\n                                    fragment['duration'] = segment_duration\n                                fragments.append(fragment)\n                            representation_ms_info['fragments'] = fragments\n                        # If there is a fragments key available then we correctly recognized fragmented media.\n                        # Otherwise we will assume unfragmented media with direct access. Technically, such\n                        # assumption is not necessarily correct since we may simply have no support for\n                        # some forms of fragmented media renditions yet, but for now we'll use this fallback.\n                        if 'fragments' in representation_ms_info:\n                            f.update({\n                                # NB: mpd_url may be empty when MPD manifest is parsed from a string\n                                'url': mpd_url or base_url,\n                                'fragment_base_url': base_url,\n                                'fragments': [],\n                                'protocol': 'http_dash_segments',\n                            })\n                            if 'initialization_url' in representation_ms_info:\n                                initialization_url = representation_ms_info['initialization_url']\n                                if not f.get('url'):\n                                    f['url'] = initialization_url\n                                f['fragments'].append({location_key(initialization_url): initialization_url})\n                            f['fragments'].extend(representation_ms_info['fragments'])\n                        else:\n                            # Assuming direct URL to unfragmented media.\n                            f['url'] = base_url\n\n                        # According to [1, 5.3.5.2, Table 7, page 35] @id of Representation\n                        # is not necessarily unique within a Period thus formats with\n                        # the same `format_id` are quite possible. There are numerous examples\n                        # of such manifests (see https://github.com/ytdl-org/youtube-dl/issues/15111,\n                        # https://github.com/ytdl-org/youtube-dl/issues/13919)\n                        full_info = formats_dict.get(representation_id, {}).copy()\n                        full_info.update(f)\n                        formats.append(full_info)\n                    else:\n                        self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)\n        return formats",
        "begin_line": 2066,
        "end_line": 2355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005367686527106817,
            "pseudo_dstar_susp": 0.0005546311702717693,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005546311702717693,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._add_ns#2079",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._add_ns(path)",
        "snippet": "        def _add_ns(path):\n            return self._xpath_ns(path, namespace)",
        "begin_line": 2079,
        "end_line": 2080,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005327650506126798,
            "pseudo_dstar_susp": 0.0005482456140350877,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0005482456140350877,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.is_drm_protected#2082",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.is_drm_protected(element)",
        "snippet": "        def is_drm_protected(element):\n            return element.find(_add_ns('ContentProtection')) is not None",
        "begin_line": 2082,
        "end_line": 2083,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005327650506126798,
            "pseudo_dstar_susp": 0.0005482456140350877,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0005482456140350877,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_multisegment_info#2085",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_multisegment_info(element, ms_parent_info)",
        "snippet": "        def extract_multisegment_info(element, ms_parent_info):\n            ms_info = ms_parent_info.copy()\n\n            # As per [1, 5.3.9.2.2] SegmentList and SegmentTemplate share some\n            # common attributes and elements.  We will only extract relevant\n            # for us.\n            def extract_common(source):\n                segment_timeline = source.find(_add_ns('SegmentTimeline'))\n                if segment_timeline is not None:\n                    s_e = segment_timeline.findall(_add_ns('S'))\n                    if s_e:\n                        ms_info['total_number'] = 0\n                        ms_info['s'] = []\n                        for s in s_e:\n                            r = int(s.get('r', 0))\n                            ms_info['total_number'] += 1 + r\n                            ms_info['s'].append({\n                                't': int(s.get('t', 0)),\n                                # @d is mandatory (see [1, 5.3.9.6.2, Table 17, page 60])\n                                'd': int(s.attrib['d']),\n                                'r': r,\n                            })\n                start_number = source.get('startNumber')\n                if start_number:\n                    ms_info['start_number'] = int(start_number)\n                timescale = source.get('timescale')\n                if timescale:\n                    ms_info['timescale'] = int(timescale)\n                segment_duration = source.get('duration')\n                if segment_duration:\n                    ms_info['segment_duration'] = float(segment_duration)\n\n            def extract_Initialization(source):\n                initialization = source.find(_add_ns('Initialization'))\n                if initialization is not None:\n                    ms_info['initialization_url'] = initialization.attrib['sourceURL']\n\n            segment_list = element.find(_add_ns('SegmentList'))\n            if segment_list is not None:\n                extract_common(segment_list)\n                extract_Initialization(segment_list)\n                segment_urls_e = segment_list.findall(_add_ns('SegmentURL'))\n                if segment_urls_e:\n                    ms_info['segment_urls'] = [segment.attrib['media'] for segment in segment_urls_e]\n            else:\n                segment_template = element.find(_add_ns('SegmentTemplate'))\n                if segment_template is not None:\n                    extract_common(segment_template)\n                    media = segment_template.get('media')\n                    if media:\n                        ms_info['media'] = media\n                    initialization = segment_template.get('initialization')\n                    if initialization:\n                        ms_info['initialization'] = initialization\n                    else:\n                        extract_Initialization(segment_template)\n            return ms_info",
        "begin_line": 2085,
        "end_line": 2141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005022601707684581,
            "pseudo_dstar_susp": 0.0005208333333333333,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0005208333333333333,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_common#2091",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_common(source)",
        "snippet": "            def extract_common(source):\n                segment_timeline = source.find(_add_ns('SegmentTimeline'))\n                if segment_timeline is not None:\n                    s_e = segment_timeline.findall(_add_ns('S'))\n                    if s_e:\n                        ms_info['total_number'] = 0\n                        ms_info['s'] = []\n                        for s in s_e:\n                            r = int(s.get('r', 0))\n                            ms_info['total_number'] += 1 + r\n                            ms_info['s'].append({\n                                't': int(s.get('t', 0)),\n                                # @d is mandatory (see [1, 5.3.9.6.2, Table 17, page 60])\n                                'd': int(s.attrib['d']),\n                                'r': r,\n                            })\n                start_number = source.get('startNumber')\n                if start_number:\n                    ms_info['start_number'] = int(start_number)\n                timescale = source.get('timescale')\n                if timescale:\n                    ms_info['timescale'] = int(timescale)\n                segment_duration = source.get('duration')\n                if segment_duration:\n                    ms_info['segment_duration'] = float(segment_duration)",
        "begin_line": 2091,
        "end_line": 2115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004985044865403788,
            "pseudo_dstar_susp": 0.0005173305742369374,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.0005173305742369374,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_Initialization#2117",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_Initialization(source)",
        "snippet": "            def extract_Initialization(source):\n                initialization = source.find(_add_ns('Initialization'))\n                if initialization is not None:\n                    ms_info['initialization_url'] = initialization.attrib['sourceURL']",
        "begin_line": 2117,
        "end_line": 2120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.prepare_template#2200",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.prepare_template(template_name, identifiers)",
        "snippet": "                        def prepare_template(template_name, identifiers):\n                            tmpl = representation_ms_info[template_name]\n                            # First of, % characters outside $...$ templates\n                            # must be escaped by doubling for proper processing\n                            # by % operator string formatting used further (see\n                            # https://github.com/ytdl-org/youtube-dl/issues/16867).\n                            t = ''\n                            in_template = False\n                            for c in tmpl:\n                                t += c\n                                if c == '$':\n                                    in_template = not in_template\n                                elif c == '%' and not in_template:\n                                    t += c\n                            # Next, $...$ templates are translated to their\n                            # %(...) counterparts to be used with % operator\n                            t = t.replace('$RepresentationID$', representation_id)\n                            t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n                            t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n                            t.replace('$$', '$')\n                            return t",
        "begin_line": 2200,
        "end_line": 2220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000505050505050505,
            "pseudo_dstar_susp": 0.0005232862375719519,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0005232862375719519,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.add_segment_url#2269",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.add_segment_url()",
        "snippet": "                                def add_segment_url():\n                                    segment_url = media_template % {\n                                        'Time': segment_time,\n                                        'Bandwidth': bandwidth,\n                                        'Number': segment_number,\n                                    }\n                                    representation_ms_info['fragments'].append({\n                                        media_location_key: segment_url,\n                                        'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n                                    })",
        "begin_line": 2269,
        "end_line": 2278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046274872744099955,
            "pseudo_dstar_susp": 0.0004995004995004995,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004995004995004995,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_ism_formats#2357",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_ism_formats(self, ism_url, video_id, ism_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={})",
        "snippet": "    def _extract_ism_formats(self, ism_url, video_id, ism_id=None, note=None, errnote=None, fatal=True, data=None, headers={}, query={}):\n        res = self._download_xml_handle(\n            ism_url, video_id,\n            note=note or 'Downloading ISM manifest',\n            errnote=errnote or 'Failed to download ISM manifest',\n            fatal=fatal, data=data, headers=headers, query=query)\n        if res is False:\n            return []\n        ism_doc, urlh = res\n        if ism_doc is None:\n            return []\n\n        return self._parse_ism_formats(ism_doc, urlh.geturl(), ism_id)",
        "begin_line": 2357,
        "end_line": 2369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004784688995215311,
            "pseudo_dstar_susp": 0.0004734848484848485,
            "pseudo_tarantula_susp": 0.0009950248756218905,
            "pseudo_op2_susp": 0.0004734848484848485,
            "pseudo_barinel_susp": 0.0009950248756218905
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_ism_formats#2371",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_ism_formats(self, ism_doc, ism_url, ism_id=None)",
        "snippet": "    def _parse_ism_formats(self, ism_doc, ism_url, ism_id=None):\n        \"\"\"\n        Parse formats from ISM manifest.\n        References:\n         1. [MS-SSTR]: Smooth Streaming Protocol,\n            https://msdn.microsoft.com/en-us/library/ff469518.aspx\n        \"\"\"\n        if ism_doc.get('IsLive') == 'TRUE' or ism_doc.find('Protection') is not None:\n            return []\n\n        duration = int(ism_doc.attrib['Duration'])\n        timescale = int_or_none(ism_doc.get('TimeScale')) or 10000000\n\n        formats = []\n        for stream in ism_doc.findall('StreamIndex'):\n            stream_type = stream.get('Type')\n            if stream_type not in ('video', 'audio'):\n                continue\n            url_pattern = stream.attrib['Url']\n            stream_timescale = int_or_none(stream.get('TimeScale')) or timescale\n            stream_name = stream.get('Name')\n            for track in stream.findall('QualityLevel'):\n                fourcc = track.get('FourCC', 'AACL' if track.get('AudioTag') == '255' else None)\n                # TODO: add support for WVC1 and WMAP\n                if fourcc not in ('H264', 'AVC1', 'AACL'):\n                    self.report_warning('%s is not a supported codec' % fourcc)\n                    continue\n                tbr = int(track.attrib['Bitrate']) // 1000\n                # [1] does not mention Width and Height attributes. However,\n                # they're often present while MaxWidth and MaxHeight are\n                # missing, so should be used as fallbacks\n                width = int_or_none(track.get('MaxWidth') or track.get('Width'))\n                height = int_or_none(track.get('MaxHeight') or track.get('Height'))\n                sampling_rate = int_or_none(track.get('SamplingRate'))\n\n                track_url_pattern = re.sub(r'{[Bb]itrate}', track.attrib['Bitrate'], url_pattern)\n                track_url_pattern = compat_urlparse.urljoin(ism_url, track_url_pattern)\n\n                fragments = []\n                fragment_ctx = {\n                    'time': 0,\n                }\n                stream_fragments = stream.findall('c')\n                for stream_fragment_index, stream_fragment in enumerate(stream_fragments):\n                    fragment_ctx['time'] = int_or_none(stream_fragment.get('t')) or fragment_ctx['time']\n                    fragment_repeat = int_or_none(stream_fragment.get('r')) or 1\n                    fragment_ctx['duration'] = int_or_none(stream_fragment.get('d'))\n                    if not fragment_ctx['duration']:\n                        try:\n                            next_fragment_time = int(stream_fragment[stream_fragment_index + 1].attrib['t'])\n                        except IndexError:\n                            next_fragment_time = duration\n                        fragment_ctx['duration'] = (next_fragment_time - fragment_ctx['time']) / fragment_repeat\n                    for _ in range(fragment_repeat):\n                        fragments.append({\n                            'url': re.sub(r'{start[ _]time}', compat_str(fragment_ctx['time']), track_url_pattern),\n                            'duration': fragment_ctx['duration'] / stream_timescale,\n                        })\n                        fragment_ctx['time'] += fragment_ctx['duration']\n\n                format_id = []\n                if ism_id:\n                    format_id.append(ism_id)\n                if stream_name:\n                    format_id.append(stream_name)\n                format_id.append(compat_str(tbr))\n\n                formats.append({\n                    'format_id': '-'.join(format_id),\n                    'url': ism_url,\n                    'manifest_url': ism_url,\n                    'ext': 'ismv' if stream_type == 'video' else 'isma',\n                    'width': width,\n                    'height': height,\n                    'tbr': tbr,\n                    'asr': sampling_rate,\n                    'vcodec': 'none' if stream_type == 'audio' else fourcc,\n                    'acodec': 'none' if stream_type == 'video' else fourcc,\n                    'protocol': 'ism',\n                    'fragments': fragments,\n                    '_download_params': {\n                        'duration': duration,\n                        'timescale': stream_timescale,\n                        'width': width or 0,\n                        'height': height or 0,\n                        'fourcc': fourcc,\n                        'codec_private_data': track.get('CodecPrivateData'),\n                        'sampling_rate': sampling_rate,\n                        'channels': int_or_none(track.get('Channels', 2)),\n                        'bits_per_sample': int_or_none(track.get('BitsPerSample', 16)),\n                        'nal_unit_length_field': int_or_none(track.get('NALUnitLengthField', 4)),\n                    },\n                })\n        return formats",
        "begin_line": 2371,
        "end_line": 2464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004780114722753346,
            "pseudo_dstar_susp": 0.0004633920296570899,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004633920296570899,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_html5_media_entries#2466",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_html5_media_entries(self, base_url, webpage, video_id, m3u8_id=None, m3u8_entry_protocol='m3u8', mpd_id=None, preference=None)",
        "snippet": "    def _parse_html5_media_entries(self, base_url, webpage, video_id, m3u8_id=None, m3u8_entry_protocol='m3u8', mpd_id=None, preference=None):\n        def absolute_url(item_url):\n            return urljoin(base_url, item_url)\n\n        def parse_content_type(content_type):\n            if not content_type:\n                return {}\n            ctr = re.search(r'(?P<mimetype>[^/]+/[^;]+)(?:;\\s*codecs=\"?(?P<codecs>[^\"]+))?', content_type)\n            if ctr:\n                mimetype, codecs = ctr.groups()\n                f = parse_codecs(codecs)\n                f['ext'] = mimetype2ext(mimetype)\n                return f\n            return {}\n\n        def _media_formats(src, cur_media_type, type_info={}):\n            full_url = absolute_url(src)\n            ext = type_info.get('ext') or determine_ext(full_url)\n            if ext == 'm3u8':\n                is_plain_url = False\n                formats = self._extract_m3u8_formats(\n                    full_url, video_id, ext='mp4',\n                    entry_protocol=m3u8_entry_protocol, m3u8_id=m3u8_id,\n                    preference=preference, fatal=False)\n            elif ext == 'mpd':\n                is_plain_url = False\n                formats = self._extract_mpd_formats(\n                    full_url, video_id, mpd_id=mpd_id, fatal=False)\n            else:\n                is_plain_url = True\n                formats = [{\n                    'url': full_url,\n                    'vcodec': 'none' if cur_media_type == 'audio' else None,\n                }]\n            return is_plain_url, formats\n\n        entries = []\n        # amp-video and amp-audio are very similar to their HTML5 counterparts\n        # so we wll include them right here (see\n        # https://www.ampproject.org/docs/reference/components/amp-video)\n        media_tags = [(media_tag, media_type, '')\n                      for media_tag, media_type\n                      in re.findall(r'(?s)(<(?:amp-)?(video|audio)[^>]*/>)', webpage)]\n        media_tags.extend(re.findall(\n            # We only allow video|audio followed by a whitespace or '>'.\n            # Allowing more characters may end up in significant slow down (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11979, example URL:\n            # http://www.porntrex.com/maps/videositemap.xml).\n            r'(?s)(<(?P<tag>(?:amp-)?(?:video|audio))(?:\\s+[^>]*)?>)(.*?)</(?P=tag)>', webpage))\n        for media_tag, media_type, media_content in media_tags:\n            media_info = {\n                'formats': [],\n                'subtitles': {},\n            }\n            media_attributes = extract_attributes(media_tag)\n            src = strip_or_none(media_attributes.get('src'))\n            if src:\n                _, formats = _media_formats(src, media_type)\n                media_info['formats'].extend(formats)\n            media_info['thumbnail'] = absolute_url(media_attributes.get('poster'))\n            if media_content:\n                for source_tag in re.findall(r'<source[^>]+>', media_content):\n                    s_attr = extract_attributes(source_tag)\n                    # data-video-src and data-src are non standard but seen\n                    # several times in the wild\n                    src = strip_or_none(dict_get(s_attr, ('src', 'data-video-src', 'data-src')))\n                    if not src:\n                        continue\n                    f = parse_content_type(s_attr.get('type'))\n                    is_plain_url, formats = _media_formats(src, media_type, f)\n                    if is_plain_url:\n                        # width, height, res, label and title attributes are\n                        # all not standard but seen several times in the wild\n                        labels = [\n                            s_attr.get(lbl)\n                            for lbl in ('label', 'title')\n                            if str_or_none(s_attr.get(lbl))\n                        ]\n                        width = int_or_none(s_attr.get('width'))\n                        height = (int_or_none(s_attr.get('height'))\n                                  or int_or_none(s_attr.get('res')))\n                        if not width or not height:\n                            for lbl in labels:\n                                resolution = parse_resolution(lbl)\n                                if not resolution:\n                                    continue\n                                width = width or resolution.get('width')\n                                height = height or resolution.get('height')\n                        for lbl in labels:\n                            tbr = parse_bitrate(lbl)\n                            if tbr:\n                                break\n                        else:\n                            tbr = None\n                        f.update({\n                            'width': width,\n                            'height': height,\n                            'tbr': tbr,\n                            'format_id': s_attr.get('label') or s_attr.get('title'),\n                        })\n                        f.update(formats[0])\n                        media_info['formats'].append(f)\n                    else:\n                        media_info['formats'].extend(formats)\n                for track_tag in re.findall(r'<track[^>]+>', media_content):\n                    track_attributes = extract_attributes(track_tag)\n                    kind = track_attributes.get('kind')\n                    if not kind or kind in ('subtitles', 'captions'):\n                        src = strip_or_none(track_attributes.get('src'))\n                        if not src:\n                            continue\n                        lang = track_attributes.get('srclang') or track_attributes.get('lang') or track_attributes.get('label')\n                        media_info['subtitles'].setdefault(lang, []).append({\n                            'url': absolute_url(src),\n                        })\n            for f in media_info['formats']:\n                f.setdefault('http_headers', {})['Referer'] = base_url\n            if media_info['formats'] or media_info['subtitles']:\n                entries.append(media_info)\n        return entries",
        "begin_line": 2466,
        "end_line": 2585,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006485084306095979,
            "pseudo_dstar_susp": 0.0006540222367560497,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0006540222367560497,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.absolute_url#2467",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.absolute_url(item_url)",
        "snippet": "        def absolute_url(item_url):\n            return urljoin(base_url, item_url)",
        "begin_line": 2467,
        "end_line": 2468,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000449842555105713,
            "pseudo_dstar_susp": 0.0004786979415988511,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.0004786979415988511,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.parse_content_type#2470",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.parse_content_type(content_type)",
        "snippet": "        def parse_content_type(content_type):\n            if not content_type:\n                return {}\n            ctr = re.search(r'(?P<mimetype>[^/]+/[^;]+)(?:;\\s*codecs=\"?(?P<codecs>[^\"]+))?', content_type)\n            if ctr:\n                mimetype, codecs = ctr.groups()\n                f = parse_codecs(codecs)\n                f['ext'] = mimetype2ext(mimetype)\n                return f\n            return {}",
        "begin_line": 2470,
        "end_line": 2479,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004426737494466578,
            "pseudo_dstar_susp": 0.0004574565416285453,
            "pseudo_tarantula_susp": 0.0005200208008320333,
            "pseudo_op2_susp": 0.0004574565416285453,
            "pseudo_barinel_susp": 0.0005200208008320333
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._media_formats#2481",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._media_formats(src, cur_media_type, type_info={})",
        "snippet": "        def _media_formats(src, cur_media_type, type_info={}):\n            full_url = absolute_url(src)\n            ext = type_info.get('ext') or determine_ext(full_url)\n            if ext == 'm3u8':\n                is_plain_url = False\n                formats = self._extract_m3u8_formats(\n                    full_url, video_id, ext='mp4',\n                    entry_protocol=m3u8_entry_protocol, m3u8_id=m3u8_id,\n                    preference=preference, fatal=False)\n            elif ext == 'mpd':\n                is_plain_url = False\n                formats = self._extract_mpd_formats(\n                    full_url, video_id, mpd_id=mpd_id, fatal=False)\n            else:\n                is_plain_url = True\n                formats = [{\n                    'url': full_url,\n                    'vcodec': 'none' if cur_media_type == 'audio' else None,\n                }]\n            return is_plain_url, formats",
        "begin_line": 2481,
        "end_line": 2500,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004385964912280702,
            "pseudo_dstar_susp": 0.0004562043795620438,
            "pseudo_tarantula_susp": 0.0005753739930955121,
            "pseudo_op2_susp": 0.0004562043795620438,
            "pseudo_barinel_susp": 0.0005753739930955121
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_akamai_formats#2587",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_akamai_formats(self, manifest_url, video_id, hosts={})",
        "snippet": "    def _extract_akamai_formats(self, manifest_url, video_id, hosts={}):\n        formats = []\n        hdcore_sign = 'hdcore=3.7.0'\n        f4m_url = re.sub(r'(https?://[^/]+)/i/', r'\\1/z/', manifest_url).replace('/master.m3u8', '/manifest.f4m')\n        hds_host = hosts.get('hds')\n        if hds_host:\n            f4m_url = re.sub(r'(https?://)[^/]+', r'\\1' + hds_host, f4m_url)\n        if 'hdcore=' not in f4m_url:\n            f4m_url += ('&' if '?' in f4m_url else '?') + hdcore_sign\n        f4m_formats = self._extract_f4m_formats(\n            f4m_url, video_id, f4m_id='hds', fatal=False)\n        for entry in f4m_formats:\n            entry.update({'extra_param_to_segment_url': hdcore_sign})\n        formats.extend(f4m_formats)\n        m3u8_url = re.sub(r'(https?://[^/]+)/z/', r'\\1/i/', manifest_url).replace('/manifest.f4m', '/master.m3u8')\n        hls_host = hosts.get('hls')\n        if hls_host:\n            m3u8_url = re.sub(r'(https?://)[^/]+', r'\\1' + hls_host, m3u8_url)\n        formats.extend(self._extract_m3u8_formats(\n            m3u8_url, video_id, 'mp4', 'm3u8_native',\n            m3u8_id='hls', fatal=False))\n        return formats",
        "begin_line": 2587,
        "end_line": 2608,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004952947003467063,
            "pseudo_dstar_susp": 0.0004464285714285714,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004464285714285714,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_wowza_formats#2610",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_wowza_formats(self, url, video_id, m3u8_entry_protocol='m3u8_native', skip_protocols=[])",
        "snippet": "    def _extract_wowza_formats(self, url, video_id, m3u8_entry_protocol='m3u8_native', skip_protocols=[]):\n        query = compat_urlparse.urlparse(url).query\n        url = re.sub(r'/(?:manifest|playlist|jwplayer)\\.(?:m3u8|f4m|mpd|smil)', '', url)\n        mobj = re.search(\n            r'(?:(?:http|rtmp|rtsp)(?P<s>s)?:)?(?P<url>//[^?]+)', url)\n        url_base = mobj.group('url')\n        http_base_url = '%s%s:%s' % ('http', mobj.group('s') or '', url_base)\n        formats = []\n\n        def manifest_url(manifest):\n            m_url = '%s/%s' % (http_base_url, manifest)\n            if query:\n                m_url += '?%s' % query\n            return m_url\n\n        if 'm3u8' not in skip_protocols:\n            formats.extend(self._extract_m3u8_formats(\n                manifest_url('playlist.m3u8'), video_id, 'mp4',\n                m3u8_entry_protocol, m3u8_id='hls', fatal=False))\n        if 'f4m' not in skip_protocols:\n            formats.extend(self._extract_f4m_formats(\n                manifest_url('manifest.f4m'),\n                video_id, f4m_id='hds', fatal=False))\n        if 'dash' not in skip_protocols:\n            formats.extend(self._extract_mpd_formats(\n                manifest_url('manifest.mpd'),\n                video_id, mpd_id='dash', fatal=False))\n        if re.search(r'(?:/smil:|\\.smil)', url_base):\n            if 'smil' not in skip_protocols:\n                rtmp_formats = self._extract_smil_formats(\n                    manifest_url('jwplayer.smil'),\n                    video_id, fatal=False)\n                for rtmp_format in rtmp_formats:\n                    rtsp_format = rtmp_format.copy()\n                    rtsp_format['url'] = '%s/%s' % (rtmp_format['url'], rtmp_format['play_path'])\n                    del rtsp_format['play_path']\n                    del rtsp_format['ext']\n                    rtsp_format.update({\n                        'url': rtsp_format['url'].replace('rtmp://', 'rtsp://'),\n                        'format_id': rtmp_format['format_id'].replace('rtmp', 'rtsp'),\n                        'protocol': 'rtsp',\n                    })\n                    formats.extend([rtmp_format, rtsp_format])\n        else:\n            for protocol in ('rtmp', 'rtsp'):\n                if protocol not in skip_protocols:\n                    formats.append({\n                        'url': '%s:%s' % (protocol, url_base),\n                        'format_id': protocol,\n                        'protocol': protocol,\n                    })\n        return formats",
        "begin_line": 2610,
        "end_line": 2661,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004302925989672978,
            "pseudo_dstar_susp": 0.0003924646781789639,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003924646781789639,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.manifest_url#2619",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.manifest_url(manifest)",
        "snippet": "        def manifest_url(manifest):\n            m_url = '%s/%s' % (http_base_url, manifest)\n            if query:\n                m_url += '?%s' % query\n            return m_url",
        "begin_line": 2619,
        "end_line": 2623,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._find_jwplayer_data#2663",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._find_jwplayer_data(self, webpage, video_id=None, transform_source=js_to_json)",
        "snippet": "    def _find_jwplayer_data(self, webpage, video_id=None, transform_source=js_to_json):\n        mobj = re.search(\n            r'(?s)jwplayer\\((?P<quote>[\\'\"])[^\\'\" ]+(?P=quote)\\)(?!</script>).*?\\.setup\\s*\\((?P<options>[^)]+)\\)',\n            webpage)\n        if mobj:\n            try:\n                jwplayer_data = self._parse_json(mobj.group('options'),\n                                                 video_id=video_id,\n                                                 transform_source=transform_source)\n            except ExtractorError:\n                pass\n            else:\n                if isinstance(jwplayer_data, dict):\n                    return jwplayer_data",
        "begin_line": 2663,
        "end_line": 2676,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005875440658049354,
            "pseudo_dstar_susp": 0.0006045949214026602,
            "pseudo_tarantula_susp": 0.0004351610095735422,
            "pseudo_op2_susp": 0.0006045949214026602,
            "pseudo_barinel_susp": 0.0004351610095735422
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_jwplayer_data#2678",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_jwplayer_data(self, webpage, video_id, *args, **kwargs)",
        "snippet": "    def _extract_jwplayer_data(self, webpage, video_id, *args, **kwargs):\n        jwplayer_data = self._find_jwplayer_data(\n            webpage, video_id, transform_source=js_to_json)\n        return self._parse_jwplayer_data(\n            jwplayer_data, video_id, *args, **kwargs)",
        "begin_line": 2678,
        "end_line": 2682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_jwplayer_data#2684",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True, m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None)",
        "snippet": "    def _parse_jwplayer_data(self, jwplayer_data, video_id=None, require_title=True,\n                             m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None):\n        # JWPlayer backward compatibility: flattened playlists\n        # https://github.com/jwplayer/jwplayer/blob/v7.4.3/src/js/api/config.js#L81-L96\n        if 'playlist' not in jwplayer_data:\n            jwplayer_data = {'playlist': [jwplayer_data]}\n\n        entries = []\n\n        # JWPlayer backward compatibility: single playlist item\n        # https://github.com/jwplayer/jwplayer/blob/v7.7.0/src/js/playlist/playlist.js#L10\n        if not isinstance(jwplayer_data['playlist'], list):\n            jwplayer_data['playlist'] = [jwplayer_data['playlist']]\n\n        for video_data in jwplayer_data['playlist']:\n            # JWPlayer backward compatibility: flattened sources\n            # https://github.com/jwplayer/jwplayer/blob/v7.4.3/src/js/playlist/item.js#L29-L35\n            if 'sources' not in video_data:\n                video_data['sources'] = [video_data]\n\n            this_video_id = video_id or video_data['mediaid']\n\n            formats = self._parse_jwplayer_formats(\n                video_data['sources'], video_id=this_video_id, m3u8_id=m3u8_id,\n                mpd_id=mpd_id, rtmp_params=rtmp_params, base_url=base_url)\n\n            subtitles = {}\n            tracks = video_data.get('tracks')\n            if tracks and isinstance(tracks, list):\n                for track in tracks:\n                    if not isinstance(track, dict):\n                        continue\n                    track_kind = track.get('kind')\n                    if not track_kind or not isinstance(track_kind, compat_str):\n                        continue\n                    if track_kind.lower() not in ('captions', 'subtitles'):\n                        continue\n                    track_url = urljoin(base_url, track.get('file'))\n                    if not track_url:\n                        continue\n                    subtitles.setdefault(track.get('label') or 'en', []).append({\n                        'url': self._proto_relative_url(track_url)\n                    })\n\n            entry = {\n                'id': this_video_id,\n                'title': unescapeHTML(video_data['title'] if require_title else video_data.get('title')),\n                'description': clean_html(video_data.get('description')),\n                'thumbnail': urljoin(base_url, self._proto_relative_url(video_data.get('image'))),\n                'timestamp': int_or_none(video_data.get('pubdate')),\n                'duration': float_or_none(jwplayer_data.get('duration') or video_data.get('duration')),\n                'subtitles': subtitles,\n            }\n            # https://github.com/jwplayer/jwplayer/blob/master/src/js/utils/validator.js#L32\n            if len(formats) == 1 and re.search(r'^(?:http|//).*(?:youtube\\.com|youtu\\.be)/.+', formats[0]['url']):\n                entry.update({\n                    '_type': 'url_transparent',\n                    'url': formats[0]['url'],\n                })\n            else:\n                self._sort_formats(formats)\n                entry['formats'] = formats\n            entries.append(entry)\n        if len(entries) == 1:\n            return entries[0]\n        else:\n            return self.playlist_result(entries)",
        "begin_line": 2684,
        "end_line": 2750,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000425531914893617,
            "pseudo_dstar_susp": 0.0004266211604095563,
            "pseudo_tarantula_susp": 0.001893939393939394,
            "pseudo_op2_susp": 0.0004266211604095563,
            "pseudo_barinel_susp": 0.001893939393939394
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._parse_jwplayer_formats#2752",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None, m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None)",
        "snippet": "    def _parse_jwplayer_formats(self, jwplayer_sources_data, video_id=None,\n                                m3u8_id=None, mpd_id=None, rtmp_params=None, base_url=None):\n        urls = []\n        formats = []\n        for source in jwplayer_sources_data:\n            if not isinstance(source, dict):\n                continue\n            source_url = urljoin(\n                base_url, self._proto_relative_url(source.get('file')))\n            if not source_url or source_url in urls:\n                continue\n            urls.append(source_url)\n            source_type = source.get('type') or ''\n            ext = mimetype2ext(source_type) or determine_ext(source_url)\n            if source_type == 'hls' or ext == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    source_url, video_id, 'mp4', entry_protocol='m3u8_native',\n                    m3u8_id=m3u8_id, fatal=False))\n            elif source_type == 'dash' or ext == 'mpd':\n                formats.extend(self._extract_mpd_formats(\n                    source_url, video_id, mpd_id=mpd_id, fatal=False))\n            elif ext == 'smil':\n                formats.extend(self._extract_smil_formats(\n                    source_url, video_id, fatal=False))\n            # https://github.com/jwplayer/jwplayer/blob/master/src/js/providers/default.js#L67\n            elif source_type.startswith('audio') or ext in (\n                    'oga', 'aac', 'mp3', 'mpeg', 'vorbis'):\n                formats.append({\n                    'url': source_url,\n                    'vcodec': 'none',\n                    'ext': ext,\n                })\n            else:\n                height = int_or_none(source.get('height'))\n                if height is None:\n                    # Often no height is provided but there is a label in\n                    # format like \"1080p\", \"720p SD\", or 1080.\n                    height = int_or_none(self._search_regex(\n                        r'^(\\d{3,4})[pP]?(?:\\b|$)', compat_str(source.get('label') or ''),\n                        'height', default=None))\n                a_format = {\n                    'url': source_url,\n                    'width': int_or_none(source.get('width')),\n                    'height': height,\n                    'tbr': int_or_none(source.get('bitrate')),\n                    'ext': ext,\n                }\n                if source_url.startswith('rtmp'):\n                    a_format['ext'] = 'flv'\n                    # See com/longtailvideo/jwplayer/media/RTMPMediaProvider.as\n                    # of jwplayer.flash.swf\n                    rtmp_url_parts = re.split(\n                        r'((?:mp4|mp3|flv):)', source_url, 1)\n                    if len(rtmp_url_parts) == 3:\n                        rtmp_url, prefix, play_path = rtmp_url_parts\n                        a_format.update({\n                            'url': rtmp_url,\n                            'play_path': prefix + play_path,\n                        })\n                    if rtmp_params:\n                        a_format.update(rtmp_params)\n                formats.append(a_format)\n        return formats",
        "begin_line": 2752,
        "end_line": 2814,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042301184433164127,
            "pseudo_dstar_susp": 0.00041999160016799666,
            "pseudo_tarantula_susp": 0.001893939393939394,
            "pseudo_op2_susp": 0.00041999160016799666,
            "pseudo_barinel_susp": 0.001893939393939394
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._live_title#2816",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._live_title(self, name)",
        "snippet": "    def _live_title(self, name):\n        \"\"\" Generate the title for a live video \"\"\"\n        now = datetime.datetime.now()\n        now_str = now.strftime('%Y-%m-%d %H:%M')\n        return name + ' ' + now_str",
        "begin_line": 2816,
        "end_line": 2820,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005688282138794084,
            "pseudo_dstar_susp": 0.0005099439061703213,
            "pseudo_tarantula_susp": 0.0023584905660377358,
            "pseudo_op2_susp": 0.0005099439061703213,
            "pseudo_barinel_susp": 0.0023584905660377358
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._int#2822",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._int(self, v, name, fatal=False, **kwargs)",
        "snippet": "    def _int(self, v, name, fatal=False, **kwargs):\n        res = int_or_none(v, **kwargs)\n        if 'get_attr' in kwargs:\n            print(getattr(v, kwargs['get_attr']))\n        if res is None:\n            msg = 'Failed to extract %s: Could not parse value %r' % (name, v)\n            if fatal:\n                raise ExtractorError(msg)\n            else:\n                self._downloader.report_warning(msg)\n        return res",
        "begin_line": 2822,
        "end_line": 2832,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.0002678810608090008,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0002678810608090008,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._float#2834",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._float(self, v, name, fatal=False, **kwargs)",
        "snippet": "    def _float(self, v, name, fatal=False, **kwargs):\n        res = float_or_none(v, **kwargs)\n        if res is None:\n            msg = 'Failed to extract %s: Could not parse value %r' % (name, v)\n            if fatal:\n                raise ExtractorError(msg)\n            else:\n                self._downloader.report_warning(msg)\n        return res",
        "begin_line": 2834,
        "end_line": 2842,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._set_cookie#2844",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._set_cookie(self, domain, name, value, expire_time=None, port=None, path='/', secure=False, discard=False, rest={}, **kwargs)",
        "snippet": "    def _set_cookie(self, domain, name, value, expire_time=None, port=None,\n                    path='/', secure=False, discard=False, rest={}, **kwargs):\n        cookie = compat_cookiejar_Cookie(\n            0, name, value, port, port is not None, domain, True,\n            domain.startswith('.'), path, True, secure, expire_time,\n            discard, None, None, rest)\n        self._downloader.cookiejar.set_cookie(cookie)",
        "begin_line": 2844,
        "end_line": 2850,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001519756838905775,
            "pseudo_dstar_susp": 0.0009115770282588879,
            "pseudo_tarantula_susp": 0.0018214936247723133,
            "pseudo_op2_susp": 0.0009115770282588879,
            "pseudo_barinel_susp": 0.0018214936247723133
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_cookies#2852",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_cookies(self, url)",
        "snippet": "    def _get_cookies(self, url):\n        \"\"\" Return a compat_cookies.SimpleCookie with the cookies for the url \"\"\"\n        req = sanitized_Request(url)\n        self._downloader.cookiejar.add_cookie_header(req)\n        return compat_cookies.SimpleCookie(req.get_header('Cookie'))",
        "begin_line": 2852,
        "end_line": 2856,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005411255411255411,
            "pseudo_dstar_susp": 0.0005136106831022085,
            "pseudo_tarantula_susp": 0.0006574621959237344,
            "pseudo_op2_susp": 0.0005136106831022085,
            "pseudo_barinel_susp": 0.0006574621959237344
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.get_testcases#2885",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.get_testcases(self, include_onlymatching=False)",
        "snippet": "    def get_testcases(self, include_onlymatching=False):\n        t = getattr(self, '_TEST', None)\n        if t:\n            assert not hasattr(self, '_TESTS'), \\\n                '%s has _TEST and _TESTS' % type(self).__name__\n            tests = [t]\n        else:\n            tests = getattr(self, '_TESTS', [])\n        for t in tests:\n            if not include_onlymatching and t.get('only_matching', False):\n                continue\n            t['name'] = type(self).__name__[:-len('IE')]\n            yield t",
        "begin_line": 2885,
        "end_line": 2897,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.is_suitable#2899",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.is_suitable(self, age_limit)",
        "snippet": "    def is_suitable(self, age_limit):\n        \"\"\" Test whether the extractor is generally suitable for the given\n        age limit (i.e. pornographic sites are not, all others usually are) \"\"\"\n\n        any_restricted = False\n        for tc in self.get_testcases(include_onlymatching=False):\n            if tc.get('playlist', []):\n                tc = tc['playlist'][0]\n            is_restricted = age_restricted(\n                tc.get('info_dict', {}).get('age_limit'), age_limit)\n            if not is_restricted:\n                return True\n            any_restricted = any_restricted or is_restricted\n        return not any_restricted",
        "begin_line": 2899,
        "end_line": 2912,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_subtitles#2914",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_subtitles(self, *args, **kwargs)",
        "snippet": "    def extract_subtitles(self, *args, **kwargs):\n        if (self._downloader.params.get('writesubtitles', False)\n                or self._downloader.params.get('listsubtitles')):\n            return self._get_subtitles(*args, **kwargs)\n        return {}",
        "begin_line": 2914,
        "end_line": 2918,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030553009471432935,
            "pseudo_dstar_susp": 0.00030646644192460924,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00030646644192460924,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_subtitles#2920",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_subtitles(self, *args, **kwargs)",
        "snippet": "    def _get_subtitles(self, *args, **kwargs):\n        raise NotImplementedError('This method must be implemented by subclasses')",
        "begin_line": 2920,
        "end_line": 2921,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._merge_subtitle_items#2924",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._merge_subtitle_items(subtitle_list1, subtitle_list2)",
        "snippet": "    def _merge_subtitle_items(subtitle_list1, subtitle_list2):\n        \"\"\" Merge subtitle items for one language. Items with duplicated URLs\n        will be dropped. \"\"\"\n        list1_urls = set([item['url'] for item in subtitle_list1])\n        ret = list(subtitle_list1)\n        ret.extend([item for item in subtitle_list2 if item['url'] not in list1_urls])\n        return ret",
        "begin_line": 2924,
        "end_line": 2930,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.00034083162917518747,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00034083162917518747,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._merge_subtitles#2933",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._merge_subtitles(cls, subtitle_dict1, subtitle_dict2)",
        "snippet": "    def _merge_subtitles(cls, subtitle_dict1, subtitle_dict2):\n        \"\"\" Merge two subtitle dictionaries, language by language. \"\"\"\n        ret = dict(subtitle_dict1)\n        for lang in subtitle_dict2:\n            ret[lang] = cls._merge_subtitle_items(subtitle_dict1.get(lang, []), subtitle_dict2[lang])\n        return ret",
        "begin_line": 2933,
        "end_line": 2938,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.00034083162917518747,
            "pseudo_tarantula_susp": 0.0004943153732081067,
            "pseudo_op2_susp": 0.00034083162917518747,
            "pseudo_barinel_susp": 0.0004943153732081067
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.extract_automatic_captions#2940",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.extract_automatic_captions(self, *args, **kwargs)",
        "snippet": "    def extract_automatic_captions(self, *args, **kwargs):\n        if (self._downloader.params.get('writeautomaticsub', False)\n                or self._downloader.params.get('listsubtitles')):\n            return self._get_automatic_captions(*args, **kwargs)\n        return {}",
        "begin_line": 2940,
        "end_line": 2944,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_automatic_captions#2946",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_automatic_captions(self, *args, **kwargs)",
        "snippet": "    def _get_automatic_captions(self, *args, **kwargs):\n        raise NotImplementedError('This method must be implemented by subclasses')",
        "begin_line": 2946,
        "end_line": 2947,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.mark_watched#2949",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.mark_watched(self, *args, **kwargs)",
        "snippet": "    def mark_watched(self, *args, **kwargs):\n        if (self._downloader.params.get('mark_watched', False)\n                and (self._get_login_info()[0] is not None\n                     or self._downloader.params.get('cookiefile') is not None)):\n            self._mark_watched(*args, **kwargs)",
        "begin_line": 2949,
        "end_line": 2953,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._mark_watched#2955",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._mark_watched(self, *args, **kwargs)",
        "snippet": "    def _mark_watched(self, *args, **kwargs):\n        raise NotImplementedError('This method must be implemented by subclasses')",
        "begin_line": 2955,
        "end_line": 2956,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.geo_verification_headers#2958",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.geo_verification_headers(self)",
        "snippet": "    def geo_verification_headers(self):\n        headers = {}\n        geo_verification_proxy = self._downloader.params.get('geo_verification_proxy')\n        if geo_verification_proxy:\n            headers['Ytdl-request-proxy'] = geo_verification_proxy\n        return headers",
        "begin_line": 2958,
        "end_line": 2963,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009157509157509158,
            "pseudo_dstar_susp": 0.0008103727714748784,
            "pseudo_tarantula_susp": 0.0006613756613756613,
            "pseudo_op2_susp": 0.0008103727714748784,
            "pseudo_barinel_susp": 0.0006613756613756613
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._generic_id#2965",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._generic_id(self, url)",
        "snippet": "    def _generic_id(self, url):\n        return compat_urllib_parse_unquote(os.path.splitext(url.rstrip('/').split('/')[-1])[0])",
        "begin_line": 2965,
        "end_line": 2966,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._generic_title#2968",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._generic_title(self, url)",
        "snippet": "    def _generic_title(self, url):\n        return compat_urllib_parse_unquote(os.path.splitext(url_basename(url))[0])",
        "begin_line": 2968,
        "end_line": 2969,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026062027625749283,
            "pseudo_dstar_susp": 0.00026021337496747333,
            "pseudo_tarantula_susp": 0.00026143790849673205,
            "pseudo_op2_susp": 0.00026021337496747333,
            "pseudo_barinel_susp": 0.00026143790849673205
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor._make_valid_url#2980",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor._make_valid_url(cls)",
        "snippet": "    def _make_valid_url(cls):\n        return r'%s(?P<prefix>|[1-9][0-9]*|all):(?P<query>[\\s\\S]+)' % cls._SEARCH_KEY",
        "begin_line": 2980,
        "end_line": 2981,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor.suitable#2984",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return re.match(cls._make_valid_url(), url) is not None",
        "begin_line": 2984,
        "end_line": 2985,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor._real_extract#2987",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor._real_extract(self, query)",
        "snippet": "    def _real_extract(self, query):\n        mobj = re.match(self._make_valid_url(), query)\n        if mobj is None:\n            raise ExtractorError('Invalid search query \"%s\"' % query)\n\n        prefix = mobj.group('prefix')\n        query = mobj.group('query')\n        if prefix == '':\n            return self._get_n_results(query, 1)\n        elif prefix == 'all':\n            return self._get_n_results(query, self._MAX_RESULTS)\n        else:\n            n = int(prefix)\n            if n <= 0:\n                raise ExtractorError('invalid download number %s for query \"%s\"' % (n, query))\n            elif n > self._MAX_RESULTS:\n                self._downloader.report_warning('%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n))\n                n = self._MAX_RESULTS\n            return self._get_n_results(query, n)",
        "begin_line": 2987,
        "end_line": 3005,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.0002678810608090008,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0002678810608090008,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor._get_n_results#3007",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor._get_n_results(self, query, n)",
        "snippet": "    def _get_n_results(self, query, n):\n        \"\"\"Get a specified number of results for a query\"\"\"\n        raise NotImplementedError('This method must be implemented by subclasses')",
        "begin_line": 3007,
        "end_line": 3009,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.drtv.DRTVLiveIE._real_extract#316",
        "src_path": "youtube_dl/extractor/drtv.py",
        "class_name": "youtube_dl.extractor.drtv.DRTVLiveIE",
        "signature": "youtube_dl.extractor.drtv.DRTVLiveIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        channel_id = self._match_id(url)\n        channel_data = self._download_json(\n            'https://www.dr.dk/mu-online/api/1.0/channel/' + channel_id,\n            channel_id)\n        title = self._live_title(channel_data['Title'])\n\n        formats = []\n        for streaming_server in channel_data.get('StreamingServers', []):\n            server = streaming_server.get('Server')\n            if not server:\n                continue\n            link_type = streaming_server.get('LinkType')\n            for quality in streaming_server.get('Qualities', []):\n                for stream in quality.get('Streams', []):\n                    stream_path = stream.get('Stream')\n                    if not stream_path:\n                        continue\n                    stream_url = update_url_query(\n                        '%s/%s' % (server, stream_path), {'b': ''})\n                    if link_type == 'HLS':\n                        formats.extend(self._extract_m3u8_formats(\n                            stream_url, channel_id, 'mp4',\n                            m3u8_id=link_type, fatal=False, live=True))\n                    elif link_type == 'HDS':\n                        formats.extend(self._extract_f4m_formats(update_url_query(\n                            '%s/%s' % (server, stream_path), {'hdcore': '3.7.0'}),\n                            channel_id, f4m_id=link_type, fatal=False))\n        self._sort_formats(formats)\n\n        return {\n            'id': channel_id,\n            'title': title,\n            'thumbnail': channel_data.get('PrimaryImageUri'),\n            'formats': formats,\n            'is_live': True,\n        }",
        "begin_line": 316,
        "end_line": 352,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor._extract_mgid#256",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor",
        "signature": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor._extract_mgid(self, webpage)",
        "snippet": "    def _extract_mgid(self, webpage):\n        try:\n            # the url can be http://media.mtvnservices.com/fb/{mgid}.swf\n            # or http://media.mtvnservices.com/{mgid}\n            og_url = self._og_search_video_url(webpage)\n            mgid = url_basename(og_url)\n            if mgid.endswith('.swf'):\n                mgid = mgid[:-4]\n        except RegexNotFoundError:\n            mgid = None\n\n        if mgid is None or ':' not in mgid:\n            mgid = self._search_regex(\n                [r'data-mgid=\"(.*?)\"', r'swfobject\\.embedSWF\\(\".*?(mgid:.*?)\"'],\n                webpage, 'mgid', default=None)\n\n        if not mgid:\n            sm4_embed = self._html_search_meta(\n                'sm4:video:embed', webpage, 'sm4 embed', default='')\n            mgid = self._search_regex(\n                r'embed/(mgid:.+?)[\"\\'&?/]', sm4_embed, 'mgid', default=None)\n\n        if not mgid:\n            mgid = self._extract_triforce_mgid(webpage)\n\n        return mgid",
        "begin_line": 256,
        "end_line": 281,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor._real_extract#283",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor",
        "signature": "youtube_dl.extractor.mtv.MTVServicesInfoExtractor._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        title = url_basename(url)\n        webpage = self._download_webpage(url, title)\n        mgid = self._extract_mgid(webpage)\n        videos_info = self._get_videos_info(mgid)\n        return videos_info",
        "begin_line": 283,
        "end_line": 288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE._extract_url#310",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE",
        "signature": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE._extract_url(webpage)",
        "snippet": "    def _extract_url(webpage):\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//media.mtvnservices.com/embed/.+?)\\1', webpage)\n        if mobj:\n            return mobj.group('url')",
        "begin_line": 310,
        "end_line": 314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE._real_extract#322",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE",
        "signature": "youtube_dl.extractor.mtv.MTVServicesEmbeddedIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        mgid = mobj.group('mgid')\n        return self._get_videos_info(mgid)",
        "begin_line": 322,
        "end_line": 325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVJapanIE._get_feed_query#371",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVJapanIE",
        "signature": "youtube_dl.extractor.mtv.MTVJapanIE._get_feed_query(self, uri)",
        "snippet": "    def _get_feed_query(self, uri):\n        return {\n            'arcEp': 'mtvjapan.com',\n            'mgid': uri,\n        }",
        "begin_line": 371,
        "end_line": 375,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVVideoIE._get_thumbnail_url#401",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVVideoIE",
        "signature": "youtube_dl.extractor.mtv.MTVVideoIE._get_thumbnail_url(self, uri, itemdoc)",
        "snippet": "    def _get_thumbnail_url(self, uri, itemdoc):\n        return 'http://mtv.mtvnimages.com/uri/' + uri",
        "begin_line": 401,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVVideoIE._real_extract#404",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVVideoIE",
        "signature": "youtube_dl.extractor.mtv.MTVVideoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('videoid')\n        uri = mobj.groupdict().get('mgid')\n        if uri is None:\n            webpage = self._download_webpage(url, video_id)\n\n            # Some videos come from Vevo.com\n            m_vevo = re.search(\n                r'(?s)isVevoVideo = true;.*?vevoVideoId = \"(.*?)\";', webpage)\n            if m_vevo:\n                vevo_id = m_vevo.group(1)\n                self.to_screen('Vevo video detected: %s' % vevo_id)\n                return self.url_result('vevo:%s' % vevo_id, ie='Vevo')\n\n            uri = self._html_search_regex(r'/uri/(.*?)\\?', webpage, 'uri')\n        return self._get_videos_info(uri)",
        "begin_line": 404,
        "end_line": 420,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVDEIE._get_feed_query#470",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVDEIE",
        "signature": "youtube_dl.extractor.mtv.MTVDEIE._get_feed_query(self, uri)",
        "snippet": "    def _get_feed_query(self, uri):\n        return {\n            'arcEp': 'mtv.de',\n            'mgid': uri,\n        }",
        "begin_line": 470,
        "end_line": 474,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.adobetv.AdobeTVBaseIE._call_api#21",
        "src_path": "youtube_dl/extractor/adobetv.py",
        "class_name": "youtube_dl.extractor.adobetv.AdobeTVBaseIE",
        "signature": "youtube_dl.extractor.adobetv.AdobeTVBaseIE._call_api(self, path, video_id, query, note=None)",
        "snippet": "    def _call_api(self, path, video_id, query, note=None):\n        return self._download_json(\n            'http://tv.adobe.com/api/v4/' + path,\n            video_id, note, query=query)['data']",
        "begin_line": 21,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000741839762611276,
            "pseudo_dstar_susp": 0.000708215297450425,
            "pseudo_tarantula_susp": 0.000547645125958379,
            "pseudo_op2_susp": 0.000708215297450425,
            "pseudo_barinel_susp": 0.000547645125958379
        }
    },
    {
        "name": "youtube_dl.extractor.adobetv.AdobeTVVideoIE._real_extract#250",
        "src_path": "youtube_dl/extractor/adobetv.py",
        "class_name": "youtube_dl.extractor.adobetv.AdobeTVVideoIE",
        "signature": "youtube_dl.extractor.adobetv.AdobeTVVideoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        video_data = self._parse_json(self._search_regex(\n            r'var\\s+bridge\\s*=\\s*([^;]+);', webpage, 'bridged data'), video_id)\n        title = video_data['title']\n\n        formats = []\n        sources = video_data.get('sources') or []\n        for source in sources:\n            source_src = source.get('src')\n            if not source_src:\n                continue\n            formats.append({\n                'filesize': int_or_none(source.get('kilobytes') or None, invscale=1000),\n                'format_id': '-'.join(filter(None, [source.get('format'), source.get('label')])),\n                'height': int_or_none(source.get('height') or None),\n                'tbr': int_or_none(source.get('bitrate') or None),\n                'width': int_or_none(source.get('width') or None),\n                'url': source_src,\n            })\n        self._sort_formats(formats)\n\n        # For both metadata and downloaded files the duration varies among\n        # formats. I just pick the max one\n        duration = max(filter(None, [\n            float_or_none(source.get('duration'), scale=1000)\n            for source in sources]))\n\n        return {\n            'id': video_id,\n            'formats': formats,\n            'title': title,\n            'description': video_data.get('description'),\n            'thumbnail': video_data.get('video', {}).get('poster'),\n            'duration': duration,\n            'subtitles': self._parse_subtitles(video_data, 'vttPath'),\n        }",
        "begin_line": 250,
        "end_line": 288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.udn.UDNEmbedIE._real_extract#41",
        "src_path": "youtube_dl/extractor/udn.py",
        "class_name": "youtube_dl.extractor.udn.UDNEmbedIE",
        "signature": "youtube_dl.extractor.udn.UDNEmbedIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        page = self._download_webpage(url, video_id)\n\n        options_str = self._html_search_regex(\n            r'var\\s+options\\s*=\\s*([^;]+);', page, 'options')\n        trans_options_str = js_to_json(options_str)\n        options = self._parse_json(trans_options_str, 'options', fatal=False) or {}\n        if options:\n            video_urls = options['video']\n            title = options['title']\n            poster = options.get('poster')\n        else:\n            video_urls = self._parse_json(self._html_search_regex(\n                r'\"video\"\\s*:\\s*({.+?})\\s*,', trans_options_str, 'video urls'), 'video urls')\n            title = self._html_search_regex(\n                r\"title\\s*:\\s*'(.+?)'\\s*,\", options_str, 'title')\n            poster = self._html_search_regex(\n                r\"poster\\s*:\\s*'(.+?)'\\s*,\", options_str, 'poster', default=None)\n\n        if video_urls.get('youtube'):\n            return self.url_result(video_urls.get('youtube'), 'Youtube')\n\n        formats = []\n        for video_type, api_url in video_urls.items():\n            if not api_url:\n                continue\n\n            video_url = self._download_webpage(\n                compat_urlparse.urljoin(url, api_url), video_id,\n                note='retrieve url for %s video' % video_type)\n\n            ext = determine_ext(video_url)\n            if ext == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    video_url, video_id, ext='mp4', m3u8_id='hls'))\n            elif ext == 'f4m':\n                formats.extend(self._extract_f4m_formats(\n                    video_url, video_id, f4m_id='hds'))\n            else:\n                mobj = re.search(r'_(?P<height>\\d+)p_(?P<tbr>\\d+)\\.mp4', video_url)\n                a_format = {\n                    'url': video_url,\n                    # video_type may be 'mp4', which confuses YoutubeDL\n                    'format_id': 'http-' + video_type,\n                }\n                if mobj:\n                    a_format.update({\n                        'height': int_or_none(mobj.group('height')),\n                        'tbr': int_or_none(mobj.group('tbr')),\n                    })\n                formats.append(a_format)\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'formats': formats,\n            'title': title,\n            'thumbnail': poster,\n        }",
        "begin_line": 41,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011364927832708262,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.theplatform.ThePlatformBaseIE._extract_theplatform_smil#37",
        "src_path": "youtube_dl/extractor/theplatform.py",
        "class_name": "youtube_dl.extractor.theplatform.ThePlatformBaseIE",
        "signature": "youtube_dl.extractor.theplatform.ThePlatformBaseIE._extract_theplatform_smil(self, smil_url, video_id, note='Downloading SMIL data')",
        "snippet": "    def _extract_theplatform_smil(self, smil_url, video_id, note='Downloading SMIL data'):\n        meta = self._download_xml(\n            smil_url, video_id, note=note, query={'format': 'SMIL'},\n            headers=self.geo_verification_headers())\n        error_element = find_xpath_attr(meta, _x('.//smil:ref'), 'src')\n        if error_element is not None:\n            exception = find_xpath_attr(\n                error_element, _x('.//smil:param'), 'name', 'exception')\n            if exception is not None:\n                if exception.get('value') == 'GeoLocationBlocked':\n                    self.raise_geo_restricted(error_element.attrib['abstract'])\n                elif error_element.attrib['src'].startswith(\n                        'http://link.theplatform.%s/s/errorFiles/Unavailable.'\n                        % self._TP_TLD):\n                    raise ExtractorError(\n                        error_element.attrib['abstract'], expected=True)\n\n        smil_formats = self._parse_smil_formats(\n            meta, smil_url, video_id, namespace=default_ns,\n            # the parameters are from syfy.com, other sites may use others,\n            # they also work for nbc.com\n            f4m_params={'g': 'UXWGVKRWHFSP', 'hdcore': '3.0.3'},\n            transform_rtmp_url=lambda streamer, src: (streamer, 'mp4:' + src))\n\n        formats = []\n        for _format in smil_formats:\n            if OnceIE.suitable(_format['url']):\n                formats.extend(self._extract_once_formats(_format['url']))\n            else:\n                media_url = _format['url']\n                if determine_ext(media_url) == 'm3u8':\n                    hdnea2 = self._get_cookies(media_url).get('hdnea2')\n                    if hdnea2:\n                        _format['url'] = update_url_query(media_url, {'hdnea3': hdnea2.value})\n\n                formats.append(_format)\n\n        subtitles = self._parse_smil_subtitles(meta, default_ns)\n\n        return formats, subtitles",
        "begin_line": 37,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.theplatform.ThePlatformIE._extract_urls#201",
        "src_path": "youtube_dl/extractor/theplatform.py",
        "class_name": "youtube_dl.extractor.theplatform.ThePlatformIE",
        "signature": "youtube_dl.extractor.theplatform.ThePlatformIE._extract_urls(cls, webpage)",
        "snippet": "    def _extract_urls(cls, webpage):\n        m = re.search(\n            r'''(?x)\n                    <meta\\s+\n                        property=([\"'])(?:og:video(?::(?:secure_)?url)?|twitter:player)\\1\\s+\n                        content=([\"'])(?P<url>https?://player\\.theplatform\\.com/p/.+?)\\2\n            ''', webpage)\n        if m:\n            return [m.group('url')]\n\n        # Are whitesapces ignored in URLs?\n        # https://github.com/ytdl-org/youtube-dl/issues/12044\n        matches = re.findall(\n            r'(?s)<(?:iframe|script)[^>]+src=([\"\\'])((?:https?:)?//player\\.theplatform\\.com/p/.+?)\\1', webpage)\n        if matches:\n            return [re.sub(r'\\s', '', list(zip(*matches))[1][0])]",
        "begin_line": 201,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.theplatform.ThePlatformIE._real_extract#235",
        "src_path": "youtube_dl/extractor/theplatform.py",
        "class_name": "youtube_dl.extractor.theplatform.ThePlatformIE",
        "signature": "youtube_dl.extractor.theplatform.ThePlatformIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n\n        mobj = re.match(self._VALID_URL, url)\n        provider_id = mobj.group('provider_id')\n        video_id = mobj.group('id')\n\n        if not provider_id:\n            provider_id = 'dJ5BDC'\n\n        path = provider_id + '/'\n        if mobj.group('media'):\n            path += mobj.group('media')\n        path += video_id\n\n        qs_dict = compat_parse_qs(compat_urllib_parse_urlparse(url).query)\n        if 'guid' in qs_dict:\n            webpage = self._download_webpage(url, video_id)\n            scripts = re.findall(r'<script[^>]+src=\"([^\"]+)\"', webpage)\n            feed_id = None\n            # feed id usually locates in the last script.\n            # Seems there's no pattern for the interested script filename, so\n            # I try one by one\n            for script in reversed(scripts):\n                feed_script = self._download_webpage(\n                    self._proto_relative_url(script, 'http:'),\n                    video_id, 'Downloading feed script')\n                feed_id = self._search_regex(\n                    r'defaultFeedId\\s*:\\s*\"([^\"]+)\"', feed_script,\n                    'default feed id', default=None)\n                if feed_id is not None:\n                    break\n            if feed_id is None:\n                raise ExtractorError('Unable to find feed id')\n            return self.url_result('http://feed.theplatform.com/f/%s/%s?byGuid=%s' % (\n                provider_id, feed_id, qs_dict['guid'][0]))\n\n        if smuggled_data.get('force_smil_url', False):\n            smil_url = url\n        # Explicitly specified SMIL (see https://github.com/ytdl-org/youtube-dl/issues/7385)\n        elif '/guid/' in url:\n            headers = {}\n            source_url = smuggled_data.get('source_url')\n            if source_url:\n                headers['Referer'] = source_url\n            request = sanitized_Request(url, headers=headers)\n            webpage = self._download_webpage(request, video_id)\n            smil_url = self._search_regex(\n                r'<link[^>]+href=([\"\\'])(?P<url>.+?)\\1[^>]+type=[\"\\']application/smil\\+xml',\n                webpage, 'smil url', group='url')\n            path = self._search_regex(\n                r'link\\.theplatform\\.com/s/((?:[^/?#&]+/)+[^/?#&]+)', smil_url, 'path')\n            smil_url += '?' if '?' not in smil_url else '&' + 'formats=m3u,mpeg4'\n        elif mobj.group('config'):\n            config_url = url + '&form=json'\n            config_url = config_url.replace('swf/', 'config/')\n            config_url = config_url.replace('onsite/', 'onsite/config/')\n            config = self._download_json(config_url, video_id, 'Downloading config')\n            if 'releaseUrl' in config:\n                release_url = config['releaseUrl']\n            else:\n                release_url = 'http://link.theplatform.com/s/%s?mbr=true' % path\n            smil_url = release_url + '&formats=MPEG4&manifest=f4m'\n        else:\n            smil_url = 'http://link.theplatform.com/s/%s?mbr=true' % path\n\n        sig = smuggled_data.get('sig')\n        if sig:\n            smil_url = self._sign_url(smil_url, sig['key'], sig['secret'])\n\n        formats, subtitles = self._extract_theplatform_smil(smil_url, video_id)\n        self._sort_formats(formats)\n\n        ret = self._extract_theplatform_metadata(path, video_id)\n        combined_subtitles = self._merge_subtitles(ret.get('subtitles', {}), subtitles)\n        ret.update({\n            'id': video_id,\n            'formats': formats,\n            'subtitles': combined_subtitles,\n        })\n\n        return ret",
        "begin_line": 235,
        "end_line": 316,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.theplatform.ThePlatformFeedIE._extract_feed_info#343",
        "src_path": "youtube_dl/extractor/theplatform.py",
        "class_name": "youtube_dl.extractor.theplatform.ThePlatformFeedIE",
        "signature": "youtube_dl.extractor.theplatform.ThePlatformFeedIE._extract_feed_info(self, provider_id, feed_id, filter_query, video_id, custom_fields=None, asset_types_query={}, account_id=None)",
        "snippet": "    def _extract_feed_info(self, provider_id, feed_id, filter_query, video_id, custom_fields=None, asset_types_query={}, account_id=None):\n        real_url = self._URL_TEMPLATE % (self.http_scheme(), provider_id, feed_id, filter_query)\n        entry = self._download_json(real_url, video_id)['entries'][0]\n        main_smil_url = 'http://link.theplatform.com/s/%s/media/guid/%d/%s' % (provider_id, account_id, entry['guid']) if account_id else entry.get('plmedia$publicUrl')\n\n        formats = []\n        subtitles = {}\n        first_video_id = None\n        duration = None\n        asset_types = []\n        for item in entry['media$content']:\n            smil_url = item['plfile$url']\n            cur_video_id = ThePlatformIE._match_id(smil_url)\n            if first_video_id is None:\n                first_video_id = cur_video_id\n                duration = float_or_none(item.get('plfile$duration'))\n            file_asset_types = item.get('plfile$assetTypes') or compat_parse_qs(compat_urllib_parse_urlparse(smil_url).query)['assetTypes']\n            for asset_type in file_asset_types:\n                if asset_type in asset_types:\n                    continue\n                asset_types.append(asset_type)\n                query = {\n                    'mbr': 'true',\n                    'formats': item['plfile$format'],\n                    'assetTypes': asset_type,\n                }\n                if asset_type in asset_types_query:\n                    query.update(asset_types_query[asset_type])\n                cur_formats, cur_subtitles = self._extract_theplatform_smil(update_url_query(\n                    main_smil_url or smil_url, query), video_id, 'Downloading SMIL data for %s' % asset_type)\n                formats.extend(cur_formats)\n                subtitles = self._merge_subtitles(subtitles, cur_subtitles)\n\n        self._sort_formats(formats)\n\n        thumbnails = [{\n            'url': thumbnail['plfile$url'],\n            'width': int_or_none(thumbnail.get('plfile$width')),\n            'height': int_or_none(thumbnail.get('plfile$height')),\n        } for thumbnail in entry.get('media$thumbnails', [])]\n\n        timestamp = int_or_none(entry.get('media$availableDate'), scale=1000)\n        categories = [item['media$name'] for item in entry.get('media$categories', [])]\n\n        ret = self._extract_theplatform_metadata('%s/%s' % (provider_id, first_video_id), video_id)\n        subtitles = self._merge_subtitles(subtitles, ret['subtitles'])\n        ret.update({\n            'id': video_id,\n            'formats': formats,\n            'subtitles': subtitles,\n            'thumbnails': thumbnails,\n            'duration': duration,\n            'timestamp': timestamp,\n            'categories': categories,\n        })\n        if custom_fields:\n            ret.update(custom_fields(entry))\n\n        return ret",
        "begin_line": 343,
        "end_line": 401,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.theplatform.ThePlatformFeedIE._real_extract#403",
        "src_path": "youtube_dl/extractor/theplatform.py",
        "class_name": "youtube_dl.extractor.theplatform.ThePlatformFeedIE",
        "signature": "youtube_dl.extractor.theplatform.ThePlatformFeedIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n\n        video_id = mobj.group('id')\n        provider_id = mobj.group('provider_id')\n        feed_id = mobj.group('feed_id')\n        filter_query = mobj.group('filter')\n\n        return self._extract_feed_info(provider_id, feed_id, filter_query, video_id)",
        "begin_line": 403,
        "end_line": 411,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.safari.SafariCourseIE.suitable#243",
        "src_path": "youtube_dl/extractor/safari.py",
        "class_name": "youtube_dl.extractor.safari.SafariCourseIE",
        "signature": "youtube_dl.extractor.safari.SafariCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if SafariIE.suitable(url) or SafariApiIE.suitable(url)\n                else super(SafariCourseIE, cls).suitable(url))",
        "begin_line": 243,
        "end_line": 245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.kinja.KinjaEmbedIE._extract_urls#126",
        "src_path": "youtube_dl/extractor/kinja.py",
        "class_name": "youtube_dl.extractor.kinja.KinjaEmbedIE",
        "signature": "youtube_dl.extractor.kinja.KinjaEmbedIE._extract_urls(webpage, url)",
        "snippet": "    def _extract_urls(webpage, url):\n        return [urljoin(url, unescapeHTML(mobj.group('url'))) for mobj in re.finditer(\n            r'(?x)<iframe[^>]+?src=(?P<q>[\"\\'])(?P<url>(?:(?:https?:)?//%s)?%s(?:(?!\\1).)+)\\1' % (KinjaEmbedIE._DOMAIN_REGEX, KinjaEmbedIE._COMMON_REGEX),\n            webpage)]",
        "begin_line": 126,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00017812611328820805,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.tvanouvelles.TVANouvellesArticleIE.suitable#47",
        "src_path": "youtube_dl/extractor/tvanouvelles.py",
        "class_name": "youtube_dl.extractor.tvanouvelles.TVANouvellesArticleIE",
        "signature": "youtube_dl.extractor.tvanouvelles.TVANouvellesArticleIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if TVANouvellesIE.suitable(url) else super(TVANouvellesArticleIE, cls).suitable(url)",
        "begin_line": 47,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.daum.DaumClipIE.suitable#124",
        "src_path": "youtube_dl/extractor/daum.py",
        "class_name": "youtube_dl.extractor.daum.DaumClipIE",
        "signature": "youtube_dl.extractor.daum.DaumClipIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if DaumPlaylistIE.suitable(url) or DaumUserIE.suitable(url) else super(DaumClipIE, cls).suitable(url)",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030759766225776686,
            "pseudo_dstar_susp": 0.0003598416696653472,
            "pseudo_tarantula_susp": 0.0002757859900717044,
            "pseudo_op2_susp": 0.0003598416696653472,
            "pseudo_barinel_susp": 0.0002757859900717044
        }
    },
    {
        "name": "youtube_dl.extractor.daum.DaumPlaylistIE.suitable#197",
        "src_path": "youtube_dl/extractor/daum.py",
        "class_name": "youtube_dl.extractor.daum.DaumPlaylistIE",
        "signature": "youtube_dl.extractor.daum.DaumPlaylistIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if DaumUserIE.suitable(url) else super(DaumPlaylistIE, cls).suitable(url)",
        "begin_line": 197,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.zattoo.QuicklineLiveIE.suitable#233",
        "src_path": "youtube_dl/extractor/zattoo.py",
        "class_name": "youtube_dl.extractor.zattoo.QuicklineLiveIE",
        "signature": "youtube_dl.extractor.zattoo.QuicklineLiveIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if QuicklineIE.suitable(url) else super(QuicklineLiveIE, cls).suitable(url)",
        "begin_line": 233,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011788282447247436,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.zattoo.ZattooLiveIE.suitable#278",
        "src_path": "youtube_dl/extractor/zattoo.py",
        "class_name": "youtube_dl.extractor.zattoo.ZattooLiveIE",
        "signature": "youtube_dl.extractor.zattoo.ZattooLiveIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ZattooIE.suitable(url) else super(ZattooLiveIE, cls).suitable(url)",
        "begin_line": 278,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011810558639423645,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.f4m.remove_encrypted_media#240",
        "src_path": "youtube_dl/downloader/f4m.py",
        "class_name": "youtube_dl.downloader.f4m",
        "signature": "youtube_dl.downloader.f4m.remove_encrypted_media(media)",
        "snippet": "def remove_encrypted_media(media):\n    return list(filter(lambda e: 'drmAdditionalHeaderId' not in e.attrib\n                                 and 'drmAdditionalHeaderSetId' not in e.attrib,\n                       media))",
        "begin_line": 240,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.f4m._add_ns#246",
        "src_path": "youtube_dl/downloader/f4m.py",
        "class_name": "youtube_dl.downloader.f4m",
        "signature": "youtube_dl.downloader.f4m._add_ns(prop, ver=1)",
        "snippet": "def _add_ns(prop, ver=1):\n    return '{http://ns.adobe.com/f4m/%d.0}%s' % (ver, prop)",
        "begin_line": 246,
        "end_line": 247,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.downloader.f4m.get_base_url#250",
        "src_path": "youtube_dl/downloader/f4m.py",
        "class_name": "youtube_dl.downloader.f4m",
        "signature": "youtube_dl.downloader.f4m.get_base_url(manifest)",
        "snippet": "def get_base_url(manifest):\n    base_url = xpath_text(\n        manifest, [_add_ns('baseURL'), _add_ns('baseURL', 2)],\n        'base URL', default=None)\n    if base_url:\n        base_url = base_url.strip()\n    return base_url",
        "begin_line": 250,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.beampro.BeamProLiveIE.suitable#59",
        "src_path": "youtube_dl/extractor/beampro.py",
        "class_name": "youtube_dl.extractor.beampro.BeamProLiveIE",
        "signature": "youtube_dl.extractor.beampro.BeamProLiveIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if BeamProVodIE.suitable(url) else super(BeamProLiveIE, cls).suitable(url)",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.__init__#55",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.__init__(self, downloader=None)",
        "snippet": "    def __init__(self, downloader=None):\n        PostProcessor.__init__(self, downloader)\n        self._determine_executables()",
        "begin_line": 55,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_versions#72",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_versions(downloader=None)",
        "snippet": "    def get_versions(downloader=None):\n        return FFmpegPostProcessor(downloader)._versions",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor._determine_executables#75",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor._determine_executables(self)",
        "snippet": "    def _determine_executables(self):\n        programs = ['avprobe', 'avconv', 'ffmpeg', 'ffprobe']\n        prefer_ffmpeg = True\n\n        def get_ffmpeg_version(path):\n            ver = get_exe_version(path, args=['-version'])\n            if ver:\n                regexs = [\n                    r'(?:\\d+:)?([0-9.]+)-[0-9]+ubuntu[0-9.]+$',  # Ubuntu, see [1]\n                    r'n([0-9.]+)$',  # Arch Linux\n                    # 1. http://www.ducea.com/2006/06/17/ubuntu-package-version-naming-explanation/\n                ]\n                for regex in regexs:\n                    mobj = re.match(regex, ver)\n                    if mobj:\n                        ver = mobj.group(1)\n            return ver\n\n        self.basename = None\n        self.probe_basename = None\n\n        self._paths = None\n        self._versions = None\n        if self._downloader:\n            prefer_ffmpeg = self._downloader.params.get('prefer_ffmpeg', True)\n            location = self._downloader.params.get('ffmpeg_location')\n            if location is not None:\n                if not os.path.exists(location):\n                    self._downloader.report_warning(\n                        'ffmpeg-location %s does not exist! '\n                        'Continuing without avconv/ffmpeg.' % (location))\n                    self._versions = {}\n                    return\n                elif not os.path.isdir(location):\n                    basename = os.path.splitext(os.path.basename(location))[0]\n                    if basename not in programs:\n                        self._downloader.report_warning(\n                            'Cannot identify executable %s, its basename should be one of %s. '\n                            'Continuing without avconv/ffmpeg.' %\n                            (location, ', '.join(programs)))\n                        self._versions = {}\n                        return None\n                    location = os.path.dirname(os.path.abspath(location))\n                    if basename in ('ffmpeg', 'ffprobe'):\n                        prefer_ffmpeg = True\n\n                self._paths = dict(\n                    (p, os.path.join(location, p)) for p in programs)\n                self._versions = dict(\n                    (p, get_ffmpeg_version(self._paths[p])) for p in programs)\n        if self._versions is None:\n            self._versions = dict(\n                (p, get_ffmpeg_version(p)) for p in programs)\n            self._paths = dict((p, p) for p in programs)\n\n        if prefer_ffmpeg is False:\n            prefs = ('avconv', 'ffmpeg')\n        else:\n            prefs = ('ffmpeg', 'avconv')\n        for p in prefs:\n            if self._versions[p]:\n                self.basename = p\n                break\n\n        if prefer_ffmpeg is False:\n            prefs = ('avprobe', 'ffprobe')\n        else:\n            prefs = ('ffprobe', 'avprobe')\n        for p in prefs:\n            if self._versions[p]:\n                self.probe_basename = p\n                break",
        "begin_line": 75,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004033884630899556,
            "pseudo_dstar_susp": 0.00035829451809387314,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00035829451809387314,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_ffmpeg_version#79",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_ffmpeg_version(path)",
        "snippet": "        def get_ffmpeg_version(path):\n            ver = get_exe_version(path, args=['-version'])\n            if ver:\n                regexs = [\n                    r'(?:\\d+:)?([0-9.]+)-[0-9]+ubuntu[0-9.]+$',  # Ubuntu, see [1]\n                    r'n([0-9.]+)$',  # Arch Linux\n                    # 1. http://www.ducea.com/2006/06/17/ubuntu-package-version-naming-explanation/\n                ]\n                for regex in regexs:\n                    mobj = re.match(regex, ver)\n                    if mobj:\n                        ver = mobj.group(1)\n            return ver",
        "begin_line": 79,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004033884630899556,
            "pseudo_dstar_susp": 0.00035829451809387314,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00035829451809387314,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegMetadataPP.run#434",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegMetadataPP",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegMetadataPP.run(self, info)",
        "snippet": "    def run(self, info):\n        metadata = {}\n\n        def add(meta_list, info_list=None):\n            if not info_list:\n                info_list = meta_list\n            if not isinstance(meta_list, (list, tuple)):\n                meta_list = (meta_list,)\n            if not isinstance(info_list, (list, tuple)):\n                info_list = (info_list,)\n            for info_f in info_list:\n                if info.get(info_f) is not None:\n                    for meta_f in meta_list:\n                        metadata[meta_f] = info[info_f]\n                    break\n\n        # See [1-4] for some info on media metadata/metadata supported\n        # by ffmpeg.\n        # 1. https://kdenlive.org/en/project/adding-meta-data-to-mp4-video/\n        # 2. https://wiki.multimedia.cx/index.php/FFmpeg_Metadata\n        # 3. https://kodi.wiki/view/Video_file_tagging\n        # 4. http://atomicparsley.sourceforge.net/mpeg-4files.html\n\n        add('title', ('track', 'title'))\n        add('date', 'upload_date')\n        add(('description', 'comment'), 'description')\n        add('purl', 'webpage_url')\n        add('track', 'track_number')\n        add('artist', ('artist', 'creator', 'uploader', 'uploader_id'))\n        add('genre')\n        add('album')\n        add('album_artist')\n        add('disc', 'disc_number')\n        add('show', 'series')\n        add('season_number')\n        add('episode_id', ('episode', 'episode_id'))\n        add('episode_sort', 'episode_number')\n\n        if not metadata:\n            self._downloader.to_screen('[ffmpeg] There isn\\'t any metadata to add')\n            return [], info\n\n        filename = info['filepath']\n        temp_filename = prepend_extension(filename, 'temp')\n        in_filenames = [filename]\n        options = []\n\n        if info['ext'] == 'm4a':\n            options.extend(['-vn', '-acodec', 'copy'])\n        else:\n            options.extend(['-c', 'copy'])\n\n        for (name, value) in metadata.items():\n            options.extend(['-metadata', '%s=%s' % (name, value)])\n\n        chapters = info.get('chapters', [])\n        if chapters:\n            metadata_filename = replace_extension(filename, 'meta')\n            with io.open(metadata_filename, 'wt', encoding='utf-8') as f:\n                def ffmpeg_escape(text):\n                    return re.sub(r'(=|;|#|\\\\|\\n)', r'\\\\\\1', text)\n\n                metadata_file_content = ';FFMETADATA1\\n'\n                for chapter in chapters:\n                    metadata_file_content += '[CHAPTER]\\nTIMEBASE=1/1000\\n'\n                    metadata_file_content += 'START=%d\\n' % (chapter['start_time'] * 1000)\n                    metadata_file_content += 'END=%d\\n' % (chapter['end_time'] * 1000)\n                    chapter_title = chapter.get('title')\n                    if chapter_title:\n                        metadata_file_content += 'title=%s\\n' % ffmpeg_escape(chapter_title)\n                f.write(metadata_file_content)\n                in_filenames.append(metadata_filename)\n                options.extend(['-map_metadata', '1'])\n\n        self._downloader.to_screen('[ffmpeg] Adding metadata to \\'%s\\'' % filename)\n        self.run_ffmpeg_multiple_files(in_filenames, temp_filename, options)\n        if chapters:\n            os.remove(metadata_filename)\n        os.remove(encodeFilename(filename))\n        os.rename(encodeFilename(temp_filename), encodeFilename(filename))\n        return [], info",
        "begin_line": 434,
        "end_line": 514,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegMergerPP.can_merge#527",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegMergerPP",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegMergerPP.can_merge(self)",
        "snippet": "    def can_merge(self):\n        # TODO: figure out merge-capable ffmpeg version\n        if self.basename != 'avconv':\n            return True\n\n        required_version = '10-0'\n        if is_outdated_version(\n                self._versions[self.basename], required_version):\n            warning = ('Your copy of %s is outdated and unable to properly mux separate video and audio files, '\n                       'youtube-dl will download single file media. '\n                       'Update %s to version %s or newer to fix this.') % (\n                           self.basename, self.basename, required_version)\n            if self._downloader:\n                self._downloader.report_warning(warning)\n            return False\n        return True",
        "begin_line": 527,
        "end_line": 542,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM4aPP.run#565",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM4aPP",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM4aPP.run(self, info)",
        "snippet": "    def run(self, info):\n        if info.get('container') != 'm4a_dash':\n            return [], info\n\n        filename = info['filepath']\n        temp_filename = prepend_extension(filename, 'temp')\n\n        options = ['-c', 'copy', '-f', 'mp4']\n        self._downloader.to_screen('[ffmpeg] Correcting container in \"%s\"' % filename)\n        self.run_ffmpeg(filename, temp_filename, options)\n\n        os.remove(encodeFilename(filename))\n        os.rename(encodeFilename(temp_filename), encodeFilename(filename))\n\n        return [], info",
        "begin_line": 565,
        "end_line": 579,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM3u8PP.run#583",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM3u8PP",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegFixupM3u8PP.run(self, info)",
        "snippet": "    def run(self, info):\n        filename = info['filepath']\n        if self.get_audio_codec(filename) == 'aac':\n            temp_filename = prepend_extension(filename, 'temp')\n\n            options = ['-c', 'copy', '-f', 'mp4', '-bsf:a', 'aac_adtstoasc']\n            self._downloader.to_screen('[ffmpeg] Fixing malformed AAC bitstream in \"%s\"' % filename)\n            self.run_ffmpeg(filename, temp_filename, options)\n\n            os.remove(encodeFilename(filename))\n            os.rename(encodeFilename(temp_filename), encodeFilename(filename))\n        return [], info",
        "begin_line": 583,
        "end_line": 594,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nexx.NexxIE._extract_urls#116",
        "src_path": "youtube_dl/extractor/nexx.py",
        "class_name": "youtube_dl.extractor.nexx.NexxIE",
        "signature": "youtube_dl.extractor.nexx.NexxIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        # Reference:\n        # 1. https://nx-s.akamaized.net/files/201510/44.pdf\n\n        entries = []\n\n        # JavaScript Integration\n        domain_id = NexxIE._extract_domain_id(webpage)\n        if domain_id:\n            for video_id in re.findall(\n                    r'(?is)onPLAYReady.+?_play\\.(?:init|(?:control\\.)?addPlayer)\\s*\\(.+?\\s*,\\s*[\"\\']?(\\d+)',\n                    webpage):\n                entries.append(\n                    'https://api.nexx.cloud/v3/%s/videos/byid/%s'\n                    % (domain_id, video_id))\n\n        # TODO: support more embed formats\n\n        return entries",
        "begin_line": 116,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nexx.NexxEmbedIE._extract_urls#439",
        "src_path": "youtube_dl/extractor/nexx.py",
        "class_name": "youtube_dl.extractor.nexx.NexxEmbedIE",
        "signature": "youtube_dl.extractor.nexx.NexxEmbedIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        # Reference:\n        # 1. https://nx-s.akamaized.net/files/201510/44.pdf\n\n        # iFrame Embed Integration\n        return [mobj.group('url') for mobj in re.finditer(\n            r'<iframe[^>]+\\bsrc=([\"\\'])(?P<url>(?:https?:)?//embed\\.nexx(?:\\.cloud|cdn\\.com)/\\d+/(?:(?!\\1).)+)\\1',\n            webpage)]",
        "begin_line": 439,
        "end_line": 446,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000778816199376947,
            "pseudo_dstar_susp": 0.0007880220646178094,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0007880220646178094,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.register_socks_protocols#75",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.register_socks_protocols()",
        "snippet": "def register_socks_protocols():\n    # \"Register\" SOCKS protocols\n    # In Python < 2.6.5, urlsplit() suffers from bug https://bugs.python.org/issue7904\n    # URLs with protocols not in urlparse.uses_netloc are not handled correctly\n    for scheme in ('socks', 'socks4', 'socks4a', 'socks5'):\n        if scheme not in compat_urlparse.uses_netloc:\n            compat_urlparse.uses_netloc.append(scheme)",
        "begin_line": 75,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.08333333333333333,
            "pseudo_dstar_susp": 0.07142857142857142,
            "pseudo_tarantula_susp": 0.0009049773755656109,
            "pseudo_op2_susp": 0.07142857142857142,
            "pseudo_barinel_susp": 0.0009049773755656109
        }
    },
    {
        "name": "youtube_dl.utils.random_user_agent#88",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.random_user_agent()",
        "snippet": "def random_user_agent():\n    _USER_AGENT_TPL = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/%s Safari/537.36'\n    _CHROME_VERSIONS = (\n        '74.0.3729.129',\n        '76.0.3780.3',\n        '76.0.3780.2',\n        '74.0.3729.128',\n        '76.0.3780.1',\n        '76.0.3780.0',\n        '75.0.3770.15',\n        '74.0.3729.127',\n        '74.0.3729.126',\n        '76.0.3779.1',\n        '76.0.3779.0',\n        '75.0.3770.14',\n        '74.0.3729.125',\n        '76.0.3778.1',\n        '76.0.3778.0',\n        '75.0.3770.13',\n        '74.0.3729.124',\n        '74.0.3729.123',\n        '73.0.3683.121',\n        '76.0.3777.1',\n        '76.0.3777.0',\n        '75.0.3770.12',\n        '74.0.3729.122',\n        '76.0.3776.4',\n        '75.0.3770.11',\n        '74.0.3729.121',\n        '76.0.3776.3',\n        '76.0.3776.2',\n        '73.0.3683.120',\n        '74.0.3729.120',\n        '74.0.3729.119',\n        '74.0.3729.118',\n        '76.0.3776.1',\n        '76.0.3776.0',\n        '76.0.3775.5',\n        '75.0.3770.10',\n        '74.0.3729.117',\n        '76.0.3775.4',\n        '76.0.3775.3',\n        '74.0.3729.116',\n        '75.0.3770.9',\n        '76.0.3775.2',\n        '76.0.3775.1',\n        '76.0.3775.0',\n        '75.0.3770.8',\n        '74.0.3729.115',\n        '74.0.3729.114',\n        '76.0.3774.1',\n        '76.0.3774.0',\n        '75.0.3770.7',\n        '74.0.3729.113',\n        '74.0.3729.112',\n        '74.0.3729.111',\n        '76.0.3773.1',\n        '76.0.3773.0',\n        '75.0.3770.6',\n        '74.0.3729.110',\n        '74.0.3729.109',\n        '76.0.3772.1',\n        '76.0.3772.0',\n        '75.0.3770.5',\n        '74.0.3729.108',\n        '74.0.3729.107',\n        '76.0.3771.1',\n        '76.0.3771.0',\n        '75.0.3770.4',\n        '74.0.3729.106',\n        '74.0.3729.105',\n        '75.0.3770.3',\n        '74.0.3729.104',\n        '74.0.3729.103',\n        '74.0.3729.102',\n        '75.0.3770.2',\n        '74.0.3729.101',\n        '75.0.3770.1',\n        '75.0.3770.0',\n        '74.0.3729.100',\n        '75.0.3769.5',\n        '75.0.3769.4',\n        '74.0.3729.99',\n        '75.0.3769.3',\n        '75.0.3769.2',\n        '75.0.3768.6',\n        '74.0.3729.98',\n        '75.0.3769.1',\n        '75.0.3769.0',\n        '74.0.3729.97',\n        '73.0.3683.119',\n        '73.0.3683.118',\n        '74.0.3729.96',\n        '75.0.3768.5',\n        '75.0.3768.4',\n        '75.0.3768.3',\n        '75.0.3768.2',\n        '74.0.3729.95',\n        '74.0.3729.94',\n        '75.0.3768.1',\n        '75.0.3768.0',\n        '74.0.3729.93',\n        '74.0.3729.92',\n        '73.0.3683.117',\n        '74.0.3729.91',\n        '75.0.3766.3',\n        '74.0.3729.90',\n        '75.0.3767.2',\n        '75.0.3767.1',\n        '75.0.3767.0',\n        '74.0.3729.89',\n        '73.0.3683.116',\n        '75.0.3766.2',\n        '74.0.3729.88',\n        '75.0.3766.1',\n        '75.0.3766.0',\n        '74.0.3729.87',\n        '73.0.3683.115',\n        '74.0.3729.86',\n        '75.0.3765.1',\n        '75.0.3765.0',\n        '74.0.3729.85',\n        '73.0.3683.114',\n        '74.0.3729.84',\n        '75.0.3764.1',\n        '75.0.3764.0',\n        '74.0.3729.83',\n        '73.0.3683.113',\n        '75.0.3763.2',\n        '75.0.3761.4',\n        '74.0.3729.82',\n        '75.0.3763.1',\n        '75.0.3763.0',\n        '74.0.3729.81',\n        '73.0.3683.112',\n        '75.0.3762.1',\n        '75.0.3762.0',\n        '74.0.3729.80',\n        '75.0.3761.3',\n        '74.0.3729.79',\n        '73.0.3683.111',\n        '75.0.3761.2',\n        '74.0.3729.78',\n        '74.0.3729.77',\n        '75.0.3761.1',\n        '75.0.3761.0',\n        '73.0.3683.110',\n        '74.0.3729.76',\n        '74.0.3729.75',\n        '75.0.3760.0',\n        '74.0.3729.74',\n        '75.0.3759.8',\n        '75.0.3759.7',\n        '75.0.3759.6',\n        '74.0.3729.73',\n        '75.0.3759.5',\n        '74.0.3729.72',\n        '73.0.3683.109',\n        '75.0.3759.4',\n        '75.0.3759.3',\n        '74.0.3729.71',\n        '75.0.3759.2',\n        '74.0.3729.70',\n        '73.0.3683.108',\n        '74.0.3729.69',\n        '75.0.3759.1',\n        '75.0.3759.0',\n        '74.0.3729.68',\n        '73.0.3683.107',\n        '74.0.3729.67',\n        '75.0.3758.1',\n        '75.0.3758.0',\n        '74.0.3729.66',\n        '73.0.3683.106',\n        '74.0.3729.65',\n        '75.0.3757.1',\n        '75.0.3757.0',\n        '74.0.3729.64',\n        '73.0.3683.105',\n        '74.0.3729.63',\n        '75.0.3756.1',\n        '75.0.3756.0',\n        '74.0.3729.62',\n        '73.0.3683.104',\n        '75.0.3755.3',\n        '75.0.3755.2',\n        '73.0.3683.103',\n        '75.0.3755.1',\n        '75.0.3755.0',\n        '74.0.3729.61',\n        '73.0.3683.102',\n        '74.0.3729.60',\n        '75.0.3754.2',\n        '74.0.3729.59',\n        '75.0.3753.4',\n        '74.0.3729.58',\n        '75.0.3754.1',\n        '75.0.3754.0',\n        '74.0.3729.57',\n        '73.0.3683.101',\n        '75.0.3753.3',\n        '75.0.3752.2',\n        '75.0.3753.2',\n        '74.0.3729.56',\n        '75.0.3753.1',\n        '75.0.3753.0',\n        '74.0.3729.55',\n        '73.0.3683.100',\n        '74.0.3729.54',\n        '75.0.3752.1',\n        '75.0.3752.0',\n        '74.0.3729.53',\n        '73.0.3683.99',\n        '74.0.3729.52',\n        '75.0.3751.1',\n        '75.0.3751.0',\n        '74.0.3729.51',\n        '73.0.3683.98',\n        '74.0.3729.50',\n        '75.0.3750.0',\n        '74.0.3729.49',\n        '74.0.3729.48',\n        '74.0.3729.47',\n        '75.0.3749.3',\n        '74.0.3729.46',\n        '73.0.3683.97',\n        '75.0.3749.2',\n        '74.0.3729.45',\n        '75.0.3749.1',\n        '75.0.3749.0',\n        '74.0.3729.44',\n        '73.0.3683.96',\n        '74.0.3729.43',\n        '74.0.3729.42',\n        '75.0.3748.1',\n        '75.0.3748.0',\n        '74.0.3729.41',\n        '75.0.3747.1',\n        '73.0.3683.95',\n        '75.0.3746.4',\n        '74.0.3729.40',\n        '74.0.3729.39',\n        '75.0.3747.0',\n        '75.0.3746.3',\n        '75.0.3746.2',\n        '74.0.3729.38',\n        '75.0.3746.1',\n        '75.0.3746.0',\n        '74.0.3729.37',\n        '73.0.3683.94',\n        '75.0.3745.5',\n        '75.0.3745.4',\n        '75.0.3745.3',\n        '75.0.3745.2',\n        '74.0.3729.36',\n        '75.0.3745.1',\n        '75.0.3745.0',\n        '75.0.3744.2',\n        '74.0.3729.35',\n        '73.0.3683.93',\n        '74.0.3729.34',\n        '75.0.3744.1',\n        '75.0.3744.0',\n        '74.0.3729.33',\n        '73.0.3683.92',\n        '74.0.3729.32',\n        '74.0.3729.31',\n        '73.0.3683.91',\n        '75.0.3741.2',\n        '75.0.3740.5',\n        '74.0.3729.30',\n        '75.0.3741.1',\n        '75.0.3741.0',\n        '74.0.3729.29',\n        '75.0.3740.4',\n        '73.0.3683.90',\n        '74.0.3729.28',\n        '75.0.3740.3',\n        '73.0.3683.89',\n        '75.0.3740.2',\n        '74.0.3729.27',\n        '75.0.3740.1',\n        '75.0.3740.0',\n        '74.0.3729.26',\n        '73.0.3683.88',\n        '73.0.3683.87',\n        '74.0.3729.25',\n        '75.0.3739.1',\n        '75.0.3739.0',\n        '73.0.3683.86',\n        '74.0.3729.24',\n        '73.0.3683.85',\n        '75.0.3738.4',\n        '75.0.3738.3',\n        '75.0.3738.2',\n        '75.0.3738.1',\n        '75.0.3738.0',\n        '74.0.3729.23',\n        '73.0.3683.84',\n        '74.0.3729.22',\n        '74.0.3729.21',\n        '75.0.3737.1',\n        '75.0.3737.0',\n        '74.0.3729.20',\n        '73.0.3683.83',\n        '74.0.3729.19',\n        '75.0.3736.1',\n        '75.0.3736.0',\n        '74.0.3729.18',\n        '73.0.3683.82',\n        '74.0.3729.17',\n        '75.0.3735.1',\n        '75.0.3735.0',\n        '74.0.3729.16',\n        '73.0.3683.81',\n        '75.0.3734.1',\n        '75.0.3734.0',\n        '74.0.3729.15',\n        '73.0.3683.80',\n        '74.0.3729.14',\n        '75.0.3733.1',\n        '75.0.3733.0',\n        '75.0.3732.1',\n        '74.0.3729.13',\n        '74.0.3729.12',\n        '73.0.3683.79',\n        '74.0.3729.11',\n        '75.0.3732.0',\n        '74.0.3729.10',\n        '73.0.3683.78',\n        '74.0.3729.9',\n        '74.0.3729.8',\n        '74.0.3729.7',\n        '75.0.3731.3',\n        '75.0.3731.2',\n        '75.0.3731.0',\n        '74.0.3729.6',\n        '73.0.3683.77',\n        '73.0.3683.76',\n        '75.0.3730.5',\n        '75.0.3730.4',\n        '73.0.3683.75',\n        '74.0.3729.5',\n        '73.0.3683.74',\n        '75.0.3730.3',\n        '75.0.3730.2',\n        '74.0.3729.4',\n        '73.0.3683.73',\n        '73.0.3683.72',\n        '75.0.3730.1',\n        '75.0.3730.0',\n        '74.0.3729.3',\n        '73.0.3683.71',\n        '74.0.3729.2',\n        '73.0.3683.70',\n        '74.0.3729.1',\n        '74.0.3729.0',\n        '74.0.3726.4',\n        '73.0.3683.69',\n        '74.0.3726.3',\n        '74.0.3728.0',\n        '74.0.3726.2',\n        '73.0.3683.68',\n        '74.0.3726.1',\n        '74.0.3726.0',\n        '74.0.3725.4',\n        '73.0.3683.67',\n        '73.0.3683.66',\n        '74.0.3725.3',\n        '74.0.3725.2',\n        '74.0.3725.1',\n        '74.0.3724.8',\n        '74.0.3725.0',\n        '73.0.3683.65',\n        '74.0.3724.7',\n        '74.0.3724.6',\n        '74.0.3724.5',\n        '74.0.3724.4',\n        '74.0.3724.3',\n        '74.0.3724.2',\n        '74.0.3724.1',\n        '74.0.3724.0',\n        '73.0.3683.64',\n        '74.0.3723.1',\n        '74.0.3723.0',\n        '73.0.3683.63',\n        '74.0.3722.1',\n        '74.0.3722.0',\n        '73.0.3683.62',\n        '74.0.3718.9',\n        '74.0.3702.3',\n        '74.0.3721.3',\n        '74.0.3721.2',\n        '74.0.3721.1',\n        '74.0.3721.0',\n        '74.0.3720.6',\n        '73.0.3683.61',\n        '72.0.3626.122',\n        '73.0.3683.60',\n        '74.0.3720.5',\n        '72.0.3626.121',\n        '74.0.3718.8',\n        '74.0.3720.4',\n        '74.0.3720.3',\n        '74.0.3718.7',\n        '74.0.3720.2',\n        '74.0.3720.1',\n        '74.0.3720.0',\n        '74.0.3718.6',\n        '74.0.3719.5',\n        '73.0.3683.59',\n        '74.0.3718.5',\n        '74.0.3718.4',\n        '74.0.3719.4',\n        '74.0.3719.3',\n        '74.0.3719.2',\n        '74.0.3719.1',\n        '73.0.3683.58',\n        '74.0.3719.0',\n        '73.0.3683.57',\n        '73.0.3683.56',\n        '74.0.3718.3',\n        '73.0.3683.55',\n        '74.0.3718.2',\n        '74.0.3718.1',\n        '74.0.3718.0',\n        '73.0.3683.54',\n        '74.0.3717.2',\n        '73.0.3683.53',\n        '74.0.3717.1',\n        '74.0.3717.0',\n        '73.0.3683.52',\n        '74.0.3716.1',\n        '74.0.3716.0',\n        '73.0.3683.51',\n        '74.0.3715.1',\n        '74.0.3715.0',\n        '73.0.3683.50',\n        '74.0.3711.2',\n        '74.0.3714.2',\n        '74.0.3713.3',\n        '74.0.3714.1',\n        '74.0.3714.0',\n        '73.0.3683.49',\n        '74.0.3713.1',\n        '74.0.3713.0',\n        '72.0.3626.120',\n        '73.0.3683.48',\n        '74.0.3712.2',\n        '74.0.3712.1',\n        '74.0.3712.0',\n        '73.0.3683.47',\n        '72.0.3626.119',\n        '73.0.3683.46',\n        '74.0.3710.2',\n        '72.0.3626.118',\n        '74.0.3711.1',\n        '74.0.3711.0',\n        '73.0.3683.45',\n        '72.0.3626.117',\n        '74.0.3710.1',\n        '74.0.3710.0',\n        '73.0.3683.44',\n        '72.0.3626.116',\n        '74.0.3709.1',\n        '74.0.3709.0',\n        '74.0.3704.9',\n        '73.0.3683.43',\n        '72.0.3626.115',\n        '74.0.3704.8',\n        '74.0.3704.7',\n        '74.0.3708.0',\n        '74.0.3706.7',\n        '74.0.3704.6',\n        '73.0.3683.42',\n        '72.0.3626.114',\n        '74.0.3706.6',\n        '72.0.3626.113',\n        '74.0.3704.5',\n        '74.0.3706.5',\n        '74.0.3706.4',\n        '74.0.3706.3',\n        '74.0.3706.2',\n        '74.0.3706.1',\n        '74.0.3706.0',\n        '73.0.3683.41',\n        '72.0.3626.112',\n        '74.0.3705.1',\n        '74.0.3705.0',\n        '73.0.3683.40',\n        '72.0.3626.111',\n        '73.0.3683.39',\n        '74.0.3704.4',\n        '73.0.3683.38',\n        '74.0.3704.3',\n        '74.0.3704.2',\n        '74.0.3704.1',\n        '74.0.3704.0',\n        '73.0.3683.37',\n        '72.0.3626.110',\n        '72.0.3626.109',\n        '74.0.3703.3',\n        '74.0.3703.2',\n        '73.0.3683.36',\n        '74.0.3703.1',\n        '74.0.3703.0',\n        '73.0.3683.35',\n        '72.0.3626.108',\n        '74.0.3702.2',\n        '74.0.3699.3',\n        '74.0.3702.1',\n        '74.0.3702.0',\n        '73.0.3683.34',\n        '72.0.3626.107',\n        '73.0.3683.33',\n        '74.0.3701.1',\n        '74.0.3701.0',\n        '73.0.3683.32',\n        '73.0.3683.31',\n        '72.0.3626.105',\n        '74.0.3700.1',\n        '74.0.3700.0',\n        '73.0.3683.29',\n        '72.0.3626.103',\n        '74.0.3699.2',\n        '74.0.3699.1',\n        '74.0.3699.0',\n        '73.0.3683.28',\n        '72.0.3626.102',\n        '73.0.3683.27',\n        '73.0.3683.26',\n        '74.0.3698.0',\n        '74.0.3696.2',\n        '72.0.3626.101',\n        '73.0.3683.25',\n        '74.0.3696.1',\n        '74.0.3696.0',\n        '74.0.3694.8',\n        '72.0.3626.100',\n        '74.0.3694.7',\n        '74.0.3694.6',\n        '74.0.3694.5',\n        '74.0.3694.4',\n        '72.0.3626.99',\n        '72.0.3626.98',\n        '74.0.3694.3',\n        '73.0.3683.24',\n        '72.0.3626.97',\n        '72.0.3626.96',\n        '72.0.3626.95',\n        '73.0.3683.23',\n        '72.0.3626.94',\n        '73.0.3683.22',\n        '73.0.3683.21',\n        '72.0.3626.93',\n        '74.0.3694.2',\n        '72.0.3626.92',\n        '74.0.3694.1',\n        '74.0.3694.0',\n        '74.0.3693.6',\n        '73.0.3683.20',\n        '72.0.3626.91',\n        '74.0.3693.5',\n        '74.0.3693.4',\n        '74.0.3693.3',\n        '74.0.3693.2',\n        '73.0.3683.19',\n        '74.0.3693.1',\n        '74.0.3693.0',\n        '73.0.3683.18',\n        '72.0.3626.90',\n        '74.0.3692.1',\n        '74.0.3692.0',\n        '73.0.3683.17',\n        '72.0.3626.89',\n        '74.0.3687.3',\n        '74.0.3691.1',\n        '74.0.3691.0',\n        '73.0.3683.16',\n        '72.0.3626.88',\n        '72.0.3626.87',\n        '73.0.3683.15',\n        '74.0.3690.1',\n        '74.0.3690.0',\n        '73.0.3683.14',\n        '72.0.3626.86',\n        '73.0.3683.13',\n        '73.0.3683.12',\n        '74.0.3689.1',\n        '74.0.3689.0',\n        '73.0.3683.11',\n        '72.0.3626.85',\n        '73.0.3683.10',\n        '72.0.3626.84',\n        '73.0.3683.9',\n        '74.0.3688.1',\n        '74.0.3688.0',\n        '73.0.3683.8',\n        '72.0.3626.83',\n        '74.0.3687.2',\n        '74.0.3687.1',\n        '74.0.3687.0',\n        '73.0.3683.7',\n        '72.0.3626.82',\n        '74.0.3686.4',\n        '72.0.3626.81',\n        '74.0.3686.3',\n        '74.0.3686.2',\n        '74.0.3686.1',\n        '74.0.3686.0',\n        '73.0.3683.6',\n        '72.0.3626.80',\n        '74.0.3685.1',\n        '74.0.3685.0',\n        '73.0.3683.5',\n        '72.0.3626.79',\n        '74.0.3684.1',\n        '74.0.3684.0',\n        '73.0.3683.4',\n        '72.0.3626.78',\n        '72.0.3626.77',\n        '73.0.3683.3',\n        '73.0.3683.2',\n        '72.0.3626.76',\n        '73.0.3683.1',\n        '73.0.3683.0',\n        '72.0.3626.75',\n        '71.0.3578.141',\n        '73.0.3682.1',\n        '73.0.3682.0',\n        '72.0.3626.74',\n        '71.0.3578.140',\n        '73.0.3681.4',\n        '73.0.3681.3',\n        '73.0.3681.2',\n        '73.0.3681.1',\n        '73.0.3681.0',\n        '72.0.3626.73',\n        '71.0.3578.139',\n        '72.0.3626.72',\n        '72.0.3626.71',\n        '73.0.3680.1',\n        '73.0.3680.0',\n        '72.0.3626.70',\n        '71.0.3578.138',\n        '73.0.3678.2',\n        '73.0.3679.1',\n        '73.0.3679.0',\n        '72.0.3626.69',\n        '71.0.3578.137',\n        '73.0.3678.1',\n        '73.0.3678.0',\n        '71.0.3578.136',\n        '73.0.3677.1',\n        '73.0.3677.0',\n        '72.0.3626.68',\n        '72.0.3626.67',\n        '71.0.3578.135',\n        '73.0.3676.1',\n        '73.0.3676.0',\n        '73.0.3674.2',\n        '72.0.3626.66',\n        '71.0.3578.134',\n        '73.0.3674.1',\n        '73.0.3674.0',\n        '72.0.3626.65',\n        '71.0.3578.133',\n        '73.0.3673.2',\n        '73.0.3673.1',\n        '73.0.3673.0',\n        '72.0.3626.64',\n        '71.0.3578.132',\n        '72.0.3626.63',\n        '72.0.3626.62',\n        '72.0.3626.61',\n        '72.0.3626.60',\n        '73.0.3672.1',\n        '73.0.3672.0',\n        '72.0.3626.59',\n        '71.0.3578.131',\n        '73.0.3671.3',\n        '73.0.3671.2',\n        '73.0.3671.1',\n        '73.0.3671.0',\n        '72.0.3626.58',\n        '71.0.3578.130',\n        '73.0.3670.1',\n        '73.0.3670.0',\n        '72.0.3626.57',\n        '71.0.3578.129',\n        '73.0.3669.1',\n        '73.0.3669.0',\n        '72.0.3626.56',\n        '71.0.3578.128',\n        '73.0.3668.2',\n        '73.0.3668.1',\n        '73.0.3668.0',\n        '72.0.3626.55',\n        '71.0.3578.127',\n        '73.0.3667.2',\n        '73.0.3667.1',\n        '73.0.3667.0',\n        '72.0.3626.54',\n        '71.0.3578.126',\n        '73.0.3666.1',\n        '73.0.3666.0',\n        '72.0.3626.53',\n        '71.0.3578.125',\n        '73.0.3665.4',\n        '73.0.3665.3',\n        '72.0.3626.52',\n        '73.0.3665.2',\n        '73.0.3664.4',\n        '73.0.3665.1',\n        '73.0.3665.0',\n        '72.0.3626.51',\n        '71.0.3578.124',\n        '72.0.3626.50',\n        '73.0.3664.3',\n        '73.0.3664.2',\n        '73.0.3664.1',\n        '73.0.3664.0',\n        '73.0.3663.2',\n        '72.0.3626.49',\n        '71.0.3578.123',\n        '73.0.3663.1',\n        '73.0.3663.0',\n        '72.0.3626.48',\n        '71.0.3578.122',\n        '73.0.3662.1',\n        '73.0.3662.0',\n        '72.0.3626.47',\n        '71.0.3578.121',\n        '73.0.3661.1',\n        '72.0.3626.46',\n        '73.0.3661.0',\n        '72.0.3626.45',\n        '71.0.3578.120',\n        '73.0.3660.2',\n        '73.0.3660.1',\n        '73.0.3660.0',\n        '72.0.3626.44',\n        '71.0.3578.119',\n        '73.0.3659.1',\n        '73.0.3659.0',\n        '72.0.3626.43',\n        '71.0.3578.118',\n        '73.0.3658.1',\n        '73.0.3658.0',\n        '72.0.3626.42',\n        '71.0.3578.117',\n        '73.0.3657.1',\n        '73.0.3657.0',\n        '72.0.3626.41',\n        '71.0.3578.116',\n        '73.0.3656.1',\n        '73.0.3656.0',\n        '72.0.3626.40',\n        '71.0.3578.115',\n        '73.0.3655.1',\n        '73.0.3655.0',\n        '72.0.3626.39',\n        '71.0.3578.114',\n        '73.0.3654.1',\n        '73.0.3654.0',\n        '72.0.3626.38',\n        '71.0.3578.113',\n        '73.0.3653.1',\n        '73.0.3653.0',\n        '72.0.3626.37',\n        '71.0.3578.112',\n        '73.0.3652.1',\n        '73.0.3652.0',\n        '72.0.3626.36',\n        '71.0.3578.111',\n        '73.0.3651.1',\n        '73.0.3651.0',\n        '72.0.3626.35',\n        '71.0.3578.110',\n        '73.0.3650.1',\n        '73.0.3650.0',\n        '72.0.3626.34',\n        '71.0.3578.109',\n        '73.0.3649.1',\n        '73.0.3649.0',\n        '72.0.3626.33',\n        '71.0.3578.108',\n        '73.0.3648.2',\n        '73.0.3648.1',\n        '73.0.3648.0',\n        '72.0.3626.32',\n        '71.0.3578.107',\n        '73.0.3647.2',\n        '73.0.3647.1',\n        '73.0.3647.0',\n        '72.0.3626.31',\n        '71.0.3578.106',\n        '73.0.3635.3',\n        '73.0.3646.2',\n        '73.0.3646.1',\n        '73.0.3646.0',\n        '72.0.3626.30',\n        '71.0.3578.105',\n        '72.0.3626.29',\n        '73.0.3645.2',\n        '73.0.3645.1',\n        '73.0.3645.0',\n        '72.0.3626.28',\n        '71.0.3578.104',\n        '72.0.3626.27',\n        '72.0.3626.26',\n        '72.0.3626.25',\n        '72.0.3626.24',\n        '73.0.3644.0',\n        '73.0.3643.2',\n        '72.0.3626.23',\n        '71.0.3578.103',\n        '73.0.3643.1',\n        '73.0.3643.0',\n        '72.0.3626.22',\n        '71.0.3578.102',\n        '73.0.3642.1',\n        '73.0.3642.0',\n        '72.0.3626.21',\n        '71.0.3578.101',\n        '73.0.3641.1',\n        '73.0.3641.0',\n        '72.0.3626.20',\n        '71.0.3578.100',\n        '72.0.3626.19',\n        '73.0.3640.1',\n        '73.0.3640.0',\n        '72.0.3626.18',\n        '73.0.3639.1',\n        '71.0.3578.99',\n        '73.0.3639.0',\n        '72.0.3626.17',\n        '73.0.3638.2',\n        '72.0.3626.16',\n        '73.0.3638.1',\n        '73.0.3638.0',\n        '72.0.3626.15',\n        '71.0.3578.98',\n        '73.0.3635.2',\n        '71.0.3578.97',\n        '73.0.3637.1',\n        '73.0.3637.0',\n        '72.0.3626.14',\n        '71.0.3578.96',\n        '71.0.3578.95',\n        '72.0.3626.13',\n        '71.0.3578.94',\n        '73.0.3636.2',\n        '71.0.3578.93',\n        '73.0.3636.1',\n        '73.0.3636.0',\n        '72.0.3626.12',\n        '71.0.3578.92',\n        '73.0.3635.1',\n        '73.0.3635.0',\n        '72.0.3626.11',\n        '71.0.3578.91',\n        '73.0.3634.2',\n        '73.0.3634.1',\n        '73.0.3634.0',\n        '72.0.3626.10',\n        '71.0.3578.90',\n        '71.0.3578.89',\n        '73.0.3633.2',\n        '73.0.3633.1',\n        '73.0.3633.0',\n        '72.0.3610.4',\n        '72.0.3626.9',\n        '71.0.3578.88',\n        '73.0.3632.5',\n        '73.0.3632.4',\n        '73.0.3632.3',\n        '73.0.3632.2',\n        '73.0.3632.1',\n        '73.0.3632.0',\n        '72.0.3626.8',\n        '71.0.3578.87',\n        '73.0.3631.2',\n        '73.0.3631.1',\n        '73.0.3631.0',\n        '72.0.3626.7',\n        '71.0.3578.86',\n        '72.0.3626.6',\n        '73.0.3630.1',\n        '73.0.3630.0',\n        '72.0.3626.5',\n        '71.0.3578.85',\n        '72.0.3626.4',\n        '73.0.3628.3',\n        '73.0.3628.2',\n        '73.0.3629.1',\n        '73.0.3629.0',\n        '72.0.3626.3',\n        '71.0.3578.84',\n        '73.0.3628.1',\n        '73.0.3628.0',\n        '71.0.3578.83',\n        '73.0.3627.1',\n        '73.0.3627.0',\n        '72.0.3626.2',\n        '71.0.3578.82',\n        '71.0.3578.81',\n        '71.0.3578.80',\n        '72.0.3626.1',\n        '72.0.3626.0',\n        '71.0.3578.79',\n        '70.0.3538.124',\n        '71.0.3578.78',\n        '72.0.3623.4',\n        '72.0.3625.2',\n        '72.0.3625.1',\n        '72.0.3625.0',\n        '71.0.3578.77',\n        '70.0.3538.123',\n        '72.0.3624.4',\n        '72.0.3624.3',\n        '72.0.3624.2',\n        '71.0.3578.76',\n        '72.0.3624.1',\n        '72.0.3624.0',\n        '72.0.3623.3',\n        '71.0.3578.75',\n        '70.0.3538.122',\n        '71.0.3578.74',\n        '72.0.3623.2',\n        '72.0.3610.3',\n        '72.0.3623.1',\n        '72.0.3623.0',\n        '72.0.3622.3',\n        '72.0.3622.2',\n        '71.0.3578.73',\n        '70.0.3538.121',\n        '72.0.3622.1',\n        '72.0.3622.0',\n        '71.0.3578.72',\n        '70.0.3538.120',\n        '72.0.3621.1',\n        '72.0.3621.0',\n        '71.0.3578.71',\n        '70.0.3538.119',\n        '72.0.3620.1',\n        '72.0.3620.0',\n        '71.0.3578.70',\n        '70.0.3538.118',\n        '71.0.3578.69',\n        '72.0.3619.1',\n        '72.0.3619.0',\n        '71.0.3578.68',\n        '70.0.3538.117',\n        '71.0.3578.67',\n        '72.0.3618.1',\n        '72.0.3618.0',\n        '71.0.3578.66',\n        '70.0.3538.116',\n        '72.0.3617.1',\n        '72.0.3617.0',\n        '71.0.3578.65',\n        '70.0.3538.115',\n        '72.0.3602.3',\n        '71.0.3578.64',\n        '72.0.3616.1',\n        '72.0.3616.0',\n        '71.0.3578.63',\n        '70.0.3538.114',\n        '71.0.3578.62',\n        '72.0.3615.1',\n        '72.0.3615.0',\n        '71.0.3578.61',\n        '70.0.3538.113',\n        '72.0.3614.1',\n        '72.0.3614.0',\n        '71.0.3578.60',\n        '70.0.3538.112',\n        '72.0.3613.1',\n        '72.0.3613.0',\n        '71.0.3578.59',\n        '70.0.3538.111',\n        '72.0.3612.2',\n        '72.0.3612.1',\n        '72.0.3612.0',\n        '70.0.3538.110',\n        '71.0.3578.58',\n        '70.0.3538.109',\n        '72.0.3611.2',\n        '72.0.3611.1',\n        '72.0.3611.0',\n        '71.0.3578.57',\n        '70.0.3538.108',\n        '72.0.3610.2',\n        '71.0.3578.56',\n        '71.0.3578.55',\n        '72.0.3610.1',\n        '72.0.3610.0',\n        '71.0.3578.54',\n        '70.0.3538.107',\n        '71.0.3578.53',\n        '72.0.3609.3',\n        '71.0.3578.52',\n        '72.0.3609.2',\n        '71.0.3578.51',\n        '72.0.3608.5',\n        '72.0.3609.1',\n        '72.0.3609.0',\n        '71.0.3578.50',\n        '70.0.3538.106',\n        '72.0.3608.4',\n        '72.0.3608.3',\n        '72.0.3608.2',\n        '71.0.3578.49',\n        '72.0.3608.1',\n        '72.0.3608.0',\n        '70.0.3538.105',\n        '71.0.3578.48',\n        '72.0.3607.1',\n        '72.0.3607.0',\n        '71.0.3578.47',\n        '70.0.3538.104',\n        '72.0.3606.2',\n        '72.0.3606.1',\n        '72.0.3606.0',\n        '71.0.3578.46',\n        '70.0.3538.103',\n        '70.0.3538.102',\n        '72.0.3605.3',\n        '72.0.3605.2',\n        '72.0.3605.1',\n        '72.0.3605.0',\n        '71.0.3578.45',\n        '70.0.3538.101',\n        '71.0.3578.44',\n        '71.0.3578.43',\n        '70.0.3538.100',\n        '70.0.3538.99',\n        '71.0.3578.42',\n        '72.0.3604.1',\n        '72.0.3604.0',\n        '71.0.3578.41',\n        '70.0.3538.98',\n        '71.0.3578.40',\n        '72.0.3603.2',\n        '72.0.3603.1',\n        '72.0.3603.0',\n        '71.0.3578.39',\n        '70.0.3538.97',\n        '72.0.3602.2',\n        '71.0.3578.38',\n        '71.0.3578.37',\n        '72.0.3602.1',\n        '72.0.3602.0',\n        '71.0.3578.36',\n        '70.0.3538.96',\n        '72.0.3601.1',\n        '72.0.3601.0',\n        '71.0.3578.35',\n        '70.0.3538.95',\n        '72.0.3600.1',\n        '72.0.3600.0',\n        '71.0.3578.34',\n        '70.0.3538.94',\n        '72.0.3599.3',\n        '72.0.3599.2',\n        '72.0.3599.1',\n        '72.0.3599.0',\n        '71.0.3578.33',\n        '70.0.3538.93',\n        '72.0.3598.1',\n        '72.0.3598.0',\n        '71.0.3578.32',\n        '70.0.3538.87',\n        '72.0.3597.1',\n        '72.0.3597.0',\n        '72.0.3596.2',\n        '71.0.3578.31',\n        '70.0.3538.86',\n        '71.0.3578.30',\n        '71.0.3578.29',\n        '72.0.3596.1',\n        '72.0.3596.0',\n        '71.0.3578.28',\n        '70.0.3538.85',\n        '72.0.3595.2',\n        '72.0.3591.3',\n        '72.0.3595.1',\n        '72.0.3595.0',\n        '71.0.3578.27',\n        '70.0.3538.84',\n        '72.0.3594.1',\n        '72.0.3594.0',\n        '71.0.3578.26',\n        '70.0.3538.83',\n        '72.0.3593.2',\n        '72.0.3593.1',\n        '72.0.3593.0',\n        '71.0.3578.25',\n        '70.0.3538.82',\n        '72.0.3589.3',\n        '72.0.3592.2',\n        '72.0.3592.1',\n        '72.0.3592.0',\n        '71.0.3578.24',\n        '72.0.3589.2',\n        '70.0.3538.81',\n        '70.0.3538.80',\n        '72.0.3591.2',\n        '72.0.3591.1',\n        '72.0.3591.0',\n        '71.0.3578.23',\n        '70.0.3538.79',\n        '71.0.3578.22',\n        '72.0.3590.1',\n        '72.0.3590.0',\n        '71.0.3578.21',\n        '70.0.3538.78',\n        '70.0.3538.77',\n        '72.0.3589.1',\n        '72.0.3589.0',\n        '71.0.3578.20',\n        '70.0.3538.76',\n        '71.0.3578.19',\n        '70.0.3538.75',\n        '72.0.3588.1',\n        '72.0.3588.0',\n        '71.0.3578.18',\n        '70.0.3538.74',\n        '72.0.3586.2',\n        '72.0.3587.0',\n        '71.0.3578.17',\n        '70.0.3538.73',\n        '72.0.3586.1',\n        '72.0.3586.0',\n        '71.0.3578.16',\n        '70.0.3538.72',\n        '72.0.3585.1',\n        '72.0.3585.0',\n        '71.0.3578.15',\n        '70.0.3538.71',\n        '71.0.3578.14',\n        '72.0.3584.1',\n        '72.0.3584.0',\n        '71.0.3578.13',\n        '70.0.3538.70',\n        '72.0.3583.2',\n        '71.0.3578.12',\n        '72.0.3583.1',\n        '72.0.3583.0',\n        '71.0.3578.11',\n        '70.0.3538.69',\n        '71.0.3578.10',\n        '72.0.3582.0',\n        '72.0.3581.4',\n        '71.0.3578.9',\n        '70.0.3538.67',\n        '72.0.3581.3',\n        '72.0.3581.2',\n        '72.0.3581.1',\n        '72.0.3581.0',\n        '71.0.3578.8',\n        '70.0.3538.66',\n        '72.0.3580.1',\n        '72.0.3580.0',\n        '71.0.3578.7',\n        '70.0.3538.65',\n        '71.0.3578.6',\n        '72.0.3579.1',\n        '72.0.3579.0',\n        '71.0.3578.5',\n        '70.0.3538.64',\n        '71.0.3578.4',\n        '71.0.3578.3',\n        '71.0.3578.2',\n        '71.0.3578.1',\n        '71.0.3578.0',\n        '70.0.3538.63',\n        '69.0.3497.128',\n        '70.0.3538.62',\n        '70.0.3538.61',\n        '70.0.3538.60',\n        '70.0.3538.59',\n        '71.0.3577.1',\n        '71.0.3577.0',\n        '70.0.3538.58',\n        '69.0.3497.127',\n        '71.0.3576.2',\n        '71.0.3576.1',\n        '71.0.3576.0',\n        '70.0.3538.57',\n        '70.0.3538.56',\n        '71.0.3575.2',\n        '70.0.3538.55',\n        '69.0.3497.126',\n        '70.0.3538.54',\n        '71.0.3575.1',\n        '71.0.3575.0',\n        '71.0.3574.1',\n        '71.0.3574.0',\n        '70.0.3538.53',\n        '69.0.3497.125',\n        '70.0.3538.52',\n        '71.0.3573.1',\n        '71.0.3573.0',\n        '70.0.3538.51',\n        '69.0.3497.124',\n        '71.0.3572.1',\n        '71.0.3572.0',\n        '70.0.3538.50',\n        '69.0.3497.123',\n        '71.0.3571.2',\n        '70.0.3538.49',\n        '69.0.3497.122',\n        '71.0.3571.1',\n        '71.0.3571.0',\n        '70.0.3538.48',\n        '69.0.3497.121',\n        '71.0.3570.1',\n        '71.0.3570.0',\n        '70.0.3538.47',\n        '69.0.3497.120',\n        '71.0.3568.2',\n        '71.0.3569.1',\n        '71.0.3569.0',\n        '70.0.3538.46',\n        '69.0.3497.119',\n        '70.0.3538.45',\n        '71.0.3568.1',\n        '71.0.3568.0',\n        '70.0.3538.44',\n        '69.0.3497.118',\n        '70.0.3538.43',\n        '70.0.3538.42',\n        '71.0.3567.1',\n        '71.0.3567.0',\n        '70.0.3538.41',\n        '69.0.3497.117',\n        '71.0.3566.1',\n        '71.0.3566.0',\n        '70.0.3538.40',\n        '69.0.3497.116',\n        '71.0.3565.1',\n        '71.0.3565.0',\n        '70.0.3538.39',\n        '69.0.3497.115',\n        '71.0.3564.1',\n        '71.0.3564.0',\n        '70.0.3538.38',\n        '69.0.3497.114',\n        '71.0.3563.0',\n        '71.0.3562.2',\n        '70.0.3538.37',\n        '69.0.3497.113',\n        '70.0.3538.36',\n        '70.0.3538.35',\n        '71.0.3562.1',\n        '71.0.3562.0',\n        '70.0.3538.34',\n        '69.0.3497.112',\n        '70.0.3538.33',\n        '71.0.3561.1',\n        '71.0.3561.0',\n        '70.0.3538.32',\n        '69.0.3497.111',\n        '71.0.3559.6',\n        '71.0.3560.1',\n        '71.0.3560.0',\n        '71.0.3559.5',\n        '71.0.3559.4',\n        '70.0.3538.31',\n        '69.0.3497.110',\n        '71.0.3559.3',\n        '70.0.3538.30',\n        '69.0.3497.109',\n        '71.0.3559.2',\n        '71.0.3559.1',\n        '71.0.3559.0',\n        '70.0.3538.29',\n        '69.0.3497.108',\n        '71.0.3558.2',\n        '71.0.3558.1',\n        '71.0.3558.0',\n        '70.0.3538.28',\n        '69.0.3497.107',\n        '71.0.3557.2',\n        '71.0.3557.1',\n        '71.0.3557.0',\n        '70.0.3538.27',\n        '69.0.3497.106',\n        '71.0.3554.4',\n        '70.0.3538.26',\n        '71.0.3556.1',\n        '71.0.3556.0',\n        '70.0.3538.25',\n        '71.0.3554.3',\n        '69.0.3497.105',\n        '71.0.3554.2',\n        '70.0.3538.24',\n        '69.0.3497.104',\n        '71.0.3555.2',\n        '70.0.3538.23',\n        '71.0.3555.1',\n        '71.0.3555.0',\n        '70.0.3538.22',\n        '69.0.3497.103',\n        '71.0.3554.1',\n        '71.0.3554.0',\n        '70.0.3538.21',\n        '69.0.3497.102',\n        '71.0.3553.3',\n        '70.0.3538.20',\n        '69.0.3497.101',\n        '71.0.3553.2',\n        '69.0.3497.100',\n        '71.0.3553.1',\n        '71.0.3553.0',\n        '70.0.3538.19',\n        '69.0.3497.99',\n        '69.0.3497.98',\n        '69.0.3497.97',\n        '71.0.3552.6',\n        '71.0.3552.5',\n        '71.0.3552.4',\n        '71.0.3552.3',\n        '71.0.3552.2',\n        '71.0.3552.1',\n        '71.0.3552.0',\n        '70.0.3538.18',\n        '69.0.3497.96',\n        '71.0.3551.3',\n        '71.0.3551.2',\n        '71.0.3551.1',\n        '71.0.3551.0',\n        '70.0.3538.17',\n        '69.0.3497.95',\n        '71.0.3550.3',\n        '71.0.3550.2',\n        '71.0.3550.1',\n        '71.0.3550.0',\n        '70.0.3538.16',\n        '69.0.3497.94',\n        '71.0.3549.1',\n        '71.0.3549.0',\n        '70.0.3538.15',\n        '69.0.3497.93',\n        '69.0.3497.92',\n        '71.0.3548.1',\n        '71.0.3548.0',\n        '70.0.3538.14',\n        '69.0.3497.91',\n        '71.0.3547.1',\n        '71.0.3547.0',\n        '70.0.3538.13',\n        '69.0.3497.90',\n        '71.0.3546.2',\n        '69.0.3497.89',\n        '71.0.3546.1',\n        '71.0.3546.0',\n        '70.0.3538.12',\n        '69.0.3497.88',\n        '71.0.3545.4',\n        '71.0.3545.3',\n        '71.0.3545.2',\n        '71.0.3545.1',\n        '71.0.3545.0',\n        '70.0.3538.11',\n        '69.0.3497.87',\n        '71.0.3544.5',\n        '71.0.3544.4',\n        '71.0.3544.3',\n        '71.0.3544.2',\n        '71.0.3544.1',\n        '71.0.3544.0',\n        '69.0.3497.86',\n        '70.0.3538.10',\n        '69.0.3497.85',\n        '70.0.3538.9',\n        '69.0.3497.84',\n        '71.0.3543.4',\n        '70.0.3538.8',\n        '71.0.3543.3',\n        '71.0.3543.2',\n        '71.0.3543.1',\n        '71.0.3543.0',\n        '70.0.3538.7',\n        '69.0.3497.83',\n        '71.0.3542.2',\n        '71.0.3542.1',\n        '71.0.3542.0',\n        '70.0.3538.6',\n        '69.0.3497.82',\n        '69.0.3497.81',\n        '71.0.3541.1',\n        '71.0.3541.0',\n        '70.0.3538.5',\n        '69.0.3497.80',\n        '71.0.3540.1',\n        '71.0.3540.0',\n        '70.0.3538.4',\n        '69.0.3497.79',\n        '70.0.3538.3',\n        '71.0.3539.1',\n        '71.0.3539.0',\n        '69.0.3497.78',\n        '68.0.3440.134',\n        '69.0.3497.77',\n        '70.0.3538.2',\n        '70.0.3538.1',\n        '70.0.3538.0',\n        '69.0.3497.76',\n        '68.0.3440.133',\n        '69.0.3497.75',\n        '70.0.3537.2',\n        '70.0.3537.1',\n        '70.0.3537.0',\n        '69.0.3497.74',\n        '68.0.3440.132',\n        '70.0.3536.0',\n        '70.0.3535.5',\n        '70.0.3535.4',\n        '70.0.3535.3',\n        '69.0.3497.73',\n        '68.0.3440.131',\n        '70.0.3532.8',\n        '70.0.3532.7',\n        '69.0.3497.72',\n        '69.0.3497.71',\n        '70.0.3535.2',\n        '70.0.3535.1',\n        '70.0.3535.0',\n        '69.0.3497.70',\n        '68.0.3440.130',\n        '69.0.3497.69',\n        '68.0.3440.129',\n        '70.0.3534.4',\n        '70.0.3534.3',\n        '70.0.3534.2',\n        '70.0.3534.1',\n        '70.0.3534.0',\n        '69.0.3497.68',\n        '68.0.3440.128',\n        '70.0.3533.2',\n        '70.0.3533.1',\n        '70.0.3533.0',\n        '69.0.3497.67',\n        '68.0.3440.127',\n        '70.0.3532.6',\n        '70.0.3532.5',\n        '70.0.3532.4',\n        '69.0.3497.66',\n        '68.0.3440.126',\n        '70.0.3532.3',\n        '70.0.3532.2',\n        '70.0.3532.1',\n        '69.0.3497.60',\n        '69.0.3497.65',\n        '69.0.3497.64',\n        '70.0.3532.0',\n        '70.0.3531.0',\n        '70.0.3530.4',\n        '70.0.3530.3',\n        '70.0.3530.2',\n        '69.0.3497.58',\n        '68.0.3440.125',\n        '69.0.3497.57',\n        '69.0.3497.56',\n        '69.0.3497.55',\n        '69.0.3497.54',\n        '70.0.3530.1',\n        '70.0.3530.0',\n        '69.0.3497.53',\n        '68.0.3440.124',\n        '69.0.3497.52',\n        '70.0.3529.3',\n        '70.0.3529.2',\n        '70.0.3529.1',\n        '70.0.3529.0',\n        '69.0.3497.51',\n        '70.0.3528.4',\n        '68.0.3440.123',\n        '70.0.3528.3',\n        '70.0.3528.2',\n        '70.0.3528.1',\n        '70.0.3528.0',\n        '69.0.3497.50',\n        '68.0.3440.122',\n        '70.0.3527.1',\n        '70.0.3527.0',\n        '69.0.3497.49',\n        '68.0.3440.121',\n        '70.0.3526.1',\n        '70.0.3526.0',\n        '68.0.3440.120',\n        '69.0.3497.48',\n        '69.0.3497.47',\n        '68.0.3440.119',\n        '68.0.3440.118',\n        '70.0.3525.5',\n        '70.0.3525.4',\n        '70.0.3525.3',\n        '68.0.3440.117',\n        '69.0.3497.46',\n        '70.0.3525.2',\n        '70.0.3525.1',\n        '70.0.3525.0',\n        '69.0.3497.45',\n        '68.0.3440.116',\n        '70.0.3524.4',\n        '70.0.3524.3',\n        '69.0.3497.44',\n        '70.0.3524.2',\n        '70.0.3524.1',\n        '70.0.3524.0',\n        '70.0.3523.2',\n        '69.0.3497.43',\n        '68.0.3440.115',\n        '70.0.3505.9',\n        '69.0.3497.42',\n        '70.0.3505.8',\n        '70.0.3523.1',\n        '70.0.3523.0',\n        '69.0.3497.41',\n        '68.0.3440.114',\n        '70.0.3505.7',\n        '69.0.3497.40',\n        '70.0.3522.1',\n        '70.0.3522.0',\n        '70.0.3521.2',\n        '69.0.3497.39',\n        '68.0.3440.113',\n        '70.0.3505.6',\n        '70.0.3521.1',\n        '70.0.3521.0',\n        '69.0.3497.38',\n        '68.0.3440.112',\n        '70.0.3520.1',\n        '70.0.3520.0',\n        '69.0.3497.37',\n        '68.0.3440.111',\n        '70.0.3519.3',\n        '70.0.3519.2',\n        '70.0.3519.1',\n        '70.0.3519.0',\n        '69.0.3497.36',\n        '68.0.3440.110',\n        '70.0.3518.1',\n        '70.0.3518.0',\n        '69.0.3497.35',\n        '69.0.3497.34',\n        '68.0.3440.109',\n        '70.0.3517.1',\n        '70.0.3517.0',\n        '69.0.3497.33',\n        '68.0.3440.108',\n        '69.0.3497.32',\n        '70.0.3516.3',\n        '70.0.3516.2',\n        '70.0.3516.1',\n        '70.0.3516.0',\n        '69.0.3497.31',\n        '68.0.3440.107',\n        '70.0.3515.4',\n        '68.0.3440.106',\n        '70.0.3515.3',\n        '70.0.3515.2',\n        '70.0.3515.1',\n        '70.0.3515.0',\n        '69.0.3497.30',\n        '68.0.3440.105',\n        '68.0.3440.104',\n        '70.0.3514.2',\n        '70.0.3514.1',\n        '70.0.3514.0',\n        '69.0.3497.29',\n        '68.0.3440.103',\n        '70.0.3513.1',\n        '70.0.3513.0',\n        '69.0.3497.28',\n    )\n    return _USER_AGENT_TPL % random.choice(_CHROME_VERSIONS)",
        "begin_line": 88,
        "end_line": 1668,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.preferredencoding#1780",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.preferredencoding()",
        "snippet": "def preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref",
        "begin_line": 1780,
        "end_line": 1792,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.write_json_file#1795",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.write_json_file(obj, fn)",
        "snippet": "def write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    fn = encodeFilename(fn)\n    if sys.version_info < (3, 0) and sys.platform != 'win32':\n        encoding = get_filesystem_encoding()\n        # os.path.basename returns a bytes object, but NamedTemporaryFile\n        # will fail if the filename contains non ascii characters unless we\n        # use a unicode object\n        path_basename = lambda f: os.path.basename(fn).decode(encoding)\n        # the same for os.path.dirname\n        path_dirname = lambda f: os.path.dirname(fn).decode(encoding)\n    else:\n        path_basename = os.path.basename\n        path_dirname = os.path.dirname\n\n    args = {\n        'suffix': '.tmp',\n        'prefix': path_basename(fn) + '.',\n        'dir': path_dirname(fn),\n        'delete': False,\n    }\n\n    # In Python 2.x, json.dump expects a bytestream.\n    # In Python 3.x, it writes to a character stream\n    if sys.version_info < (3, 0):\n        args['mode'] = 'wb'\n    else:\n        args.update({\n            'mode': 'w',\n            'encoding': 'utf-8',\n        })\n\n    tf = tempfile.NamedTemporaryFile(**compat_kwargs(args))\n\n    try:\n        with tf:\n            json.dump(obj, tf)\n        if sys.platform == 'win32':\n            # Need to remove existing file on Windows, else os.rename raises\n            # WindowsError or FileExistsError.\n            try:\n                os.unlink(fn)\n            except OSError:\n                pass\n        try:\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        except OSError:\n            pass\n        os.rename(tf.name, fn)\n    except Exception:\n        try:\n            os.remove(tf.name)\n        except OSError:\n            pass\n        raise",
        "begin_line": 1795,
        "end_line": 1852,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012195121951219512,
            "pseudo_dstar_susp": 0.0012853470437017994,
            "pseudo_tarantula_susp": 0.0003901677721420211,
            "pseudo_op2_susp": 0.0012853470437017994,
            "pseudo_barinel_susp": 0.0003901677721420211
        }
    },
    {
        "name": "youtube_dl.utils.find_xpath_attr#1856",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.find_xpath_attr(node, xpath, key, val=None)",
        "snippet": "    def find_xpath_attr(node, xpath, key, val=None):\n        \"\"\" Find the xpath xpath[@key=val] \"\"\"\n        assert re.match(r'^[a-zA-Z_-]+$', key)\n        expr = xpath + ('[@%s]' % key if val is None else \"[@%s='%s']\" % (key, val))\n        return node.find(expr)",
        "begin_line": 1856,
        "end_line": 1860,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002760905577029266,
            "pseudo_dstar_susp": 0.000271370420624152,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.000271370420624152,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.utils.xpath_with_ns#1874",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.xpath_with_ns(path, ns_map)",
        "snippet": "def xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)",
        "begin_line": 1874,
        "end_line": 1883,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004512635379061372,
            "pseudo_dstar_susp": 0.00048192771084337347,
            "pseudo_tarantula_susp": 0.00033178500331785003,
            "pseudo_op2_susp": 0.00048192771084337347,
            "pseudo_barinel_susp": 0.00033178500331785003
        }
    },
    {
        "name": "youtube_dl.utils.xpath_element#1886",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT)",
        "snippet": "def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(compat_xpath(xpath))\n\n    if isinstance(xpath, (str, compat_str)):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n",
        "begin_line": 1886,
        "end_line": 1906,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006075334143377885,
            "pseudo_dstar_susp": 0.0006101281269066504,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0006101281269066504,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils._find_xpath#1887",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._find_xpath(xpath)",
        "snippet": "    def _find_xpath(xpath):\n        return node.find(compat_xpath(xpath))",
        "begin_line": 1887,
        "end_line": 1888,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006075334143377885,
            "pseudo_dstar_susp": 0.0006101281269066504,
            "pseudo_tarantula_susp": 0.0004578754578754579,
            "pseudo_op2_susp": 0.0006101281269066504,
            "pseudo_barinel_susp": 0.0004578754578754579
        }
    },
    {
        "name": "youtube_dl.utils.xpath_text#1909",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT)",
        "snippet": "def xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text",
        "begin_line": 1909,
        "end_line": 1921,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006093845216331506,
            "pseudo_dstar_susp": 0.0006108735491753207,
            "pseudo_tarantula_susp": 0.0006108735491753207,
            "pseudo_op2_susp": 0.0006108735491753207,
            "pseudo_barinel_susp": 0.0006108735491753207
        }
    },
    {
        "name": "youtube_dl.utils.xpath_attr#1924",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT)",
        "snippet": "def xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = '%s[@%s]' % (xpath, key) if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]",
        "begin_line": 1924,
        "end_line": 1934,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.get_element_by_id#1937",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_element_by_id(id, html)",
        "snippet": "def get_element_by_id(id, html):\n    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_by_attribute('id', id, html)",
        "begin_line": 1937,
        "end_line": 1939,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.get_element_by_class#1942",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_element_by_class(class_name, html)",
        "snippet": "def get_element_by_class(class_name, html):\n    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_by_class(class_name, html)\n    return retval[0] if retval else None",
        "begin_line": 1942,
        "end_line": 1945,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.get_element_by_attribute#1948",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_element_by_attribute(attribute, value, html, escape_value=True)",
        "snippet": "def get_element_by_attribute(attribute, value, html, escape_value=True):\n    retval = get_elements_by_attribute(attribute, value, html, escape_value)\n    return retval[0] if retval else None",
        "begin_line": 1948,
        "end_line": 1950,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006872852233676976,
            "pseudo_dstar_susp": 0.0006906077348066298,
            "pseudo_tarantula_susp": 0.00046904315196998124,
            "pseudo_op2_susp": 0.0006906077348066298,
            "pseudo_barinel_susp": 0.00046904315196998124
        }
    },
    {
        "name": "youtube_dl.utils.get_elements_by_class#1953",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_elements_by_class(class_name, html)",
        "snippet": "def get_elements_by_class(class_name, html):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*\\b%s\\b[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)",
        "begin_line": 1953,
        "end_line": 1957,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005141388174807198,
            "pseudo_dstar_susp": 0.0004830917874396135,
            "pseudo_tarantula_susp": 0.001483679525222552,
            "pseudo_op2_susp": 0.0004830917874396135,
            "pseudo_barinel_susp": 0.001483679525222552
        }
    },
    {
        "name": "youtube_dl.utils.get_elements_by_attribute#1960",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_elements_by_attribute(attribute, value, html, escape_value=True)",
        "snippet": "def get_elements_by_attribute(attribute, value, html, escape_value=True):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n\n    value = re.escape(value) if escape_value else value\n\n    retlist = []\n    for m in re.finditer(r'''(?xs)\n        <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n         \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n        \\s*>\n        (?P<content>.*?)\n        </\\1>\n    ''' % (re.escape(attribute), value), html):\n        res = m.group('content')\n\n        if res.startswith('\"') or res.startswith(\"'\"):\n            res = res[1:-1]\n\n        retlist.append(unescapeHTML(res))\n\n    return retlist",
        "begin_line": 1960,
        "end_line": 1982,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007434944237918215,
            "pseudo_dstar_susp": 0.000731528895391368,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.000731528895391368,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.HTMLAttributeParser.__init__#1987",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.HTMLAttributeParser",
        "signature": "youtube_dl.utils.HTMLAttributeParser.__init__(self)",
        "snippet": "    def __init__(self):\n        self.attrs = {}\n        compat_HTMLParser.__init__(self)",
        "begin_line": 1987,
        "end_line": 1989,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005549389567147614,
            "pseudo_dstar_susp": 0.0005571030640668524,
            "pseudo_tarantula_susp": 0.0005379236148466917,
            "pseudo_op2_susp": 0.0005571030640668524,
            "pseudo_barinel_susp": 0.0005379236148466917
        }
    },
    {
        "name": "youtube_dl.utils.HTMLAttributeParser.handle_starttag#1991",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.HTMLAttributeParser",
        "signature": "youtube_dl.utils.HTMLAttributeParser.handle_starttag(self, tag, attrs)",
        "snippet": "    def handle_starttag(self, tag, attrs):\n        self.attrs = dict(attrs)",
        "begin_line": 1991,
        "end_line": 1992,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005549389567147614,
            "pseudo_dstar_susp": 0.0005571030640668524,
            "pseudo_tarantula_susp": 0.0005379236148466917,
            "pseudo_op2_susp": 0.0005571030640668524,
            "pseudo_barinel_susp": 0.0005379236148466917
        }
    },
    {
        "name": "youtube_dl.utils.extract_attributes#1995",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.extract_attributes(html_element)",
        "snippet": "def extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n    but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n    \"\"\"\n    parser = HTMLAttributeParser()\n    try:\n        parser.feed(html_element)\n        parser.close()\n    # Older Python may throw HTMLParseError in case of malformed HTML\n    except compat_HTMLParseError:\n        pass\n    return parser.attrs",
        "begin_line": 1995,
        "end_line": 2018,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005549389567147614,
            "pseudo_dstar_susp": 0.0005571030640668524,
            "pseudo_tarantula_susp": 0.0005379236148466917,
            "pseudo_op2_susp": 0.0005571030640668524,
            "pseudo_barinel_susp": 0.0005379236148466917
        }
    },
    {
        "name": "youtube_dl.utils.clean_html#2021",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.clean_html(html)",
        "snippet": "def clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    # Newline vs <br />\n    html = html.replace('\\n', ' ')\n    html = re.sub(r'(?u)\\s*<\\s*br\\s*/?\\s*>\\s*', '\\n', html)\n    html = re.sub(r'(?u)<\\s*/\\s*p\\s*>\\s*<\\s*p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()",
        "begin_line": 2021,
        "end_line": 2035,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001142857142857143,
            "pseudo_dstar_susp": 0.0010638297872340426,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0010638297872340426,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_open#2038",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_open(filename, open_mode)",
        "snippet": "def sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    try:\n        if filename == '-':\n            if sys.platform == 'win32':\n                import msvcrt\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n            return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n        stream = open(encodeFilename(filename), open_mode)\n        return (stream, filename)\n    except (IOError, OSError) as err:\n        if err.errno in (errno.EACCES,):\n            raise\n\n        # In case of error, try to remove win32 forbidden chars\n        alt_filename = sanitize_path(filename)\n        if alt_filename == filename:\n            raise\n        else:\n            # An exception here should be caught in the caller\n            stream = open(encodeFilename(alt_filename), open_mode)\n            return (stream, alt_filename)",
        "begin_line": 2038,
        "end_line": 2067,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008264462809917355,
            "pseudo_dstar_susp": 0.0008857395925597874,
            "pseudo_tarantula_susp": 0.00030873726458783575,
            "pseudo_op2_susp": 0.0008857395925597874,
            "pseudo_barinel_susp": 0.00030873726458783575
        }
    },
    {
        "name": "youtube_dl.utils.timeconvert#2070",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.timeconvert(timestr)",
        "snippet": "def timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp",
        "begin_line": 2070,
        "end_line": 2076,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006934812760055479,
            "pseudo_dstar_susp": 0.0008084074373484236,
            "pseudo_tarantula_susp": 0.0002942041776993233,
            "pseudo_op2_susp": 0.0008084074373484236,
            "pseudo_barinel_susp": 0.0002942041776993233
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_filename#2079",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_filename(s, restricted=False, is_id=False)",
        "snippet": "def sanitize_filename(s, restricted=False, is_id=False):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    If restricted is set, use a stricter subset of allowed characters.\n    Set is_id if this is not an arbitrary string, but an ID that should be kept\n    if possible.\n    \"\"\"\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        if char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '_-' if restricted else ' -'\n        elif char in '\\\\/|*<>':\n            return '_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n            return '_'\n        if restricted and ord(char) > 127:\n            return '_'\n        return char\n\n    # Handle timestamps\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)\n    result = ''.join(map(replace_insane, s))\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result",
        "begin_line": 2079,
        "end_line": 2117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0035211267605633804,
            "pseudo_dstar_susp": 0.003952569169960474,
            "pseudo_tarantula_susp": 0.000931098696461825,
            "pseudo_op2_susp": 0.003952569169960474,
            "pseudo_barinel_susp": 0.000931098696461825
        }
    },
    {
        "name": "youtube_dl.utils.replace_insane#2085",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.replace_insane(char)",
        "snippet": "    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        if char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '_-' if restricted else ' -'\n        elif char in '\\\\/|*<>':\n            return '_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n            return '_'\n        if restricted and ord(char) > 127:\n            return '_'\n        return char",
        "begin_line": 2085,
        "end_line": 2100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0034129692832764505,
            "pseudo_dstar_susp": 0.003816793893129771,
            "pseudo_tarantula_susp": 0.0009233610341643582,
            "pseudo_op2_susp": 0.003816793893129771,
            "pseudo_barinel_susp": 0.0009233610341643582
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_path#2120",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_path(s)",
        "snippet": "def sanitize_path(s):\n    \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n    if sys.platform != 'win32':\n        return s\n    drive_or_unc, _ = os.path.splitdrive(s)\n    if sys.version_info < (2, 7) and not drive_or_unc:\n        drive_or_unc, _ = os.path.splitunc(s)\n    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)\n    if drive_or_unc:\n        norm_path.pop(0)\n    sanitized_path = [\n        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n        for path_part in norm_path]\n    if drive_or_unc:\n        sanitized_path.insert(0, drive_or_unc + os.path.sep)\n    return os.path.join(*sanitized_path)",
        "begin_line": 2120,
        "end_line": 2135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0036496350364963502,
            "pseudo_dstar_susp": 0.00411522633744856,
            "pseudo_tarantula_susp": 0.0009407337723424271,
            "pseudo_op2_susp": 0.00411522633744856,
            "pseudo_barinel_susp": 0.0009407337723424271
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_url#2138",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_url(url)",
        "snippet": "def sanitize_url(url):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url.startswith('//'):\n        return 'http:%s' % url\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url",
        "begin_line": 2138,
        "end_line": 2153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007874015748031496,
            "pseudo_dstar_susp": 0.004830917874396135,
            "pseudo_tarantula_susp": 0.001277139208173691,
            "pseudo_op2_susp": 0.004830917874396135,
            "pseudo_barinel_susp": 0.001277139208173691
        }
    },
    {
        "name": "youtube_dl.utils.sanitized_Request#2156",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitized_Request(url, *args, **kwargs)",
        "snippet": "def sanitized_Request(url, *args, **kwargs):\n    return compat_urllib_request.Request(sanitize_url(url), *args, **kwargs)",
        "begin_line": 2156,
        "end_line": 2157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008064516129032258,
            "pseudo_dstar_susp": 0.004901960784313725,
            "pseudo_tarantula_susp": 0.001282051282051282,
            "pseudo_op2_susp": 0.004901960784313725,
            "pseudo_barinel_susp": 0.001282051282051282
        }
    },
    {
        "name": "youtube_dl.utils.expand_path#2160",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.expand_path(s)",
        "snippet": "def expand_path(s):\n    \"\"\"Expand shell variables and ~\"\"\"\n    return os.path.expandvars(compat_expanduser(s))",
        "begin_line": 2160,
        "end_line": 2162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0038461538461538464,
            "pseudo_dstar_susp": 0.0043859649122807015,
            "pseudo_tarantula_susp": 0.0009337068160597573,
            "pseudo_op2_susp": 0.0043859649122807015,
            "pseudo_barinel_susp": 0.0009337068160597573
        }
    },
    {
        "name": "youtube_dl.utils.orderedSet#2165",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.orderedSet(iterable)",
        "snippet": "def orderedSet(iterable):\n    \"\"\" Remove all duplicates from the input iterable \"\"\"\n    res = []\n    for el in iterable:\n        if el not in res:\n            res.append(el)\n    return res",
        "begin_line": 2165,
        "end_line": 2171,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006501950585175553,
            "pseudo_dstar_susp": 0.000643915003219575,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.000643915003219575,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils._htmlentity_transform#2174",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._htmlentity_transform(entity_with_semicolon)",
        "snippet": "def _htmlentity_transform(entity_with_semicolon):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    entity = entity_with_semicolon[:-1]\n\n    # Known non-numeric HTML entity\n    if entity in compat_html_entities.name2codepoint:\n        return compat_chr(compat_html_entities.name2codepoint[entity])\n\n    # TODO: HTML5 allows entities without a semicolon. For example,\n    # '&Eacuteric' should be decoded as '\u00c9ric'.\n    if entity_with_semicolon in compat_html_entities_html5:\n        return compat_html_entities_html5[entity_with_semicolon]\n\n    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith('x'):\n            base = 16\n            numstr = '0%s' % numstr\n        else:\n            base = 10\n        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n        try:\n            return compat_chr(int(numstr, base))\n        except ValueError:\n            pass\n\n    # Unknown entity in name, return its literal representation\n    return '&%s;' % entity",
        "begin_line": 2174,
        "end_line": 2202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000864304235090752,
            "pseudo_dstar_susp": 0.0008071025020177562,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0008071025020177562,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.unescapeHTML#2205",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unescapeHTML(s)",
        "snippet": "def unescapeHTML(s):\n    if s is None:\n        return None\n    assert type(s) == compat_str\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)",
        "begin_line": 2205,
        "end_line": 2211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001941747572815534,
            "pseudo_dstar_susp": 0.001893939393939394,
            "pseudo_tarantula_susp": 0.0006207324643078833,
            "pseudo_op2_susp": 0.001893939393939394,
            "pseudo_barinel_susp": 0.0006207324643078833
        }
    },
    {
        "name": "youtube_dl.utils.get_subprocess_encoding#2214",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_subprocess_encoding()",
        "snippet": "def get_subprocess_encoding():\n    if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n        # For subprocess calls, encode with locale encoding\n        # Refer to http://stackoverflow.com/a/9951851/35070\n        encoding = preferredencoding()\n    else:\n        encoding = sys.getfilesystemencoding()\n    if encoding is None:\n        encoding = 'utf-8'\n    return encoding",
        "begin_line": 2214,
        "end_line": 2223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.encodeFilename#2226",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encodeFilename(s, for_subprocess=False)",
        "snippet": "def encodeFilename(s, for_subprocess=False):\n    \"\"\"\n    @param s The name of the file\n    \"\"\"\n\n    assert type(s) == compat_str\n\n    # Python 3 has a Unicode API\n    if sys.version_info >= (3, 0):\n        return s\n\n    # Pass '' directly to use Unicode APIs on Windows 2000 and up\n    # (Detecting Windows NT 4 is tricky because 'major >= 4' would\n    # match Windows 9x series as well. Besides, NT 4 is obsolete.)\n    if not for_subprocess and sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n        return s\n\n    # Jython assumes filenames are Unicode strings though reported as Python 2.x compatible\n    if sys.platform.startswith('java'):\n        return s\n\n    return s.encode(get_subprocess_encoding(), 'ignore')",
        "begin_line": 2226,
        "end_line": 2247,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012004801920768306,
            "pseudo_dstar_susp": 0.0013054830287206266,
            "pseudo_tarantula_susp": 0.0003705075954057058,
            "pseudo_op2_susp": 0.0013054830287206266,
            "pseudo_barinel_susp": 0.0003705075954057058
        }
    },
    {
        "name": "youtube_dl.utils.decodeFilename#2250",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.decodeFilename(b, for_subprocess=False)",
        "snippet": "def decodeFilename(b, for_subprocess=False):\n\n    if sys.version_info >= (3, 0):\n        return b\n\n    if not isinstance(b, bytes):\n        return b\n\n    return b.decode(get_subprocess_encoding(), 'ignore')",
        "begin_line": 2250,
        "end_line": 2258,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.encodeArgument#2261",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encodeArgument(s)",
        "snippet": "def encodeArgument(s):\n    if not isinstance(s, compat_str):\n        # Legacy code that uses byte strings\n        # Uncomment the following line after fixing all post processors\n        # assert False, 'Internal error: %r should be of type %r, is %r' % (s, compat_str, type(s))\n        s = s.decode('ascii')\n    return encodeFilename(s, True)",
        "begin_line": 2261,
        "end_line": 2267,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004280821917808219,
            "pseudo_dstar_susp": 0.00043782837127845885,
            "pseudo_tarantula_susp": 0.0006064281382656155,
            "pseudo_op2_susp": 0.00043782837127845885,
            "pseudo_barinel_susp": 0.0006064281382656155
        }
    },
    {
        "name": "youtube_dl.utils.decodeArgument#2270",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.decodeArgument(b)",
        "snippet": "def decodeArgument(b):\n    return decodeFilename(b, True)",
        "begin_line": 2270,
        "end_line": 2271,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.decodeOption#2274",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.decodeOption(optval)",
        "snippet": "def decodeOption(optval):\n    if optval is None:\n        return optval\n    if isinstance(optval, bytes):\n        optval = optval.decode(preferredencoding())\n\n    assert isinstance(optval, compat_str)\n    return optval",
        "begin_line": 2274,
        "end_line": 2281,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018083182640144665,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.formatSeconds#2284",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.formatSeconds(secs)",
        "snippet": "def formatSeconds(secs):\n    if secs > 3600:\n        return '%d:%02d:%02d' % (secs // 3600, (secs % 3600) // 60, secs % 60)\n    elif secs > 60:\n        return '%d:%02d' % (secs // 60, secs % 60)\n    else:\n        return '%d' % secs",
        "begin_line": 2284,
        "end_line": 2290,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.make_HTTPS_handler#2293",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.make_HTTPS_handler(params, **kwargs)",
        "snippet": "def make_HTTPS_handler(params, **kwargs):\n    opts_no_check_certificate = params.get('nocheckcertificate', False)\n    if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        if opts_no_check_certificate:\n            context.check_hostname = False\n            context.verify_mode = ssl.CERT_NONE\n        try:\n            return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n        except TypeError:\n            # Python 2.7.8\n            # (create_default_context present but HTTPSHandler has no context=)\n            pass\n\n    if sys.version_info < (3, 2):\n        return YoutubeDLHTTPSHandler(params, **kwargs)\n    else:  # Python < 3.4\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n        context.verify_mode = (ssl.CERT_NONE\n                               if opts_no_check_certificate\n                               else ssl.CERT_REQUIRED)\n        context.set_default_verify_paths()\n        return YoutubeDLHTTPSHandler(params, context=context, **kwargs)",
        "begin_line": 2293,
        "end_line": 2315,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005208333333333333,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.utils.bug_reports_message#2318",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.bug_reports_message()",
        "snippet": "def bug_reports_message():\n    if ytdl_is_updateable():\n        update_cmd = 'type  youtube-dl -U  to update'\n    else:\n        update_cmd = 'see  https://yt-dl.org/update  on how to update'\n    msg = '; please report this issue on https://yt-dl.org/bug .'\n    msg += ' Make sure you are using the latest version; %s.' % update_cmd\n    msg += ' Be sure to call youtube-dl with the --verbose flag and include its complete output.'\n    return msg",
        "begin_line": 2318,
        "end_line": 2326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0032679738562091504,
            "pseudo_dstar_susp": 0.0026246719160104987,
            "pseudo_tarantula_susp": 0.002702702702702703,
            "pseudo_op2_susp": 0.0026246719160104987,
            "pseudo_barinel_susp": 0.002702702702702703
        }
    },
    {
        "name": "youtube_dl.utils.ExtractorError.__init__#2337",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ExtractorError",
        "signature": "youtube_dl.utils.ExtractorError.__init__(self, msg, tb=None, expected=False, cause=None, video_id=None)",
        "snippet": "    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in youtube-dl.\n        \"\"\"\n\n        if sys.exc_info()[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError):\n            expected = True\n        if video_id is not None:\n            msg = video_id + ': ' + msg\n        if cause:\n            msg += ' (caused by %r)' % cause\n        if not expected:\n            msg += bug_reports_message()\n        super(ExtractorError, self).__init__(msg)\n\n        self.traceback = tb\n        self.exc_info = sys.exc_info()  # preserve original exception\n        self.cause = cause\n        self.video_id = video_id",
        "begin_line": 2337,
        "end_line": 2355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004201680672268907,
            "pseudo_dstar_susp": 0.003003003003003003,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.003003003003003003,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.ExtractorError.format_traceback#2357",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ExtractorError",
        "signature": "youtube_dl.utils.ExtractorError.format_traceback(self)",
        "snippet": "    def format_traceback(self):\n        if self.traceback is None:\n            return None\n        return ''.join(traceback.format_tb(self.traceback))",
        "begin_line": 2357,
        "end_line": 2360,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004081632653061225,
            "pseudo_dstar_susp": 0.0029239766081871343,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0029239766081871343,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.UnsupportedError.__init__#2364",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.UnsupportedError",
        "signature": "youtube_dl.utils.UnsupportedError.__init__(self, url)",
        "snippet": "    def __init__(self, url):\n        super(UnsupportedError, self).__init__(\n            'Unsupported URL: %s' % url, expected=True)\n        self.url = url",
        "begin_line": 2364,
        "end_line": 2367,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.GeoRestrictedError.__init__#2381",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.GeoRestrictedError",
        "signature": "youtube_dl.utils.GeoRestrictedError.__init__(self, msg, countries=None)",
        "snippet": "    def __init__(self, msg, countries=None):\n        super(GeoRestrictedError, self).__init__(msg, expected=True)\n        self.msg = msg\n        self.countries = countries",
        "begin_line": 2381,
        "end_line": 2384,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005497526113249038,
            "pseudo_dstar_susp": 0.0004930966469428008,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004930966469428008,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.DownloadError.__init__#2395",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DownloadError",
        "signature": "youtube_dl.utils.DownloadError.__init__(self, msg, exc_info=None)",
        "snippet": "    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super(DownloadError, self).__init__(msg)\n        self.exc_info = exc_info",
        "begin_line": 2395,
        "end_line": 2398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004464285714285714,
            "pseudo_dstar_susp": 0.0031446540880503146,
            "pseudo_tarantula_susp": 0.002070393374741201,
            "pseudo_op2_susp": 0.0031446540880503146,
            "pseudo_barinel_susp": 0.002070393374741201
        }
    },
    {
        "name": "youtube_dl.utils.PostProcessingError.__init__#2417",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.PostProcessingError",
        "signature": "youtube_dl.utils.PostProcessingError.__init__(self, msg)",
        "snippet": "    def __init__(self, msg):\n        super(PostProcessingError, self).__init__(msg)\n        self.msg = msg",
        "begin_line": 2417,
        "end_line": 2419,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.ContentTooShortError.__init__#2444",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ContentTooShortError",
        "signature": "youtube_dl.utils.ContentTooShortError.__init__(self, downloaded, expected)",
        "snippet": "    def __init__(self, downloaded, expected):\n        super(ContentTooShortError, self).__init__(\n            'Downloaded {0} bytes, expected {1} bytes'.format(downloaded, expected)\n        )\n        # Both in bytes\n        self.downloaded = downloaded\n        self.expected = expected",
        "begin_line": 2444,
        "end_line": 2450,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.XAttrMetadataError.__init__#2454",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.XAttrMetadataError",
        "signature": "youtube_dl.utils.XAttrMetadataError.__init__(self, code=None, msg='Unknown error')",
        "snippet": "    def __init__(self, code=None, msg='Unknown error'):\n        super(XAttrMetadataError, self).__init__(msg)\n        self.code = code\n        self.msg = msg\n\n        # Parsing code and msg\n        if (self.code in (errno.ENOSPC, errno.EDQUOT)\n                or 'No space left' in self.msg or 'Disk quota excedded' in self.msg):\n            self.reason = 'NO_SPACE'\n        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:\n            self.reason = 'VALUE_TOO_LONG'\n        else:\n            self.reason = 'NOT_SUPPORTED'",
        "begin_line": 2454,
        "end_line": 2466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils._create_http_connection#2473",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._create_http_connection(ydl_handler, http_class, is_https, *args, **kwargs)",
        "snippet": "def _create_http_connection(ydl_handler, http_class, is_https, *args, **kwargs):\n    # Working around python 2 bug (see http://bugs.python.org/issue17849) by limiting\n    # expected HTTP responses to meet HTTP/1.0 or later (see also\n    # https://github.com/ytdl-org/youtube-dl/issues/6727)\n    if sys.version_info < (3, 0):\n        kwargs['strict'] = True\n    hc = http_class(*args, **compat_kwargs(kwargs))\n    source_address = ydl_handler._params.get('source_address')\n\n    if source_address is not None:\n        # This is to workaround _create_connection() from socket where it will try all\n        # address data from getaddrinfo() including IPv6. This filters the result from\n        # getaddrinfo() based on the source_address value.\n        # This is based on the cpython socket.create_connection() function.\n        # https://github.com/python/cpython/blob/master/Lib/socket.py#L691\n        def _create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n            host, port = address\n            err = None\n            addrs = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\n            af = socket.AF_INET if '.' in source_address[0] else socket.AF_INET6\n            ip_addrs = [addr for addr in addrs if addr[0] == af]\n            if addrs and not ip_addrs:\n                ip_version = 'v4' if af == socket.AF_INET else 'v6'\n                raise socket.error(\n                    \"No remote IP%s addresses available for connect, can't use '%s' as source address\"\n                    % (ip_version, source_address[0]))\n            for res in ip_addrs:\n                af, socktype, proto, canonname, sa = res\n                sock = None\n                try:\n                    sock = socket.socket(af, socktype, proto)\n                    if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                        sock.settimeout(timeout)\n                    sock.bind(source_address)\n                    sock.connect(sa)\n                    err = None  # Explicitly break reference cycle\n                    return sock\n                except socket.error as _:\n                    err = _\n                    if sock is not None:\n                        sock.close()\n            if err is not None:\n                raise err\n            else:\n                raise socket.error('getaddrinfo returns an empty list')\n        if hasattr(hc, '_create_connection'):\n            hc._create_connection = _create_connection\n        sa = (source_address, 0)\n        if hasattr(hc, 'source_address'):  # Python 2.7+\n            hc.source_address = sa\n        else:  # Python 2.6\n            def _hc_connect(self, *args, **kwargs):\n                sock = _create_connection(\n                    (self.host, self.port), self.timeout, sa)\n                if is_https:\n                    self.sock = ssl.wrap_socket(\n                        sock, self.key_file, self.cert_file,\n                        ssl_version=ssl.PROTOCOL_TLSv1)\n                else:\n                    self.sock = sock\n            hc.connect = functools.partial(_hc_connect, hc)\n\n    return hc",
        "begin_line": 2473,
        "end_line": 2535,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011627906976744186,
            "pseudo_dstar_susp": 0.0072992700729927005,
            "pseudo_tarantula_susp": 0.001182033096926714,
            "pseudo_op2_susp": 0.0072992700729927005,
            "pseudo_barinel_susp": 0.001182033096926714
        }
    },
    {
        "name": "youtube_dl.utils.handle_youtubedl_headers#2538",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.handle_youtubedl_headers(headers)",
        "snippet": "def handle_youtubedl_headers(headers):\n    filtered_headers = headers\n\n    if 'Youtubedl-no-compression' in filtered_headers:\n        filtered_headers = dict((k, v) for k, v in filtered_headers.items() if k.lower() != 'accept-encoding')\n        del filtered_headers['Youtubedl-no-compression']\n\n    return filtered_headers",
        "begin_line": 2538,
        "end_line": 2545,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011627906976744186,
            "pseudo_dstar_susp": 0.0072992700729927005,
            "pseudo_tarantula_susp": 0.001182033096926714,
            "pseudo_op2_susp": 0.0072992700729927005,
            "pseudo_barinel_susp": 0.001182033096926714
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.__init__#2566",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.__init__(self, params, *args, **kwargs)",
        "snippet": "    def __init__(self, params, *args, **kwargs):\n        compat_urllib_request.HTTPHandler.__init__(self, *args, **kwargs)\n        self._params = params",
        "begin_line": 2566,
        "end_line": 2568,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005208333333333333,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.http_open#2570",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.http_open(self, req)",
        "snippet": "    def http_open(self, req):\n        conn_class = compat_http_client.HTTPConnection\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        return self.do_open(functools.partial(\n            _create_http_connection, self, conn_class, False),\n            req)",
        "begin_line": 2570,
        "end_line": 2580,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0028735632183908046,
            "pseudo_dstar_susp": 0.0028653295128939827,
            "pseudo_tarantula_susp": 0.001288659793814433,
            "pseudo_op2_susp": 0.0028653295128939827,
            "pseudo_barinel_susp": 0.001288659793814433
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.http_request#2589",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.http_request(self, req)",
        "snippet": "    def http_request(self, req):\n        # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n        # always respected by websites, some tend to give out URLs with non percent-encoded\n        # non-ASCII characters (see telemb.py, ard.py [#3412])\n        # urllib chokes on URLs with non-ASCII characters (see http://bugs.python.org/issue3991)\n        # To work around aforementioned issue we will replace request's original URL with\n        # percent-encoded one\n        # Since redirects are also affected (e.g. http://www.southpark.de/alle-episoden/s18e09)\n        # the code of this workaround has been moved here from YoutubeDL.urlopen()\n        url = req.get_full_url()\n        url_escaped = escape_url(url)\n\n        # Substitute URL if any change after escaping\n        if url != url_escaped:\n            req = update_Request(req, url=url_escaped)\n\n        for h, v in std_headers.items():\n            # Capitalize is needed because of Python bug 2275: http://bugs.python.org/issue2275\n            # The dict keys are capitalized because of this bug by urllib\n            if h.capitalize() not in req.headers:\n                req.add_header(h, v)\n\n        req.headers = handle_youtubedl_headers(req.headers)\n\n        if sys.version_info < (2, 7) and '#' in req.get_full_url():\n            # Python 2.6 is brain-dead when it comes to fragments\n            req._Request__original = req._Request__original.partition('#')[0]\n            req._Request__r_type = req._Request__r_type.partition('#')[0]\n\n        return req",
        "begin_line": 2589,
        "end_line": 2618,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010309278350515464,
            "pseudo_dstar_susp": 0.006802721088435374,
            "pseudo_tarantula_susp": 0.0018450184501845018,
            "pseudo_op2_susp": 0.006802721088435374,
            "pseudo_barinel_susp": 0.0018450184501845018
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.http_response#2620",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.http_response(self, req, resp)",
        "snippet": "    def http_response(self, req, resp):\n        old_resp = resp\n        # gzip\n        if resp.headers.get('Content-encoding', '') == 'gzip':\n            content = resp.read()\n            gz = gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb')\n            try:\n                uncompressed = io.BytesIO(gz.read())\n            except IOError as original_ioerror:\n                # There may be junk add the end of the file\n                # See http://stackoverflow.com/q/4928560/35070 for details\n                for i in range(1, 1024):\n                    try:\n                        gz = gzip.GzipFile(fileobj=io.BytesIO(content[:-i]), mode='rb')\n                        uncompressed = io.BytesIO(gz.read())\n                    except IOError:\n                        continue\n                    break\n                else:\n                    raise original_ioerror\n            resp = compat_urllib_request.addinfourl(uncompressed, old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n            del resp.headers['Content-encoding']\n        # deflate\n        if resp.headers.get('Content-encoding', '') == 'deflate':\n            gz = io.BytesIO(self.deflate(resp.read()))\n            resp = compat_urllib_request.addinfourl(gz, old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n            del resp.headers['Content-encoding']\n        # Percent-encode redirect URL of Location HTTP header to satisfy RFC 3986 (see\n        # https://github.com/ytdl-org/youtube-dl/issues/6457).\n        if 300 <= resp.code < 400:\n            location = resp.headers.get('Location')\n            if location:\n                # As of RFC 2616 default charset is iso-8859-1 that is respected by python 3\n                if sys.version_info >= (3, 0):\n                    location = location.encode('iso-8859-1').decode('utf-8')\n                else:\n                    location = location.decode('utf-8')\n                location_escaped = escape_url(location)\n                if location != location_escaped:\n                    del resp.headers['Location']\n                    if sys.version_info < (3, 0):\n                        location_escaped = location_escaped.encode('utf-8')\n                    resp.headers['Location'] = location_escaped\n        return resp",
        "begin_line": 2620,
        "end_line": 2665,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.029411764705882353,
            "pseudo_dstar_susp": 0.004761904761904762,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.004761904761904762,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.utils.make_socks_conn_class#2671",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.make_socks_conn_class(base_class, socks_proxy)",
        "snippet": "def make_socks_conn_class(base_class, socks_proxy):\n    assert issubclass(base_class, (\n        compat_http_client.HTTPConnection, compat_http_client.HTTPSConnection))\n\n    url_components = compat_urlparse.urlparse(socks_proxy)\n    if url_components.scheme.lower() == 'socks5':\n        socks_type = ProxyType.SOCKS5\n    elif url_components.scheme.lower() in ('socks', 'socks4'):\n        socks_type = ProxyType.SOCKS4\n    elif url_components.scheme.lower() == 'socks4a':\n        socks_type = ProxyType.SOCKS4A\n\n    def unquote_if_non_empty(s):\n        if not s:\n            return s\n        return compat_urllib_parse_unquote_plus(s)\n\n    proxy_args = (\n        socks_type,\n        url_components.hostname, url_components.port or 1080,\n        True,  # Remote DNS\n        unquote_if_non_empty(url_components.username),\n        unquote_if_non_empty(url_components.password),\n    )\n\n    class SocksConnection(base_class):\n        def connect(self):\n            self.sock = sockssocket()\n            self.sock.setproxy(*proxy_args)\n            if type(self.timeout) in (int, float):\n                self.sock.settimeout(self.timeout)\n            self.sock.connect((self.host, self.port))\n\n            if isinstance(self, compat_http_client.HTTPSConnection):\n                if hasattr(self, '_context'):  # Python > 2.6\n                    self.sock = self._context.wrap_socket(\n                        self.sock, server_hostname=self.host)\n                else:\n                    self.sock = ssl.wrap_socket(self.sock)\n\n    return SocksConnection",
        "begin_line": 2671,
        "end_line": 2711,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHTTPSHandler.__init__#2715",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHTTPSHandler",
        "signature": "youtube_dl.utils.YoutubeDLHTTPSHandler.__init__(self, params, https_conn_class=None, *args, **kwargs)",
        "snippet": "    def __init__(self, params, https_conn_class=None, *args, **kwargs):\n        compat_urllib_request.HTTPSHandler.__init__(self, *args, **kwargs)\n        self._https_conn_class = https_conn_class or compat_http_client.HTTPSConnection\n        self._params = params",
        "begin_line": 2715,
        "end_line": 2718,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005208333333333333,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHTTPSHandler.https_open#2720",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHTTPSHandler",
        "signature": "youtube_dl.utils.YoutubeDLHTTPSHandler.https_open(self, req)",
        "snippet": "    def https_open(self, req):\n        kwargs = {}\n        conn_class = self._https_conn_class\n\n        if hasattr(self, '_context'):  # python > 2.6\n            kwargs['context'] = self._context\n        if hasattr(self, '_check_hostname'):  # python 3.x\n            kwargs['check_hostname'] = self._check_hostname\n\n        socks_proxy = req.headers.get('Ytdl-socks-proxy')\n        if socks_proxy:\n            conn_class = make_socks_conn_class(conn_class, socks_proxy)\n            del req.headers['Ytdl-socks-proxy']\n\n        return self.do_open(functools.partial(\n            _create_http_connection, self, conn_class, True),\n            req, **kwargs)",
        "begin_line": 2720,
        "end_line": 2736,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00398406374501992,
            "pseudo_dstar_susp": 0.003663003663003663,
            "pseudo_tarantula_susp": 0.0011013215859030838,
            "pseudo_op2_susp": 0.003663003663003663,
            "pseudo_barinel_susp": 0.0011013215859030838
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLCookieJar.save#2755",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLCookieJar",
        "signature": "youtube_dl.utils.YoutubeDLCookieJar.save(self, filename=None, ignore_discard=False, ignore_expires=False)",
        "snippet": "    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"\n        Save cookies to a file.\n\n        Most of the code is taken from CPython 3.8 and slightly adapted\n        to support cookie files with UTF-8 in both python 2 and 3.\n        \"\"\"\n        if filename is None:\n            if self.filename is not None:\n                filename = self.filename\n            else:\n                raise ValueError(compat_cookiejar.MISSING_FILENAME_TEXT)\n\n        # Store session cookies with `expires` set to 0 instead of an empty\n        # string\n        for cookie in self:\n            if cookie.expires is None:\n                cookie.expires = 0\n\n        with io.open(filename, 'w', encoding='utf-8') as f:\n            f.write(self._HEADER)\n            now = time.time()\n            for cookie in self:\n                if not ignore_discard and cookie.discard:\n                    continue\n                if not ignore_expires and cookie.is_expired(now):\n                    continue\n                if cookie.secure:\n                    secure = 'TRUE'\n                else:\n                    secure = 'FALSE'\n                if cookie.domain.startswith('.'):\n                    initial_dot = 'TRUE'\n                else:\n                    initial_dot = 'FALSE'\n                if cookie.expires is not None:\n                    expires = compat_str(cookie.expires)\n                else:\n                    expires = ''\n                if cookie.value is None:\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas http.cookiejar regards it as a\n                    # cookie with no value.\n                    name = ''\n                    value = cookie.name\n                else:\n                    name = cookie.name\n                    value = cookie.value\n                f.write(\n                    '\\t'.join([cookie.domain, initial_dot, cookie.path,\n                               secure, expires, name, value]) + '\\n')",
        "begin_line": 2755,
        "end_line": 2805,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLCookieJar.load#2807",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLCookieJar",
        "signature": "youtube_dl.utils.YoutubeDLCookieJar.load(self, filename=None, ignore_discard=False, ignore_expires=False)",
        "snippet": "    def load(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"Load cookies from a file.\"\"\"\n        if filename is None:\n            if self.filename is not None:\n                filename = self.filename\n            else:\n                raise ValueError(compat_cookiejar.MISSING_FILENAME_TEXT)\n\n        def prepare_line(line):\n            if line.startswith(self._HTTPONLY_PREFIX):\n                line = line[len(self._HTTPONLY_PREFIX):]\n            # comments and empty lines are fine\n            if line.startswith('#') or not line.strip():\n                return line\n            cookie_list = line.split('\\t')\n            if len(cookie_list) != self._ENTRY_LEN:\n                raise compat_cookiejar.LoadError('invalid length %d' % len(cookie_list))\n            cookie = self._CookieFileEntry(*cookie_list)\n            if cookie.expires_at and not cookie.expires_at.isdigit():\n                raise compat_cookiejar.LoadError('invalid expires at %s' % cookie.expires_at)\n            return line\n\n        cf = io.StringIO()\n        with io.open(filename, encoding='utf-8') as f:\n            for line in f:\n                try:\n                    cf.write(prepare_line(line))\n                except compat_cookiejar.LoadError as e:\n                    write_string(\n                        'WARNING: skipping cookie file entry due to %s: %r\\n'\n                        % (e, line), sys.stderr)\n                    continue\n        cf.seek(0)\n        self._really_load(cf, filename, ignore_discard, ignore_expires)\n        # Session cookies are denoted by either `expires` field set to\n        # an empty string or 0. MozillaCookieJar only recognizes the former\n        # (see [1]). So we need force the latter to be recognized as session\n        # cookies on our own.\n        # Session cookies may be important for cookies-based authentication,\n        # e.g. usually, when user does not check 'Remember me' check box while\n        # logging in on a site, some important cookies are stored as session\n        # cookies so that not recognizing them will result in failed login.\n        # 1. https://bugs.python.org/issue17164\n        for cookie in self:\n            # Treat `expires=0` cookies as session cookies\n            if cookie.expires == 0:\n                cookie.expires = None\n                cookie.discard = True",
        "begin_line": 2807,
        "end_line": 2854,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLCookieJar.prepare_line#2815",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLCookieJar",
        "signature": "youtube_dl.utils.YoutubeDLCookieJar.prepare_line(line)",
        "snippet": "        def prepare_line(line):\n            if line.startswith(self._HTTPONLY_PREFIX):\n                line = line[len(self._HTTPONLY_PREFIX):]\n            # comments and empty lines are fine\n            if line.startswith('#') or not line.strip():\n                return line\n            cookie_list = line.split('\\t')\n            if len(cookie_list) != self._ENTRY_LEN:\n                raise compat_cookiejar.LoadError('invalid length %d' % len(cookie_list))\n            cookie = self._CookieFileEntry(*cookie_list)\n            if cookie.expires_at and not cookie.expires_at.isdigit():\n                raise compat_cookiejar.LoadError('invalid expires at %s' % cookie.expires_at)\n            return line",
        "begin_line": 2815,
        "end_line": 2827,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLCookieProcessor.__init__#2858",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLCookieProcessor",
        "signature": "youtube_dl.utils.YoutubeDLCookieProcessor.__init__(self, cookiejar=None)",
        "snippet": "    def __init__(self, cookiejar=None):\n        compat_urllib_request.HTTPCookieProcessor.__init__(self, cookiejar)",
        "begin_line": 2858,
        "end_line": 2859,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005208333333333333,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLCookieProcessor.http_response#2861",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLCookieProcessor",
        "signature": "youtube_dl.utils.YoutubeDLCookieProcessor.http_response(self, request, response)",
        "snippet": "    def http_response(self, request, response):\n        # Python 2 will choke on next HTTP request in row if there are non-ASCII\n        # characters in Set-Cookie HTTP header of last response (see\n        # https://github.com/ytdl-org/youtube-dl/issues/6769).\n        # In order to at least prevent crashing we will percent encode Set-Cookie\n        # header before HTTPCookieProcessor starts processing it.\n        # if sys.version_info < (3, 0) and response.headers:\n        #     for set_cookie_header in ('Set-Cookie', 'Set-Cookie2'):\n        #         set_cookie = response.headers.get(set_cookie_header)\n        #         if set_cookie:\n        #             set_cookie_escaped = compat_urllib_parse.quote(set_cookie, b\"%/;:@&=+$,!~*'()?#[] \")\n        #             if set_cookie != set_cookie_escaped:\n        #                 del response.headers[set_cookie_header]\n        #                 response.headers[set_cookie_header] = set_cookie_escaped\n        return compat_urllib_request.HTTPCookieProcessor.http_response(self, request, response)",
        "begin_line": 2861,
        "end_line": 2875,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02702702702702703,
            "pseudo_dstar_susp": 0.004694835680751174,
            "pseudo_tarantula_susp": 0.0012970168612191958,
            "pseudo_op2_susp": 0.004694835680751174,
            "pseudo_barinel_susp": 0.0012970168612191958
        }
    },
    {
        "name": "youtube_dl.utils.extract_timezone#2890",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.extract_timezone(date_str)",
        "snippet": "def extract_timezone(date_str):\n    m = re.search(\n        r'^.{8,}?(?P<tz>Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',\n        date_str)\n    if not m:\n        timezone = datetime.timedelta()\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str",
        "begin_line": 2890,
        "end_line": 2905,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009124087591240876,
            "pseudo_dstar_susp": 0.0009514747859181732,
            "pseudo_tarantula_susp": 0.00048661800486618007,
            "pseudo_op2_susp": 0.0009514747859181732,
            "pseudo_barinel_susp": 0.00048661800486618007
        }
    },
    {
        "name": "youtube_dl.utils.parse_iso8601#2908",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_iso8601(date_str, delimiter='T', timezone=None)",
        "snippet": "def parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    try:\n        date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())\n    except ValueError:\n        pass",
        "begin_line": 2908,
        "end_line": 2924,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000655307994757536,
            "pseudo_dstar_susp": 0.0006548788474132286,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0006548788474132286,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.date_formats#2927",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.date_formats(day_first=True)",
        "snippet": "def date_formats(day_first=True):\n    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST",
        "begin_line": 2927,
        "end_line": 2928,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007423904974016332,
            "pseudo_dstar_susp": 0.0007575757575757576,
            "pseudo_tarantula_susp": 0.0003968253968253968,
            "pseudo_op2_susp": 0.0007575757575757576,
            "pseudo_barinel_susp": 0.0003968253968253968
        }
    },
    {
        "name": "youtube_dl.utils.unified_strdate#2931",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unified_strdate(date_str, day_first=True)",
        "snippet": "def unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        try:\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n        except ValueError:\n            pass\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            try:\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n            except ValueError:\n                pass\n    if upload_date is not None:\n        return compat_str(upload_date)",
        "begin_line": 2931,
        "end_line": 2956,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008650519031141869,
            "pseudo_dstar_susp": 0.0008382229673093043,
            "pseudo_tarantula_susp": 0.00045475216007276033,
            "pseudo_op2_susp": 0.0008382229673093043,
            "pseudo_barinel_susp": 0.00045475216007276033
        }
    },
    {
        "name": "youtube_dl.utils.unified_timestamp#2959",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unified_timestamp(date_str, day_first=True)",
        "snippet": "def unified_timestamp(date_str, day_first=True):\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'[,|]', '', date_str)\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        try:\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n        except ValueError:\n            pass\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600",
        "begin_line": 2959,
        "end_line": 2989,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006418485237483953,
            "pseudo_dstar_susp": 0.0006514657980456026,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0006514657980456026,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.determine_ext#2992",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.determine_ext(url, default_ext='unknown_video')",
        "snippet": "def determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext",
        "begin_line": 2992,
        "end_line": 3002,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010741138560687433,
            "pseudo_dstar_susp": 0.0011363636363636363,
            "pseudo_tarantula_susp": 0.00040112314480545525,
            "pseudo_op2_susp": 0.0011363636363636363,
            "pseudo_barinel_susp": 0.00040112314480545525
        }
    },
    {
        "name": "youtube_dl.utils.subtitles_filename#3005",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None)",
        "snippet": "def subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):\n    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)",
        "begin_line": 3005,
        "end_line": 3006,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.date_from_str#3009",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.date_from_str(date_str)",
        "snippet": "def date_from_str(date_str):\n    \"\"\"\n    Return a datetime object from a string in the format YYYYMMDD or\n    (now|today)[+-][0-9](day|week|month|year)(s)?\"\"\"\n    today = datetime.date.today()\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(r'(now|today)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>day|week|month|year)(s)?', date_str)\n    if match is not None:\n        sign = match.group('sign')\n        time = int(match.group('time'))\n        if sign == '-':\n            time = -time\n        unit = match.group('unit')\n        # A bad approximation?\n        if unit == 'month':\n            unit = 'day'\n            time *= 30\n        elif unit == 'year':\n            unit = 'day'\n            time *= 365\n        unit += 's'\n        delta = datetime.timedelta(**{unit: time})\n        return today + delta\n    return datetime.datetime.strptime(date_str, '%Y%m%d').date()",
        "begin_line": 3009,
        "end_line": 3035,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009328358208955224,
            "pseudo_dstar_susp": 0.0009765625,
            "pseudo_tarantula_susp": 0.0003980891719745223,
            "pseudo_op2_susp": 0.0009765625,
            "pseudo_barinel_susp": 0.0003980891719745223
        }
    },
    {
        "name": "youtube_dl.utils.hyphenate_date#3038",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.hyphenate_date(date_str)",
        "snippet": "def hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str",
        "begin_line": 3038,
        "end_line": 3045,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.DateRange.__init__#3051",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DateRange",
        "signature": "youtube_dl.utils.DateRange.__init__(self, start=None, end=None)",
        "snippet": "    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)",
        "begin_line": 3051,
        "end_line": 3062,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009225092250922509,
            "pseudo_dstar_susp": 0.0009671179883945841,
            "pseudo_tarantula_susp": 0.00039323633503735744,
            "pseudo_op2_susp": 0.0009671179883945841,
            "pseudo_barinel_susp": 0.00039323633503735744
        }
    },
    {
        "name": "youtube_dl.utils.DateRange.__contains__#3069",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DateRange",
        "signature": "youtube_dl.utils.DateRange.__contains__(self, date)",
        "snippet": "    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end",
        "begin_line": 3069,
        "end_line": 3073,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009328358208955224,
            "pseudo_dstar_susp": 0.0009765625,
            "pseudo_tarantula_susp": 0.0003980891719745223,
            "pseudo_op2_susp": 0.0009765625,
            "pseudo_barinel_susp": 0.0003980891719745223
        }
    },
    {
        "name": "youtube_dl.utils.DateRange.__str__#3075",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DateRange",
        "signature": "youtube_dl.utils.DateRange.__str__(self)",
        "snippet": "    def __str__(self):\n        return '%s - %s' % (self.start.isoformat(), self.end.isoformat())",
        "begin_line": 3075,
        "end_line": 3076,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.platform_name#3079",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.platform_name()",
        "snippet": "def platform_name():\n    \"\"\" Returns the platform name as a compat_str \"\"\"\n    res = platform.platform()\n    if isinstance(res, bytes):\n        res = res.decode(preferredencoding())\n\n    assert isinstance(res, compat_str)\n    return res",
        "begin_line": 3079,
        "end_line": 3086,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026455026455026457,
            "pseudo_dstar_susp": 0.0002639218791237794,
            "pseudo_tarantula_susp": 0.0002726281352235551,
            "pseudo_op2_susp": 0.0002639218791237794,
            "pseudo_barinel_susp": 0.0002729257641921397
        }
    },
    {
        "name": "youtube_dl.utils._windows_write_string#3089",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._windows_write_string(s, out)",
        "snippet": "def _windows_write_string(s, out):\n    \"\"\" Returns True if the string was written using special methods,\n    False if it has yet to be written out.\"\"\"\n    # Adapted from http://stackoverflow.com/a/3259271/35070\n\n    import ctypes\n    import ctypes.wintypes\n\n    WIN_OUTPUT_IDS = {\n        1: -11,\n        2: -12,\n    }\n\n    try:\n        fileno = out.fileno()\n    except AttributeError:\n        # If the output stream doesn't have a fileno, it's virtual\n        return False\n    except io.UnsupportedOperation:\n        # Some strange Windows pseudo files?\n        return False\n    if fileno not in WIN_OUTPUT_IDS:\n        return False\n\n    GetStdHandle = compat_ctypes_WINFUNCTYPE(\n        ctypes.wintypes.HANDLE, ctypes.wintypes.DWORD)(\n        ('GetStdHandle', ctypes.windll.kernel32))\n    h = GetStdHandle(WIN_OUTPUT_IDS[fileno])\n\n    WriteConsoleW = compat_ctypes_WINFUNCTYPE(\n        ctypes.wintypes.BOOL, ctypes.wintypes.HANDLE, ctypes.wintypes.LPWSTR,\n        ctypes.wintypes.DWORD, ctypes.POINTER(ctypes.wintypes.DWORD),\n        ctypes.wintypes.LPVOID)(('WriteConsoleW', ctypes.windll.kernel32))\n    written = ctypes.wintypes.DWORD(0)\n\n    GetFileType = compat_ctypes_WINFUNCTYPE(ctypes.wintypes.DWORD, ctypes.wintypes.DWORD)(('GetFileType', ctypes.windll.kernel32))\n    FILE_TYPE_CHAR = 0x0002\n    FILE_TYPE_REMOTE = 0x8000\n    GetConsoleMode = compat_ctypes_WINFUNCTYPE(\n        ctypes.wintypes.BOOL, ctypes.wintypes.HANDLE,\n        ctypes.POINTER(ctypes.wintypes.DWORD))(\n        ('GetConsoleMode', ctypes.windll.kernel32))\n    INVALID_HANDLE_VALUE = ctypes.wintypes.DWORD(-1).value\n\n    def not_a_console(handle):\n        if handle == INVALID_HANDLE_VALUE or handle is None:\n            return True\n        return ((GetFileType(handle) & ~FILE_TYPE_REMOTE) != FILE_TYPE_CHAR\n                or GetConsoleMode(handle, ctypes.byref(ctypes.wintypes.DWORD())) == 0)\n\n    if not_a_console(h):\n        return False\n\n    def next_nonbmp_pos(s):\n        try:\n            return next(i for i, c in enumerate(s) if ord(c) > 0xffff)\n        except StopIteration:\n            return len(s)\n\n    while s:\n        count = min(next_nonbmp_pos(s), 1024)\n\n        ret = WriteConsoleW(\n            h, s, count if count else 2, ctypes.byref(written), None)\n        if ret == 0:\n            raise OSError('Failed to write string')\n        if not count:  # We just wrote a non-BMP character\n            assert written.value == 2\n            s = s[1:]\n        else:\n            assert written.value > 0\n            s = s[written.value:]\n    return True",
        "begin_line": 3089,
        "end_line": 3161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.write_string#3164",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.write_string(s, out=None, encoding=None)",
        "snippet": "def write_string(s, out=None, encoding=None):\n    if out is None:\n        out = sys.stderr\n    assert type(s) == compat_str\n\n    if sys.platform == 'win32' and encoding is None and hasattr(out, 'fileno'):\n        if _windows_write_string(s, out):\n            return\n\n    if ('b' in getattr(out, 'mode', '')\n            or sys.version_info[0] < 3):  # Python 2 lies about mode of sys.stderr\n        byt = s.encode(encoding or preferredencoding(), 'ignore')\n        out.write(byt)\n    elif hasattr(out, 'buffer'):\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n        byt = s.encode(enc, 'ignore')\n        out.buffer.write(byt)\n    else:\n        out.write(s)\n    out.flush()",
        "begin_line": 3164,
        "end_line": 3183,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006756756756756757,
            "pseudo_dstar_susp": 0.0053475935828877,
            "pseudo_tarantula_susp": 0.0011198208286674132,
            "pseudo_op2_susp": 0.0053475935828877,
            "pseudo_barinel_susp": 0.0011198208286674132
        }
    },
    {
        "name": "youtube_dl.utils.bytes_to_intlist#3186",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.bytes_to_intlist(bs)",
        "snippet": "def bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]",
        "begin_line": 3186,
        "end_line": 3192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00038955979742890534,
            "pseudo_dstar_susp": 0.00040551500405515005,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00040551500405515005,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.intlist_to_bytes#3195",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.intlist_to_bytes(xs)",
        "snippet": "def intlist_to_bytes(xs):\n    if not xs:\n        return b''\n    return compat_struct_pack('%dB' % len(xs), *xs)",
        "begin_line": 3195,
        "end_line": 3198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003850596842510589,
            "pseudo_dstar_susp": 0.0004048582995951417,
            "pseudo_tarantula_susp": 0.0004943153732081067,
            "pseudo_op2_susp": 0.0004048582995951417,
            "pseudo_barinel_susp": 0.0004943153732081067
        }
    },
    {
        "name": "youtube_dl.utils._lock_file#3261",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._lock_file(f, exclusive)",
        "snippet": "        def _lock_file(f, exclusive):\n            fcntl.flock(f, fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH)",
        "begin_line": 3261,
        "end_line": 3262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils._unlock_file#3264",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._unlock_file(f)",
        "snippet": "        def _unlock_file(f):\n            fcntl.flock(f, fcntl.LOCK_UN)",
        "begin_line": 3264,
        "end_line": 3265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.__init__#3277",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.__init__(self, filename, mode, encoding=None)",
        "snippet": "    def __init__(self, filename, mode, encoding=None):\n        assert mode in ['r', 'a', 'w']\n        self.f = io.open(filename, mode, encoding=encoding)\n        self.mode = mode",
        "begin_line": 3277,
        "end_line": 3280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.__enter__#3282",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.__enter__(self)",
        "snippet": "    def __enter__(self):\n        exclusive = self.mode != 'r'\n        try:\n            _lock_file(self.f, exclusive)\n        except IOError:\n            self.f.close()\n            raise\n        return self",
        "begin_line": 3282,
        "end_line": 3289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.__exit__#3291",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.__exit__(self, etype, value, traceback)",
        "snippet": "    def __exit__(self, etype, value, traceback):\n        try:\n            _unlock_file(self.f)\n        finally:\n            self.f.close()",
        "begin_line": 3291,
        "end_line": 3295,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.__iter__#3297",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self.f)",
        "begin_line": 3297,
        "end_line": 3298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.write#3300",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.write(self, *args)",
        "snippet": "    def write(self, *args):\n        return self.f.write(*args)",
        "begin_line": 3300,
        "end_line": 3301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.locked_file.read#3303",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.locked_file",
        "signature": "youtube_dl.utils.locked_file.read(self, *args)",
        "snippet": "    def read(self, *args):\n        return self.f.read(*args)",
        "begin_line": 3303,
        "end_line": 3304,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.get_filesystem_encoding#3307",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_filesystem_encoding()",
        "snippet": "def get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'",
        "begin_line": 3307,
        "end_line": 3309,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.shell_quote#3312",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.shell_quote(args)",
        "snippet": "def shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)",
        "begin_line": 3312,
        "end_line": 3320,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.smuggle_url#3323",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.smuggle_url(url, data)",
        "snippet": "def smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = compat_urllib_parse_urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata",
        "begin_line": 3323,
        "end_line": 3330,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004899559039686428,
            "pseudo_dstar_susp": 0.0005073566717402334,
            "pseudo_tarantula_susp": 0.000328515111695138,
            "pseudo_op2_susp": 0.0005073566717402334,
            "pseudo_barinel_susp": 0.000328515111695138
        }
    },
    {
        "name": "youtube_dl.utils.unsmuggle_url#3333",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unsmuggle_url(smug_url, default=None)",
        "snippet": "def unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = compat_parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data",
        "begin_line": 3333,
        "end_line": 3339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016366612111292963,
            "pseudo_dstar_susp": 0.0011560693641618498,
            "pseudo_tarantula_susp": 0.0005711022272986865,
            "pseudo_op2_susp": 0.0011560693641618498,
            "pseudo_barinel_susp": 0.0005711022272986865
        }
    },
    {
        "name": "youtube_dl.utils.format_bytes#3342",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.format_bytes(bytes)",
        "snippet": "def format_bytes(bytes):\n    if bytes is None:\n        return 'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return '%.2f%s' % (converted, suffix)",
        "begin_line": 3342,
        "end_line": 3353,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008264462809917355,
            "pseudo_dstar_susp": 0.0008857395925597874,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0008857395925597874,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.lookup_unit_table#3356",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.lookup_unit_table(unit_table, s)",
        "snippet": "def lookup_unit_table(unit_table, s):\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = re.match(\n        r'(?P<num>[0-9]+(?:[,.][0-9]*)?)\\s*(?P<unit>%s)\\b' % units_re, s)\n    if not m:\n        return None\n    num_str = m.group('num').replace(',', '.')\n    mult = unit_table[m.group('unit')]\n    return int(float(num_str) * mult)",
        "begin_line": 3356,
        "end_line": 3364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002760905577029266,
            "pseudo_dstar_susp": 0.000271370420624152,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.000271370420624152,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.utils.parse_filesize#3367",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_filesize(s)",
        "snippet": "def parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)",
        "begin_line": 3367,
        "end_line": 3435,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002666666666666667,
            "pseudo_dstar_susp": 0.00026595744680851064,
            "pseudo_tarantula_susp": 0.00028336639274582036,
            "pseudo_op2_susp": 0.00026595744680851064,
            "pseudo_barinel_susp": 0.00028336639274582036
        }
    },
    {
        "name": "youtube_dl.utils.parse_count#3438",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_count(s)",
        "snippet": "def parse_count(s):\n    if s is None:\n        return None\n\n    s = s.strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)",
        "begin_line": 3438,
        "end_line": 3456,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004060089321965083,
            "pseudo_dstar_susp": 0.0004152823920265781,
            "pseudo_tarantula_susp": 0.000572737686139748,
            "pseudo_op2_susp": 0.0004152823920265781,
            "pseudo_barinel_susp": 0.000572737686139748
        }
    },
    {
        "name": "youtube_dl.utils.parse_resolution#3459",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_resolution(s)",
        "snippet": "def parse_resolution(s):\n    if s is None:\n        return {}\n\n    mobj = re.search(r'\\b(?P<w>\\d+)\\s*[xX\u00d7]\\s*(?P<h>\\d+)\\b', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'\\b(\\d+)[pPiI]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}",
        "begin_line": 3459,
        "end_line": 3478,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004,
            "pseudo_dstar_susp": 0.00041459369817578774,
            "pseudo_tarantula_susp": 0.000591715976331361,
            "pseudo_op2_susp": 0.00041459369817578774,
            "pseudo_barinel_susp": 0.000591715976331361
        }
    },
    {
        "name": "youtube_dl.utils.parse_bitrate#3481",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_bitrate(s)",
        "snippet": "def parse_bitrate(s):\n    if not isinstance(s, compat_str):\n        return\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))",
        "begin_line": 3481,
        "end_line": 3486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00038955979742890534,
            "pseudo_dstar_susp": 0.00040551500405515005,
            "pseudo_tarantula_susp": 0.0005293806246691371,
            "pseudo_op2_susp": 0.00040551500405515005,
            "pseudo_barinel_susp": 0.0005293806246691371
        }
    },
    {
        "name": "youtube_dl.utils.month_by_name#3489",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.month_by_name(name, lang='en')",
        "snippet": "def month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None",
        "begin_line": 3489,
        "end_line": 3497,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.month_by_abbreviation#3500",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.month_by_abbreviation(abbrev)",
        "snippet": "def month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None",
        "begin_line": 3500,
        "end_line": 3507,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.fix_xml_ampersands#3510",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fix_xml_ampersands(xml_str)",
        "snippet": "def fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        '&amp;',\n        xml_str)",
        "begin_line": 3510,
        "end_line": 3515,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006265664160401002,
            "pseudo_dstar_susp": 0.0005920663114268798,
            "pseudo_tarantula_susp": 0.0006082725060827251,
            "pseudo_op2_susp": 0.0005920663114268798,
            "pseudo_barinel_susp": 0.0006082725060827251
        }
    },
    {
        "name": "youtube_dl.utils.setproctitle#3518",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.setproctitle(title)",
        "snippet": "def setproctitle(title):\n    assert isinstance(title, compat_str)\n\n    # ctypes in Jython is not complete\n    # http://bugs.jython.org/issue2148\n    if sys.platform.startswith('java'):\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode('utf-8')\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this",
        "begin_line": 3518,
        "end_line": 3541,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.remove_start#3544",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.remove_start(s, start)",
        "snippet": "def remove_start(s, start):\n    return s[len(start):] if s is not None and s.startswith(start) else s",
        "begin_line": 3544,
        "end_line": 3545,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005047955577990914,
            "pseudo_dstar_susp": 0.00045495905368516835,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.00045495905368516835,
            "pseudo_barinel_susp": 0.002531645569620253
        }
    },
    {
        "name": "youtube_dl.utils.remove_end#3548",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.remove_end(s, end)",
        "snippet": "def remove_end(s, end):\n    return s[:-len(end)] if s is not None and s.endswith(end) else s",
        "begin_line": 3548,
        "end_line": 3549,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045871559633027525,
            "pseudo_dstar_susp": 0.00045829514207149406,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00045829514207149406,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.remove_quotes#3552",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.remove_quotes(s)",
        "snippet": "def remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s",
        "begin_line": 3552,
        "end_line": 3558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.url_basename#3561",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.url_basename(url)",
        "snippet": "def url_basename(url):\n    path = compat_urlparse.urlparse(url).path\n    return path.strip('/').split('/')[-1]",
        "begin_line": 3561,
        "end_line": 3563,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002178649237472767,
            "pseudo_dstar_susp": 0.002207505518763797,
            "pseudo_tarantula_susp": 0.0005577244841048522,
            "pseudo_op2_susp": 0.002207505518763797,
            "pseudo_barinel_susp": 0.0005577244841048522
        }
    },
    {
        "name": "youtube_dl.utils.base_url#3566",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.base_url(url)",
        "snippet": "def base_url(url):\n    return re.match(r'https?://[^?#&]+/', url).group()",
        "begin_line": 3566,
        "end_line": 3567,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005125576627370579,
            "pseudo_dstar_susp": 0.0005350454788657035,
            "pseudo_tarantula_susp": 0.0003198976327575176,
            "pseudo_op2_susp": 0.0005350454788657035,
            "pseudo_barinel_susp": 0.0003198976327575176
        }
    },
    {
        "name": "youtube_dl.utils.urljoin#3570",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.urljoin(base, path)",
        "snippet": "def urljoin(base, path):\n    if isinstance(path, bytes):\n        path = path.decode('utf-8')\n    if not isinstance(path, compat_str) or not path:\n        return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n        return path\n    if isinstance(base, bytes):\n        base = base.decode('utf-8')\n    if not isinstance(base, compat_str) or not re.match(\n            r'^(?:https?:)?//', base):\n        return None\n    return compat_urlparse.urljoin(base, path)",
        "begin_line": 3570,
        "end_line": 3582,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005837711617046118,
            "pseudo_dstar_susp": 0.0005685048322910744,
            "pseudo_tarantula_susp": 0.0006184291898577613,
            "pseudo_op2_susp": 0.0005685048322910744,
            "pseudo_barinel_susp": 0.0006184291898577613
        }
    },
    {
        "name": "youtube_dl.utils.HEADRequest.get_method#3586",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.HEADRequest",
        "signature": "youtube_dl.utils.HEADRequest.get_method(self)",
        "snippet": "    def get_method(self):\n        return 'HEAD'",
        "begin_line": 3586,
        "end_line": 3587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008496176720475786,
            "pseudo_dstar_susp": 0.0008305647840531562,
            "pseudo_tarantula_susp": 0.00038654812524159255,
            "pseudo_op2_susp": 0.0008305647840531562,
            "pseudo_barinel_susp": 0.00038654812524159255
        }
    },
    {
        "name": "youtube_dl.utils.PUTRequest.get_method#3591",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.PUTRequest",
        "signature": "youtube_dl.utils.PUTRequest.get_method(self)",
        "snippet": "    def get_method(self):\n        return 'PUT'",
        "begin_line": 3591,
        "end_line": 3592,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.int_or_none#3595",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.int_or_none(v, scale=1, default=None, get_attr=None, invscale=1)",
        "snippet": "def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr:\n        if v is not None:\n            v = getattr(v, get_attr, None)\n    if v == '':\n        v = None\n    if v is None:\n        return default\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError):\n        return default",
        "begin_line": 3595,
        "end_line": 3606,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014184397163120568,
            "pseudo_dstar_susp": 0.001564945226917058,
            "pseudo_tarantula_susp": 0.00045829514207149406,
            "pseudo_op2_susp": 0.001564945226917058,
            "pseudo_barinel_susp": 0.00045829514207149406
        }
    },
    {
        "name": "youtube_dl.utils.str_or_none#3609",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.str_or_none(v, default=None)",
        "snippet": "def str_or_none(v, default=None):\n    return default if v is None else compat_str(v)",
        "begin_line": 3609,
        "end_line": 3610,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005417118093174431,
            "pseudo_dstar_susp": 0.0005688282138794084,
            "pseudo_tarantula_susp": 0.00029877502240812666,
            "pseudo_op2_susp": 0.0005688282138794084,
            "pseudo_barinel_susp": 0.00029877502240812666
        }
    },
    {
        "name": "youtube_dl.utils.str_to_int#3613",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.str_to_int(int_str)",
        "snippet": "def str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, compat_integer_types):\n        return int_str\n    elif isinstance(int_str, compat_str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)",
        "begin_line": 3613,
        "end_line": 3619,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00045955882352941176,
            "pseudo_dstar_susp": 0.00047281323877068556,
            "pseudo_tarantula_susp": 0.0006253908692933083,
            "pseudo_op2_susp": 0.00047281323877068556,
            "pseudo_barinel_susp": 0.0006253908692933083
        }
    },
    {
        "name": "youtube_dl.utils.float_or_none#3622",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.float_or_none(v, scale=1, invscale=1, default=None)",
        "snippet": "def float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default",
        "begin_line": 3622,
        "end_line": 3628,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009823182711198428,
            "pseudo_dstar_susp": 0.0010384215991692627,
            "pseudo_tarantula_susp": 0.00041459369817578774,
            "pseudo_op2_susp": 0.0010384215991692627,
            "pseudo_barinel_susp": 0.00041459369817578774
        }
    },
    {
        "name": "youtube_dl.utils.bool_or_none#3631",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.bool_or_none(v, default=None)",
        "snippet": "def bool_or_none(v, default=None):\n    return v if isinstance(v, bool) else default",
        "begin_line": 3631,
        "end_line": 3632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00042716787697565144,
            "pseudo_dstar_susp": 0.00040899795501022495,
            "pseudo_tarantula_susp": 0.0017953321364452424,
            "pseudo_op2_susp": 0.00040899795501022495,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.utils.strip_or_none#3635",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.strip_or_none(v, default=None)",
        "snippet": "def strip_or_none(v, default=None):\n    return v.strip() if isinstance(v, compat_str) else default",
        "begin_line": 3635,
        "end_line": 3636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005656108597285068,
            "pseudo_dstar_susp": 0.0006013229104028864,
            "pseudo_tarantula_susp": 0.00032030749519538755,
            "pseudo_op2_susp": 0.0006013229104028864,
            "pseudo_barinel_susp": 0.00032030749519538755
        }
    },
    {
        "name": "youtube_dl.utils.url_or_none#3639",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.url_or_none(url)",
        "snippet": "def url_or_none(url):\n    if not url or not isinstance(url, compat_str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:[a-zA-Z][\\da-zA-Z.+-]*:)?//', url) else None",
        "begin_line": 3639,
        "end_line": 3643,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011560693641618498,
            "pseudo_dstar_susp": 0.000980392156862745,
            "pseudo_tarantula_susp": 0.0015772870662460567,
            "pseudo_op2_susp": 0.000980392156862745,
            "pseudo_barinel_susp": 0.0015772870662460567
        }
    },
    {
        "name": "youtube_dl.utils.parse_duration#3646",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_duration(s)",
        "snippet": "def parse_duration(s):\n    if not isinstance(s, compat_basestring):\n        return None\n\n    s = s.strip()\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'(?:(?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?(?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?Z?$', s)\n    if m:\n        days, hours, mins, secs, ms = m.groups()\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:ours?)?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    duration = 0\n    if secs:\n        duration += float(secs)\n    if mins:\n        duration += float(mins) * 60\n    if hours:\n        duration += float(hours) * 60 * 60\n    if days:\n        duration += float(days) * 24 * 60 * 60\n    if ms:\n        duration += float(ms)\n    return duration",
        "begin_line": 3646,
        "end_line": 3701,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006402048655569782,
            "pseudo_dstar_susp": 0.0006557377049180328,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0006557377049180328,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.utils.prepend_extension#3704",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.prepend_extension(filename, ext, expected_real_ext=None)",
        "snippet": "def prepend_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return (\n        '{0}.{1}{2}'.format(name, ext, real_ext)\n        if not expected_real_ext or real_ext[1:] == expected_real_ext\n        else '{0}.{1}'.format(filename, ext))",
        "begin_line": 3704,
        "end_line": 3709,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.replace_extension#3712",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.replace_extension(filename, ext, expected_real_ext=None)",
        "snippet": "def replace_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return '{0}.{1}'.format(\n        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n        ext)",
        "begin_line": 3712,
        "end_line": 3716,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011160714285714285,
            "pseudo_dstar_susp": 0.0011918951132300357,
            "pseudo_tarantula_susp": 0.00035435861091424523,
            "pseudo_op2_susp": 0.0011918951132300357,
            "pseudo_barinel_susp": 0.00035435861091424523
        }
    },
    {
        "name": "youtube_dl.utils.check_executable#3719",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.check_executable(exe, args=[])",
        "snippet": "def check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n    except OSError:\n        return False\n    return exe",
        "begin_line": 3719,
        "end_line": 3726,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.get_exe_version#3729",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_exe_version(exe, args=['--version'], version_re=None, unrecognized='present')",
        "snippet": "def get_exe_version(exe, args=['--version'],\n                    version_re=None, unrecognized='present'):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    try:\n        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n        # SIGTTOU if youtube-dl is run in the background.\n        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n        out, _ = subprocess.Popen(\n            [encodeArgument(exe)] + args,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()\n    except OSError:\n        return False\n    if isinstance(out, bytes):  # Python 2.x\n        out = out.decode('ascii', 'ignore')\n    return detect_exe_version(out, version_re, unrecognized)",
        "begin_line": 3729,
        "end_line": 3745,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043271311120726956,
            "pseudo_dstar_susp": 0.00043878894251864854,
            "pseudo_tarantula_susp": 0.0006353240152477764,
            "pseudo_op2_susp": 0.00043878894251864854,
            "pseudo_barinel_susp": 0.0006353240152477764
        }
    },
    {
        "name": "youtube_dl.utils.detect_exe_version#3748",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.detect_exe_version(output, version_re=None, unrecognized='present')",
        "snippet": "def detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, compat_str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized",
        "begin_line": 3748,
        "end_line": 3756,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.PagedList.__len__#3760",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.PagedList",
        "signature": "youtube_dl.utils.PagedList.__len__(self)",
        "snippet": "    def __len__(self):\n        # This is only useful for tests\n        return len(self.getslice())",
        "begin_line": 3760,
        "end_line": 3762,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.OnDemandPagedList.__init__#3766",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.OnDemandPagedList",
        "signature": "youtube_dl.utils.OnDemandPagedList.__init__(self, pagefunc, pagesize, use_cache=True)",
        "snippet": "    def __init__(self, pagefunc, pagesize, use_cache=True):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize\n        self._use_cache = use_cache\n        if use_cache:\n            self._cache = {}",
        "begin_line": 3766,
        "end_line": 3771,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003795066413662239,
            "pseudo_dstar_susp": 0.0004032258064516129,
            "pseudo_tarantula_susp": 0.0004210526315789474,
            "pseudo_op2_susp": 0.0004032258064516129,
            "pseudo_barinel_susp": 0.0004210526315789474
        }
    },
    {
        "name": "youtube_dl.utils.OnDemandPagedList.getslice#3773",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.OnDemandPagedList",
        "signature": "youtube_dl.utils.OnDemandPagedList.getslice(self, start=0, end=None)",
        "snippet": "    def getslice(self, start=0, end=None):\n        res = []\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            page_results = None\n            if self._use_cache:\n                page_results = self._cache.get(pagenum)\n            if page_results is None:\n                page_results = list(self._pagefunc(pagenum))\n            if self._use_cache:\n                self._cache[pagenum] = page_results\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            res.extend(page_results)\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n        return res",
        "begin_line": 3773,
        "end_line": 3814,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003795066413662239,
            "pseudo_dstar_susp": 0.0004032258064516129,
            "pseudo_tarantula_susp": 0.0004210526315789474,
            "pseudo_op2_susp": 0.0004032258064516129,
            "pseudo_barinel_susp": 0.0004210526315789474
        }
    },
    {
        "name": "youtube_dl.utils.InAdvancePagedList.__init__#3818",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.InAdvancePagedList",
        "signature": "youtube_dl.utils.InAdvancePagedList.__init__(self, pagefunc, pagecount, pagesize)",
        "snippet": "    def __init__(self, pagefunc, pagecount, pagesize):\n        self._pagefunc = pagefunc\n        self._pagecount = pagecount\n        self._pagesize = pagesize",
        "begin_line": 3818,
        "end_line": 3821,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.utils.InAdvancePagedList.getslice#3823",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.InAdvancePagedList",
        "signature": "youtube_dl.utils.InAdvancePagedList.getslice(self, start=0, end=None)",
        "snippet": "    def getslice(self, start=0, end=None):\n        res = []\n        start_page = start // self._pagesize\n        end_page = (\n            self._pagecount if end is None else (end // self._pagesize + 1))\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page = list(self._pagefunc(pagenum))\n            if skip_elems:\n                page = page[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page) < only_more:\n                    only_more -= len(page)\n                else:\n                    page = page[:only_more]\n                    res.extend(page)\n                    break\n            res.extend(page)\n        return res",
        "begin_line": 3823,
        "end_line": 3843,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.utils.uppercase_escape#3846",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.uppercase_escape(s)",
        "snippet": "def uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)",
        "begin_line": 3846,
        "end_line": 3851,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.lowercase_escape#3854",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.lowercase_escape(s)",
        "snippet": "def lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)",
        "begin_line": 3854,
        "end_line": 3859,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.escape_rfc3986#3862",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.escape_rfc3986(s)",
        "snippet": "def escape_rfc3986(s):\n    \"\"\"Escape non-ASCII characters as suggested by RFC 3986\"\"\"\n    if sys.version_info < (3, 0) and isinstance(s, compat_str):\n        s = s.encode('utf-8')\n    return compat_urllib_parse.quote(s, b\"%/;:@&=+$,!~*'()?#[]\")",
        "begin_line": 3862,
        "end_line": 3866,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008547008547008548,
            "pseudo_dstar_susp": 0.006097560975609756,
            "pseudo_tarantula_susp": 0.0011363636363636363,
            "pseudo_op2_susp": 0.006097560975609756,
            "pseudo_barinel_susp": 0.0011363636363636363
        }
    },
    {
        "name": "youtube_dl.utils.escape_url#3869",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.escape_url(url)",
        "snippet": "def escape_url(url):\n    \"\"\"Escape URL as suggested by RFC 3986\"\"\"\n    url_parsed = compat_urllib_parse_urlparse(url)\n    return url_parsed._replace(\n        netloc=url_parsed.netloc.encode('idna').decode('ascii'),\n        path=escape_rfc3986(url_parsed.path),\n        params=escape_rfc3986(url_parsed.params),\n        query=escape_rfc3986(url_parsed.query),\n        fragment=escape_rfc3986(url_parsed.fragment)\n    ).geturl()",
        "begin_line": 3869,
        "end_line": 3878,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00909090909090909,
            "pseudo_dstar_susp": 0.006329113924050633,
            "pseudo_tarantula_susp": 0.001145475372279496,
            "pseudo_op2_susp": 0.006329113924050633,
            "pseudo_barinel_susp": 0.001145475372279496
        }
    },
    {
        "name": "youtube_dl.utils.read_batch_urls#3881",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.read_batch_urls(batch_fd)",
        "snippet": "def read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, compat_str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = '\\xef\\xbb\\xbf'\n        if url.startswith(BOM_UTF8):\n            url = url[len(BOM_UTF8):]\n        url = url.strip()\n        if url.startswith(('#', ';', ']')):\n            return False\n        return url\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]",
        "begin_line": 3881,
        "end_line": 3894,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.fixup#3882",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fixup(url)",
        "snippet": "    def fixup(url):\n        if not isinstance(url, compat_str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = '\\xef\\xbb\\xbf'\n        if url.startswith(BOM_UTF8):\n            url = url[len(BOM_UTF8):]\n        url = url.strip()\n        if url.startswith(('#', ';', ']')):\n            return False\n        return url",
        "begin_line": 3882,
        "end_line": 3891,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.urlencode_postdata#3897",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.urlencode_postdata(*args, **kargs)",
        "snippet": "def urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse_urlencode(*args, **kargs).encode('ascii')",
        "begin_line": 3897,
        "end_line": 3898,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000758150113722517,
            "pseudo_dstar_susp": 0.0006309148264984228,
            "pseudo_tarantula_susp": 0.0015360983102918587,
            "pseudo_op2_susp": 0.0006309148264984228,
            "pseudo_barinel_susp": 0.0015360983102918587
        }
    },
    {
        "name": "youtube_dl.utils.update_url_query#3901",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.update_url_query(url, query)",
        "snippet": "def update_url_query(url, query):\n    if not query:\n        return url\n    parsed_url = compat_urlparse.urlparse(url)\n    qs = compat_parse_qs(parsed_url.query)\n    qs.update(query)\n    return compat_urlparse.urlunparse(parsed_url._replace(\n        query=compat_urllib_parse_urlencode(qs, True)))",
        "begin_line": 3901,
        "end_line": 3908,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021551724137931034,
            "pseudo_dstar_susp": 0.002012072434607646,
            "pseudo_tarantula_susp": 0.0008920606601248885,
            "pseudo_op2_susp": 0.002012072434607646,
            "pseudo_barinel_susp": 0.0008920606601248885
        }
    },
    {
        "name": "youtube_dl.utils.update_Request#3911",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.update_Request(req, url=None, data=None, headers={}, query={})",
        "snippet": "def update_Request(req, url=None, data=None, headers={}, query={}):\n    req_headers = req.headers.copy()\n    req_headers.update(headers)\n    req_data = data or req.data\n    req_url = update_url_query(url or req.get_full_url(), query)\n    req_get_method = req.get_method()\n    if req_get_method == 'HEAD':\n        req_type = HEADRequest\n    elif req_get_method == 'PUT':\n        req_type = PUTRequest\n    else:\n        req_type = compat_urllib_request.Request\n    new_req = req_type(\n        req_url, data=req_data, headers=req_headers,\n        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n    if hasattr(req, 'timeout'):\n        new_req.timeout = req.timeout\n    return new_req",
        "begin_line": 3911,
        "end_line": 3928,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009950248756218905,
            "pseudo_dstar_susp": 0.0010141987829614604,
            "pseudo_tarantula_susp": 0.0018450184501845018,
            "pseudo_op2_susp": 0.0010141987829614604,
            "pseudo_barinel_susp": 0.0018450184501845018
        }
    },
    {
        "name": "youtube_dl.utils._multipart_encode_impl#3931",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._multipart_encode_impl(data, boundary)",
        "snippet": "def _multipart_encode_impl(data, boundary):\n    content_type = 'multipart/form-data; boundary=%s' % boundary\n\n    out = b''\n    for k, v in data.items():\n        out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n        if isinstance(k, compat_str):\n            k = k.encode('utf-8')\n        if isinstance(v, compat_str):\n            v = v.encode('utf-8')\n        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n        # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n        if boundary.encode('ascii') in content:\n            raise ValueError('Boundary overlaps with data')\n        out += content\n\n    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'\n\n    return out, content_type",
        "begin_line": 3931,
        "end_line": 3950,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.multipart_encode#3953",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.multipart_encode(data, boundary=None)",
        "snippet": "def multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type",
        "begin_line": 3953,
        "end_line": 3980,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.dict_get#3983",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.dict_get(d, key_or_keys, default=None, skip_false_values=True)",
        "snippet": "def dict_get(d, key_or_keys, default=None, skip_false_values=True):\n    if isinstance(key_or_keys, (list, tuple)):\n        for key in key_or_keys:\n            if key not in d or d[key] is None or skip_false_values and not d[key]:\n                continue\n            return d[key]\n        return default\n    return d.get(key_or_keys, default)",
        "begin_line": 3983,
        "end_line": 3990,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004520795660036166,
            "pseudo_dstar_susp": 0.0005045408678102926,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0005045408678102926,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.utils.try_get#3993",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.try_get(src, getter, expected_type=None)",
        "snippet": "def try_get(src, getter, expected_type=None):\n    if not isinstance(getter, (list, tuple)):\n        getter = [getter]\n    for get in getter:\n        try:\n            v = get(src)\n        except (AttributeError, KeyError, TypeError, IndexError):\n            pass\n        else:\n            if expected_type is None or isinstance(v, expected_type):\n                return v",
        "begin_line": 3993,
        "end_line": 4003,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0015060240963855422,
            "pseudo_dstar_susp": 0.0010245901639344263,
            "pseudo_tarantula_susp": 0.0018115942028985507,
            "pseudo_op2_susp": 0.0010245901639344263,
            "pseudo_barinel_susp": 0.0018115942028985507
        }
    },
    {
        "name": "youtube_dl.utils.merge_dicts#4006",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.merge_dicts(*dicts)",
        "snippet": "def merge_dicts(*dicts):\n    merged = {}\n    for a_dict in dicts:\n        for k, v in a_dict.items():\n            if v is None:\n                continue\n            if (k not in merged\n                    or (isinstance(v, compat_str) and v\n                        and isinstance(merged[k], compat_str)\n                        and not merged[k])):\n                merged[k] = v\n    return merged",
        "begin_line": 4006,
        "end_line": 4017,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004127115146512588,
            "pseudo_dstar_susp": 0.0004710315591144607,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0004710315591144607,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.utils.encode_compat_str#4020",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encode_compat_str(string, encoding=preferredencoding(), errors='strict')",
        "snippet": "def encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n    return string if isinstance(string, compat_str) else compat_str(string, encoding, errors)",
        "begin_line": 4020,
        "end_line": 4021,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026954177897574125,
            "pseudo_dstar_susp": 0.00234192037470726,
            "pseudo_tarantula_susp": 0.0021008403361344537,
            "pseudo_op2_susp": 0.00234192037470726,
            "pseudo_barinel_susp": 0.0021008403361344537
        }
    },
    {
        "name": "youtube_dl.utils.parse_age_limit#4043",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_age_limit(s)",
        "snippet": "def parse_age_limit(s):\n    if type(s) == int:\n        return s if 0 <= s <= 21 else None\n    if not isinstance(s, compat_basestring):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None",
        "begin_line": 4043,
        "end_line": 4056,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003850596842510589,
            "pseudo_dstar_susp": 0.00039888312724371757,
            "pseudo_tarantula_susp": 0.002232142857142857,
            "pseudo_op2_susp": 0.00039888312724371757,
            "pseudo_barinel_susp": 0.002232142857142857
        }
    },
    {
        "name": "youtube_dl.utils.strip_jsonp#4059",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.strip_jsonp(code)",
        "snippet": "def strip_jsonp(code):\n    return re.sub(\n        r'''(?sx)^\n            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)\n            (?:\\s*&&\\s*(?P=func_name))?\n            \\s*\\(\\s*(?P<callback_data>.*)\\);?\n            \\s*?(?://[^\\n]*)*$''',\n        r'\\g<callback_data>', code)",
        "begin_line": 4059,
        "end_line": 4066,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005104645227156713,
            "pseudo_dstar_susp": 0.0004901960784313725,
            "pseudo_tarantula_susp": 0.0008880994671403197,
            "pseudo_op2_susp": 0.0004901960784313725,
            "pseudo_barinel_susp": 0.0008880994671403197
        }
    },
    {
        "name": "youtube_dl.utils.js_to_json#4069",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.js_to_json(code)",
        "snippet": "def js_to_json(code):\n    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*'\n    SKIP_RE = r'\\s*(?:{comment})?\\s*'.format(comment=COMMENT_RE)\n    INTEGER_TABLE = (\n        (r'(?s)^(0[xX][0-9a-fA-F]+){skip}:?$'.format(skip=SKIP_RE), 16),\n        (r'(?s)^(0+[0-7]+){skip}:?$'.format(skip=SKIP_RE), 8),\n    )\n\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v.startswith('/*') or v.startswith('//') or v == ',':\n            return \"\"\n\n        if v[0] in (\"'\", '\"'):\n            v = re.sub(r'(?s)\\\\.|\"', lambda m: {\n                '\"': '\\\\\"',\n                \"\\\\'\": \"'\",\n                '\\\\\\n': '',\n                '\\\\x': '\\\\u00',\n            }.get(m.group(0), m.group(0)), v[1:-1])\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return '\"%d\":' % i if v.endswith(':') else '%d' % i\n\n        return '\"%s\"' % v\n\n    return re.sub(r'''(?sx)\n        \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^\"\\\\]*\"|\n        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\['\"nurtbfx/\\n]))*[^'\\\\]*'|\n        {comment}|,(?={skip}[\\]}}])|\n        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|\n        [0-9]+(?={skip}:)\n        '''.format(comment=COMMENT_RE, skip=SKIP_RE), fix_kv, code)",
        "begin_line": 4069,
        "end_line": 4107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000722543352601156,
            "pseudo_dstar_susp": 0.0006605019815059445,
            "pseudo_tarantula_susp": 0.0006049606775559589,
            "pseudo_op2_susp": 0.0006605019815059445,
            "pseudo_barinel_susp": 0.0006049606775559589
        }
    },
    {
        "name": "youtube_dl.utils.fix_kv#4077",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fix_kv(m)",
        "snippet": "    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v.startswith('/*') or v.startswith('//') or v == ',':\n            return \"\"\n\n        if v[0] in (\"'\", '\"'):\n            v = re.sub(r'(?s)\\\\.|\"', lambda m: {\n                '\"': '\\\\\"',\n                \"\\\\'\": \"'\",\n                '\\\\\\n': '',\n                '\\\\x': '\\\\u00',\n            }.get(m.group(0), m.group(0)), v[1:-1])\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return '\"%d\":' % i if v.endswith(':') else '%d' % i\n\n        return '\"%s\"' % v",
        "begin_line": 4077,
        "end_line": 4098,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000625,
            "pseudo_dstar_susp": 0.0005984440454817474,
            "pseudo_tarantula_susp": 0.0017953321364452424,
            "pseudo_op2_susp": 0.0005984440454817474,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.utils.qualities#4110",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.qualities(quality_ids)",
        "snippet": "def qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q",
        "begin_line": 4110,
        "end_line": 4117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006459948320413437,
            "pseudo_dstar_susp": 0.0005931198102016608,
            "pseudo_tarantula_susp": 0.0009157509157509158,
            "pseudo_op2_susp": 0.0005931198102016608,
            "pseudo_barinel_susp": 0.0009165902841429881
        }
    },
    {
        "name": "youtube_dl.utils.q#4112",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.q(qid)",
        "snippet": "    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1",
        "begin_line": 4112,
        "end_line": 4116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005892751915144372,
            "pseudo_dstar_susp": 0.0005617977528089888,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005617977528089888,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.limit_length#4123",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.limit_length(s, length)",
        "snippet": "def limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s",
        "begin_line": 4123,
        "end_line": 4130,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003952569169960474,
            "pseudo_dstar_susp": 0.0004068348250610252,
            "pseudo_tarantula_susp": 0.0005640157924421884,
            "pseudo_op2_susp": 0.0004068348250610252,
            "pseudo_barinel_susp": 0.0005636978579481398
        }
    },
    {
        "name": "youtube_dl.utils.version_tuple#4133",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.version_tuple(v)",
        "snippet": "def version_tuple(v):\n    return tuple(int(e) for e in re.split(r'[-.]', v))",
        "begin_line": 4133,
        "end_line": 4134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.is_outdated_version#4137",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.is_outdated_version(version, limit, assume_new=True)",
        "snippet": "def is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new",
        "begin_line": 4137,
        "end_line": 4143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.ytdl_is_updateable#4146",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.ytdl_is_updateable()",
        "snippet": "def ytdl_is_updateable():\n    \"\"\" Returns if youtube-dl can be updated with -U \"\"\"\n    from zipimport import zipimporter\n\n    return isinstance(globals().get('__loader__'), zipimporter) or hasattr(sys, 'frozen')",
        "begin_line": 4146,
        "end_line": 4150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0032679738562091504,
            "pseudo_dstar_susp": 0.0026246719160104987,
            "pseudo_tarantula_susp": 0.0026666666666666666,
            "pseudo_op2_susp": 0.0026246719160104987,
            "pseudo_barinel_susp": 0.0026666666666666666
        }
    },
    {
        "name": "youtube_dl.utils.args_to_str#4153",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.args_to_str(args)",
        "snippet": "def args_to_str(args):\n    # Get a short string representation for a subprocess command\n    return ' '.join(compat_shlex_quote(a) for a in args)",
        "begin_line": 4153,
        "end_line": 4155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.error_to_compat_str#4158",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.error_to_compat_str(err)",
        "snippet": "def error_to_compat_str(err):\n    err_str = str(err)\n    # On python 2 error byte string must be decoded with proper\n    # encoding rather than ascii\n    if sys.version_info[0] < 3:\n        err_str = err_str.decode(preferredencoding())\n    return err_str",
        "begin_line": 4158,
        "end_line": 4164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002331002331002331,
            "pseudo_dstar_susp": 0.001984126984126984,
            "pseudo_tarantula_susp": 0.0015455950540958269,
            "pseudo_op2_susp": 0.001984126984126984,
            "pseudo_barinel_susp": 0.0015455950540958269
        }
    },
    {
        "name": "youtube_dl.utils.mimetype2ext#4167",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.mimetype2ext(mt)",
        "snippet": "def mimetype2ext(mt):\n    if mt is None:\n        return None\n\n    ext = {\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3. Here use .mp3 as\n        # it's the most popular one\n        'audio/mpeg': 'mp3',\n    }.get(mt)\n    if ext is not None:\n        return ext\n\n    _, _, res = mt.rpartition('/')\n    res = res.split(';')[0].strip().lower()\n\n    return {\n        '3gpp': '3gp',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-flv': 'flv',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-sami': 'sami',\n        'x-ms-wmv': 'wmv',\n        'mpegurl': 'm3u8',\n        'x-mpegurl': 'm3u8',\n        'vnd.apple.mpegurl': 'm3u8',\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.ms-sstr+xml': 'ism',\n        'quicktime': 'mov',\n        'mp2t': 'ts',\n    }.get(res, res)",
        "begin_line": 4167,
        "end_line": 4201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006868131868131869,
            "pseudo_dstar_susp": 0.0006858710562414266,
            "pseudo_tarantula_susp": 0.0005076142131979696,
            "pseudo_op2_susp": 0.0006858710562414266,
            "pseudo_barinel_susp": 0.0005076142131979696
        }
    },
    {
        "name": "youtube_dl.utils.parse_codecs#4204",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_codecs(codecs_str)",
        "snippet": "def parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    splited_codecs = list(filter(None, map(\n        lambda str: str.strip(), codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec = None, None\n    for full_codec in splited_codecs:\n        codec = full_codec.split('.')[0]\n        if codec in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2', 'h263', 'h264', 'mp4v', 'hvc1', 'av01', 'theora'):\n            if not vcodec:\n                vcodec = full_codec\n        elif codec in ('mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            if not acodec:\n                acodec = full_codec\n        else:\n            write_string('WARNING: Unknown codec %s\\n' % full_codec, sys.stderr)\n    if not vcodec and not acodec:\n        if len(splited_codecs) == 2:\n            return {\n                'vcodec': splited_codecs[0],\n                'acodec': splited_codecs[1],\n            }\n    else:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n        }\n    return {}",
        "begin_line": 4204,
        "end_line": 4232,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009165902841429881,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0009551098376313276,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.urlhandle_detect_ext#4235",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.urlhandle_detect_ext(url_handle)",
        "snippet": "def urlhandle_detect_ext(url_handle):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    return mimetype2ext(getheader('Content-Type'))",
        "begin_line": 4235,
        "end_line": 4246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003732736095558044,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003732736095558044,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.encode_data_uri#4249",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encode_data_uri(data, mime_type)",
        "snippet": "def encode_data_uri(data, mime_type):\n    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))",
        "begin_line": 4249,
        "end_line": 4250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00031201248049921997,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00031201248049921997,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.age_restricted#4253",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.age_restricted(content_limit, age_limit)",
        "snippet": "def age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit",
        "begin_line": 4253,
        "end_line": 4260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001692047377326565,
            "pseudo_dstar_susp": 0.0017985611510791368,
            "pseudo_tarantula_susp": 0.00033692722371967657,
            "pseudo_op2_susp": 0.0017985611510791368,
            "pseudo_barinel_susp": 0.00033692722371967657
        }
    },
    {
        "name": "youtube_dl.utils.is_html#4263",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.is_html(first_bytes)",
        "snippet": "def is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    BOMS = [\n        (b'\\xef\\xbb\\xbf', 'utf-8'),\n        (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n        (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n        (b'\\xff\\xfe', 'utf-16-le'),\n        (b'\\xfe\\xff', 'utf-16-be'),\n    ]\n    for bom, enc in BOMS:\n        if first_bytes.startswith(bom):\n            s = first_bytes[len(bom):].decode(enc, 'replace')\n            break\n    else:\n        s = first_bytes.decode('utf-8', 'replace')\n\n    return re.match(r'^\\s*<', s)",
        "begin_line": 4263,
        "end_line": 4280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007496251874062968,
            "pseudo_dstar_susp": 0.0007763975155279503,
            "pseudo_tarantula_susp": 0.00041718815185648727,
            "pseudo_op2_susp": 0.0007763975155279503,
            "pseudo_barinel_susp": 0.00041718815185648727
        }
    },
    {
        "name": "youtube_dl.utils.determine_protocol#4283",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.determine_protocol(info_dict)",
        "snippet": "def determine_protocol(info_dict):\n    protocol = info_dict.get('protocol')\n    if protocol is not None:\n        return protocol\n\n    url = info_dict['url']\n    if url.startswith('rtmp'):\n        return 'rtmp'\n    elif url.startswith('mms'):\n        return 'mms'\n    elif url.startswith('rtsp'):\n        return 'rtsp'\n\n    ext = determine_ext(url)\n    if ext == 'm3u8':\n        return 'm3u8'\n    elif ext == 'f4m':\n        return 'f4m'\n\n    return compat_urllib_parse_urlparse(url).scheme",
        "begin_line": 4283,
        "end_line": 4302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010504201680672268,
            "pseudo_dstar_susp": 0.0011248593925759281,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0011248593925759281,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.utils.render_table#4305",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.render_table(header_row, data)",
        "snippet": "def render_table(header_row, data):\n    \"\"\" Render a list of rows, each as a list of values \"\"\"\n    table = [header_row] + data\n    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]\n    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'\n    return '\\n'.join(format_str % tuple(row) for row in table)",
        "begin_line": 4305,
        "end_line": 4310,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils._match_one#4313",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._match_one(filter_part, dct)",
        "snippet": "def _match_one(filter_part, dct):\n    COMPARISON_OPERATORS = {\n        '<': operator.lt,\n        '<=': operator.le,\n        '>': operator.gt,\n        '>=': operator.ge,\n        '=': operator.eq,\n        '!=': operator.ne,\n    }\n    operator_rex = re.compile(r'''(?x)\\s*\n        (?P<key>[a-z_]+)\n        \\s*(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n        (?:\n            (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|\n            (?P<quote>[\"\\'])(?P<quotedstrval>(?:\\\\.|(?!(?P=quote)|\\\\).)+?)(?P=quote)|\n            (?P<strval>(?![0-9.])[a-z0-9A-Z]*)\n        )\n        \\s*$\n        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n    m = operator_rex.search(filter_part)\n    if m:\n        op = COMPARISON_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if (m.group('quotedstrval') is not None\n            or m.group('strval') is not None\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11082).\n            or actual_value is not None and m.group('intval') is not None\n                and isinstance(actual_value, compat_str)):\n            if m.group('op') not in ('=', '!='):\n                raise ValueError(\n                    'Operator %s does not support string values!' % m.group('op'))\n            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')\n            quote = m.group('quote')\n            if quote is not None:\n                comparison_value = comparison_value.replace(r'\\%s' % quote, quote)\n        else:\n            try:\n                comparison_value = int(m.group('intval'))\n            except ValueError:\n                comparison_value = parse_filesize(m.group('intval'))\n                if comparison_value is None:\n                    comparison_value = parse_filesize(m.group('intval') + 'B')\n                if comparison_value is None:\n                    raise ValueError(\n                        'Invalid integer value %r in filter part %r' % (\n                            m.group('intval'), filter_part))\n        if actual_value is None:\n            return m.group('none_inclusive')\n        return op(actual_value, comparison_value)\n\n    UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n    }\n    operator_rex = re.compile(r'''(?x)\\s*\n        (?P<op>%s)\\s*(?P<key>[a-z_]+)\n        \\s*$\n        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))\n    m = operator_rex.search(filter_part)\n    if m:\n        op = UNARY_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        return op(actual_value)\n\n    raise ValueError('Invalid filter part %r' % filter_part)",
        "begin_line": 4313,
        "end_line": 4380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.match_str#4383",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.match_str(filter_str, dct)",
        "snippet": "def match_str(filter_str, dct):\n    \"\"\" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false \"\"\"\n\n    return all(\n        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))",
        "begin_line": 4383,
        "end_line": 4387,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.match_filter_func#4390",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.match_filter_func(filter_str)",
        "snippet": "def match_filter_func(filter_str):\n    def _match_func(info_dict):\n        if match_str(filter_str, info_dict):\n            return None\n        else:\n            video_title = info_dict.get('title', info_dict.get('id', 'video'))\n            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)\n    return _match_func",
        "begin_line": 4390,
        "end_line": 4397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils._match_func#4391",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._match_func(info_dict)",
        "snippet": "    def _match_func(info_dict):\n        if match_str(filter_str, info_dict):\n            return None\n        else:\n            video_title = info_dict.get('title', info_dict.get('id', 'video'))\n            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)",
        "begin_line": 4391,
        "end_line": 4396,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.parse_dfxp_time_expr#4400",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_dfxp_time_expr(time_expr)",
        "snippet": "def parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))",
        "begin_line": 4400,
        "end_line": 4410,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.srt_subtitles_timecode#4413",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.srt_subtitles_timecode(seconds)",
        "snippet": "def srt_subtitles_timecode(seconds):\n    return '%02d:%02d:%02d,%03d' % (seconds / 3600, (seconds % 3600) / 60, seconds % 60, (seconds % 1) * 1000)",
        "begin_line": 4413,
        "end_line": 4414,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.dfxp2srt#4417",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.dfxp2srt(dfxp_data)",
        "snippet": "def dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser(object):\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)",
        "begin_line": 4417,
        "end_line": 4578,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.TTMLPElementParser.dfxp2srt#4417",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TTMLPElementParser",
        "signature": "youtube_dl.utils.TTMLPElementParser.dfxp2srt(dfxp_data)",
        "snippet": "def dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser(object):\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)",
        "begin_line": 4417,
        "end_line": 4578,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.TTMLPElementParser.start#4456",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TTMLPElementParser",
        "signature": "youtube_dl.utils.TTMLPElementParser.start(self, tag, attrib)",
        "snippet": "        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)",
        "begin_line": 4456,
        "end_line": 4499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.TTMLPElementParser.end#4501",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TTMLPElementParser",
        "signature": "youtube_dl.utils.TTMLPElementParser.end(self, tag)",
        "snippet": "        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()",
        "begin_line": 4501,
        "end_line": 4507,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.TTMLPElementParser.data#4509",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TTMLPElementParser",
        "signature": "youtube_dl.utils.TTMLPElementParser.data(self, data)",
        "snippet": "        def data(self, data):\n            self._out += data",
        "begin_line": 4509,
        "end_line": 4510,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.TTMLPElementParser.close#4512",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TTMLPElementParser",
        "signature": "youtube_dl.utils.TTMLPElementParser.close(self)",
        "snippet": "        def close(self):\n            return self._out.strip()",
        "begin_line": 4512,
        "end_line": 4513,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.parse_node#4515",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_node(node)",
        "snippet": "    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()",
        "begin_line": 4515,
        "end_line": 4519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.cli_option#4581",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.cli_option(params, command_option, param)",
        "snippet": "def cli_option(params, command_option, param):\n    param = params.get(param)\n    if param:\n        param = compat_str(param)\n    return [command_option, param] if param is not None else []",
        "begin_line": 4581,
        "end_line": 4585,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.cli_bool_option#4588",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None)",
        "snippet": "def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    if param is None:\n        return []\n    assert isinstance(param, bool)\n    if separator:\n        return [command_option + separator + (true_value if param else false_value)]\n    return [command_option, true_value if param else false_value]",
        "begin_line": 4588,
        "end_line": 4595,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.cli_valueless_option#4598",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.cli_valueless_option(params, command_option, param, expected_value=True)",
        "snippet": "def cli_valueless_option(params, command_option, param, expected_value=True):\n    param = params.get(param)\n    return [command_option] if param == expected_value else []",
        "begin_line": 4598,
        "end_line": 4600,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.cli_configuration_args#4603",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.cli_configuration_args(params, param, default=[])",
        "snippet": "def cli_configuration_args(params, param, default=[]):\n    ex_args = params.get(param)\n    if ex_args is None:\n        return default\n    assert isinstance(ex_args, list)\n    return ex_args",
        "begin_line": 4603,
        "end_line": 4608,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.ISO639Utils.long2short#4809",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ISO639Utils",
        "signature": "youtube_dl.utils.ISO639Utils.long2short(cls, code)",
        "snippet": "    def long2short(cls, code):\n        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n        for short_name, long_name in cls._lang_map.items():\n            if long_name == code:\n                return short_name",
        "begin_line": 4809,
        "end_line": 4813,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.utils.ISO3166Utils.short2full#5071",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ISO3166Utils",
        "signature": "youtube_dl.utils.ISO3166Utils.short2full(cls, code)",
        "snippet": "    def short2full(cls, code):\n        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n        return cls._country_map.get(code.upper())",
        "begin_line": 5071,
        "end_line": 5073,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005359056806002144,
            "pseudo_dstar_susp": 0.0004803073967339097,
            "pseudo_tarantula_susp": 0.0023752969121140144,
            "pseudo_op2_susp": 0.0004803073967339097,
            "pseudo_barinel_susp": 0.0023752969121140144
        }
    },
    {
        "name": "youtube_dl.utils.GeoUtils.random_ipv4#5322",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.GeoUtils",
        "signature": "youtube_dl.utils.GeoUtils.random_ipv4(cls, code_or_block)",
        "snippet": "    def random_ipv4(cls, code_or_block):\n        if len(code_or_block) == 2:\n            block = cls._country_ip_map.get(code_or_block.upper())\n            if not block:\n                return None\n        else:\n            block = code_or_block\n        addr, preflen = block.split('/')\n        addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n        addr_max = addr_min | (0xffffffff >> int(preflen))\n        return compat_str(socket.inet_ntoa(\n            compat_struct_pack('!L', random.randint(addr_min, addr_max))))",
        "begin_line": 5322,
        "end_line": 5333,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009746588693957114,
            "pseudo_dstar_susp": 0.0008019246190858059,
            "pseudo_tarantula_susp": 0.0015267175572519084,
            "pseudo_op2_susp": 0.0008019246190858059,
            "pseudo_barinel_susp": 0.0015267175572519084
        }
    },
    {
        "name": "youtube_dl.utils.PerRequestProxyHandler.__init__#5337",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.PerRequestProxyHandler",
        "signature": "youtube_dl.utils.PerRequestProxyHandler.__init__(self, proxies=None)",
        "snippet": "    def __init__(self, proxies=None):\n        # Set default handlers\n        for type in ('http', 'https'):\n            setattr(self, '%s_open' % type,\n                    lambda r, proxy='__noproxy__', type=type, meth=self.proxy_open:\n                        meth(r, proxy, type))\n        compat_urllib_request.ProxyHandler.__init__(self, proxies)",
        "begin_line": 5337,
        "end_line": 5343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01282051282051282,
            "pseudo_dstar_susp": 0.012195121951219513,
            "pseudo_tarantula_susp": 0.0011933174224343676,
            "pseudo_op2_susp": 0.012195121951219513,
            "pseudo_barinel_susp": 0.0011933174224343676
        }
    },
    {
        "name": "youtube_dl.utils.PerRequestProxyHandler.proxy_open#5345",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.PerRequestProxyHandler",
        "signature": "youtube_dl.utils.PerRequestProxyHandler.proxy_open(self, req, proxy, type)",
        "snippet": "    def proxy_open(self, req, proxy, type):\n        req_proxy = req.headers.get('Ytdl-request-proxy')\n        if req_proxy is not None:\n            proxy = req_proxy\n            del req.headers['Ytdl-request-proxy']\n\n        if proxy == '__noproxy__':\n            return None  # No Proxy\n        if compat_urlparse.urlparse(proxy).scheme.lower() in ('socks', 'socks4', 'socks4a', 'socks5'):\n            req.add_header('Ytdl-socks-proxy', proxy)\n            # youtube-dl's http/https handlers do wrapping the socket with socks\n            return None\n        return compat_urllib_request.ProxyHandler.proxy_open(\n            self, req, proxy, type)",
        "begin_line": 5345,
        "end_line": 5358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01282051282051282,
            "pseudo_dstar_susp": 0.007518796992481203,
            "pseudo_tarantula_susp": 0.0011933174224343676,
            "pseudo_op2_susp": 0.007518796992481203,
            "pseudo_barinel_susp": 0.0011933174224343676
        }
    },
    {
        "name": "youtube_dl.utils.long_to_bytes#5365",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.long_to_bytes(n, blocksize=0)",
        "snippet": "def long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = compat_struct_pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s",
        "begin_line": 5365,
        "end_line": 5392,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.bytes_to_long#5395",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.bytes_to_long(s)",
        "snippet": "def bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n    return acc",
        "begin_line": 5395,
        "end_line": 5409,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.ohdave_rsa_encrypt#5412",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.ohdave_rsa_encrypt(data, exponent, modulus)",
        "snippet": "def ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted",
        "begin_line": 5412,
        "end_line": 5426,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.pkcs1pad#5429",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.pkcs1pad(data, length)",
        "snippet": "def pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data",
        "begin_line": 5429,
        "end_line": 5441,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.encode_base_n#5444",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encode_base_n(num, n, table=None)",
        "snippet": "def encode_base_n(num, n, table=None):\n    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if not table:\n        table = FULL_TABLE[:n]\n\n    if n > len(table):\n        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n\n    if num == 0:\n        return table[0]\n\n    ret = ''\n    while num:\n        ret = table[num % n] + ret\n        num = num // n\n    return ret",
        "begin_line": 5444,
        "end_line": 5459,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.decode_packed_codes#5462",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.decode_packed_codes(code)",
        "snippet": "def decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfucasted_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfucasted_code)",
        "begin_line": 5462,
        "end_line": 5477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.caesar#5480",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.caesar(s, alphabet, shift)",
        "snippet": "def caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)",
        "begin_line": 5480,
        "end_line": 5486,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.rot47#5489",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.rot47(s)",
        "snippet": "def rot47(s):\n    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)",
        "begin_line": 5489,
        "end_line": 5490,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.parse_m3u8_attributes#5493",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_m3u8_attributes(attrib)",
        "snippet": "def parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info",
        "begin_line": 5493,
        "end_line": 5499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008795074758135445,
            "pseudo_dstar_susp": 0.0009267840593141798,
            "pseudo_tarantula_susp": 0.00045187528242205153,
            "pseudo_op2_susp": 0.0009267840593141798,
            "pseudo_barinel_susp": 0.00045187528242205153
        }
    },
    {
        "name": "youtube_dl.utils.urshift#5502",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.urshift(val, n)",
        "snippet": "def urshift(val, n):\n    return val >> n if val >= 0 else (val + 0x100000000) >> n",
        "begin_line": 5502,
        "end_line": 5503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00039761431411530816,
            "pseudo_dstar_susp": 0.0003763643206624012,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0003763643206624012,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.utils.decode_png#5508",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.decode_png(png_data)",
        "snippet": "def decode_png(png_data):\n    # Reference: https://www.w3.org/TR/PNG/\n    header = png_data[8:]\n\n    if png_data[:8] != b'\\x89PNG\\x0d\\x0a\\x1a\\x0a' or header[4:8] != b'IHDR':\n        raise IOError('Not a valid PNG file.')\n\n    int_map = {1: '>B', 2: '>H', 4: '>I'}\n    unpack_integer = lambda x: compat_struct_unpack(int_map[len(x)], x)[0]\n\n    chunks = []\n\n    while header:\n        length = unpack_integer(header[:4])\n        header = header[4:]\n\n        chunk_type = header[:4]\n        header = header[4:]\n\n        chunk_data = header[:length]\n        header = header[length:]\n\n        header = header[4:]  # Skip CRC\n\n        chunks.append({\n            'type': chunk_type,\n            'length': length,\n            'data': chunk_data\n        })\n\n    ihdr = chunks[0]['data']\n\n    width = unpack_integer(ihdr[:4])\n    height = unpack_integer(ihdr[4:8])\n\n    idat = b''\n\n    for chunk in chunks:\n        if chunk['type'] == b'IDAT':\n            idat += chunk['data']\n\n    if not idat:\n        raise IOError('Unable to read PNG data.')\n\n    decompressed_data = bytearray(zlib.decompress(idat))\n\n    stride = width * 3\n    pixels = []\n\n    def _get_pixel(idx):\n        x = idx % stride\n        y = idx // stride\n        return pixels[y][x]\n\n    for y in range(height):\n        basePos = y * (1 + stride)\n        filter_type = decompressed_data[basePos]\n\n        current_row = []\n\n        pixels.append(current_row)\n\n        for x in range(stride):\n            color = decompressed_data[1 + basePos + x]\n            basex = y * stride + x\n            left = 0\n            up = 0\n\n            if x > 2:\n                left = _get_pixel(basex - 3)\n            if y > 0:\n                up = _get_pixel(basex - stride)\n\n            if filter_type == 1:  # Sub\n                color = (color + left) & 0xff\n            elif filter_type == 2:  # Up\n                color = (color + up) & 0xff\n            elif filter_type == 3:  # Average\n                color = (color + ((left + up) >> 1)) & 0xff\n            elif filter_type == 4:  # Paeth\n                a = left\n                b = up\n                c = 0\n\n                if x > 2 and y > 0:\n                    c = _get_pixel(basex - stride - 3)\n\n                p = a + b - c\n\n                pa = abs(p - a)\n                pb = abs(p - b)\n                pc = abs(p - c)\n\n                if pa <= pb and pa <= pc:\n                    color = (color + a) & 0xff\n                elif pb <= pc:\n                    color = (color + b) & 0xff\n                else:\n                    color = (color + c) & 0xff\n\n            current_row.append(color)\n\n    return width, height, pixels",
        "begin_line": 5508,
        "end_line": 5610,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.write_xattr#5613",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.write_xattr(path, key, value)",
        "snippet": "def write_xattr(path, key, value):\n    # This mess below finds the best xattr tool for the job\n    try:\n        # try the pyxattr module...\n        import xattr\n\n        if hasattr(xattr, 'set'):  # pyxattr\n            # Unicode arguments are not supported in python-pyxattr until\n            # version 0.5.0\n            # See https://github.com/ytdl-org/youtube-dl/issues/5498\n            pyxattr_required_version = '0.5.0'\n            if version_tuple(xattr.__version__) < version_tuple(pyxattr_required_version):\n                # TODO: fallback to CLI tools\n                raise XAttrUnavailableError(\n                    'python-pyxattr is detected but is too old. '\n                    'youtube-dl requires %s or above while your version is %s. '\n                    'Falling back to other xattr implementations' % (\n                        pyxattr_required_version, xattr.__version__))\n\n            setxattr = xattr.set\n        else:  # xattr\n            setxattr = xattr.setxattr\n\n        try:\n            setxattr(path, key, value)\n        except EnvironmentError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n\n    except ImportError:\n        if compat_os_name == 'nt':\n            # Write xattrs to NTFS Alternate Data Streams:\n            # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n            assert ':' not in key\n            assert os.path.exists(path)\n\n            ads_fn = path + ':' + key\n            try:\n                with open(ads_fn, 'wb') as f:\n                    f.write(value)\n            except EnvironmentError as e:\n                raise XAttrMetadataError(e.errno, e.strerror)\n        else:\n            user_has_setfattr = check_executable('setfattr', ['--version'])\n            user_has_xattr = check_executable('xattr', ['-h'])\n\n            if user_has_setfattr or user_has_xattr:\n\n                value = value.decode('utf-8')\n                if user_has_setfattr:\n                    executable = 'setfattr'\n                    opts = ['-n', key, '-v', value]\n                elif user_has_xattr:\n                    executable = 'xattr'\n                    opts = ['-w', key, value]\n\n                cmd = ([encodeFilename(executable, True)]\n                       + [encodeArgument(o) for o in opts]\n                       + [encodeFilename(path, True)])\n\n                try:\n                    p = subprocess.Popen(\n                        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n                except EnvironmentError as e:\n                    raise XAttrMetadataError(e.errno, e.strerror)\n                stdout, stderr = p.communicate()\n                stderr = stderr.decode('utf-8', 'replace')\n                if p.returncode != 0:\n                    raise XAttrMetadataError(p.returncode, stderr)\n\n            else:\n                # On Unix, and can't find pyxattr, setfattr, or xattr.\n                if sys.platform.startswith('linux'):\n                    raise XAttrUnavailableError(\n                        \"Couldn't find a tool to set the xattrs. \"\n                        \"Install either the python 'pyxattr' or 'xattr' \"\n                        \"modules, or the GNU 'attr' package \"\n                        \"(which contains the 'setfattr' tool).\")\n                else:\n                    raise XAttrUnavailableError(\n                        \"Couldn't find a tool to set the xattrs. \"\n                        \"Install either the python 'xattr' module, \"\n                        \"or the 'xattr' binary.\")",
        "begin_line": 5613,
        "end_line": 5694,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.utils.random_birthday#5697",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.random_birthday(year_field, month_field, day_field)",
        "snippet": "def random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }",
        "begin_line": 5697,
        "end_line": 5706,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.lynda.LyndaBaseIE._real_initialize#24",
        "src_path": "youtube_dl/extractor/lynda.py",
        "class_name": "youtube_dl.extractor.lynda.LyndaBaseIE",
        "signature": "youtube_dl.extractor.lynda.LyndaBaseIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 24,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.lynda.LyndaIE._fix_subtitles#231",
        "src_path": "youtube_dl/extractor/lynda.py",
        "class_name": "youtube_dl.extractor.lynda.LyndaIE",
        "signature": "youtube_dl.extractor.lynda.LyndaIE._fix_subtitles(self, subs)",
        "snippet": "    def _fix_subtitles(self, subs):\n        srt = ''\n        seq_counter = 0\n        for pos in range(0, len(subs) - 1):\n            seq_current = subs[pos]\n            m_current = re.match(self._TIMECODE_REGEX, seq_current['Timecode'])\n            if m_current is None:\n                continue\n            seq_next = subs[pos + 1]\n            m_next = re.match(self._TIMECODE_REGEX, seq_next['Timecode'])\n            if m_next is None:\n                continue\n            appear_time = m_current.group('timecode')\n            disappear_time = m_next.group('timecode')\n            text = seq_current['Caption'].strip()\n            if text:\n                seq_counter += 1\n                srt += '%s\\r\\n%s --> %s\\r\\n%s\\r\\n\\r\\n' % (seq_counter, appear_time, disappear_time, text)\n        if srt:\n            return srt",
        "begin_line": 231,
        "end_line": 250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018083182640144665,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.lynda.LyndaIE._get_subtitles#252",
        "src_path": "youtube_dl/extractor/lynda.py",
        "class_name": "youtube_dl.extractor.lynda.LyndaIE",
        "signature": "youtube_dl.extractor.lynda.LyndaIE._get_subtitles(self, video_id)",
        "snippet": "    def _get_subtitles(self, video_id):\n        url = 'https://www.lynda.com/ajax/player?videoId=%s&type=transcript' % video_id\n        subs = self._download_webpage(\n            url, video_id, 'Downloading subtitles JSON', fatal=False)\n        if not subs or 'Status=\"NotFound\"' in subs:\n            return {}\n        subs = self._parse_json(subs, video_id, fatal=False)\n        if not subs:\n            return {}\n        fixed_subs = self._fix_subtitles(subs)\n        if fixed_subs:\n            return {'en': [{'ext': 'srt', 'data': fixed_subs}]}\n        return {}",
        "begin_line": 252,
        "end_line": 264,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.pbs.PBSIE._extract_webpage#438",
        "src_path": "youtube_dl/extractor/pbs.py",
        "class_name": "youtube_dl.extractor.pbs.PBSIE",
        "signature": "youtube_dl.extractor.pbs.PBSIE._extract_webpage(self, url)",
        "snippet": "    def _extract_webpage(self, url):\n        mobj = re.match(self._VALID_URL, url)\n\n        description = None\n\n        presumptive_id = mobj.group('presumptive_id')\n        display_id = presumptive_id\n        if presumptive_id:\n            webpage = self._download_webpage(url, display_id)\n\n            description = strip_or_none(self._og_search_description(\n                webpage, default=None) or self._html_search_meta(\n                'description', webpage, default=None))\n            upload_date = unified_strdate(self._search_regex(\n                r'<input type=\"hidden\" id=\"air_date_[0-9]+\" value=\"([^\"]+)\"',\n                webpage, 'upload date', default=None))\n\n            # tabbed frontline videos\n            MULTI_PART_REGEXES = (\n                r'<div[^>]+class=\"videotab[^\"]*\"[^>]+vid=\"(\\d+)\"',\n                r'<a[^>]+href=[\"\\']#(?:video-|part)\\d+[\"\\'][^>]+data-cove[Ii]d=[\"\\'](\\d+)',\n            )\n            for p in MULTI_PART_REGEXES:\n                tabbed_videos = orderedSet(re.findall(p, webpage))\n                if tabbed_videos:\n                    return tabbed_videos, presumptive_id, upload_date, description\n\n            MEDIA_ID_REGEXES = [\n                r\"div\\s*:\\s*'videoembed'\\s*,\\s*mediaid\\s*:\\s*'(\\d+)'\",  # frontline video embed\n                r'class=\"coveplayerid\">([^<]+)<',                       # coveplayer\n                r'<section[^>]+data-coveid=\"(\\d+)\"',                    # coveplayer from http://www.pbs.org/wgbh/frontline/film/real-csi/\n                r'<input type=\"hidden\" id=\"pbs_video_id_[0-9]+\" value=\"([0-9]+)\"/>',  # jwplayer\n                r\"(?s)window\\.PBS\\.playerConfig\\s*=\\s*{.*?id\\s*:\\s*'([0-9]+)',\",\n                r'<div[^>]+\\bdata-cove-id=[\"\\'](\\d+)\"',  # http://www.pbs.org/wgbh/roadshow/watch/episode/2105-indianapolis-hour-2/\n                r'<iframe[^>]+\\bsrc=[\"\\'](?:https?:)?//video\\.pbs\\.org/widget/partnerplayer/(\\d+)',  # https://www.pbs.org/wgbh/masterpiece/episodes/victoria-s2-e1/\n            ]\n\n            media_id = self._search_regex(\n                MEDIA_ID_REGEXES, webpage, 'media ID', fatal=False, default=None)\n            if media_id:\n                return media_id, presumptive_id, upload_date, description\n\n            # Fronline video embedded via flp\n            video_id = self._search_regex(\n                r'videoid\\s*:\\s*\"([\\d+a-z]{7,})\"', webpage, 'videoid', default=None)\n            if video_id:\n                # pkg_id calculation is reverse engineered from\n                # http://www.pbs.org/wgbh/pages/frontline/js/flp2012.js\n                prg_id = self._search_regex(\n                    r'videoid\\s*:\\s*\"([\\d+a-z]{7,})\"', webpage, 'videoid')[7:]\n                if 'q' in prg_id:\n                    prg_id = prg_id.split('q')[1]\n                prg_id = int(prg_id, 16)\n                getdir = self._download_json(\n                    'http://www.pbs.org/wgbh/pages/frontline/.json/getdir/getdir%d.json' % prg_id,\n                    presumptive_id, 'Downloading getdir JSON',\n                    transform_source=strip_jsonp)\n                return getdir['mid'], presumptive_id, upload_date, description\n\n            for iframe in re.findall(r'(?s)<iframe(.+?)></iframe>', webpage):\n                url = self._search_regex(\n                    r'src=([\"\\'])(?P<url>.+?partnerplayer.+?)\\1', iframe,\n                    'player URL', default=None, group='url')\n                if url:\n                    break\n\n            if not url:\n                url = self._og_search_url(webpage)\n\n            mobj = re.match(\n                self._VALID_URL, self._proto_relative_url(url.strip()))\n\n        player_id = mobj.group('player_id')\n        if not display_id:\n            display_id = player_id\n        if player_id:\n            player_page = self._download_webpage(\n                url, display_id, note='Downloading player page',\n                errnote='Could not download player page')\n            video_id = self._search_regex(\n                r'<div\\s+id=[\"\\']video_(\\d+)', player_page, 'video ID',\n                default=None)\n            if not video_id:\n                video_info = self._extract_video_data(\n                    player_page, 'video data', display_id)\n                video_id = compat_str(\n                    video_info.get('id') or video_info['contentID'])\n        else:\n            video_id = mobj.group('id')\n            display_id = video_id\n\n        return video_id, display_id, None, description",
        "begin_line": 438,
        "end_line": 529,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00036469730123997083,
            "pseudo_tarantula_susp": 0.00034352456200618345,
            "pseudo_op2_susp": 0.00036469730123997083,
            "pseudo_barinel_susp": 0.00034423407917383823
        }
    },
    {
        "name": "youtube_dl.extractor.pbs.PBSIE.extract_redirect_urls#552",
        "src_path": "youtube_dl/extractor/pbs.py",
        "class_name": "youtube_dl.extractor.pbs.PBSIE",
        "signature": "youtube_dl.extractor.pbs.PBSIE.extract_redirect_urls(info)",
        "snippet": "        def extract_redirect_urls(info):\n            for encoding_name in ('recommended_encoding', 'alternate_encoding'):\n                redirect = info.get(encoding_name)\n                if not redirect:\n                    continue\n                redirect_url = redirect.get('url')\n                if redirect_url and redirect_url not in redirect_urls:\n                    redirects.append(redirect)\n                    redirect_urls.add(redirect_url)\n            encodings = info.get('encodings')\n            if isinstance(encodings, list):\n                for encoding in encodings:\n                    encoding_url = url_or_none(encoding)\n                    if encoding_url and encoding_url not in redirect_urls:\n                        redirects.append({'url': encoding_url})\n                        redirect_urls.add(encoding_url)",
        "begin_line": 552,
        "end_line": 567,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.pbs.PBSIE._real_extract#539",
        "src_path": "youtube_dl/extractor/pbs.py",
        "class_name": "youtube_dl.extractor.pbs.PBSIE",
        "signature": "youtube_dl.extractor.pbs.PBSIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id, display_id, upload_date, description = self._extract_webpage(url)\n\n        if isinstance(video_id, list):\n            entries = [self.url_result(\n                'http://video.pbs.org/video/%s' % vid_id, 'PBS', vid_id)\n                for vid_id in video_id]\n            return self.playlist_result(entries, display_id)\n\n        info = None\n        redirects = []\n        redirect_urls = set()\n\n        def extract_redirect_urls(info):\n            for encoding_name in ('recommended_encoding', 'alternate_encoding'):\n                redirect = info.get(encoding_name)\n                if not redirect:\n                    continue\n                redirect_url = redirect.get('url')\n                if redirect_url and redirect_url not in redirect_urls:\n                    redirects.append(redirect)\n                    redirect_urls.add(redirect_url)\n            encodings = info.get('encodings')\n            if isinstance(encodings, list):\n                for encoding in encodings:\n                    encoding_url = url_or_none(encoding)\n                    if encoding_url and encoding_url not in redirect_urls:\n                        redirects.append({'url': encoding_url})\n                        redirect_urls.add(encoding_url)\n\n        chapters = []\n        # Player pages may also serve different qualities\n        for page in ('widget/partnerplayer', 'portalplayer'):\n            player = self._download_webpage(\n                'http://player.pbs.org/%s/%s' % (page, video_id),\n                display_id, 'Downloading %s page' % page, fatal=False)\n            if player:\n                video_info = self._extract_video_data(\n                    player, '%s video data' % page, display_id, fatal=False)\n                if video_info:\n                    extract_redirect_urls(video_info)\n                    if not info:\n                        info = video_info\n                if not chapters:\n                    raw_chapters = video_info.get('chapters') or []\n                    if not raw_chapters:\n                        for chapter_data in re.findall(r'(?s)chapters\\.push\\(({.*?})\\)', player):\n                            chapter = self._parse_json(chapter_data, video_id, js_to_json, fatal=False)\n                            if not chapter:\n                                continue\n                            raw_chapters.append(chapter)\n                    for chapter in raw_chapters:\n                        start_time = float_or_none(chapter.get('start_time'), 1000)\n                        duration = float_or_none(chapter.get('duration'), 1000)\n                        if start_time is None or duration is None:\n                            continue\n                        chapters.append({\n                            'start_time': start_time,\n                            'end_time': start_time + duration,\n                            'title': chapter.get('title'),\n                        })\n\n        formats = []\n        http_url = None\n        for num, redirect in enumerate(redirects):\n            redirect_id = redirect.get('eeid')\n\n            redirect_info = self._download_json(\n                '%s?format=json' % redirect['url'], display_id,\n                'Downloading %s video url info' % (redirect_id or num),\n                headers=self.geo_verification_headers())\n\n            if redirect_info['status'] == 'error':\n                message = self._ERRORS.get(\n                    redirect_info['http_code'], redirect_info['message'])\n                if redirect_info['http_code'] == 403:\n                    self.raise_geo_restricted(\n                        msg=message, countries=self._GEO_COUNTRIES)\n                raise ExtractorError(\n                    '%s said: %s' % (self.IE_NAME, message), expected=True)\n\n            format_url = redirect_info.get('url')\n            if not format_url:\n                continue\n\n            if determine_ext(format_url) == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    format_url, display_id, 'mp4', m3u8_id='hls', fatal=False))\n            else:\n                formats.append({\n                    'url': format_url,\n                    'format_id': redirect_id,\n                })\n                if re.search(r'^https?://.*(?:\\d+k|baseline)', format_url):\n                    http_url = format_url\n        self._remove_duplicate_formats(formats)\n        m3u8_formats = list(filter(\n            lambda f: f.get('protocol') == 'm3u8' and f.get('vcodec') != 'none',\n            formats))\n        if http_url:\n            for m3u8_format in m3u8_formats:\n                bitrate = self._search_regex(r'(\\d+)k', m3u8_format['url'], 'bitrate', default=None)\n                # Lower qualities (150k and 192k) are not available as HTTP formats (see [1]),\n                # we won't try extracting them.\n                # Since summer 2016 higher quality formats (4500k and 6500k) are also available\n                # albeit they are not documented in [2].\n                # 1. https://github.com/ytdl-org/youtube-dl/commit/cbc032c8b70a038a69259378c92b4ba97b42d491#commitcomment-17313656\n                # 2. https://projects.pbs.org/confluence/display/coveapi/COVE+Video+Specifications\n                if not bitrate or int(bitrate) < 400:\n                    continue\n                f_url = re.sub(r'\\d+k|baseline', bitrate + 'k', http_url)\n                # This may produce invalid links sometimes (e.g.\n                # http://www.pbs.org/wgbh/frontline/film/suicide-plan)\n                if not self._is_valid_url(f_url, display_id, 'http-%sk video' % bitrate):\n                    continue\n                f = m3u8_format.copy()\n                f.update({\n                    'url': f_url,\n                    'format_id': m3u8_format['format_id'].replace('hls', 'http'),\n                    'protocol': 'http',\n                })\n                formats.append(f)\n        self._sort_formats(formats)\n\n        rating_str = info.get('rating')\n        if rating_str is not None:\n            rating_str = rating_str.rpartition('-')[2]\n        age_limit = US_RATINGS.get(rating_str)\n\n        subtitles = {}\n        closed_captions_url = info.get('closed_captions_url')\n        if closed_captions_url:\n            subtitles['en'] = [{\n                'ext': 'ttml',\n                'url': closed_captions_url,\n            }]\n            mobj = re.search(r'/(\\d+)_Encoded\\.dfxp', closed_captions_url)\n            if mobj:\n                ttml_caption_suffix, ttml_caption_id = mobj.group(0, 1)\n                ttml_caption_id = int(ttml_caption_id)\n                subtitles['en'].extend([{\n                    'url': closed_captions_url.replace(\n                        ttml_caption_suffix, '/%d_Encoded.srt' % (ttml_caption_id + 1)),\n                    'ext': 'srt',\n                }, {\n                    'url': closed_captions_url.replace(\n                        ttml_caption_suffix, '/%d_Encoded.vtt' % (ttml_caption_id + 2)),\n                    'ext': 'vtt',\n                }])\n\n        # info['title'] is often incomplete (e.g. 'Full Episode', 'Episode 5', etc)\n        # Try turning it to 'program - title' naming scheme if possible\n        alt_title = info.get('program', {}).get('title')\n        if alt_title:\n            info['title'] = alt_title + ' - ' + re.sub(r'^' + alt_title + r'[\\s\\-:]+', '', info['title'])\n\n        description = info.get('description') or info.get(\n            'program', {}).get('description') or description\n\n        return {\n            'id': video_id,\n            'display_id': display_id,\n            'title': info['title'],\n            'description': description,\n            'thumbnail': info.get('image_url'),\n            'duration': int_or_none(info.get('duration')),\n            'age_limit': age_limit,\n            'upload_date': upload_date,\n            'formats': formats,\n            'subtitles': subtitles,\n            'chapters': chapters,\n        }",
        "begin_line": 539,
        "end_line": 710,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.amp.AMPIE._extract_feed_info#17",
        "src_path": "youtube_dl/extractor/amp.py",
        "class_name": "youtube_dl.extractor.amp.AMPIE",
        "signature": "youtube_dl.extractor.amp.AMPIE._extract_feed_info(self, url)",
        "snippet": "    def _extract_feed_info(self, url):\n        feed = self._download_json(\n            url, None, 'Downloading Akamai AMP feed',\n            'Unable to download Akamai AMP feed')\n        item = feed.get('channel', {}).get('item')\n        if not item:\n            raise ExtractorError('%s said: %s' % (self.IE_NAME, feed['error']))\n\n        video_id = item['guid']\n\n        def get_media_node(name, default=None):\n            media_name = 'media-%s' % name\n            media_group = item.get('media-group') or item\n            return media_group.get(media_name) or item.get(media_name) or item.get(name, default)\n\n        thumbnails = []\n        media_thumbnail = get_media_node('thumbnail')\n        if media_thumbnail:\n            if isinstance(media_thumbnail, dict):\n                media_thumbnail = [media_thumbnail]\n            for thumbnail_data in media_thumbnail:\n                thumbnail = thumbnail_data.get('@attributes', {})\n                thumbnail_url = url_or_none(thumbnail.get('url'))\n                if not thumbnail_url:\n                    continue\n                thumbnails.append({\n                    'url': self._proto_relative_url(thumbnail_url, 'http:'),\n                    'width': int_or_none(thumbnail.get('width')),\n                    'height': int_or_none(thumbnail.get('height')),\n                })\n\n        subtitles = {}\n        media_subtitle = get_media_node('subTitle')\n        if media_subtitle:\n            if isinstance(media_subtitle, dict):\n                media_subtitle = [media_subtitle]\n            for subtitle_data in media_subtitle:\n                subtitle = subtitle_data.get('@attributes', {})\n                subtitle_href = url_or_none(subtitle.get('href'))\n                if not subtitle_href:\n                    continue\n                subtitles.setdefault(subtitle.get('lang') or 'en', []).append({\n                    'url': subtitle_href,\n                    'ext': mimetype2ext(subtitle.get('type')) or determine_ext(subtitle_href),\n                })\n\n        formats = []\n        media_content = get_media_node('content')\n        if isinstance(media_content, dict):\n            media_content = [media_content]\n        for media_data in media_content:\n            media = media_data.get('@attributes', {})\n            media_url = url_or_none(media.get('url'))\n            if not media_url:\n                continue\n            ext = mimetype2ext(media.get('type')) or determine_ext(media_url)\n            if ext == 'f4m':\n                formats.extend(self._extract_f4m_formats(\n                    media_url + '?hdcore=3.4.0&plugin=aasp-3.4.0.132.124',\n                    video_id, f4m_id='hds', fatal=False))\n            elif ext == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    media_url, video_id, 'mp4', m3u8_id='hls', fatal=False))\n            else:\n                formats.append({\n                    'format_id': media_data.get('media-category', {}).get('@attributes', {}).get('label'),\n                    'url': media_url,\n                    'tbr': int_or_none(media.get('bitrate')),\n                    'filesize': int_or_none(media.get('fileSize')),\n                    'ext': ext,\n                })\n\n        self._sort_formats(formats)\n\n        timestamp = parse_iso8601(item.get('pubDate'), ' ') or parse_iso8601(item.get('dc-date'))\n\n        return {\n            'id': video_id,\n            'title': get_media_node('title'),\n            'description': get_media_node('description'),\n            'thumbnails': thumbnails,\n            'timestamp': timestamp,\n            'duration': int_or_none(media_content[0].get('@attributes', {}).get('duration')),\n            'subtitles': subtitles,\n            'formats': formats,\n        }",
        "begin_line": 17,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000779423226812159,
            "pseudo_dstar_susp": 0.000791765637371338,
            "pseudo_tarantula_susp": 0.0004885197850512946,
            "pseudo_op2_susp": 0.000791765637371338,
            "pseudo_barinel_susp": 0.0004885197850512946
        }
    },
    {
        "name": "youtube_dl.extractor.videomore.VideomoreVideoIE.suitable#206",
        "src_path": "youtube_dl/extractor/videomore.py",
        "class_name": "youtube_dl.extractor.videomore.VideomoreVideoIE",
        "signature": "youtube_dl.extractor.videomore.VideomoreVideoIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if VideomoreIE.suitable(url) else super(VideomoreVideoIE, cls).suitable(url)",
        "begin_line": 206,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiSDK.date#96",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiSDK",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiSDK.date(self, scheme)",
        "snippet": "    def date(self, scheme):\n        self.target = md5_text(self.target)\n        d = time.localtime(self.timestamp)\n        strings = {\n            'y': compat_str(d.tm_year),\n            'm': '%02d' % d.tm_mon,\n            'd': '%02d' % d.tm_mday,\n        }\n        self.target += ''.join(map(lambda c: strings[c], list(scheme)))",
        "begin_line": 96,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011435105774728416,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiSDK.split_time_even_odd#106",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiSDK",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiSDK.split_time_even_odd(self)",
        "snippet": "    def split_time_even_odd(self):\n        even, odd = self.even_odd()\n        self.target = odd + md5_text(self.target) + even",
        "begin_line": 106,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011437721605856113,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiSDK.split_time_ip_sum#118",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiSDK",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiSDK.split_time_ip_sum(self)",
        "snippet": "    def split_time_ip_sum(self):\n        chunks, ip = self.preprocess(32)\n        self.target = self.digit_sum(self.timestamp) + chunks[0] + compat_str(sum(ip))",
        "begin_line": 118,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011473152822395595,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiSDKInterpreter.__init__#124",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiSDKInterpreter",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiSDKInterpreter.__init__(self, sdk_code)",
        "snippet": "    def __init__(self, sdk_code):\n        self.sdk_code = sdk_code",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011471836641046231,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiIE._login#241",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiIE",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiIE._login(self)",
        "snippet": "    def _login(self):\n        username, password = self._get_login_info()\n\n        # No authentication to be performed\n        if not username:\n            return True\n\n        data = self._download_json(\n            'http://kylin.iqiyi.com/get_token', None,\n            note='Get token for logging', errnote='Unable to get token for logging')\n        sdk = data['sdk']\n        timestamp = int(time.time())\n        target = '/apis/reglogin/login.action?lang=zh_TW&area_code=null&email=%s&passwd=%s&agenttype=1&from=undefined&keeplogin=0&piccode=&fromurl=&_pos=1' % (\n            username, self._rsa_fun(password.encode('utf-8')))\n\n        interp = IqiyiSDKInterpreter(sdk)\n        sign = interp.run(target, data['ip'], timestamp)\n\n        validation_params = {\n            'target': target,\n            'server': 'BEA3AA1908656AABCCFF76582C4C6660',\n            'token': data['token'],\n            'bird_src': 'f8d91d57af224da7893dd397d52d811a',\n            'sign': sign,\n            'bird_t': timestamp,\n        }\n        validation_result = self._download_json(\n            'http://kylin.iqiyi.com/validate?' + compat_urllib_parse_urlencode(validation_params), None,\n            note='Validate credentials', errnote='Unable to validate credentials')\n\n        MSG_MAP = {\n            'P00107': 'please login via the web interface and enter the CAPTCHA code',\n            'P00117': 'bad username or password',\n        }\n\n        code = validation_result['code']\n        if code != 'A00000':\n            msg = MSG_MAP.get(code)\n            if not msg:\n                msg = 'error %s' % code\n                if validation_result.get('msg'):\n                    msg += ': ' + validation_result['msg']\n            self._downloader.report_warning('unable to log in: ' + msg)\n            return False\n\n        return True",
        "begin_line": 241,
        "end_line": 286,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiIE.get_raw_data#288",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiIE",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiIE.get_raw_data(self, tvid, video_id)",
        "snippet": "    def get_raw_data(self, tvid, video_id):\n        tm = int(time.time() * 1000)\n\n        key = 'd5fb4bd9d50c4be6948c97edd7254b0e'\n        sc = md5_text(compat_str(tm) + key + tvid)\n        params = {\n            'tvid': tvid,\n            'vid': video_id,\n            'src': '76f90cbd92f94a2e925d83e8ccd22cb7',\n            'sc': sc,\n            't': tm,\n        }\n\n        return self._download_json(\n            'http://cache.m.iqiyi.com/jp/tmts/%s/%s/' % (tvid, video_id),\n            video_id, transform_source=lambda s: remove_start(s, 'var tvInfoJs='),\n            query=params, headers=self.geo_verification_headers())",
        "begin_line": 288,
        "end_line": 304,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.iqiyi.IqiyiIE._real_extract#339",
        "src_path": "youtube_dl/extractor/iqiyi.py",
        "class_name": "youtube_dl.extractor.iqiyi.IqiyiIE",
        "signature": "youtube_dl.extractor.iqiyi.IqiyiIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        webpage = self._download_webpage(\n            url, 'temp_id', note='download video page')\n\n        # There's no simple way to determine whether an URL is a playlist or not\n        # Sometimes there are playlist links in individual videos, so treat it\n        # as a single video first\n        tvid = self._search_regex(\n            r'data-(?:player|shareplattrigger)-tvid\\s*=\\s*[\\'\"](\\d+)', webpage, 'tvid', default=None)\n        if tvid is None:\n            playlist_result = self._extract_playlist(webpage)\n            if playlist_result:\n                return playlist_result\n            raise ExtractorError('Can\\'t find any video')\n\n        video_id = self._search_regex(\n            r'data-(?:player|shareplattrigger)-videoid\\s*=\\s*[\\'\"]([a-f\\d]+)', webpage, 'video_id')\n\n        formats = []\n        for _ in range(5):\n            raw_data = self.get_raw_data(tvid, video_id)\n\n            if raw_data['code'] != 'A00000':\n                if raw_data['code'] == 'A00111':\n                    self.raise_geo_restricted()\n                raise ExtractorError('Unable to load data. Error code: ' + raw_data['code'])\n\n            data = raw_data['data']\n\n            for stream in data['vidl']:\n                if 'm3utx' not in stream:\n                    continue\n                vd = compat_str(stream['vd'])\n                formats.append({\n                    'url': stream['m3utx'],\n                    'format_id': vd,\n                    'ext': 'mp4',\n                    'preference': self._FORMATS_MAP.get(vd, -1),\n                    'protocol': 'm3u8_native',\n                })\n\n            if formats:\n                break\n\n            self._sleep(5, video_id)\n\n        self._sort_formats(formats)\n        title = (get_element_by_id('widget-videotitle', webpage)\n                 or clean_html(get_element_by_attribute('class', 'mod-play-tit', webpage))\n                 or self._html_search_regex(r'<span[^>]+data-videochanged-title=\"word\"[^>]*>([^<]+)</span>', webpage, 'title'))\n\n        return {\n            'id': video_id,\n            'title': title,\n            'formats': formats,\n        }",
        "begin_line": 339,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.apa.APAIE._real_extract#48",
        "src_path": "youtube_dl/extractor/apa.py",
        "class_name": "youtube_dl.extractor.apa.APAIE",
        "signature": "youtube_dl.extractor.apa.APAIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, video_id)\n\n        jwplatform_id = self._search_regex(\n            r'media[iI]d\\s*:\\s*[\"\\'](?P<id>[a-zA-Z0-9]{8})', webpage,\n            'jwplatform id', default=None)\n\n        if jwplatform_id:\n            return self.url_result(\n                'jwplatform:' + jwplatform_id, ie='JWPlatform',\n                video_id=video_id)\n\n        sources = self._parse_json(\n            self._search_regex(\n                r'sources\\s*=\\s*(\\[.+?\\])\\s*;', webpage, 'sources'),\n            video_id, transform_source=js_to_json)\n\n        formats = []\n        for source in sources:\n            if not isinstance(source, dict):\n                continue\n            source_url = url_or_none(source.get('file'))\n            if not source_url:\n                continue\n            ext = determine_ext(source_url)\n            if ext == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    source_url, video_id, 'mp4', entry_protocol='m3u8_native',\n                    m3u8_id='hls', fatal=False))\n            else:\n                formats.append({\n                    'url': source_url,\n                })\n        self._sort_formats(formats)\n\n        thumbnail = self._search_regex(\n            r'image\\s*:\\s*([\"\\'])(?P<url>(?:(?!\\1).)+)\\1', webpage,\n            'thumbnail', fatal=False, group='url')\n\n        return {\n            'id': video_id,\n            'title': video_id,\n            'thumbnail': thumbnail,\n            'formats': formats,\n        }",
        "begin_line": 48,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0023148148148148147,
            "pseudo_dstar_susp": 0.0024813895781637717,
            "pseudo_tarantula_susp": 0.0005157297576070139,
            "pseudo_op2_susp": 0.0024813895781637717,
            "pseudo_barinel_susp": 0.0005157297576070139
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE._extract_brightcove_urls#251",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE._extract_brightcove_urls(cls, webpage)",
        "snippet": "    def _extract_brightcove_urls(cls, webpage):\n        \"\"\"Return a list of all Brightcove URLs from the webpage \"\"\"\n\n        url_m = re.search(\n            r'''(?x)\n                <meta\\s+\n                    (?:property|itemprop)=([\\'\"])(?:og:video|embedURL)\\1[^>]+\n                    content=([\\'\"])(?P<url>https?://(?:secure|c)\\.brightcove.com/(?:(?!\\2).)+)\\2\n            ''', webpage)\n        if url_m:\n            url = unescapeHTML(url_m.group('url'))\n            # Some sites don't add it, we can't download with this url, for example:\n            # http://www.ktvu.com/videos/news/raw-video-caltrain-releases-video-of-man-almost/vCTZdY/\n            if 'playerKey' in url or 'videoId' in url or 'idVideo' in url:\n                return [url]\n\n        matches = re.findall(\n            r'''(?sx)<object\n            (?:\n                [^>]+?class=[\\'\"][^>]*?BrightcoveExperience.*?[\\'\"] |\n                [^>]*?>\\s*<param\\s+name=\"movie\"\\s+value=\"https?://[^/]*brightcove\\.com/\n            ).+?>\\s*</object>''',\n            webpage)\n        if matches:\n            return list(filter(None, [cls._build_brighcove_url(m) for m in matches]))\n\n        matches = re.findall(r'(customBC\\.createVideo\\(.+?\\);)', webpage)\n        if matches:\n            return list(filter(None, [\n                cls._build_brighcove_url_from_js(custom_bc)\n                for custom_bc in matches]))\n        return [src for _, src in re.findall(\n            r'<iframe[^>]+src=([\\'\"])((?:https?:)?//link\\.brightcove\\.com/services/player/(?!\\1).+)\\1', webpage)]",
        "begin_line": 251,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE._real_extract#285",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveLegacyIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n\n        # Change the 'videoId' and others field to '@videoPlayer'\n        url = re.sub(r'(?<=[?&])(videoI(d|D)|idVideo|bctid)', '%40videoPlayer', url)\n        # Change bckey (used by bcove.me urls) to playerKey\n        url = re.sub(r'(?<=[?&])bckey', 'playerKey', url)\n        mobj = re.match(self._VALID_URL, url)\n        query_str = mobj.group('query')\n        query = compat_urlparse.parse_qs(query_str)\n\n        videoPlayer = query.get('@videoPlayer')\n        if videoPlayer:\n            # We set the original url as the default 'Referer' header\n            referer = query.get('linkBaseURL', [None])[0] or smuggled_data.get('Referer', url)\n            video_id = videoPlayer[0]\n            if 'playerID' not in query:\n                mobj = re.search(r'/bcpid(\\d+)', url)\n                if mobj is not None:\n                    query['playerID'] = [mobj.group(1)]\n            publisher_id = query.get('publisherId')\n            if publisher_id and publisher_id[0].isdigit():\n                publisher_id = publisher_id[0]\n            if not publisher_id:\n                player_key = query.get('playerKey')\n                if player_key and ',' in player_key[0]:\n                    player_key = player_key[0]\n                else:\n                    player_id = query.get('playerID')\n                    if player_id and player_id[0].isdigit():\n                        headers = {}\n                        if referer:\n                            headers['Referer'] = referer\n                        player_page = self._download_webpage(\n                            'http://link.brightcove.com/services/player/bcpid' + player_id[0],\n                            video_id, headers=headers, fatal=False)\n                        if player_page:\n                            player_key = self._search_regex(\n                                r'<param\\s+name=\"playerKey\"\\s+value=\"([\\w~,-]+)\"',\n                                player_page, 'player key', fatal=False)\n                if player_key:\n                    enc_pub_id = player_key.split(',')[1].replace('~', '=')\n                    publisher_id = struct.unpack('>Q', base64.urlsafe_b64decode(enc_pub_id))[0]\n            if publisher_id:\n                brightcove_new_url = 'http://players.brightcove.net/%s/default_default/index.html?videoId=%s' % (publisher_id, video_id)\n                if referer:\n                    brightcove_new_url = smuggle_url(brightcove_new_url, {'referrer': referer})\n                return self.url_result(brightcove_new_url, BrightcoveNewIE.ie_key(), video_id)\n        # TODO: figure out if it's possible to extract playlistId from playerKey\n        # elif 'playerKey' in query:\n        #     player_key = query['playerKey']\n        #     return self._get_playlist_info(player_key[0])\n        raise UnsupportedError(url)",
        "begin_line": 285,
        "end_line": 337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveNewIE._extract_urls#410",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveNewIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveNewIE._extract_urls(ie, webpage)",
        "snippet": "    def _extract_urls(ie, webpage):\n        # Reference:\n        # 1. http://docs.brightcove.com/en/video-cloud/brightcove-player/guides/publish-video.html#setvideoiniframe\n        # 2. http://docs.brightcove.com/en/video-cloud/brightcove-player/guides/publish-video.html#tag\n        # 3. http://docs.brightcove.com/en/video-cloud/brightcove-player/guides/publish-video.html#setvideousingjavascript\n        # 4. http://docs.brightcove.com/en/video-cloud/brightcove-player/guides/in-page-embed-player-implementation.html\n        # 5. https://support.brightcove.com/en/video-cloud/docs/dynamically-assigning-videos-player\n\n        entries = []\n\n        # Look for iframe embeds [1]\n        for _, url in re.findall(\n                r'<iframe[^>]+src=([\"\\'])((?:https?:)?//players\\.brightcove\\.net/\\d+/[^/]+/index\\.html.+?)\\1', webpage):\n            entries.append(url if url.startswith('http') else 'http:' + url)\n\n        # Look for <video> tags [2] and embed_in_page embeds [3]\n        # [2] looks like:\n        for video, script_tag, account_id, player_id, embed in re.findall(\n                r'''(?isx)\n                    (<video\\s+[^>]*\\bdata-video-id\\s*=\\s*['\"]?[^>]+>)\n                    (?:.*?\n                        (<script[^>]+\n                            src=[\"\\'](?:https?:)?//players\\.brightcove\\.net/\n                            (\\d+)/([^/]+)_([^/]+)/index(?:\\.min)?\\.js\n                        )\n                    )?\n                ''', webpage):\n            attrs = extract_attributes(video)\n\n            # According to examples from [4] it's unclear whether video id\n            # may be optional and what to do when it is\n            video_id = attrs.get('data-video-id')\n            if not video_id:\n                continue\n\n            account_id = account_id or attrs.get('data-account')\n            if not account_id:\n                continue\n\n            player_id = player_id or attrs.get('data-player') or 'default'\n            embed = embed or attrs.get('data-embed') or 'default'\n\n            bc_url = 'http://players.brightcove.net/%s/%s_%s/index.html?videoId=%s' % (\n                account_id, player_id, embed, video_id)\n\n            # Some brightcove videos may be embedded with video tag only and\n            # without script tag or any mentioning of brightcove at all. Such\n            # embeds are considered ambiguous since they are matched based only\n            # on data-video-id and data-account attributes and in the wild may\n            # not be brightcove embeds at all. Let's check reconstructed\n            # brightcove URLs in case of such embeds and only process valid\n            # ones. By this we ensure there is indeed a brightcove embed.\n            if not script_tag and not ie._is_valid_url(\n                    bc_url, video_id, 'possible brightcove video'):\n                continue\n\n            entries.append(bc_url)\n\n        return entries",
        "begin_line": 410,
        "end_line": 468,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000744047619047619,
            "pseudo_dstar_susp": 0.0007564296520423601,
            "pseudo_tarantula_susp": 0.0004415011037527594,
            "pseudo_op2_susp": 0.0007564296520423601,
            "pseudo_barinel_susp": 0.0004415011037527594
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveNewIE._parse_brightcove_metadata#470",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveNewIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveNewIE._parse_brightcove_metadata(self, json_data, video_id, headers={})",
        "snippet": "    def _parse_brightcove_metadata(self, json_data, video_id, headers={}):\n        title = json_data['name'].strip()\n\n        formats = []\n        for source in json_data.get('sources', []):\n            container = source.get('container')\n            ext = mimetype2ext(source.get('type'))\n            src = source.get('src')\n            # https://support.brightcove.com/playback-api-video-fields-reference#key_systems_object\n            if ext == 'ism' or container == 'WVM' or source.get('key_systems'):\n                continue\n            elif ext == 'm3u8' or container == 'M2TS':\n                if not src:\n                    continue\n                formats.extend(self._extract_m3u8_formats(\n                    src, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))\n            elif ext == 'mpd':\n                if not src:\n                    continue\n                formats.extend(self._extract_mpd_formats(src, video_id, 'dash', fatal=False))\n            else:\n                streaming_src = source.get('streaming_src')\n                stream_name, app_name = source.get('stream_name'), source.get('app_name')\n                if not src and not streaming_src and (not stream_name or not app_name):\n                    continue\n                tbr = float_or_none(source.get('avg_bitrate'), 1000)\n                height = int_or_none(source.get('height'))\n                width = int_or_none(source.get('width'))\n                f = {\n                    'tbr': tbr,\n                    'filesize': int_or_none(source.get('size')),\n                    'container': container,\n                    'ext': ext or container.lower(),\n                }\n                if width == 0 and height == 0:\n                    f.update({\n                        'vcodec': 'none',\n                    })\n                else:\n                    f.update({\n                        'width': width,\n                        'height': height,\n                        'vcodec': source.get('codec'),\n                    })\n\n                def build_format_id(kind):\n                    format_id = kind\n                    if tbr:\n                        format_id += '-%dk' % int(tbr)\n                    if height:\n                        format_id += '-%dp' % height\n                    return format_id\n\n                if src or streaming_src:\n                    f.update({\n                        'url': src or streaming_src,\n                        'format_id': build_format_id('http' if src else 'http-streaming'),\n                        'source_preference': 0 if src else -1,\n                    })\n                else:\n                    f.update({\n                        'url': app_name,\n                        'play_path': stream_name,\n                        'format_id': build_format_id('rtmp'),\n                    })\n                formats.append(f)\n        if not formats:\n            # for sonyliv.com DRM protected videos\n            s3_source_url = json_data.get('custom_fields', {}).get('s3sourceurl')\n            if s3_source_url:\n                formats.append({\n                    'url': s3_source_url,\n                    'format_id': 'source',\n                })\n\n        errors = json_data.get('errors')\n        if not formats and errors:\n            error = errors[0]\n            raise ExtractorError(\n                error.get('message') or error.get('error_subcode') or error['error_code'], expected=True)\n\n        self._sort_formats(formats)\n\n        for f in formats:\n            f.setdefault('http_headers', {}).update(headers)\n\n        subtitles = {}\n        for text_track in json_data.get('text_tracks', []):\n            if text_track.get('kind') != 'captions':\n                continue\n            text_track_url = url_or_none(text_track.get('src'))\n            if not text_track_url:\n                continue\n            lang = (str_or_none(text_track.get('srclang'))\n                    or str_or_none(text_track.get('label')) or 'en').lower()\n            subtitles.setdefault(lang, []).append({\n                'url': text_track_url,\n            })\n\n        is_live = False\n        duration = float_or_none(json_data.get('duration'), 1000)\n        if duration is not None and duration <= 0:\n            is_live = True\n\n        return {\n            'id': video_id,\n            'title': self._live_title(title) if is_live else title,\n            'description': clean_html(json_data.get('description')),\n            'thumbnail': json_data.get('thumbnail') or json_data.get('poster'),\n            'duration': duration,\n            'timestamp': parse_iso8601(json_data.get('published_at')),\n            'uploader_id': json_data.get('account_id'),\n            'formats': formats,\n            'subtitles': subtitles,\n            'tags': json_data.get('tags', []),\n            'is_live': is_live,\n        }",
        "begin_line": 470,
        "end_line": 586,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00040551500405515005,
            "pseudo_dstar_susp": 0.00042589437819420784,
            "pseudo_tarantula_susp": 0.000501002004008016,
            "pseudo_op2_susp": 0.00042589437819420784,
            "pseudo_barinel_susp": 0.000501002004008016
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveNewIE.build_format_id#515",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveNewIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveNewIE.build_format_id(kind)",
        "snippet": "                def build_format_id(kind):\n                    format_id = kind\n                    if tbr:\n                        format_id += '-%dk' % int(tbr)\n                    if height:\n                        format_id += '-%dp' % height\n                    return format_id",
        "begin_line": 515,
        "end_line": 521,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034674063800277393,
            "pseudo_dstar_susp": 0.0003834355828220859,
            "pseudo_tarantula_susp": 0.00030339805825242716,
            "pseudo_op2_susp": 0.0003834355828220859,
            "pseudo_barinel_susp": 0.00030339805825242716
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveNewIE._real_extract#588",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveNewIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveNewIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n        self._initialize_geo_bypass({\n            'countries': smuggled_data.get('geo_countries'),\n            'ip_blocks': smuggled_data.get('geo_ip_blocks'),\n        })\n\n        account_id, player_id, embed, content_type, video_id = re.match(self._VALID_URL, url).groups()\n\n        policy_key_id = '%s_%s' % (account_id, player_id)\n        policy_key = self._downloader.cache.load('brightcove', policy_key_id)\n        policy_key_extracted = False\n        store_pk = lambda x: self._downloader.cache.store('brightcove', policy_key_id, x)\n\n        def extract_policy_key():\n            webpage = self._download_webpage(\n                'http://players.brightcove.net/%s/%s_%s/index.min.js'\n                % (account_id, player_id, embed), video_id)\n\n            policy_key = None\n\n            catalog = self._search_regex(\n                r'catalog\\(({.+?})\\);', webpage, 'catalog', default=None)\n            if catalog:\n                catalog = self._parse_json(\n                    js_to_json(catalog), video_id, fatal=False)\n                if catalog:\n                    policy_key = catalog.get('policyKey')\n\n            if not policy_key:\n                policy_key = self._search_regex(\n                    r'policyKey\\s*:\\s*([\"\\'])(?P<pk>.+?)\\1',\n                    webpage, 'policy key', group='pk')\n\n            store_pk(policy_key)\n            return policy_key\n\n        api_url = 'https://edge.api.brightcove.com/playback/v1/accounts/%s/%ss/%s' % (account_id, content_type, video_id)\n        headers = {}\n        referrer = smuggled_data.get('referrer')\n        if referrer:\n            headers.update({\n                'Referer': referrer,\n                'Origin': re.search(r'https?://[^/]+', referrer).group(0),\n            })\n\n        for _ in range(2):\n            if not policy_key:\n                policy_key = extract_policy_key()\n                policy_key_extracted = True\n            headers['Accept'] = 'application/json;pk=%s' % policy_key\n            try:\n                json_data = self._download_json(api_url, video_id, headers=headers)\n                break\n            except ExtractorError as e:\n                if isinstance(e.cause, compat_HTTPError) and e.cause.code in (401, 403):\n                    json_data = self._parse_json(e.cause.read().decode(), video_id)[0]\n                    message = json_data.get('message') or json_data['error_code']\n                    if json_data.get('error_subcode') == 'CLIENT_GEO':\n                        self.raise_geo_restricted(msg=message)\n                    elif json_data.get('error_code') == 'INVALID_POLICY_KEY' and not policy_key_extracted:\n                        policy_key = None\n                        store_pk(None)\n                        continue\n                    raise ExtractorError(message, expected=True)\n                raise\n\n        errors = json_data.get('errors')\n        if errors and errors[0].get('error_subcode') == 'TVE_AUTH':\n            custom_fields = json_data['custom_fields']\n            tve_token = self._extract_mvpd_auth(\n                smuggled_data['source_url'], video_id,\n                custom_fields['bcadobepassrequestorid'],\n                custom_fields['bcadobepassresourceid'])\n            json_data = self._download_json(\n                api_url, video_id, headers={\n                    'Accept': 'application/json;pk=%s' % policy_key\n                }, query={\n                    'tveToken': tve_token,\n                })\n\n        if content_type == 'playlist':\n            return self.playlist_result(\n                [self._parse_brightcove_metadata(vid, vid.get('id'), headers)\n                 for vid in json_data.get('videos', []) if vid.get('id')],\n                json_data.get('id'), json_data.get('name'),\n                json_data.get('description'))\n\n        return self._parse_brightcove_metadata(\n            json_data, video_id, headers=headers)",
        "begin_line": 588,
        "end_line": 677,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveNewIE.extract_policy_key#602",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveNewIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveNewIE.extract_policy_key()",
        "snippet": "        def extract_policy_key():\n            webpage = self._download_webpage(\n                'http://players.brightcove.net/%s/%s_%s/index.min.js'\n                % (account_id, player_id, embed), video_id)\n\n            policy_key = None\n\n            catalog = self._search_regex(\n                r'catalog\\(({.+?})\\);', webpage, 'catalog', default=None)\n            if catalog:\n                catalog = self._parse_json(\n                    js_to_json(catalog), video_id, fatal=False)\n                if catalog:\n                    policy_key = catalog.get('policyKey')\n\n            if not policy_key:\n                policy_key = self._search_regex(\n                    r'policyKey\\s*:\\s*([\"\\'])(?P<pk>.+?)\\1',\n                    webpage, 'policy key', group='pk')\n\n            store_pk(policy_key)\n            return policy_key",
        "begin_line": 602,
        "end_line": 623,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00046533271288971617,
            "pseudo_dstar_susp": 0.0004589261128958238,
            "pseudo_tarantula_susp": 0.0010384215991692627,
            "pseudo_op2_susp": 0.0004589261128958238,
            "pseudo_barinel_susp": 0.0010384215991692627
        }
    },
    {
        "name": "youtube_dl.extractor.vine.VineUserIE.suitable#131",
        "src_path": "youtube_dl/extractor/vine.py",
        "class_name": "youtube_dl.extractor.vine.VineUserIE",
        "signature": "youtube_dl.extractor.vine.VineUserIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if VineIE.suitable(url) else super(VineUserIE, cls).suitable(url)",
        "begin_line": 131,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.keezmovies.KeezMoviesIE._real_extract#127",
        "src_path": "youtube_dl/extractor/keezmovies.py",
        "class_name": "youtube_dl.extractor.keezmovies.KeezMoviesIE",
        "signature": "youtube_dl.extractor.keezmovies.KeezMoviesIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        webpage, info = self._extract_info(url, fatal=False)\n        if not info['formats']:\n            return self.url_result(url, 'Generic')\n        info['view_count'] = str_to_int(self._search_regex(\n            r'<b>([\\d,.]+)</b> Views?', webpage, 'view count', fatal=False))\n        return info",
        "begin_line": 127,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.0001145475372279496,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.__init__#31",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.__init__(self, code, objects=None)",
        "snippet": "    def __init__(self, code, objects=None):\n        if objects is None:\n            objects = {}\n        self.code = code\n        self._functions = {}\n        self._objects = objects",
        "begin_line": 31,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011690437222352116,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.interpret_statement#38",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.interpret_statement(self, stmt, local_vars, allow_recursion=100)",
        "snippet": "    def interpret_statement(self, stmt, local_vars, allow_recursion=100):\n        if allow_recursion < 0:\n            raise ExtractorError('Recursion limit reached')\n\n        should_abort = False\n        stmt = stmt.lstrip()\n        stmt_m = re.match(r'var\\s', stmt)\n        if stmt_m:\n            expr = stmt[len(stmt_m.group(0)):]\n        else:\n            return_m = re.match(r'return(?:\\s+|$)', stmt)\n            if return_m:\n                expr = stmt[len(return_m.group(0)):]\n                should_abort = True\n            else:\n                # Try interpreting it as an expression\n                expr = stmt\n\n        v = self.interpret_expression(expr, local_vars, allow_recursion)\n        return v, should_abort",
        "begin_line": 38,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.interpret_expression#59",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.interpret_expression(self, expr, local_vars, allow_recursion)",
        "snippet": "    def interpret_expression(self, expr, local_vars, allow_recursion):\n        expr = expr.strip()\n        if expr == '':  # Empty expression\n            return None\n\n        if expr.startswith('('):\n            parens_count = 0\n            for m in re.finditer(r'[()]', expr):\n                if m.group(0) == '(':\n                    parens_count += 1\n                else:\n                    parens_count -= 1\n                    if parens_count == 0:\n                        sub_expr = expr[1:m.start()]\n                        sub_result = self.interpret_expression(\n                            sub_expr, local_vars, allow_recursion)\n                        remaining_expr = expr[m.end():].strip()\n                        if not remaining_expr:\n                            return sub_result\n                        else:\n                            expr = json.dumps(sub_result) + remaining_expr\n                        break\n            else:\n                raise ExtractorError('Premature end of parens in %r' % expr)\n\n        for op, opfunc in _ASSIGN_OPERATORS:\n            m = re.match(r'''(?x)\n                (?P<out>%s)(?:\\[(?P<index>[^\\]]+?)\\])?\n                \\s*%s\n                (?P<expr>.*)$''' % (_NAME_RE, re.escape(op)), expr)\n            if not m:\n                continue\n            right_val = self.interpret_expression(\n                m.group('expr'), local_vars, allow_recursion - 1)\n\n            if m.groupdict().get('index'):\n                lvar = local_vars[m.group('out')]\n                idx = self.interpret_expression(\n                    m.group('index'), local_vars, allow_recursion)\n                assert isinstance(idx, int)\n                cur = lvar[idx]\n                val = opfunc(cur, right_val)\n                lvar[idx] = val\n                return val\n            else:\n                cur = local_vars.get(m.group('out'))\n                val = opfunc(cur, right_val)\n                local_vars[m.group('out')] = val\n                return val\n\n        if expr.isdigit():\n            return int(expr)\n\n        var_m = re.match(\n            r'(?!if|return|true|false)(?P<name>%s)$' % _NAME_RE,\n            expr)\n        if var_m:\n            return local_vars[var_m.group('name')]\n\n        try:\n            return json.loads(expr)\n        except ValueError:\n            pass\n\n        m = re.match(\n            r'(?P<in>%s)\\[(?P<idx>.+)\\]$' % _NAME_RE, expr)\n        if m:\n            val = local_vars[m.group('in')]\n            idx = self.interpret_expression(\n                m.group('idx'), local_vars, allow_recursion - 1)\n            return val[idx]\n\n        m = re.match(\n            r'(?P<var>%s)(?:\\.(?P<member>[^(]+)|\\[(?P<member2>[^]]+)\\])\\s*(?:\\(+(?P<args>[^()]*)\\))?$' % _NAME_RE,\n            expr)\n        if m:\n            variable = m.group('var')\n            member = remove_quotes(m.group('member') or m.group('member2'))\n            arg_str = m.group('args')\n\n            if variable in local_vars:\n                obj = local_vars[variable]\n            else:\n                if variable not in self._objects:\n                    self._objects[variable] = self.extract_object(variable)\n                obj = self._objects[variable]\n\n            if arg_str is None:\n                # Member access\n                if member == 'length':\n                    return len(obj)\n                return obj[member]\n\n            assert expr.endswith(')')\n            # Function call\n            if arg_str == '':\n                argvals = tuple()\n            else:\n                argvals = tuple([\n                    self.interpret_expression(v, local_vars, allow_recursion)\n                    for v in arg_str.split(',')])\n\n            if member == 'split':\n                assert argvals == ('',)\n                return list(obj)\n            if member == 'join':\n                assert len(argvals) == 1\n                return argvals[0].join(obj)\n            if member == 'reverse':\n                assert len(argvals) == 0\n                obj.reverse()\n                return obj\n            if member == 'slice':\n                assert len(argvals) == 1\n                return obj[argvals[0]:]\n            if member == 'splice':\n                assert isinstance(obj, list)\n                index, howMany = argvals\n                res = []\n                for i in range(index, min(index + howMany, len(obj))):\n                    res.append(obj.pop(index))\n                return res\n\n            return obj[member](argvals)\n\n        for op, opfunc in _OPERATORS:\n            m = re.match(r'(?P<x>.+?)%s(?P<y>.+)' % re.escape(op), expr)\n            if not m:\n                continue\n            x, abort = self.interpret_statement(\n                m.group('x'), local_vars, allow_recursion - 1)\n            if abort:\n                raise ExtractorError(\n                    'Premature left-side return of %s in %r' % (op, expr))\n            y, abort = self.interpret_statement(\n                m.group('y'), local_vars, allow_recursion - 1)\n            if abort:\n                raise ExtractorError(\n                    'Premature right-side return of %s in %r' % (op, expr))\n            return opfunc(x, y)\n\n        m = re.match(\n            r'^(?P<func>%s)\\((?P<args>[a-zA-Z0-9_$,]*)\\)$' % _NAME_RE, expr)\n        if m:\n            fname = m.group('func')\n            argvals = tuple([\n                int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()\n            if fname not in self._functions:\n                self._functions[fname] = self.extract_function(fname)\n            return self._functions[fname](argvals)\n\n        raise ExtractorError('Unsupported JS expression %r' % expr)",
        "begin_line": 59,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.extract_object#213",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.extract_object(self, objname)",
        "snippet": "    def extract_object(self, objname):\n        _FUNC_NAME_RE = r'''(?:[a-zA-Z$0-9]+|\"[a-zA-Z$0-9]+\"|'[a-zA-Z$0-9]+')'''\n        obj = {}\n        obj_m = re.search(\n            r'''(?x)\n                (?<!this\\.)%s\\s*=\\s*{\\s*\n                    (?P<fields>(%s\\s*:\\s*function\\s*\\(.*?\\)\\s*{.*?}(?:,\\s*)?)*)\n                }\\s*;\n            ''' % (re.escape(objname), _FUNC_NAME_RE),\n            self.code)\n        fields = obj_m.group('fields')\n        # Currently, it only supports function definitions\n        fields_m = re.finditer(\n            r'''(?x)\n                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>[a-z,]+)\\){(?P<code>[^}]+)}\n            ''' % _FUNC_NAME_RE,\n            fields)\n        for f in fields_m:\n            argnames = f.group('args').split(',')\n            obj[remove_quotes(f.group('key'))] = self.build_function(argnames, f.group('code'))\n\n        return obj",
        "begin_line": 213,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018083182640144665,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.extract_function#236",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.extract_function(self, funcname)",
        "snippet": "    def extract_function(self, funcname):\n        func_m = re.search(\n            r'''(?x)\n                (?:function\\s+%s|[{;,]\\s*%s\\s*=\\s*function|var\\s+%s\\s*=\\s*function)\\s*\n                \\((?P<args>[^)]*)\\)\\s*\n                \\{(?P<code>[^}]+)\\}''' % (\n                re.escape(funcname), re.escape(funcname), re.escape(funcname)),\n            self.code)\n        if func_m is None:\n            raise ExtractorError('Could not find JS function %r' % funcname)\n        argnames = func_m.group('args').split(',')\n\n        return self.build_function(argnames, func_m.group('code'))",
        "begin_line": 236,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011690437222352116,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.call_function#250",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.call_function(self, funcname, *args)",
        "snippet": "    def call_function(self, funcname, *args):\n        f = self.extract_function(funcname)\n        return f(args)",
        "begin_line": 250,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011844131232974062,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.build_function#254",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.build_function(self, argnames, code)",
        "snippet": "    def build_function(self, argnames, code):\n        def resf(args):\n            local_vars = dict(zip(argnames, args))\n            for stmt in code.split(';'):\n                res, abort = self.interpret_statement(stmt, local_vars)\n                if abort:\n                    break\n            return res\n        return resf",
        "begin_line": 254,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011690437222352116,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.resf#255",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.resf(args)",
        "snippet": "        def resf(args):\n            local_vars = dict(zip(argnames, args))\n            for stmt in code.split(';'):\n                res, abort = self.interpret_statement(stmt, local_vars)\n                if abort:\n                    break\n            return res",
        "begin_line": 255,
        "end_line": 261,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011690437222352116,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.niconico.NiconicoIE._real_extract#269",
        "src_path": "youtube_dl/extractor/niconico.py",
        "class_name": "youtube_dl.extractor.niconico.NiconicoIE",
        "signature": "youtube_dl.extractor.niconico.NiconicoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        # Get video webpage. We are not actually interested in it for normal\n        # cases, but need the cookies in order to be able to download the\n        # info webpage\n        webpage, handle = self._download_webpage_handle(\n            'http://www.nicovideo.jp/watch/' + video_id, video_id)\n        if video_id.startswith('so'):\n            video_id = self._match_id(handle.geturl())\n\n        api_data = self._parse_json(self._html_search_regex(\n            'data-api-data=\"([^\"]+)\"', webpage,\n            'API data', default='{}'), video_id)\n\n        def _format_id_from_url(video_url):\n            return 'economy' if video_real_url.endswith('low') else 'normal'\n\n        try:\n            video_real_url = api_data['video']['smileInfo']['url']\n        except KeyError:  # Flash videos\n            # Get flv info\n            flv_info_webpage = self._download_webpage(\n                'http://flapi.nicovideo.jp/api/getflv/' + video_id + '?as3=1',\n                video_id, 'Downloading flv info')\n\n            flv_info = compat_urlparse.parse_qs(flv_info_webpage)\n            if 'url' not in flv_info:\n                if 'deleted' in flv_info:\n                    raise ExtractorError('The video has been deleted.',\n                                         expected=True)\n                elif 'closed' in flv_info:\n                    raise ExtractorError('Niconico videos now require logging in',\n                                         expected=True)\n                elif 'error' in flv_info:\n                    raise ExtractorError('%s reports error: %s' % (\n                        self.IE_NAME, flv_info['error'][0]), expected=True)\n                else:\n                    raise ExtractorError('Unable to find video URL')\n\n            video_info_xml = self._download_xml(\n                'http://ext.nicovideo.jp/api/getthumbinfo/' + video_id,\n                video_id, note='Downloading video info page')\n\n            def get_video_info(items):\n                if not isinstance(items, list):\n                    items = [items]\n                for item in items:\n                    ret = xpath_text(video_info_xml, './/' + item)\n                    if ret:\n                        return ret\n\n            video_real_url = flv_info['url'][0]\n\n            extension = get_video_info('movie_type')\n            if not extension:\n                extension = determine_ext(video_real_url)\n\n            formats = [{\n                'url': video_real_url,\n                'ext': extension,\n                'format_id': _format_id_from_url(video_real_url),\n            }]\n        else:\n            formats = []\n\n            dmc_info = api_data['video'].get('dmcInfo')\n            if dmc_info:  # \"New\" HTML5 videos\n                quality_info = dmc_info['quality']\n                for audio_quality in quality_info['audios']:\n                    for video_quality in quality_info['videos']:\n                        if not audio_quality['available'] or not video_quality['available']:\n                            continue\n                        formats.append(self._extract_format_for_quality(\n                            api_data, video_id, audio_quality, video_quality))\n\n                self._sort_formats(formats)\n            else:  # \"Old\" HTML5 videos\n                formats = [{\n                    'url': video_real_url,\n                    'ext': 'mp4',\n                    'format_id': _format_id_from_url(video_real_url),\n                }]\n\n            def get_video_info(items):\n                return dict_get(api_data['video'], items)\n\n        # Start extracting information\n        title = get_video_info('title')\n        if not title:\n            title = self._og_search_title(webpage, default=None)\n        if not title:\n            title = self._html_search_regex(\n                r'<span[^>]+class=\"videoHeaderTitle\"[^>]*>([^<]+)</span>',\n                webpage, 'video title')\n\n        watch_api_data_string = self._html_search_regex(\n            r'<div[^>]+id=\"watchAPIDataContainer\"[^>]+>([^<]+)</div>',\n            webpage, 'watch api data', default=None)\n        watch_api_data = self._parse_json(watch_api_data_string, video_id) if watch_api_data_string else {}\n        video_detail = watch_api_data.get('videoDetail', {})\n\n        thumbnail = (\n            get_video_info(['thumbnail_url', 'thumbnailURL'])\n            or self._html_search_meta('image', webpage, 'thumbnail', default=None)\n            or video_detail.get('thumbnail'))\n\n        description = get_video_info('description')\n\n        timestamp = (parse_iso8601(get_video_info('first_retrieve'))\n                     or unified_timestamp(get_video_info('postedDateTime')))\n        if not timestamp:\n            match = self._html_search_meta('datePublished', webpage, 'date published', default=None)\n            if match:\n                timestamp = parse_iso8601(match.replace('+', ':00+'))\n        if not timestamp and video_detail.get('postedAt'):\n            timestamp = parse_iso8601(\n                video_detail['postedAt'].replace('/', '-'),\n                delimiter=' ', timezone=datetime.timedelta(hours=9))\n\n        view_count = int_or_none(get_video_info(['view_counter', 'viewCount']))\n        if not view_count:\n            match = self._html_search_regex(\n                r'>Views: <strong[^>]*>([^<]+)</strong>',\n                webpage, 'view count', default=None)\n            if match:\n                view_count = int_or_none(match.replace(',', ''))\n        view_count = view_count or video_detail.get('viewCount')\n\n        comment_count = (int_or_none(get_video_info('comment_num'))\n                         or video_detail.get('commentCount')\n                         or try_get(api_data, lambda x: x['thread']['commentCount']))\n        if not comment_count:\n            match = self._html_search_regex(\n                r'>Comments: <strong[^>]*>([^<]+)</strong>',\n                webpage, 'comment count', default=None)\n            if match:\n                comment_count = int_or_none(match.replace(',', ''))\n\n        duration = (parse_duration(\n            get_video_info('length')\n            or self._html_search_meta(\n                'video:duration', webpage, 'video duration', default=None))\n            or video_detail.get('length')\n            or get_video_info('duration'))\n\n        webpage_url = get_video_info('watch_url') or url\n\n        # Note: cannot use api_data.get('owner', {}) because owner may be set to \"null\"\n        # in the JSON, which will cause None to be returned instead of {}.\n        owner = try_get(api_data, lambda x: x.get('owner'), dict) or {}\n        uploader_id = get_video_info(['ch_id', 'user_id']) or owner.get('id')\n        uploader = get_video_info(['ch_name', 'user_nickname']) or owner.get('nickname')\n\n        return {\n            'id': video_id,\n            'title': title,\n            'formats': formats,\n            'thumbnail': thumbnail,\n            'description': description,\n            'uploader': uploader,\n            'timestamp': timestamp,\n            'uploader_id': uploader_id,\n            'view_count': view_count,\n            'comment_count': comment_count,\n            'duration': duration,\n            'webpage_url': webpage_url,\n        }",
        "begin_line": 269,
        "end_line": 436,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.niconico.NiconicoPlaylistIE._real_extract#451",
        "src_path": "youtube_dl/extractor/niconico.py",
        "class_name": "youtube_dl.extractor.niconico.NiconicoPlaylistIE",
        "signature": "youtube_dl.extractor.niconico.NiconicoPlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        list_id = self._match_id(url)\n        webpage = self._download_webpage(url, list_id)\n\n        entries_json = self._search_regex(r'Mylist\\.preload\\(\\d+, (\\[.*\\])\\);',\n                                          webpage, 'entries')\n        entries = json.loads(entries_json)\n        entries = [{\n            '_type': 'url',\n            'ie_key': NiconicoIE.ie_key(),\n            'url': ('http://www.nicovideo.jp/watch/%s' %\n                    entry['item_data']['video_id']),\n        } for entry in entries]\n\n        return {\n            '_type': 'playlist',\n            'title': self._search_regex(r'\\s+name: \"(.*?)\"', webpage, 'title'),\n            'id': list_id,\n            'entries': entries,\n        }",
        "begin_line": 451,
        "end_line": 470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.abc.ABCIE._real_extract#63",
        "src_path": "youtube_dl/extractor/abc.py",
        "class_name": "youtube_dl.extractor.abc.ABCIE",
        "signature": "youtube_dl.extractor.abc.ABCIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        mobj = re.search(\n            r'inline(?P<type>Video|Audio|YouTube)Data\\.push\\((?P<json_data>[^)]+)\\);',\n            webpage)\n        if mobj is None:\n            expired = self._html_search_regex(r'(?s)class=\"expired-(?:video|audio)\".+?<span>(.+?)</span>', webpage, 'expired', None)\n            if expired:\n                raise ExtractorError('%s said: %s' % (self.IE_NAME, expired), expected=True)\n            raise ExtractorError('Unable to extract video urls')\n\n        urls_info = self._parse_json(\n            mobj.group('json_data'), video_id, transform_source=js_to_json)\n\n        if not isinstance(urls_info, list):\n            urls_info = [urls_info]\n\n        if mobj.group('type') == 'YouTube':\n            return self.playlist_result([\n                self.url_result(url_info['url']) for url_info in urls_info])\n\n        formats = [{\n            'url': url_info['url'],\n            'vcodec': url_info.get('codec') if mobj.group('type') == 'Video' else 'none',\n            'width': int_or_none(url_info.get('width')),\n            'height': int_or_none(url_info.get('height')),\n            'tbr': int_or_none(url_info.get('bitrate')),\n            'filesize': int_or_none(url_info.get('filesize')),\n        } for url_info in urls_info]\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': self._og_search_title(webpage),\n            'formats': formats,\n            'description': self._og_search_description(webpage),\n            'thumbnail': self._og_search_thumbnail(webpage),\n        }",
        "begin_line": 63,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0025188916876574307,
            "pseudo_dstar_susp": 0.002932551319648094,
            "pseudo_tarantula_susp": 0.0005678591709256105,
            "pseudo_op2_susp": 0.002932551319648094,
            "pseudo_barinel_susp": 0.0005678591709256105
        }
    },
    {
        "name": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE.suitable#300",
        "src_path": "youtube_dl/extractor/bilibili.py",
        "class_name": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE",
        "signature": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if BiliBiliIE.suitable(url) else super(BiliBiliBangumiIE, cls).suitable(url)",
        "begin_line": 300,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004484304932735426,
            "pseudo_dstar_susp": 0.004484304932735426,
            "pseudo_tarantula_susp": 0.0011049723756906078,
            "pseudo_op2_susp": 0.004484304932735426,
            "pseudo_barinel_susp": 0.0011049723756906078
        }
    },
    {
        "name": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE._real_extract#303",
        "src_path": "youtube_dl/extractor/bilibili.py",
        "class_name": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE",
        "signature": "youtube_dl.extractor.bilibili.BiliBiliBangumiIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        bangumi_id = self._match_id(url)\n\n        # Sometimes this API returns a JSONP response\n        season_info = self._download_json(\n            'http://bangumi.bilibili.com/jsonp/seasoninfo/%s.ver' % bangumi_id,\n            bangumi_id, transform_source=strip_jsonp)['result']\n\n        entries = [{\n            '_type': 'url_transparent',\n            'url': smuggle_url(episode['webplay_url'], {'no_bangumi_tip': 1}),\n            'ie_key': BiliBiliIE.ie_key(),\n            'timestamp': parse_iso8601(episode.get('update_time'), delimiter=' '),\n            'episode': episode.get('index_title'),\n            'episode_number': int_or_none(episode.get('index')),\n        } for episode in season_info['episodes']]\n\n        entries = sorted(entries, key=lambda entry: entry.get('episode_number'))\n\n        return self.playlist_result(\n            entries, bangumi_id,\n            season_info.get('bangumi_title'), season_info.get('evaluate'))",
        "begin_line": 303,
        "end_line": 324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003641660597232338,
            "pseudo_dstar_susp": 0.0003484320557491289,
            "pseudo_tarantula_susp": 0.0013605442176870747,
            "pseudo_op2_susp": 0.0003484320557491289,
            "pseudo_barinel_susp": 0.0013605442176870747
        }
    },
    {
        "name": "youtube_dl.extractor.bilibili.BilibiliAudioBaseIE._call_api#328",
        "src_path": "youtube_dl/extractor/bilibili.py",
        "class_name": "youtube_dl.extractor.bilibili.BilibiliAudioBaseIE",
        "signature": "youtube_dl.extractor.bilibili.BilibiliAudioBaseIE._call_api(self, path, sid, query=None)",
        "snippet": "    def _call_api(self, path, sid, query=None):\n        if not query:\n            query = {'sid': sid}\n        return self._download_json(\n            'https://www.bilibili.com/audio/music-service-c/web/' + path,\n            sid, query=query)['data']",
        "begin_line": 328,
        "end_line": 333,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bilibili.BilibiliAudioIE._real_extract#362",
        "src_path": "youtube_dl/extractor/bilibili.py",
        "class_name": "youtube_dl.extractor.bilibili.BilibiliAudioIE",
        "signature": "youtube_dl.extractor.bilibili.BilibiliAudioIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        au_id = self._match_id(url)\n\n        play_data = self._call_api('url', au_id)\n        formats = [{\n            'url': play_data['cdns'][0],\n            'filesize': int_or_none(play_data.get('size')),\n        }]\n\n        song = self._call_api('song/info', au_id)\n        title = song['title']\n        statistic = song.get('statistic') or {}\n\n        subtitles = None\n        lyric = song.get('lyric')\n        if lyric:\n            subtitles = {\n                'origin': [{\n                    'url': lyric,\n                }]\n            }\n\n        return {\n            'id': au_id,\n            'title': title,\n            'formats': formats,\n            'artist': song.get('author'),\n            'comment_count': int_or_none(statistic.get('comment')),\n            'description': song.get('intro'),\n            'duration': int_or_none(song.get('duration')),\n            'subtitles': subtitles,\n            'thumbnail': song.get('cover'),\n            'timestamp': int_or_none(song.get('passtime')),\n            'uploader': song.get('uname'),\n            'view_count': int_or_none(statistic.get('play')),\n        }",
        "begin_line": 362,
        "end_line": 397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.bilibili.BilibiliAudioAlbumIE._real_extract#412",
        "src_path": "youtube_dl/extractor/bilibili.py",
        "class_name": "youtube_dl.extractor.bilibili.BilibiliAudioAlbumIE",
        "signature": "youtube_dl.extractor.bilibili.BilibiliAudioAlbumIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        am_id = self._match_id(url)\n\n        songs = self._call_api(\n            'song/of-menu', am_id, {'sid': am_id, 'pn': 1, 'ps': 100})['data']\n\n        entries = []\n        for song in songs:\n            sid = str_or_none(song.get('id'))\n            if not sid:\n                continue\n            entries.append(self.url_result(\n                'https://www.bilibili.com/audio/au' + sid,\n                BilibiliAudioIE.ie_key(), sid))\n\n        if entries:\n            album_data = self._call_api('menu/info', am_id) or {}\n            album_title = album_data.get('title')\n            if album_title:\n                for entry in entries:\n                    entry['album'] = album_title\n                return self.playlist_result(\n                    entries, am_id, album_title, album_data.get('intro'))\n\n        return self.playlist_result(entries, am_id)",
        "begin_line": 412,
        "end_line": 436,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.xhamster.XHamsterEmbedIE._extract_urls#318",
        "src_path": "youtube_dl/extractor/xhamster.py",
        "class_name": "youtube_dl.extractor.xhamster.XHamsterEmbedIE",
        "signature": "youtube_dl.extractor.xhamster.XHamsterEmbedIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        return [url for _, url in re.findall(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?xhamster\\.com/xembed\\.php\\?video=\\d+)\\1',\n            webpage)]",
        "begin_line": 318,
        "end_line": 321,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.nbc.NBCSportsVPlayerIE._extract_url#181",
        "src_path": "youtube_dl/extractor/nbc.py",
        "class_name": "youtube_dl.extractor.nbc.NBCSportsVPlayerIE",
        "signature": "youtube_dl.extractor.nbc.NBCSportsVPlayerIE._extract_url(webpage)",
        "snippet": "    def _extract_url(webpage):\n        iframe_m = re.search(\n            r'<iframe[^>]+src=\"(?P<url>https?://vplayer\\.nbcsports\\.com/[^\"]+)\"', webpage)\n        if iframe_m:\n            return iframe_m.group('url')",
        "begin_line": 181,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.foxgay.FoxgayIE._real_extract#28",
        "src_path": "youtube_dl/extractor/foxgay.py",
        "class_name": "youtube_dl.extractor.foxgay.FoxgayIE",
        "signature": "youtube_dl.extractor.foxgay.FoxgayIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        title = remove_end(self._html_search_regex(\n            r'<title>([^<]+)</title>', webpage, 'title'), ' - Foxgay.com')\n        description = get_element_by_id('inf_tit', webpage)\n\n        # The default user-agent with foxgay cookies leads to pages without videos\n        self._downloader.cookiejar.clear('.foxgay.com')\n        # Find the URL for the iFrame which contains the actual video.\n        iframe_url = self._html_search_regex(\n            r'<iframe[^>]+src=([\\'\"])(?P<url>[^\\'\"]+)\\1', webpage,\n            'video frame', group='url')\n        iframe = self._download_webpage(\n            iframe_url, video_id, headers={'User-Agent': 'curl/7.50.1'},\n            note='Downloading video frame')\n        video_data = self._parse_json(self._search_regex(\n            r'video_data\\s*=\\s*([^;]+);', iframe, 'video data'), video_id)\n\n        formats = [{\n            'url': source,\n            'height': int_or_none(resolution),\n        } for source, resolution in zip(\n            video_data['sources'], video_data.get('resolutions', itertools.repeat(None)))]\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'formats': formats,\n            'description': description,\n            'thumbnail': video_data.get('act_vid', {}).get('thumb'),\n            'age_limit': 18,\n        }",
        "begin_line": 28,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011337868480725624,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.adobepass.AdobePassIE._download_webpage_handle#1332",
        "src_path": "youtube_dl/extractor/adobepass.py",
        "class_name": "youtube_dl.extractor.adobepass.AdobePassIE",
        "signature": "youtube_dl.extractor.adobepass.AdobePassIE._download_webpage_handle(self, *args, **kwargs)",
        "snippet": "    def _download_webpage_handle(self, *args, **kwargs):\n        headers = self.geo_verification_headers()\n        headers.update(kwargs.get('headers', {}))\n        kwargs['headers'] = headers\n        return super(AdobePassIE, self)._download_webpage_handle(\n            *args, **compat_kwargs(kwargs))",
        "begin_line": 1332,
        "end_line": 1337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006747638326585695,
            "pseudo_dstar_susp": 0.0006297229219143577,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0006297229219143577,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.adobepass.AdobePassIE.post_form#1363",
        "src_path": "youtube_dl/extractor/adobepass.py",
        "class_name": "youtube_dl.extractor.adobepass.AdobePassIE",
        "signature": "youtube_dl.extractor.adobepass.AdobePassIE.post_form(form_page_res, note, data={})",
        "snippet": "        def post_form(form_page_res, note, data={}):\n            form_page, urlh = form_page_res\n            post_url = self._html_search_regex(r'<form[^>]+action=([\"\\'])(?P<url>.+?)\\1', form_page, 'post url', group='url')\n            if not re.match(r'https?://', post_url):\n                post_url = compat_urlparse.urljoin(urlh.geturl(), post_url)\n            form_data = self._hidden_inputs(form_page)\n            form_data.update(data)\n            return self._download_webpage_handle(\n                post_url, video_id, note, data=urlencode_postdata(form_data), headers={\n                    'Content-Type': 'application/x-www-form-urlencoded',\n                })",
        "begin_line": 1363,
        "end_line": 1373,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00031735956839098697,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.adobepass.AdobePassIE._extract_mvpd_auth#1354",
        "src_path": "youtube_dl/extractor/adobepass.py",
        "class_name": "youtube_dl.extractor.adobepass.AdobePassIE",
        "signature": "youtube_dl.extractor.adobepass.AdobePassIE._extract_mvpd_auth(self, url, video_id, requestor_id, resource)",
        "snippet": "    def _extract_mvpd_auth(self, url, video_id, requestor_id, resource):\n        def xml_text(xml_str, tag):\n            return self._search_regex(\n                '<%s>(.+?)</%s>' % (tag, tag), xml_str, tag)\n\n        def is_expired(token, date_ele):\n            token_expires = unified_timestamp(re.sub(r'[_ ]GMT', '', xml_text(token, date_ele)))\n            return token_expires and token_expires <= int(time.time())\n\n        def post_form(form_page_res, note, data={}):\n            form_page, urlh = form_page_res\n            post_url = self._html_search_regex(r'<form[^>]+action=([\"\\'])(?P<url>.+?)\\1', form_page, 'post url', group='url')\n            if not re.match(r'https?://', post_url):\n                post_url = compat_urlparse.urljoin(urlh.geturl(), post_url)\n            form_data = self._hidden_inputs(form_page)\n            form_data.update(data)\n            return self._download_webpage_handle(\n                post_url, video_id, note, data=urlencode_postdata(form_data), headers={\n                    'Content-Type': 'application/x-www-form-urlencoded',\n                })\n\n        def raise_mvpd_required():\n            raise ExtractorError(\n                'This video is only available for users of participating TV providers. '\n                'Use --ap-mso to specify Adobe Pass Multiple-system operator Identifier '\n                'and --ap-username and --ap-password or --netrc to provide account credentials.', expected=True)\n\n        def extract_redirect_url(html, url=None, fatal=False):\n            # TODO: eliminate code duplication with generic extractor and move\n            # redirection code into _download_webpage_handle\n            REDIRECT_REGEX = r'[0-9]{,2};\\s*(?:URL|url)=\\'?([^\\'\"]+)'\n            redirect_url = self._search_regex(\n                r'(?i)<meta\\s+(?=(?:[a-z-]+=\"[^\"]+\"\\s+)*http-equiv=\"refresh\")'\n                r'(?:[a-z-]+=\"[^\"]+\"\\s+)*?content=\"%s' % REDIRECT_REGEX,\n                html, 'meta refresh redirect',\n                default=NO_DEFAULT if fatal else None, fatal=fatal)\n            if not redirect_url:\n                return None\n            if url:\n                redirect_url = compat_urlparse.urljoin(url, unescapeHTML(redirect_url))\n            return redirect_url\n\n        mvpd_headers = {\n            'ap_42': 'anonymous',\n            'ap_11': 'Linux i686',\n            'ap_z': self._USER_AGENT,\n            'User-Agent': self._USER_AGENT,\n        }\n\n        guid = xml_text(resource, 'guid') if '<' in resource else resource\n        count = 0\n        while count < 2:\n            requestor_info = self._downloader.cache.load(self._MVPD_CACHE, requestor_id) or {}\n            authn_token = requestor_info.get('authn_token')\n            if authn_token and is_expired(authn_token, 'simpleTokenExpires'):\n                authn_token = None\n            if not authn_token:\n                # TODO add support for other TV Providers\n                mso_id = self._downloader.params.get('ap_mso')\n                if not mso_id:\n                    raise_mvpd_required()\n                username, password = self._get_login_info('ap_username', 'ap_password', mso_id)\n                if not username or not password:\n                    raise_mvpd_required()\n                mso_info = MSO_INFO[mso_id]\n\n                provider_redirect_page_res = self._download_webpage_handle(\n                    self._SERVICE_PROVIDER_TEMPLATE % 'authenticate/saml', video_id,\n                    'Downloading Provider Redirect Page', query={\n                        'noflash': 'true',\n                        'mso_id': mso_id,\n                        'requestor_id': requestor_id,\n                        'no_iframe': 'false',\n                        'domain_name': 'adobe.com',\n                        'redirect_url': url,\n                    })\n\n                if mso_id == 'Comcast_SSO':\n                    # Comcast page flow varies by video site and whether you\n                    # are on Comcast's network.\n                    provider_redirect_page, urlh = provider_redirect_page_res\n                    if 'automatically signing you in' in provider_redirect_page:\n                        oauth_redirect_url = self._html_search_regex(\n                            r'window\\.location\\s*=\\s*[\\'\"]([^\\'\"]+)',\n                            provider_redirect_page, 'oauth redirect')\n                        self._download_webpage(\n                            oauth_redirect_url, video_id, 'Confirming auto login')\n                    else:\n                        if '<form name=\"signin\"' in provider_redirect_page:\n                            provider_login_page_res = provider_redirect_page_res\n                        elif 'http-equiv=\"refresh\"' in provider_redirect_page:\n                            oauth_redirect_url = extract_redirect_url(\n                                provider_redirect_page, fatal=True)\n                            provider_login_page_res = self._download_webpage_handle(\n                                oauth_redirect_url, video_id,\n                                self._DOWNLOADING_LOGIN_PAGE)\n                        else:\n                            provider_login_page_res = post_form(\n                                provider_redirect_page_res,\n                                self._DOWNLOADING_LOGIN_PAGE)\n\n                        mvpd_confirm_page_res = post_form(\n                            provider_login_page_res, 'Logging in', {\n                                mso_info['username_field']: username,\n                                mso_info['password_field']: password,\n                            })\n                        mvpd_confirm_page, urlh = mvpd_confirm_page_res\n                        if '<button class=\"submit\" value=\"Resume\">Resume</button>' in mvpd_confirm_page:\n                            post_form(mvpd_confirm_page_res, 'Confirming Login')\n                elif mso_id == 'Verizon':\n                    # In general, if you're connecting from a Verizon-assigned IP,\n                    # you will not actually pass your credentials.\n                    provider_redirect_page, urlh = provider_redirect_page_res\n                    if 'Please wait ...' in provider_redirect_page:\n                        saml_redirect_url = self._html_search_regex(\n                            r'self\\.parent\\.location=([\"\\'])(?P<url>.+?)\\1',\n                            provider_redirect_page,\n                            'SAML Redirect URL', group='url')\n                        saml_login_page = self._download_webpage(\n                            saml_redirect_url, video_id,\n                            'Downloading SAML Login Page')\n                    else:\n                        saml_login_page_res = post_form(\n                            provider_redirect_page_res, 'Logging in', {\n                                mso_info['username_field']: username,\n                                mso_info['password_field']: password,\n                            })\n                        saml_login_page, urlh = saml_login_page_res\n                        if 'Please try again.' in saml_login_page:\n                            raise ExtractorError(\n                                'We\\'re sorry, but either the User ID or Password entered is not correct.')\n                    saml_login_url = self._search_regex(\n                        r'xmlHttp\\.open\\(\"POST\"\\s*,\\s*([\"\\'])(?P<url>.+?)\\1',\n                        saml_login_page, 'SAML Login URL', group='url')\n                    saml_response_json = self._download_json(\n                        saml_login_url, video_id, 'Downloading SAML Response',\n                        headers={'Content-Type': 'text/xml'})\n                    self._download_webpage(\n                        saml_response_json['targetValue'], video_id,\n                        'Confirming Login', data=urlencode_postdata({\n                            'SAMLResponse': saml_response_json['SAMLResponse'],\n                            'RelayState': saml_response_json['RelayState']\n                        }), headers={\n                            'Content-Type': 'application/x-www-form-urlencoded'\n                        })\n                else:\n                    # Some providers (e.g. DIRECTV NOW) have another meta refresh\n                    # based redirect that should be followed.\n                    provider_redirect_page, urlh = provider_redirect_page_res\n                    provider_refresh_redirect_url = extract_redirect_url(\n                        provider_redirect_page, url=urlh.geturl())\n                    if provider_refresh_redirect_url:\n                        provider_redirect_page_res = self._download_webpage_handle(\n                            provider_refresh_redirect_url, video_id,\n                            'Downloading Provider Redirect Page (meta refresh)')\n                    provider_login_page_res = post_form(\n                        provider_redirect_page_res, self._DOWNLOADING_LOGIN_PAGE)\n                    mvpd_confirm_page_res = post_form(provider_login_page_res, 'Logging in', {\n                        mso_info.get('username_field', 'username'): username,\n                        mso_info.get('password_field', 'password'): password,\n                    })\n                    if mso_id != 'Rogers':\n                        post_form(mvpd_confirm_page_res, 'Confirming Login')\n\n                session = self._download_webpage(\n                    self._SERVICE_PROVIDER_TEMPLATE % 'session', video_id,\n                    'Retrieving Session', data=urlencode_postdata({\n                        '_method': 'GET',\n                        'requestor_id': requestor_id,\n                    }), headers=mvpd_headers)\n                if '<pendingLogout' in session:\n                    self._downloader.cache.store(self._MVPD_CACHE, requestor_id, {})\n                    count += 1\n                    continue\n                authn_token = unescapeHTML(xml_text(session, 'authnToken'))\n                requestor_info['authn_token'] = authn_token\n                self._downloader.cache.store(self._MVPD_CACHE, requestor_id, requestor_info)\n\n            authz_token = requestor_info.get(guid)\n            if authz_token and is_expired(authz_token, 'simpleTokenTTL'):\n                authz_token = None\n            if not authz_token:\n                authorize = self._download_webpage(\n                    self._SERVICE_PROVIDER_TEMPLATE % 'authorize', video_id,\n                    'Retrieving Authorization Token', data=urlencode_postdata({\n                        'resource_id': resource,\n                        'requestor_id': requestor_id,\n                        'authentication_token': authn_token,\n                        'mso_id': xml_text(authn_token, 'simpleTokenMsoID'),\n                        'userMeta': '1',\n                    }), headers=mvpd_headers)\n                if '<pendingLogout' in authorize:\n                    self._downloader.cache.store(self._MVPD_CACHE, requestor_id, {})\n                    count += 1\n                    continue\n                if '<error' in authorize:\n                    raise ExtractorError(xml_text(authorize, 'details'), expected=True)\n                authz_token = unescapeHTML(xml_text(authorize, 'authzToken'))\n                requestor_info[guid] = authz_token\n                self._downloader.cache.store(self._MVPD_CACHE, requestor_id, requestor_info)\n\n            mvpd_headers.update({\n                'ap_19': xml_text(authn_token, 'simpleSamlNameID'),\n                'ap_23': xml_text(authn_token, 'simpleSamlSessionIndex'),\n            })\n\n            short_authorize = self._download_webpage(\n                self._SERVICE_PROVIDER_TEMPLATE % 'shortAuthorize',\n                video_id, 'Retrieving Media Token', data=urlencode_postdata({\n                    'authz_token': authz_token,\n                    'requestor_id': requestor_id,\n                    'session_guid': xml_text(authn_token, 'simpleTokenAuthenticationGuid'),\n                    'hashed_guid': 'false',\n                }), headers=mvpd_headers)\n            if '<pendingLogout' in short_authorize:\n                self._downloader.cache.store(self._MVPD_CACHE, requestor_id, {})\n                count += 1\n                continue\n            return short_authorize",
        "begin_line": 1354,
        "end_line": 1572,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00034258307639602604,
            "pseudo_dstar_susp": 0.00031735956839098697,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.update.rsa_verify#18",
        "src_path": "youtube_dl/update.py",
        "class_name": "youtube_dl.update",
        "signature": "youtube_dl.update.rsa_verify(message, signature, key)",
        "snippet": "def rsa_verify(message, signature, key):\n    from hashlib import sha256\n    assert isinstance(message, bytes)\n    byte_size = (len(bin(key[0])) - 2 + 8 - 1) // 8\n    signature = ('%x' % pow(int(signature, 16), key[1], key[0])).encode()\n    signature = (byte_size * 2 - len(signature)) * b'0' + signature\n    asn1 = b'3031300d060960864801650304020105000420'\n    asn1 += sha256(message).hexdigest().encode()\n    if byte_size < len(asn1) // 2 + 11:\n        return False\n    expected = b'0001' + (byte_size - len(asn1) // 2 - 3) * b'ff' + b'00' + asn1\n    return expected == signature",
        "begin_line": 18,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.platzi.PlatziCourseIE.suitable#181",
        "src_path": "youtube_dl/extractor/platzi.py",
        "class_name": "youtube_dl.extractor.platzi.PlatziCourseIE",
        "signature": "youtube_dl.extractor.platzi.PlatziCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if PlatziIE.suitable(url) else super(PlatziCourseIE, cls).suitable(url)",
        "begin_line": 181,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00033090668431502316,
            "pseudo_tarantula_susp": 0.0002752546105147261,
            "pseudo_op2_susp": 0.00033090668431502316,
            "pseudo_barinel_susp": 0.0002729257641921397
        }
    },
    {
        "name": "youtube_dl.extractor.platzi.PlatziCourseIE._real_extract#184",
        "src_path": "youtube_dl/extractor/platzi.py",
        "class_name": "youtube_dl.extractor.platzi.PlatziCourseIE",
        "signature": "youtube_dl.extractor.platzi.PlatziCourseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        course_name = self._match_id(url)\n\n        webpage = self._download_webpage(url, course_name)\n\n        props = self._parse_json(\n            self._search_regex(r'data\\s*=\\s*({.+?})\\s*;', webpage, 'data'),\n            course_name)['initialProps']\n\n        entries = []\n        for chapter_num, chapter in enumerate(props['concepts'], 1):\n            if not isinstance(chapter, dict):\n                continue\n            materials = chapter.get('materials')\n            if not materials or not isinstance(materials, list):\n                continue\n            chapter_title = chapter.get('title')\n            chapter_id = str_or_none(chapter.get('id'))\n            for material in materials:\n                if not isinstance(material, dict):\n                    continue\n                if material.get('material_type') != 'video':\n                    continue\n                video_url = urljoin(url, material.get('url'))\n                if not video_url:\n                    continue\n                entries.append({\n                    '_type': 'url_transparent',\n                    'url': video_url,\n                    'title': str_or_none(material.get('name')),\n                    'id': str_or_none(material.get('id')),\n                    'ie_key': PlatziIE.ie_key(),\n                    'chapter': chapter_title,\n                    'chapter_number': chapter_num,\n                    'chapter_id': chapter_id,\n                })\n\n        course_id = compat_str(try_get(props, lambda x: x['course']['id']))\n        course_title = try_get(props, lambda x: x['course']['name'], compat_str)\n\n        return self.playlist_result(entries, course_id, course_title)",
        "begin_line": 184,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011582117211026175,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.rutube.RutubeIE.suitable#132",
        "src_path": "youtube_dl/extractor/rutube.py",
        "class_name": "youtube_dl.extractor.rutube.RutubeIE",
        "signature": "youtube_dl.extractor.rutube.RutubeIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if RutubePlaylistIE.suitable(url) else super(RutubeIE, cls).suitable(url)",
        "begin_line": 132,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.00030111412225233364,
            "pseudo_tarantula_susp": 0.00026343519494204424,
            "pseudo_op2_susp": 0.00030111412225233364,
            "pseudo_barinel_susp": 0.00026343519494204424
        }
    },
    {
        "name": "youtube_dl.extractor.rutube.RutubePlaylistIE.suitable#300",
        "src_path": "youtube_dl/extractor/rutube.py",
        "class_name": "youtube_dl.extractor.rutube.RutubePlaylistIE",
        "signature": "youtube_dl.extractor.rutube.RutubePlaylistIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        if not super(RutubePlaylistIE, cls).suitable(url):\n            return False\n        params = compat_parse_qs(compat_urllib_parse_urlparse(url).query)\n        return params.get('pl_type', [None])[0] and int_or_none(params.get('pl_id', [None])[0])",
        "begin_line": 300,
        "end_line": 304,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE.report_following_redirect#2173",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE.report_following_redirect(self, new_url)",
        "snippet": "    def report_following_redirect(self, new_url):\n        \"\"\"Report information extraction.\"\"\"\n        self._downloader.to_screen('[redirect] Following redirect to %s' % new_url)",
        "begin_line": 2173,
        "end_line": 2175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE._extract_rss#2177",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE._extract_rss(self, url, video_id, doc)",
        "snippet": "    def _extract_rss(self, url, video_id, doc):\n        playlist_title = doc.find('./channel/title').text\n        playlist_desc_el = doc.find('./channel/description')\n        playlist_desc = None if playlist_desc_el is None else playlist_desc_el.text\n\n        entries = []\n        for it in doc.findall('./channel/item'):\n            next_url = None\n            enclosure_nodes = it.findall('./enclosure')\n            for e in enclosure_nodes:\n                next_url = e.attrib.get('url')\n                if next_url:\n                    break\n\n            if not next_url:\n                next_url = xpath_text(it, 'link', fatal=False)\n\n            if not next_url:\n                continue\n\n            entries.append({\n                '_type': 'url_transparent',\n                'url': next_url,\n                'title': it.find('title').text,\n            })\n\n        return {\n            '_type': 'playlist',\n            'id': url,\n            'title': playlist_title,\n            'description': playlist_desc,\n            'entries': entries,\n        }",
        "begin_line": 2177,
        "end_line": 2209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00026932399676811203,
            "pseudo_dstar_susp": 0.0002678810608090008,
            "pseudo_tarantula_susp": 0.00029129041654529564,
            "pseudo_op2_susp": 0.0002678810608090008,
            "pseudo_barinel_susp": 0.00029129041654529564
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE._extract_camtasia#2211",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE._extract_camtasia(self, url, video_id, webpage)",
        "snippet": "    def _extract_camtasia(self, url, video_id, webpage):\n        \"\"\" Returns None if no camtasia video can be found. \"\"\"\n\n        camtasia_cfg = self._search_regex(\n            r'fo\\.addVariable\\(\\s*\"csConfigFile\",\\s*\"([^\"]+)\"\\s*\\);',\n            webpage, 'camtasia configuration file', default=None)\n        if camtasia_cfg is None:\n            return None\n\n        title = self._html_search_meta('DC.title', webpage, fatal=True)\n\n        camtasia_url = compat_urlparse.urljoin(url, camtasia_cfg)\n        camtasia_cfg = self._download_xml(\n            camtasia_url, video_id,\n            note='Downloading camtasia configuration',\n            errnote='Failed to download camtasia configuration')\n        fileset_node = camtasia_cfg.find('./playlist/array/fileset')\n\n        entries = []\n        for n in fileset_node.getchildren():\n            url_n = n.find('./uri')\n            if url_n is None:\n                continue\n\n            entries.append({\n                'id': os.path.splitext(url_n.text.rpartition('/')[2])[0],\n                'title': '%s - %s' % (title, n.tag),\n                'url': compat_urlparse.urljoin(url, url_n.text),\n                'duration': float_or_none(n.find('./duration').text),\n            })\n\n        return {\n            '_type': 'playlist',\n            'entries': entries,\n            'title': title,\n        }",
        "begin_line": 2211,
        "end_line": 2246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007651109410864575,
            "pseudo_dstar_susp": 0.0007651109410864575,
            "pseudo_tarantula_susp": 0.0004716981132075472,
            "pseudo_op2_susp": 0.0007651109410864575,
            "pseudo_barinel_susp": 0.0004716981132075472
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE._real_extract#2248",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        if url.startswith('//'):\n            return self.url_result(self.http_scheme() + url)\n\n        parsed_url = compat_urlparse.urlparse(url)\n        if not parsed_url.scheme:\n            default_search = self._downloader.params.get('default_search')\n            if default_search is None:\n                default_search = 'fixup_error'\n\n            if default_search in ('auto', 'auto_warning', 'fixup_error'):\n                if re.match(r'^[^\\s/]+\\.[^\\s/]+/', url):\n                    self._downloader.report_warning('The url doesn\\'t specify the protocol, trying with http')\n                    return self.url_result('http://' + url)\n                elif default_search != 'fixup_error':\n                    if default_search == 'auto_warning':\n                        if re.match(r'^(?:url|URL)$', url):\n                            raise ExtractorError(\n                                'Invalid URL:  %r . Call youtube-dl like this:  youtube-dl -v \"https://www.youtube.com/watch?v=BaW_jenozKc\"  ' % url,\n                                expected=True)\n                        else:\n                            self._downloader.report_warning(\n                                'Falling back to youtube search for  %s . Set --default-search \"auto\" to suppress this warning.' % url)\n                    return self.url_result('ytsearch:' + url)\n\n            if default_search in ('error', 'fixup_error'):\n                raise ExtractorError(\n                    '%r is not a valid URL. '\n                    'Set --default-search \"ytsearch\" (or run  youtube-dl \"ytsearch:%s\" ) to search YouTube'\n                    % (url, url), expected=True)\n            else:\n                if ':' not in default_search:\n                    default_search += ':'\n                return self.url_result(default_search + url)\n\n        url, smuggled_data = unsmuggle_url(url)\n        force_videoid = None\n        is_intentional = smuggled_data and smuggled_data.get('to_generic')\n        if smuggled_data and 'force_videoid' in smuggled_data:\n            force_videoid = smuggled_data['force_videoid']\n            video_id = force_videoid\n        else:\n            video_id = self._generic_id(url)\n\n        self.to_screen('%s: Requesting header' % video_id)\n\n        head_req = HEADRequest(url)\n        head_response = self._request_webpage(\n            head_req, video_id,\n            note=False, errnote='Could not send HEAD request to %s' % url,\n            fatal=False)\n\n        if head_response is not False:\n            # Check for redirect\n            new_url = head_response.geturl()\n            if url != new_url:\n                self.report_following_redirect(new_url)\n                if force_videoid:\n                    new_url = smuggle_url(\n                        new_url, {'force_videoid': force_videoid})\n                return self.url_result(new_url)\n\n        full_response = None\n        if head_response is False:\n            request = sanitized_Request(url)\n            request.add_header('Accept-Encoding', '*')\n            full_response = self._request_webpage(request, video_id)\n            head_response = full_response\n\n        info_dict = {\n            'id': video_id,\n            'title': self._generic_title(url),\n            'upload_date': unified_strdate(head_response.headers.get('Last-Modified'))\n        }\n\n        # Check for direct link to a video\n        content_type = head_response.headers.get('Content-Type', '').lower()\n        m = re.match(r'^(?P<type>audio|video|application(?=/(?:ogg$|(?:vnd\\.apple\\.|x-)?mpegurl)))/(?P<format_id>[^;\\s]+)', content_type)\n        if m:\n            format_id = compat_str(m.group('format_id'))\n            if format_id.endswith('mpegurl'):\n                formats = self._extract_m3u8_formats(url, video_id, 'mp4')\n            elif format_id == 'f4m':\n                formats = self._extract_f4m_formats(url, video_id)\n            else:\n                formats = [{\n                    'format_id': format_id,\n                    'url': url,\n                    'vcodec': 'none' if m.group('type') == 'audio' else None\n                }]\n                info_dict['direct'] = True\n            self._sort_formats(formats)\n            info_dict['formats'] = formats\n            return info_dict\n\n        if not self._downloader.params.get('test', False) and not is_intentional:\n            force = self._downloader.params.get('force_generic_extractor', False)\n            self._downloader.report_warning(\n                '%s on generic information extractor.' % ('Forcing' if force else 'Falling back'))\n\n        if not full_response:\n            request = sanitized_Request(url)\n            # Some webservers may serve compressed content of rather big size (e.g. gzipped flac)\n            # making it impossible to download only chunk of the file (yet we need only 512kB to\n            # test whether it's HTML or not). According to youtube-dl default Accept-Encoding\n            # that will always result in downloading the whole file that is not desirable.\n            # Therefore for extraction pass we have to override Accept-Encoding to any in order\n            # to accept raw bytes and being able to download only a chunk.\n            # It may probably better to solve this by checking Content-Type for application/octet-stream\n            # after HEAD request finishes, but not sure if we can rely on this.\n            request.add_header('Accept-Encoding', '*')\n            full_response = self._request_webpage(request, video_id)\n\n        first_bytes = full_response.read(512)\n\n        # Is it an M3U playlist?\n        if first_bytes.startswith(b'#EXTM3U'):\n            info_dict['formats'] = self._extract_m3u8_formats(url, video_id, 'mp4')\n            self._sort_formats(info_dict['formats'])\n            return info_dict\n\n        # Maybe it's a direct link to a video?\n        # Be careful not to download the whole thing!\n        if not is_html(first_bytes):\n            self._downloader.report_warning(\n                'URL could be a direct video link, returning it as such.')\n            info_dict.update({\n                'direct': True,\n                'url': url,\n            })\n            return info_dict\n\n        webpage = self._webpage_read_content(\n            full_response, url, video_id, prefix=first_bytes)\n\n        self.report_extraction(video_id)\n\n        # Is it an RSS feed, a SMIL file, an XSPF playlist or a MPD manifest?\n        try:\n            doc = compat_etree_fromstring(webpage.encode('utf-8'))\n            if doc.tag == 'rss':\n                return self._extract_rss(url, video_id, doc)\n            elif doc.tag == 'SmoothStreamingMedia':\n                info_dict['formats'] = self._parse_ism_formats(doc, url)\n                self._sort_formats(info_dict['formats'])\n                return info_dict\n            elif re.match(r'^(?:{[^}]+})?smil$', doc.tag):\n                smil = self._parse_smil(doc, url, video_id)\n                self._sort_formats(smil['formats'])\n                return smil\n            elif doc.tag == '{http://xspf.org/ns/0/}playlist':\n                return self.playlist_result(\n                    self._parse_xspf(\n                        doc, video_id, xspf_url=url,\n                        xspf_base_url=full_response.geturl()),\n                    video_id)\n            elif re.match(r'(?i)^(?:{[^}]+})?MPD$', doc.tag):\n                info_dict['formats'] = self._parse_mpd_formats(\n                    doc,\n                    mpd_base_url=full_response.geturl().rpartition('/')[0],\n                    mpd_url=url)\n                self._sort_formats(info_dict['formats'])\n                return info_dict\n            elif re.match(r'^{http://ns\\.adobe\\.com/f4m/[12]\\.0}manifest$', doc.tag):\n                info_dict['formats'] = self._parse_f4m_formats(doc, url, video_id)\n                self._sort_formats(info_dict['formats'])\n                return info_dict\n        except compat_xml_parse_error:\n            pass\n\n        # Is it a Camtasia project?\n        camtasia_res = self._extract_camtasia(url, video_id, webpage)\n        if camtasia_res is not None:\n            return camtasia_res\n\n        # Sometimes embedded video player is hidden behind percent encoding\n        # (e.g. https://github.com/ytdl-org/youtube-dl/issues/2448)\n        # Unescaping the whole page allows to handle those cases in a generic way\n        webpage = compat_urllib_parse_unquote(webpage)\n\n        # Unescape squarespace embeds to be detected by generic extractor,\n        # see https://github.com/ytdl-org/youtube-dl/issues/21294\n        webpage = re.sub(\n            r'<div[^>]+class=[^>]*?\\bsqs-video-wrapper\\b[^>]*>',\n            lambda x: unescapeHTML(x.group(0)), webpage)\n\n        # it's tempting to parse this further, but you would\n        # have to take into account all the variations like\n        #   Video Title - Site Name\n        #   Site Name | Video Title\n        #   Video Title - Tagline | Site Name\n        # and so on and so forth; it's just not practical\n        video_title = self._og_search_title(\n            webpage, default=None) or self._html_search_regex(\n            r'(?s)<title>(.*?)</title>', webpage, 'video title',\n            default='video')\n\n        # Try to detect age limit automatically\n        age_limit = self._rta_search(webpage)\n        # And then there are the jokers who advertise that they use RTA,\n        # but actually don't.\n        AGE_LIMIT_MARKERS = [\n            r'Proudly Labeled <a href=\"http://www\\.rtalabel\\.org/\" title=\"Restricted to Adults\">RTA</a>',\n        ]\n        if any(re.search(marker, webpage) for marker in AGE_LIMIT_MARKERS):\n            age_limit = 18\n\n        # video uploader is domain name\n        video_uploader = self._search_regex(\n            r'^(?:https?://)?([^/]*)/.*', url, 'video uploader')\n\n        video_description = self._og_search_description(webpage, default=None)\n        video_thumbnail = self._og_search_thumbnail(webpage, default=None)\n\n        info_dict.update({\n            'title': video_title,\n            'description': video_description,\n            'thumbnail': video_thumbnail,\n            'age_limit': age_limit,\n        })\n\n        # Look for Brightcove Legacy Studio embeds\n        bc_urls = BrightcoveLegacyIE._extract_brightcove_urls(webpage)\n        if bc_urls:\n            entries = [{\n                '_type': 'url',\n                'url': smuggle_url(bc_url, {'Referer': url}),\n                'ie_key': 'BrightcoveLegacy'\n            } for bc_url in bc_urls]\n\n            return {\n                '_type': 'playlist',\n                'title': video_title,\n                'id': video_id,\n                'entries': entries,\n            }\n\n        # Look for Brightcove New Studio embeds\n        bc_urls = BrightcoveNewIE._extract_urls(self, webpage)\n        if bc_urls:\n            return self.playlist_from_matches(\n                bc_urls, video_id, video_title,\n                getter=lambda x: smuggle_url(x, {'referrer': url}),\n                ie='BrightcoveNew')\n\n        # Look for Nexx embeds\n        nexx_urls = NexxIE._extract_urls(webpage)\n        if nexx_urls:\n            return self.playlist_from_matches(nexx_urls, video_id, video_title, ie=NexxIE.ie_key())\n\n        # Look for Nexx iFrame embeds\n        nexx_embed_urls = NexxEmbedIE._extract_urls(webpage)\n        if nexx_embed_urls:\n            return self.playlist_from_matches(nexx_embed_urls, video_id, video_title, ie=NexxEmbedIE.ie_key())\n\n        # Look for ThePlatform embeds\n        tp_urls = ThePlatformIE._extract_urls(webpage)\n        if tp_urls:\n            return self.playlist_from_matches(tp_urls, video_id, video_title, ie='ThePlatform')\n\n        # Look for embedded rtl.nl player\n        matches = re.findall(\n            r'<iframe[^>]+?src=\"((?:https?:)?//(?:(?:www|static)\\.)?rtl\\.nl/(?:system/videoplayer/[^\"]+(?:video_)?)?embed[^\"]+)\"',\n            webpage)\n        if matches:\n            return self.playlist_from_matches(matches, video_id, video_title, ie='RtlNl')\n\n        vimeo_urls = VimeoIE._extract_urls(url, webpage)\n        if vimeo_urls:\n            return self.playlist_from_matches(vimeo_urls, video_id, video_title, ie=VimeoIE.ie_key())\n\n        vid_me_embed_url = self._search_regex(\n            r'src=[\\'\"](https?://vid\\.me/[^\\'\"]+)[\\'\"]',\n            webpage, 'vid.me embed', default=None)\n        if vid_me_embed_url is not None:\n            return self.url_result(vid_me_embed_url, 'Vidme')\n\n        # Look for YouTube embeds\n        youtube_urls = YoutubeIE._extract_urls(webpage)\n        if youtube_urls:\n            return self.playlist_from_matches(\n                youtube_urls, video_id, video_title, ie=YoutubeIE.ie_key())\n\n        matches = DailymotionIE._extract_urls(webpage)\n        if matches:\n            return self.playlist_from_matches(matches, video_id, video_title)\n\n        # Look for embedded Dailymotion playlist player (#3822)\n        m = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?dailymotion\\.[a-z]{2,3}/widget/jukebox\\?.+?)\\1', webpage)\n        if m:\n            playlists = re.findall(\n                r'list\\[\\]=/playlist/([^/]+)/', unescapeHTML(m.group('url')))\n            if playlists:\n                return self.playlist_from_matches(\n                    playlists, video_id, video_title, lambda p: '//dailymotion.com/playlist/%s' % p)\n\n        # Look for DailyMail embeds\n        dailymail_urls = DailyMailIE._extract_urls(webpage)\n        if dailymail_urls:\n            return self.playlist_from_matches(\n                dailymail_urls, video_id, video_title, ie=DailyMailIE.ie_key())\n\n        # Look for Teachable embeds, must be before Wistia\n        teachable_url = TeachableIE._extract_url(webpage, url)\n        if teachable_url:\n            return self.url_result(teachable_url)\n\n        # Look for embedded Wistia player\n        wistia_urls = WistiaIE._extract_urls(webpage)\n        if wistia_urls:\n            playlist = self.playlist_from_matches(wistia_urls, video_id, video_title, ie=WistiaIE.ie_key())\n            for entry in playlist['entries']:\n                entry.update({\n                    '_type': 'url_transparent',\n                    'uploader': video_uploader,\n                })\n            return playlist\n\n        # Look for SVT player\n        svt_url = SVTIE._extract_url(webpage)\n        if svt_url:\n            return self.url_result(svt_url, 'SVT')\n\n        # Look for Bandcamp pages with custom domain\n        mobj = re.search(r'<meta property=\"og:url\"[^>]*?content=\"(.*?bandcamp\\.com.*?)\"', webpage)\n        if mobj is not None:\n            burl = unescapeHTML(mobj.group(1))\n            # Don't set the extractor because it can be a track url or an album\n            return self.url_result(burl)\n\n        # Look for embedded Vevo player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:cache\\.)?vevo\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for embedded Viddler player\n        mobj = re.search(\n            r'<(?:iframe[^>]+?src|param[^>]+?value)=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?viddler\\.com/(?:embed|player)/.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for NYTimes player\n        mobj = re.search(\n            r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//graphics8\\.nytimes\\.com/bcvideo/[^/]+/iframe/embed\\.html.+?)\\1>',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for Libsyn player\n        mobj = re.search(\n            r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//html5-player\\.libsyn\\.com/embed/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for Ooyala videos\n        mobj = (re.search(r'player\\.ooyala\\.com/[^\"?]+[?#][^\"]*?(?:embedCode|ec)=(?P<ec>[^\"&]+)', webpage)\n                or re.search(r'OO\\.Player\\.create\\([\\'\"].*?[\\'\"],\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage)\n                or re.search(r'OO\\.Player\\.create\\.apply\\(\\s*OO\\.Player\\s*,\\s*op\\(\\s*\\[\\s*[\\'\"][^\\'\"]*[\\'\"]\\s*,\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage)\n                or re.search(r'SBN\\.VideoLinkset\\.ooyala\\([\\'\"](?P<ec>.{32})[\\'\"]\\)', webpage)\n                or re.search(r'data-ooyala-video-id\\s*=\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage))\n        if mobj is not None:\n            embed_token = self._search_regex(\n                r'embedToken[\\'\"]?\\s*:\\s*[\\'\"]([^\\'\"]+)',\n                webpage, 'ooyala embed token', default=None)\n            return OoyalaIE._build_url_result(smuggle_url(\n                mobj.group('ec'), {\n                    'domain': url,\n                    'embed_token': embed_token,\n                }))\n\n        # Look for multiple Ooyala embeds on SBN network websites\n        mobj = re.search(r'SBN\\.VideoLinkset\\.entryGroup\\((\\[.*?\\])', webpage)\n        if mobj is not None:\n            embeds = self._parse_json(mobj.group(1), video_id, fatal=False)\n            if embeds:\n                return self.playlist_from_matches(\n                    embeds, video_id, video_title,\n                    getter=lambda v: OoyalaIE._url_for_embed_code(smuggle_url(v['provider_video_id'], {'domain': url})), ie='Ooyala')\n\n        # Look for Aparat videos\n        mobj = re.search(r'<iframe .*?src=\"(http://www\\.aparat\\.com/video/[^\"]+)\"', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group(1), 'Aparat')\n\n        # Look for MPORA videos\n        mobj = re.search(r'<iframe .*?src=\"(http://mpora\\.(?:com|de)/videos/[^\"]+)\"', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group(1), 'Mpora')\n\n        # Look for embedded Facebook player\n        facebook_urls = FacebookIE._extract_urls(webpage)\n        if facebook_urls:\n            return self.playlist_from_matches(facebook_urls, video_id, video_title)\n\n        # Look for embedded VK player\n        mobj = re.search(r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://vk\\.com/video_ext\\.php.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'VK')\n\n        # Look for embedded Odnoklassniki player\n        odnoklassniki_url = OdnoklassnikiIE._extract_url(webpage)\n        if odnoklassniki_url:\n            return self.url_result(odnoklassniki_url, OdnoklassnikiIE.ie_key())\n\n        # Look for embedded ivi player\n        mobj = re.search(r'<embed[^>]+?src=([\"\\'])(?P<url>https?://(?:www\\.)?ivi\\.ru/video/player.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Ivi')\n\n        # Look for embedded Huffington Post player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://embed\\.live\\.huffingtonpost\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'HuffPost')\n\n        # Look for embed.ly\n        mobj = re.search(r'class=[\"\\']embedly-card[\"\\'][^>]href=[\"\\'](?P<url>[^\"\\']+)', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n        mobj = re.search(r'class=[\"\\']embedly-embed[\"\\'][^>]src=[\"\\'][^\"\\']*url=(?P<url>[^&]+)', webpage)\n        if mobj is not None:\n            return self.url_result(compat_urllib_parse_unquote(mobj.group('url')))\n\n        # Look for funnyordie embed\n        matches = re.findall(r'<iframe[^>]+?src=\"(https?://(?:www\\.)?funnyordie\\.com/embed/[^\"]+)\"', webpage)\n        if matches:\n            return self.playlist_from_matches(\n                matches, video_id, video_title, getter=unescapeHTML, ie='FunnyOrDie')\n\n        # Look for BBC iPlayer embed\n        matches = re.findall(r'setPlaylist\\(\"(https?://www\\.bbc\\.co\\.uk/iplayer/[^/]+/[\\da-z]{8})\"\\)', webpage)\n        if matches:\n            return self.playlist_from_matches(matches, video_id, video_title, ie='BBCCoUk')\n\n        # Look for embedded RUTV player\n        rutv_url = RUTVIE._extract_url(webpage)\n        if rutv_url:\n            return self.url_result(rutv_url, 'RUTV')\n\n        # Look for embedded TVC player\n        tvc_url = TVCIE._extract_url(webpage)\n        if tvc_url:\n            return self.url_result(tvc_url, 'TVC')\n\n        # Look for embedded SportBox player\n        sportbox_urls = SportBoxIE._extract_urls(webpage)\n        if sportbox_urls:\n            return self.playlist_from_matches(sportbox_urls, video_id, video_title, ie=SportBoxIE.ie_key())\n\n        # Look for embedded XHamster player\n        xhamster_urls = XHamsterEmbedIE._extract_urls(webpage)\n        if xhamster_urls:\n            return self.playlist_from_matches(xhamster_urls, video_id, video_title, ie='XHamsterEmbed')\n\n        # Look for embedded TNAFlixNetwork player\n        tnaflix_urls = TNAFlixNetworkEmbedIE._extract_urls(webpage)\n        if tnaflix_urls:\n            return self.playlist_from_matches(tnaflix_urls, video_id, video_title, ie=TNAFlixNetworkEmbedIE.ie_key())\n\n        # Look for embedded PornHub player\n        pornhub_urls = PornHubIE._extract_urls(webpage)\n        if pornhub_urls:\n            return self.playlist_from_matches(pornhub_urls, video_id, video_title, ie=PornHubIE.ie_key())\n\n        # Look for embedded DrTuber player\n        drtuber_urls = DrTuberIE._extract_urls(webpage)\n        if drtuber_urls:\n            return self.playlist_from_matches(drtuber_urls, video_id, video_title, ie=DrTuberIE.ie_key())\n\n        # Look for embedded RedTube player\n        redtube_urls = RedTubeIE._extract_urls(webpage)\n        if redtube_urls:\n            return self.playlist_from_matches(redtube_urls, video_id, video_title, ie=RedTubeIE.ie_key())\n\n        # Look for embedded Tube8 player\n        tube8_urls = Tube8IE._extract_urls(webpage)\n        if tube8_urls:\n            return self.playlist_from_matches(tube8_urls, video_id, video_title, ie=Tube8IE.ie_key())\n\n        # Look for embedded Mofosex player\n        mofosex_urls = MofosexEmbedIE._extract_urls(webpage)\n        if mofosex_urls:\n            return self.playlist_from_matches(mofosex_urls, video_id, video_title, ie=MofosexEmbedIE.ie_key())\n\n        # Look for embedded Spankwire player\n        spankwire_urls = SpankwireIE._extract_urls(webpage)\n        if spankwire_urls:\n            return self.playlist_from_matches(spankwire_urls, video_id, video_title, ie=SpankwireIE.ie_key())\n\n        # Look for embedded YouPorn player\n        youporn_urls = YouPornIE._extract_urls(webpage)\n        if youporn_urls:\n            return self.playlist_from_matches(youporn_urls, video_id, video_title, ie=YouPornIE.ie_key())\n\n        # Look for embedded Tvigle player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//cloud\\.tvigle\\.ru/video/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Tvigle')\n\n        # Look for embedded TED player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://embed(?:-ssl)?\\.ted\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'TED')\n\n        # Look for embedded Ustream videos\n        ustream_url = UstreamIE._extract_url(webpage)\n        if ustream_url:\n            return self.url_result(ustream_url, UstreamIE.ie_key())\n\n        # Look for embedded arte.tv player\n        mobj = re.search(\n            r'<(?:script|iframe) [^>]*?src=\"(?P<url>http://www\\.arte\\.tv/(?:playerv2/embed|arte_vp/index)[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'ArteTVEmbed')\n\n        # Look for embedded francetv player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?://)?embed\\.francetv\\.fr/\\?ue=.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for embedded smotri.com player\n        smotri_url = SmotriIE._extract_url(webpage)\n        if smotri_url:\n            return self.url_result(smotri_url, 'Smotri')\n\n        # Look for embedded Myvi.ru player\n        myvi_url = MyviIE._extract_url(webpage)\n        if myvi_url:\n            return self.url_result(myvi_url)\n\n        # Look for embedded soundcloud player\n        soundcloud_urls = SoundcloudEmbedIE._extract_urls(webpage)\n        if soundcloud_urls:\n            return self.playlist_from_matches(soundcloud_urls, video_id, video_title, getter=unescapeHTML)\n\n        # Look for tunein player\n        tunein_urls = TuneInBaseIE._extract_urls(webpage)\n        if tunein_urls:\n            return self.playlist_from_matches(tunein_urls, video_id, video_title)\n\n        # Look for embedded mtvservices player\n        mtvservices_url = MTVServicesEmbeddedIE._extract_url(webpage)\n        if mtvservices_url:\n            return self.url_result(mtvservices_url, ie='MTVServicesEmbedded')\n\n        # Look for embedded yahoo player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://(?:screen|movies)\\.yahoo\\.com/.+?\\.html\\?format=embed)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Yahoo')\n\n        # Look for embedded sbs.com.au player\n        mobj = re.search(\n            r'''(?x)\n            (?:\n                <meta\\s+property=\"og:video\"\\s+content=|\n                <iframe[^>]+?src=\n            )\n            ([\"\\'])(?P<url>https?://(?:www\\.)?sbs\\.com\\.au/ondemand/video/.+?)\\1''',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'SBS')\n\n        # Look for embedded Cinchcast player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://player\\.cinchcast\\.com/.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Cinchcast')\n\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://m(?:lb)?\\.mlb\\.com/shared/video/embed/embed\\.html\\?.+?)\\1',\n            webpage)\n        if not mobj:\n            mobj = re.search(\n                r'data-video-link=[\"\\'](?P<url>http://m.mlb.com/video/[^\"\\']+)',\n                webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'MLB')\n\n        mobj = re.search(\n            r'<(?:iframe|script)[^>]+?src=([\"\\'])(?P<url>%s)\\1' % CondeNastIE.EMBED_URL,\n            webpage)\n        if mobj is not None:\n            return self.url_result(self._proto_relative_url(mobj.group('url'), scheme='http:'), 'CondeNast')\n\n        mobj = re.search(\n            r'<iframe[^>]+src=\"(?P<url>https?://(?:new\\.)?livestream\\.com/[^\"]+/player[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Livestream')\n\n        # Look for Zapiks embed\n        mobj = re.search(\n            r'<iframe[^>]+src=\"(?P<url>https?://(?:www\\.)?zapiks\\.fr/index\\.php\\?.+?)\"', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Zapiks')\n\n        # Look for Kaltura embeds\n        kaltura_urls = KalturaIE._extract_urls(webpage)\n        if kaltura_urls:\n            return self.playlist_from_matches(\n                kaltura_urls, video_id, video_title,\n                getter=lambda x: smuggle_url(x, {'source_url': url}),\n                ie=KalturaIE.ie_key())\n\n        # Look for EaglePlatform embeds\n        eagleplatform_url = EaglePlatformIE._extract_url(webpage)\n        if eagleplatform_url:\n            return self.url_result(smuggle_url(eagleplatform_url, {'referrer': url}), EaglePlatformIE.ie_key())\n\n        # Look for ClipYou (uses EaglePlatform) embeds\n        mobj = re.search(\n            r'<iframe[^>]+src=\"https?://(?P<host>media\\.clipyou\\.ru)/index/player\\?.*\\brecord_id=(?P<id>\\d+).*\"', webpage)\n        if mobj is not None:\n            return self.url_result('eagleplatform:%(host)s:%(id)s' % mobj.groupdict(), 'EaglePlatform')\n\n        # Look for Pladform embeds\n        pladform_url = PladformIE._extract_url(webpage)\n        if pladform_url:\n            return self.url_result(pladform_url)\n\n        # Look for Videomore embeds\n        videomore_url = VideomoreIE._extract_url(webpage)\n        if videomore_url:\n            return self.url_result(videomore_url)\n\n        # Look for Webcaster embeds\n        webcaster_url = WebcasterFeedIE._extract_url(self, webpage)\n        if webcaster_url:\n            return self.url_result(webcaster_url, ie=WebcasterFeedIE.ie_key())\n\n        # Look for Playwire embeds\n        mobj = re.search(\n            r'<script[^>]+data-config=([\"\\'])(?P<url>(?:https?:)?//config\\.playwire\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for 5min embeds\n        mobj = re.search(\n            r'<meta[^>]+property=\"og:video\"[^>]+content=\"https?://embed\\.5min\\.com/(?P<id>[0-9]+)/?', webpage)\n        if mobj is not None:\n            return self.url_result('5min:%s' % mobj.group('id'), 'FiveMin')\n\n        # Look for Crooks and Liars embeds\n        mobj = re.search(\n            r'<(?:iframe[^>]+src|param[^>]+value)=([\"\\'])(?P<url>(?:https?:)?//embed\\.crooksandliars\\.com/(?:embed|v)/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for NBC Sports VPlayer embeds\n        nbc_sports_url = NBCSportsVPlayerIE._extract_url(webpage)\n        if nbc_sports_url:\n            return self.url_result(nbc_sports_url, 'NBCSportsVPlayer')\n\n        # Look for NBC News embeds\n        nbc_news_embed_url = re.search(\n            r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//www\\.nbcnews\\.com/widget/video-embed/[^\"\\']+)\\1', webpage)\n        if nbc_news_embed_url:\n            return self.url_result(nbc_news_embed_url.group('url'), 'NBCNews')\n\n        # Look for Google Drive embeds\n        google_drive_url = GoogleDriveIE._extract_url(webpage)\n        if google_drive_url:\n            return self.url_result(google_drive_url, 'GoogleDrive')\n\n        # Look for UDN embeds\n        mobj = re.search(\n            r'<iframe[^>]+src=\"(?:https?:)?(?P<url>%s)\"' % UDNEmbedIE._PROTOCOL_RELATIVE_VALID_URL, webpage)\n        if mobj is not None:\n            return self.url_result(\n                compat_urlparse.urljoin(url, mobj.group('url')), 'UDNEmbed')\n\n        # Look for Senate ISVP iframe\n        senate_isvp_url = SenateISVPIE._search_iframe_url(webpage)\n        if senate_isvp_url:\n            return self.url_result(senate_isvp_url, 'SenateISVP')\n\n        # Look for Kinja embeds\n        kinja_embed_urls = KinjaEmbedIE._extract_urls(webpage, url)\n        if kinja_embed_urls:\n            return self.playlist_from_matches(\n                kinja_embed_urls, video_id, video_title)\n\n        # Look for OnionStudios embeds\n        onionstudios_url = OnionStudiosIE._extract_url(webpage)\n        if onionstudios_url:\n            return self.url_result(onionstudios_url)\n\n        # Look for ViewLift embeds\n        viewlift_url = ViewLiftEmbedIE._extract_url(webpage)\n        if viewlift_url:\n            return self.url_result(viewlift_url)\n\n        # Look for JWPlatform embeds\n        jwplatform_urls = JWPlatformIE._extract_urls(webpage)\n        if jwplatform_urls:\n            return self.playlist_from_matches(jwplatform_urls, video_id, video_title, ie=JWPlatformIE.ie_key())\n\n        # Look for Digiteka embeds\n        digiteka_url = DigitekaIE._extract_url(webpage)\n        if digiteka_url:\n            return self.url_result(self._proto_relative_url(digiteka_url), DigitekaIE.ie_key())\n\n        # Look for Arkena embeds\n        arkena_url = ArkenaIE._extract_url(webpage)\n        if arkena_url:\n            return self.url_result(arkena_url, ArkenaIE.ie_key())\n\n        # Look for Piksel embeds\n        piksel_url = PikselIE._extract_url(webpage)\n        if piksel_url:\n            return self.url_result(piksel_url, PikselIE.ie_key())\n\n        # Look for Limelight embeds\n        limelight_urls = LimelightBaseIE._extract_urls(webpage, url)\n        if limelight_urls:\n            return self.playlist_result(\n                limelight_urls, video_id, video_title, video_description)\n\n        # Look for Anvato embeds\n        anvato_urls = AnvatoIE._extract_urls(self, webpage, video_id)\n        if anvato_urls:\n            return self.playlist_result(\n                anvato_urls, video_id, video_title, video_description)\n\n        # Look for AdobeTVVideo embeds\n        mobj = re.search(\n            r'<iframe[^>]+src=[\\'\"]((?:https?:)?//video\\.tv\\.adobe\\.com/v/\\d+[^\"]+)[\\'\"]',\n            webpage)\n        if mobj is not None:\n            return self.url_result(\n                self._proto_relative_url(unescapeHTML(mobj.group(1))),\n                'AdobeTVVideo')\n\n        # Look for Vine embeds\n        mobj = re.search(\n            r'<iframe[^>]+src=[\\'\"]((?:https?:)?//(?:www\\.)?vine\\.co/v/[^/]+/embed/(?:simple|postcard))',\n            webpage)\n        if mobj is not None:\n            return self.url_result(\n                self._proto_relative_url(unescapeHTML(mobj.group(1))), 'Vine')\n\n        # Look for VODPlatform embeds\n        mobj = re.search(\n            r'<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//(?:(?:www\\.)?vod-platform\\.net|embed\\.kwikmotion\\.com)/[eE]mbed/.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(\n                self._proto_relative_url(unescapeHTML(mobj.group('url'))), 'VODPlatform')\n\n        # Look for Mangomolo embeds\n        mobj = re.search(\n            r'''(?x)<iframe[^>]+src=([\"\\'])(?P<url>(?:https?:)?//\n                (?:\n                    admin\\.mangomolo\\.com/analytics/index\\.php/customers/embed|\n                    player\\.mangomolo\\.com/v1\n                )/\n                (?:\n                    video\\?.*?\\bid=(?P<video_id>\\d+)|\n                    (?:index|live)\\?.*?\\bchannelid=(?P<channel_id>(?:[A-Za-z0-9+/=]|%2B|%2F|%3D)+)\n                ).+?)\\1''', webpage)\n        if mobj is not None:\n            info = {\n                '_type': 'url_transparent',\n                'url': self._proto_relative_url(unescapeHTML(mobj.group('url'))),\n                'title': video_title,\n                'description': video_description,\n                'thumbnail': video_thumbnail,\n                'uploader': video_uploader,\n            }\n            video_id = mobj.group('video_id')\n            if video_id:\n                info.update({\n                    'ie_key': 'MangomoloVideo',\n                    'id': video_id,\n                })\n            else:\n                info.update({\n                    'ie_key': 'MangomoloLive',\n                    'id': mobj.group('channel_id'),\n                })\n            return info\n\n        # Look for Instagram embeds\n        instagram_embed_url = InstagramIE._extract_embed_url(webpage)\n        if instagram_embed_url is not None:\n            return self.url_result(\n                self._proto_relative_url(instagram_embed_url), InstagramIE.ie_key())\n\n        # Look for LiveLeak embeds\n        liveleak_urls = LiveLeakIE._extract_urls(webpage)\n        if liveleak_urls:\n            return self.playlist_from_matches(liveleak_urls, video_id, video_title)\n\n        # Look for 3Q SDN embeds\n        threeqsdn_url = ThreeQSDNIE._extract_url(webpage)\n        if threeqsdn_url:\n            return {\n                '_type': 'url_transparent',\n                'ie_key': ThreeQSDNIE.ie_key(),\n                'url': self._proto_relative_url(threeqsdn_url),\n                'title': video_title,\n                'description': video_description,\n                'thumbnail': video_thumbnail,\n                'uploader': video_uploader,\n            }\n\n        # Look for VBOX7 embeds\n        vbox7_url = Vbox7IE._extract_url(webpage)\n        if vbox7_url:\n            return self.url_result(vbox7_url, Vbox7IE.ie_key())\n\n        # Look for DBTV embeds\n        dbtv_urls = DBTVIE._extract_urls(webpage)\n        if dbtv_urls:\n            return self.playlist_from_matches(dbtv_urls, video_id, video_title, ie=DBTVIE.ie_key())\n\n        # Look for Videa embeds\n        videa_urls = VideaIE._extract_urls(webpage)\n        if videa_urls:\n            return self.playlist_from_matches(videa_urls, video_id, video_title, ie=VideaIE.ie_key())\n\n        # Look for 20 minuten embeds\n        twentymin_urls = TwentyMinutenIE._extract_urls(webpage)\n        if twentymin_urls:\n            return self.playlist_from_matches(\n                twentymin_urls, video_id, video_title, ie=TwentyMinutenIE.ie_key())\n\n        # Look for VideoPress embeds\n        videopress_urls = VideoPressIE._extract_urls(webpage)\n        if videopress_urls:\n            return self.playlist_from_matches(\n                videopress_urls, video_id, video_title, ie=VideoPressIE.ie_key())\n\n        # Look for Rutube embeds\n        rutube_urls = RutubeIE._extract_urls(webpage)\n        if rutube_urls:\n            return self.playlist_from_matches(\n                rutube_urls, video_id, video_title, ie=RutubeIE.ie_key())\n\n        # Look for WashingtonPost embeds\n        wapo_urls = WashingtonPostIE._extract_urls(webpage)\n        if wapo_urls:\n            return self.playlist_from_matches(\n                wapo_urls, video_id, video_title, ie=WashingtonPostIE.ie_key())\n\n        # Look for Mediaset embeds\n        mediaset_urls = MediasetIE._extract_urls(self, webpage)\n        if mediaset_urls:\n            return self.playlist_from_matches(\n                mediaset_urls, video_id, video_title, ie=MediasetIE.ie_key())\n\n        # Look for JOJ.sk embeds\n        joj_urls = JojIE._extract_urls(webpage)\n        if joj_urls:\n            return self.playlist_from_matches(\n                joj_urls, video_id, video_title, ie=JojIE.ie_key())\n\n        # Look for megaphone.fm embeds\n        mpfn_urls = MegaphoneIE._extract_urls(webpage)\n        if mpfn_urls:\n            return self.playlist_from_matches(\n                mpfn_urls, video_id, video_title, ie=MegaphoneIE.ie_key())\n\n        # Look for vzaar embeds\n        vzaar_urls = VzaarIE._extract_urls(webpage)\n        if vzaar_urls:\n            return self.playlist_from_matches(\n                vzaar_urls, video_id, video_title, ie=VzaarIE.ie_key())\n\n        channel9_urls = Channel9IE._extract_urls(webpage)\n        if channel9_urls:\n            return self.playlist_from_matches(\n                channel9_urls, video_id, video_title, ie=Channel9IE.ie_key())\n\n        vshare_urls = VShareIE._extract_urls(webpage)\n        if vshare_urls:\n            return self.playlist_from_matches(\n                vshare_urls, video_id, video_title, ie=VShareIE.ie_key())\n\n        # Look for Mediasite embeds\n        mediasite_urls = MediasiteIE._extract_urls(webpage)\n        if mediasite_urls:\n            entries = [\n                self.url_result(smuggle_url(\n                    compat_urlparse.urljoin(url, mediasite_url),\n                    {'UrlReferrer': url}), ie=MediasiteIE.ie_key())\n                for mediasite_url in mediasite_urls]\n            return self.playlist_result(entries, video_id, video_title)\n\n        springboardplatform_urls = SpringboardPlatformIE._extract_urls(webpage)\n        if springboardplatform_urls:\n            return self.playlist_from_matches(\n                springboardplatform_urls, video_id, video_title,\n                ie=SpringboardPlatformIE.ie_key())\n\n        yapfiles_urls = YapFilesIE._extract_urls(webpage)\n        if yapfiles_urls:\n            return self.playlist_from_matches(\n                yapfiles_urls, video_id, video_title, ie=YapFilesIE.ie_key())\n\n        vice_urls = ViceIE._extract_urls(webpage)\n        if vice_urls:\n            return self.playlist_from_matches(\n                vice_urls, video_id, video_title, ie=ViceIE.ie_key())\n\n        xfileshare_urls = XFileShareIE._extract_urls(webpage)\n        if xfileshare_urls:\n            return self.playlist_from_matches(\n                xfileshare_urls, video_id, video_title, ie=XFileShareIE.ie_key())\n\n        cloudflarestream_urls = CloudflareStreamIE._extract_urls(webpage)\n        if cloudflarestream_urls:\n            return self.playlist_from_matches(\n                cloudflarestream_urls, video_id, video_title, ie=CloudflareStreamIE.ie_key())\n\n        peertube_urls = PeerTubeIE._extract_urls(webpage, url)\n        if peertube_urls:\n            return self.playlist_from_matches(\n                peertube_urls, video_id, video_title, ie=PeerTubeIE.ie_key())\n\n        indavideo_urls = IndavideoEmbedIE._extract_urls(webpage)\n        if indavideo_urls:\n            return self.playlist_from_matches(\n                indavideo_urls, video_id, video_title, ie=IndavideoEmbedIE.ie_key())\n\n        apa_urls = APAIE._extract_urls(webpage)\n        if apa_urls:\n            return self.playlist_from_matches(\n                apa_urls, video_id, video_title, ie=APAIE.ie_key())\n\n        foxnews_urls = FoxNewsIE._extract_urls(webpage)\n        if foxnews_urls:\n            return self.playlist_from_matches(\n                foxnews_urls, video_id, video_title, ie=FoxNewsIE.ie_key())\n\n        sharevideos_urls = [sharevideos_mobj.group('url') for sharevideos_mobj in re.finditer(\n            r'<iframe[^>]+?\\bsrc\\s*=\\s*([\"\\'])(?P<url>(?:https?:)?//embed\\.share-videos\\.se/auto/embed/\\d+\\?.*?\\buid=\\d+.*?)\\1',\n            webpage)]\n        if sharevideos_urls:\n            return self.playlist_from_matches(\n                sharevideos_urls, video_id, video_title)\n\n        viqeo_urls = ViqeoIE._extract_urls(webpage)\n        if viqeo_urls:\n            return self.playlist_from_matches(\n                viqeo_urls, video_id, video_title, ie=ViqeoIE.ie_key())\n\n        expressen_urls = ExpressenIE._extract_urls(webpage)\n        if expressen_urls:\n            return self.playlist_from_matches(\n                expressen_urls, video_id, video_title, ie=ExpressenIE.ie_key())\n\n        zype_urls = ZypeIE._extract_urls(webpage)\n        if zype_urls:\n            return self.playlist_from_matches(\n                zype_urls, video_id, video_title, ie=ZypeIE.ie_key())\n\n        # Look for HTML5 media\n        entries = self._parse_html5_media_entries(url, webpage, video_id, m3u8_id='hls')\n        if entries:\n            if len(entries) == 1:\n                entries[0].update({\n                    'id': video_id,\n                    'title': video_title,\n                })\n            else:\n                for num, entry in enumerate(entries, start=1):\n                    entry.update({\n                        'id': '%s-%s' % (video_id, num),\n                        'title': '%s (%d)' % (video_title, num),\n                    })\n            for entry in entries:\n                self._sort_formats(entry['formats'])\n            return self.playlist_result(entries, video_id, video_title)\n\n        jwplayer_data = self._find_jwplayer_data(\n            webpage, video_id, transform_source=js_to_json)\n        if jwplayer_data:\n            try:\n                info = self._parse_jwplayer_data(\n                    jwplayer_data, video_id, require_title=False, base_url=url)\n                return merge_dicts(info, info_dict)\n            except ExtractorError:\n                # See https://github.com/ytdl-org/youtube-dl/pull/16735\n                pass\n\n        # Video.js embed\n        mobj = re.search(\n            r'(?s)\\bvideojs\\s*\\(.+?\\.src\\s*\\(\\s*((?:\\[.+?\\]|{.+?}))\\s*\\)\\s*;',\n            webpage)\n        if mobj is not None:\n            sources = self._parse_json(\n                mobj.group(1), video_id, transform_source=js_to_json,\n                fatal=False) or []\n            if not isinstance(sources, list):\n                sources = [sources]\n            formats = []\n            for source in sources:\n                src = source.get('src')\n                if not src or not isinstance(src, compat_str):\n                    continue\n                src = compat_urlparse.urljoin(url, src)\n                src_type = source.get('type')\n                if isinstance(src_type, compat_str):\n                    src_type = src_type.lower()\n                ext = determine_ext(src).lower()\n                if src_type == 'video/youtube':\n                    return self.url_result(src, YoutubeIE.ie_key())\n                if src_type == 'application/dash+xml' or ext == 'mpd':\n                    formats.extend(self._extract_mpd_formats(\n                        src, video_id, mpd_id='dash', fatal=False))\n                elif src_type == 'application/x-mpegurl' or ext == 'm3u8':\n                    formats.extend(self._extract_m3u8_formats(\n                        src, video_id, 'mp4', entry_protocol='m3u8_native',\n                        m3u8_id='hls', fatal=False))\n                else:\n                    formats.append({\n                        'url': src,\n                        'ext': (mimetype2ext(src_type)\n                                or ext if ext in KNOWN_EXTENSIONS else 'mp4'),\n                    })\n            if formats:\n                self._sort_formats(formats)\n                info_dict['formats'] = formats\n                return info_dict\n\n        # Looking for http://schema.org/VideoObject\n        json_ld = self._search_json_ld(\n            webpage, video_id, default={}, expected_type='VideoObject')\n        if json_ld.get('url'):\n            return merge_dicts(json_ld, info_dict)\n\n        def check_video(vurl):\n            if YoutubeIE.suitable(vurl):\n                return True\n            if RtmpIE.suitable(vurl):\n                return True\n            vpath = compat_urlparse.urlparse(vurl).path\n            vext = determine_ext(vpath)\n            return '.' in vpath and vext not in ('swf', 'png', 'jpg', 'srt', 'sbv', 'sub', 'vtt', 'ttml', 'js', 'xml')\n\n        def filter_video(urls):\n            return list(filter(check_video, urls))\n\n        # Start with something easy: JW Player in SWFObject\n        found = filter_video(re.findall(r'flashvars: [\\'\"](?:.*&)?file=(http[^\\'\"&]*)', webpage))\n        if not found:\n            # Look for gorilla-vid style embedding\n            found = filter_video(re.findall(r'''(?sx)\n                (?:\n                    jw_plugins|\n                    JWPlayerOptions|\n                    jwplayer\\s*\\(\\s*[\"'][^'\"]+[\"']\\s*\\)\\s*\\.setup\n                )\n                .*?\n                ['\"]?file['\"]?\\s*:\\s*[\"\\'](.*?)[\"\\']''', webpage))\n        if not found:\n            # Broaden the search a little bit\n            found = filter_video(re.findall(r'[^A-Za-z0-9]?(?:file|source)=(http[^\\'\"&]*)', webpage))\n        if not found:\n            # Broaden the findall a little bit: JWPlayer JS loader\n            found = filter_video(re.findall(\n                r'[^A-Za-z0-9]?(?:file|video_url)[\"\\']?:\\s*[\"\\'](http(?![^\\'\"]+\\.[0-9]+[\\'\"])[^\\'\"]+)[\"\\']', webpage))\n        if not found:\n            # Flow player\n            found = filter_video(re.findall(r'''(?xs)\n                flowplayer\\(\"[^\"]+\",\\s*\n                    \\{[^}]+?\\}\\s*,\n                    \\s*\\{[^}]+? [\"']?clip[\"']?\\s*:\\s*\\{\\s*\n                        [\"']?url[\"']?\\s*:\\s*[\"']([^\"']+)[\"']\n            ''', webpage))\n        if not found:\n            # Cinerama player\n            found = re.findall(\n                r\"cinerama\\.embedPlayer\\(\\s*\\'[^']+\\',\\s*'([^']+)'\", webpage)\n        if not found:\n            # Try to find twitter cards info\n            # twitter:player:stream should be checked before twitter:player since\n            # it is expected to contain a raw stream (see\n            # https://dev.twitter.com/cards/types/player#On_twitter.com_via_desktop_browser)\n            found = filter_video(re.findall(\n                r'<meta (?:property|name)=\"twitter:player:stream\" (?:content|value)=\"(.+?)\"', webpage))\n        if not found:\n            # We look for Open Graph info:\n            # We have to match any number spaces between elements, some sites try to align them (eg.: statigr.am)\n            m_video_type = re.findall(r'<meta.*?property=\"og:video:type\".*?content=\"video/(.*?)\"', webpage)\n            # We only look in og:video if the MIME type is a video, don't try if it's a Flash player:\n            if m_video_type is not None:\n                found = filter_video(re.findall(r'<meta.*?property=\"og:video\".*?content=\"(.*?)\"', webpage))\n        if not found:\n            REDIRECT_REGEX = r'[0-9]{,2};\\s*(?:URL|url)=\\'?([^\\'\"]+)'\n            found = re.search(\n                r'(?i)<meta\\s+(?=(?:[a-z-]+=\"[^\"]+\"\\s+)*http-equiv=\"refresh\")'\n                r'(?:[a-z-]+=\"[^\"]+\"\\s+)*?content=\"%s' % REDIRECT_REGEX,\n                webpage)\n            if not found:\n                # Look also in Refresh HTTP header\n                refresh_header = head_response.headers.get('Refresh')\n                if refresh_header:\n                    # In python 2 response HTTP headers are bytestrings\n                    if sys.version_info < (3, 0) and isinstance(refresh_header, str):\n                        refresh_header = refresh_header.decode('iso-8859-1')\n                    found = re.search(REDIRECT_REGEX, refresh_header)\n            if found:\n                new_url = compat_urlparse.urljoin(url, unescapeHTML(found.group(1)))\n                if new_url != url:\n                    self.report_following_redirect(new_url)\n                    return {\n                        '_type': 'url',\n                        'url': new_url,\n                    }\n                else:\n                    found = None\n\n        if not found:\n            # twitter:player is a https URL to iframe player that may or may not\n            # be supported by youtube-dl thus this is checked the very last (see\n            # https://dev.twitter.com/cards/types/player#On_twitter.com_via_desktop_browser)\n            embed_url = self._html_search_meta('twitter:player', webpage, default=None)\n            if embed_url and embed_url != url:\n                return self.url_result(embed_url)\n\n        if not found:\n            raise UnsupportedError(url)\n\n        entries = []\n        for video_url in orderedSet(found):\n            video_url = unescapeHTML(video_url)\n            video_url = video_url.replace('\\\\/', '/')\n            video_url = compat_urlparse.urljoin(url, video_url)\n            video_id = compat_urllib_parse_unquote(os.path.basename(video_url))\n\n            # Sometimes, jwplayer extraction will result in a YouTube URL\n            if YoutubeIE.suitable(video_url):\n                entries.append(self.url_result(video_url, 'Youtube'))\n                continue\n\n            # here's a fun little line of code for you:\n            video_id = os.path.splitext(video_id)[0]\n\n            entry_info_dict = {\n                'id': video_id,\n                'uploader': video_uploader,\n                'title': video_title,\n                'age_limit': age_limit,\n            }\n\n            if RtmpIE.suitable(video_url):\n                entry_info_dict.update({\n                    '_type': 'url_transparent',\n                    'ie_key': RtmpIE.ie_key(),\n                    'url': video_url,\n                })\n                entries.append(entry_info_dict)\n                continue\n\n            ext = determine_ext(video_url)\n            if ext == 'smil':\n                entry_info_dict['formats'] = self._extract_smil_formats(video_url, video_id)\n            elif ext == 'xspf':\n                return self.playlist_result(self._extract_xspf_playlist(video_url, video_id), video_id)\n            elif ext == 'm3u8':\n                entry_info_dict['formats'] = self._extract_m3u8_formats(video_url, video_id, ext='mp4')\n            elif ext == 'mpd':\n                entry_info_dict['formats'] = self._extract_mpd_formats(video_url, video_id)\n            elif ext == 'f4m':\n                entry_info_dict['formats'] = self._extract_f4m_formats(video_url, video_id)\n            elif re.search(r'(?i)\\.(?:ism|smil)/manifest', video_url) and video_url != url:\n                # Just matching .ism/manifest is not enough to be reliably sure\n                # whether it's actually an ISM manifest or some other streaming\n                # manifest since there are various streaming URL formats\n                # possible (see [1]) as well as some other shenanigans like\n                # .smil/manifest URLs that actually serve an ISM (see [2]) and\n                # so on.\n                # Thus the most reasonable way to solve this is to delegate\n                # to generic extractor in order to look into the contents of\n                # the manifest itself.\n                # 1. https://azure.microsoft.com/en-us/documentation/articles/media-services-deliver-content-overview/#streaming-url-formats\n                # 2. https://svs.itworkscdn.net/lbcivod/smil:itwfcdn/lbci/170976.smil/Manifest\n                entry_info_dict = self.url_result(\n                    smuggle_url(video_url, {'to_generic': True}),\n                    GenericIE.ie_key())\n            else:\n                entry_info_dict['url'] = video_url\n\n            if entry_info_dict.get('formats'):\n                self._sort_formats(entry_info_dict['formats'])\n\n            entries.append(entry_info_dict)\n\n        if len(entries) == 1:\n            return entries[0]\n        else:\n            for num, e in enumerate(entries, start=1):\n                # 'url' results don't have a title\n                if e.get('title') is not None:\n                    e['title'] = '%s (%d)' % (e['title'], num)\n            return {\n                '_type': 'playlist',\n                'entries': entries,\n            }",
        "begin_line": 2248,
        "end_line": 3459,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007880220646178094,
            "pseudo_dstar_susp": 0.0008271298593879239,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0008271298593879239,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE.check_video#3291",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE.check_video(vurl)",
        "snippet": "        def check_video(vurl):\n            if YoutubeIE.suitable(vurl):\n                return True\n            if RtmpIE.suitable(vurl):\n                return True\n            vpath = compat_urlparse.urlparse(vurl).path\n            vext = determine_ext(vpath)\n            return '.' in vpath and vext not in ('swf', 'png', 'jpg', 'srt', 'sbv', 'sub', 'vtt', 'ttml', 'js', 'xml')",
        "begin_line": 3291,
        "end_line": 3298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005767012687427913,
            "pseudo_dstar_susp": 0.0005817335660267597,
            "pseudo_tarantula_susp": 0.00048192771084337347,
            "pseudo_op2_susp": 0.0005817335660267597,
            "pseudo_barinel_susp": 0.00048192771084337347
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE.filter_video#3300",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE.filter_video(urls)",
        "snippet": "        def filter_video(urls):\n            return list(filter(check_video, urls))",
        "begin_line": 3300,
        "end_line": 3301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005767012687427913,
            "pseudo_dstar_susp": 0.0005817335660267597,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005817335660267597,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.kickstarter.KickStarterIE._real_extract#42",
        "src_path": "youtube_dl/extractor/kickstarter.py",
        "class_name": "youtube_dl.extractor.kickstarter.KickStarterIE",
        "signature": "youtube_dl.extractor.kickstarter.KickStarterIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        webpage = self._download_webpage(url, video_id)\n\n        title = self._html_search_regex(\n            r'<title>\\s*(.*?)(?:\\s*&mdash;\\s*Kickstarter)?\\s*</title>',\n            webpage, 'title')\n        video_url = self._search_regex(\n            r'data-video-url=\"(.*?)\"',\n            webpage, 'video URL', default=None)\n        if video_url is None:  # No native kickstarter, look for embedded videos\n            return {\n                '_type': 'url_transparent',\n                'ie_key': 'Generic',\n                'url': smuggle_url(url, {'to_generic': True}),\n                'title': title,\n            }\n\n        thumbnail = self._og_search_thumbnail(webpage, default=None)\n        if thumbnail is None:\n            thumbnail = self._html_search_regex(\n                r'<img[^>]+class=\"[^\"]+\\s*poster\\s*[^\"]+\"[^>]+src=\"([^\"]+)\"',\n                webpage, 'thumbnail image', fatal=False)\n        return {\n            'id': video_id,\n            'url': video_url,\n            'title': title,\n            'description': self._og_search_description(webpage, default=None),\n            'thumbnail': thumbnail,\n        }",
        "begin_line": 42,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011367511651699443,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.__init__.gen_extractor_classes#19",
        "src_path": "youtube_dl/extractor/__init__.py",
        "class_name": "youtube_dl.extractor.__init__",
        "signature": "youtube_dl.extractor.__init__.gen_extractor_classes()",
        "snippet": "def gen_extractor_classes():\n    \"\"\" Return a list of supported extractors.\n    The order does matter; the first extractor matched is the one handling the URL.\n    \"\"\"\n    return _ALL_CLASSES",
        "begin_line": 19,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.03333333333333333,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.0010204081632653062,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.0010204081632653062
        }
    },
    {
        "name": "youtube_dl.extractor.__init__.gen_extractors#26",
        "src_path": "youtube_dl/extractor/__init__.py",
        "class_name": "youtube_dl.extractor.__init__",
        "signature": "youtube_dl.extractor.__init__.gen_extractors()",
        "snippet": "def gen_extractors():\n    \"\"\" Return a list of an instance of every supported extractor.\n    The order does matter; the first extractor matched is the one handling the URL.\n    \"\"\"\n    return [klass() for klass in gen_extractor_classes()]",
        "begin_line": 26,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011656370206317752,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.__init__.get_info_extractor#44",
        "src_path": "youtube_dl/extractor/__init__.py",
        "class_name": "youtube_dl.extractor.__init__",
        "signature": "youtube_dl.extractor.__init__.get_info_extractor(ie_name)",
        "snippet": "def get_info_extractor(ie_name):\n    \"\"\"Returns the info extractor class with the given ie_name\"\"\"\n    return globals()[ie_name + 'IE']",
        "begin_line": 44,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.0008547008547008547,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.0008547008547008547
        }
    },
    {
        "name": "youtube_dl.extractor.teachable.TeachableCourseIE.suitable#246",
        "src_path": "youtube_dl/extractor/teachable.py",
        "class_name": "youtube_dl.extractor.teachable.TeachableCourseIE",
        "signature": "youtube_dl.extractor.teachable.TeachableCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if TeachableIE.suitable(url) else super(\n            TeachableCourseIE, cls).suitable(url)",
        "begin_line": 246,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00033534540576794097,
            "pseudo_dstar_susp": 0.0003971405877680699,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.0003971405877680699,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.picarto.PicartoIE.suitable#33",
        "src_path": "youtube_dl/extractor/picarto.py",
        "class_name": "youtube_dl.extractor.picarto.PicartoIE",
        "signature": "youtube_dl.extractor.picarto.PicartoIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if PicartoVodIE.suitable(url) else super(PicartoIE, cls).suitable(url)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003104625892579944,
            "pseudo_dstar_susp": 0.00036036036036036037,
            "pseudo_tarantula_susp": 0.0002864508736751647,
            "pseudo_op2_susp": 0.00036036036036036037,
            "pseudo_barinel_susp": 0.0002864508736751647
        }
    },
    {
        "name": "youtube_dl.cache.Cache.__init__#19",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.__init__(self, ydl)",
        "snippet": "    def __init__(self, ydl):\n        self._ydl = ydl",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.5,
            "pseudo_dstar_susp": 0.25,
            "pseudo_tarantula_susp": 0.0010845986984815619,
            "pseudo_op2_susp": 0.25,
            "pseudo_barinel_susp": 0.0010845986984815619
        }
    },
    {
        "name": "youtube_dl.cache.Cache._get_root_dir#22",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache._get_root_dir(self)",
        "snippet": "    def _get_root_dir(self):\n        res = self._ydl.params.get('cachedir')\n        if res is None:\n            cache_root = compat_getenv('XDG_CACHE_HOME', '~/.cache')\n            res = os.path.join(cache_root, 'youtube-dl')\n        return expand_path(res)",
        "begin_line": 22,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.cache.Cache._get_cache_fn#29",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache._get_cache_fn(self, section, key, dtype)",
        "snippet": "    def _get_cache_fn(self, section, key, dtype):\n        assert re.match(r'^[a-zA-Z0-9_.-]+$', section), \\\n            'invalid section %r' % section\n        assert re.match(r'^[a-zA-Z0-9_.-]+$', key), 'invalid key %r' % key\n        return os.path.join(\n            self._get_root_dir(), section, '%s.%s' % (key, dtype))",
        "begin_line": 29,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.cache.Cache.enabled#37",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.enabled(self)",
        "snippet": "    def enabled(self):\n        return self._ydl.params.get('cachedir') is not False",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.cache.Cache.store#40",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.store(self, section, key, data, dtype='json')",
        "snippet": "    def store(self, section, key, data, dtype='json'):\n        assert dtype in ('json',)\n\n        if not self.enabled:\n            return\n\n        fn = self._get_cache_fn(section, key, dtype)\n        try:\n            try:\n                os.makedirs(os.path.dirname(fn))\n            except OSError as ose:\n                if ose.errno != errno.EEXIST:\n                    raise\n            write_json_file(data, fn)\n        except Exception:\n            tb = traceback.format_exc()\n            self._ydl.report_warning(\n                'Writing cache to %r failed: %s' % (fn, tb))",
        "begin_line": 40,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.cache.Cache.load#59",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.load(self, section, key, dtype='json', default=None)",
        "snippet": "    def load(self, section, key, dtype='json', default=None):\n        assert dtype in ('json',)\n\n        if not self.enabled:\n            return default\n\n        cache_fn = self._get_cache_fn(section, key, dtype)\n        try:\n            try:\n                with io.open(cache_fn, 'r', encoding='utf-8') as cachef:\n                    return json.load(cachef)\n            except ValueError:\n                try:\n                    file_size = os.path.getsize(cache_fn)\n                except (OSError, IOError) as oe:\n                    file_size = str(oe)\n                self._ydl.report_warning(\n                    'Cache retrieval from %s failed (%s)' % (cache_fn, file_size))\n        except IOError:\n            pass  # No cache available\n\n        return default",
        "begin_line": 59,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.cache.Cache.remove#82",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.remove(self)",
        "snippet": "    def remove(self):\n        if not self.enabled:\n            self._ydl.to_screen('Cache is disabled (Did you combine --no-cache-dir and --rm-cache-dir?)')\n            return\n\n        cachedir = self._get_root_dir()\n        if not any((term in cachedir) for term in ('cache', 'tmp')):\n            raise Exception('Not removing directory %s - this does not look like a cache dir' % cachedir)\n\n        self._ydl.to_screen(\n            'Removing cache dir %s .' % cachedir, skip_eol=True)\n        if os.path.exists(cachedir):\n            self._ydl.to_screen('.', skip_eol=True)\n            shutil.rmtree(cachedir)\n        self._ydl.to_screen('.')",
        "begin_line": 82,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.microsoftvirtualacademy.MicrosoftVirtualAcademyCourseIE.suitable#150",
        "src_path": "youtube_dl/extractor/microsoftvirtualacademy.py",
        "class_name": "youtube_dl.extractor.microsoftvirtualacademy.MicrosoftVirtualAcademyCourseIE",
        "signature": "youtube_dl.extractor.microsoftvirtualacademy.MicrosoftVirtualAcademyCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if MicrosoftVirtualAcademyIE.suitable(url) else super(\n            MicrosoftVirtualAcademyCourseIE, cls).suitable(url)",
        "begin_line": 150,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000281928390188892,
            "pseudo_dstar_susp": 0.00033025099075297226,
            "pseudo_tarantula_susp": 0.00026961445133459155,
            "pseudo_op2_susp": 0.00033025099075297226,
            "pseudo_barinel_susp": 0.00026961445133459155
        }
    },
    {
        "name": "youtube_dl.extractor.anvato.AnvatoIE.__init__#164",
        "src_path": "youtube_dl/extractor/anvato.py",
        "class_name": "youtube_dl.extractor.anvato.AnvatoIE",
        "signature": "youtube_dl.extractor.anvato.AnvatoIE.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(AnvatoIE, self).__init__(*args, **kwargs)\n        self.__server_time = None",
        "begin_line": 164,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011739845034045551,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.anvato.AnvatoIE._real_extract#303",
        "src_path": "youtube_dl/extractor/anvato.py",
        "class_name": "youtube_dl.extractor.anvato.AnvatoIE",
        "signature": "youtube_dl.extractor.anvato.AnvatoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n        self._initialize_geo_bypass({\n            'countries': smuggled_data.get('geo_countries'),\n        })\n\n        mobj = re.match(self._VALID_URL, url)\n        access_key, video_id = mobj.group('access_key_or_mcp', 'id')\n        if access_key not in self._ANVACK_TABLE:\n            access_key = self._MCP_TO_ACCESS_KEY_TABLE.get(\n                access_key) or access_key\n        return self._get_anvato_videos(access_key, video_id)",
        "begin_line": 303,
        "end_line": 314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00030553009471432935,
            "pseudo_dstar_susp": 0.00030646644192460924,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00030646644192460924,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.facebook.FacebookIE._extract_urls#228",
        "src_path": "youtube_dl/extractor/facebook.py",
        "class_name": "youtube_dl.extractor.facebook.FacebookIE",
        "signature": "youtube_dl.extractor.facebook.FacebookIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        urls = []\n        for mobj in re.finditer(\n                r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://www\\.facebook\\.com/(?:video/embed|plugins/video\\.php).+?)\\1',\n                webpage):\n            urls.append(mobj.group('url'))\n        # Facebook API embed\n        # see https://developers.facebook.com/docs/plugins/embedded-video-player\n        for mobj in re.finditer(r'''(?x)<div[^>]+\n                class=(?P<q1>[\\'\"])[^\\'\"]*\\bfb-(?:video|post)\\b[^\\'\"]*(?P=q1)[^>]+\n                data-href=(?P<q2>[\\'\"])(?P<url>(?:https?:)?//(?:www\\.)?facebook.com/.+?)(?P=q2)''', webpage):\n            urls.append(mobj.group('url'))\n        return urls",
        "begin_line": 228,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.facebook.FacebookIE._extract_from_url#308",
        "src_path": "youtube_dl/extractor/facebook.py",
        "class_name": "youtube_dl.extractor.facebook.FacebookIE",
        "signature": "youtube_dl.extractor.facebook.FacebookIE._extract_from_url(self, url, video_id, fatal_if_no_video=True)",
        "snippet": "    def _extract_from_url(self, url, video_id, fatal_if_no_video=True):\n        req = sanitized_Request(url)\n        req.add_header('User-Agent', self._CHROME_USER_AGENT)\n        webpage = self._download_webpage(req, video_id)\n\n        video_data = None\n\n        def extract_video_data(instances):\n            for item in instances:\n                if item[1][0] == 'VideoConfig':\n                    video_item = item[2][0]\n                    if video_item.get('video_id'):\n                        return video_item['videoData']\n\n        server_js_data = self._parse_json(self._search_regex(\n            r'handleServerJS\\(({.+})(?:\\);|,\")', webpage,\n            'server js data', default='{}'), video_id, fatal=False)\n\n        if server_js_data:\n            video_data = extract_video_data(server_js_data.get('instances', []))\n\n        def extract_from_jsmods_instances(js_data):\n            if js_data:\n                return extract_video_data(try_get(\n                    js_data, lambda x: x['jsmods']['instances'], list) or [])\n\n        if not video_data:\n            server_js_data = self._parse_json(\n                self._search_regex(\n                    r'bigPipe\\.onPageletArrive\\(({.+?})\\)\\s*;\\s*}\\s*\\)\\s*,\\s*[\"\\']onPageletArrive\\s+(?:pagelet_group_mall|permalink_video_pagelet|hyperfeed_story_id_\\d+)',\n                    webpage, 'js data', default='{}'),\n                video_id, transform_source=js_to_json, fatal=False)\n            video_data = extract_from_jsmods_instances(server_js_data)\n\n        if not video_data:\n            if not fatal_if_no_video:\n                return webpage, False\n            m_msg = re.search(r'class=\"[^\"]*uiInterstitialContent[^\"]*\"><div>(.*?)</div>', webpage)\n            if m_msg is not None:\n                raise ExtractorError(\n                    'The video is not available, Facebook said: \"%s\"' % m_msg.group(1),\n                    expected=True)\n            elif '>You must log in to continue' in webpage:\n                self.raise_login_required()\n\n            # Video info not in first request, do a secondary request using\n            # tahoe player specific URL\n            tahoe_data = self._download_webpage(\n                self._VIDEO_PAGE_TAHOE_TEMPLATE % video_id, video_id,\n                data=urlencode_postdata({\n                    '__a': 1,\n                    '__pc': self._search_regex(\n                        r'pkg_cohort[\"\\']\\s*:\\s*[\"\\'](.+?)[\"\\']', webpage,\n                        'pkg cohort', default='PHASED:DEFAULT'),\n                    '__rev': self._search_regex(\n                        r'client_revision[\"\\']\\s*:\\s*(\\d+),', webpage,\n                        'client revision', default='3944515'),\n                    'fb_dtsg': self._search_regex(\n                        r'\"DTSGInitialData\"\\s*,\\s*\\[\\]\\s*,\\s*{\\s*\"token\"\\s*:\\s*\"([^\"]+)\"',\n                        webpage, 'dtsg token', default=''),\n                }),\n                headers={\n                    'Content-Type': 'application/x-www-form-urlencoded',\n                })\n            tahoe_js_data = self._parse_json(\n                self._search_regex(\n                    r'for\\s+\\(\\s*;\\s*;\\s*\\)\\s*;(.+)', tahoe_data,\n                    'tahoe js data', default='{}'),\n                video_id, fatal=False)\n            video_data = extract_from_jsmods_instances(tahoe_js_data)\n\n        if not video_data:\n            raise ExtractorError('Cannot parse data')\n\n        subtitles = {}\n        formats = []\n        for f in video_data:\n            format_id = f['stream_type']\n            if f and isinstance(f, dict):\n                f = [f]\n            if not f or not isinstance(f, list):\n                continue\n            for quality in ('sd', 'hd'):\n                for src_type in ('src', 'src_no_ratelimit'):\n                    src = f[0].get('%s_%s' % (quality, src_type))\n                    if src:\n                        preference = -10 if format_id == 'progressive' else 0\n                        if quality == 'hd':\n                            preference += 5\n                        formats.append({\n                            'format_id': '%s_%s_%s' % (format_id, quality, src_type),\n                            'url': src,\n                            'preference': preference,\n                        })\n            dash_manifest = f[0].get('dash_manifest')\n            if dash_manifest:\n                formats.extend(self._parse_mpd_formats(\n                    compat_etree_fromstring(compat_urllib_parse_unquote_plus(dash_manifest))))\n            subtitles_src = f[0].get('subtitles_src')\n            if subtitles_src:\n                subtitles.setdefault('en', []).append({'url': subtitles_src})\n        if not formats:\n            raise ExtractorError('Cannot find video formats')\n\n        # Downloads with browser's User-Agent are rate limited. Working around\n        # with non-browser User-Agent.\n        for f in formats:\n            f.setdefault('http_headers', {})['User-Agent'] = 'facebookexternalhit/1.1'\n\n        self._sort_formats(formats)\n\n        video_title = self._html_search_regex(\n            r'<h2\\s+[^>]*class=\"uiHeaderTitle\"[^>]*>([^<]*)</h2>', webpage,\n            'title', default=None)\n        if not video_title:\n            video_title = self._html_search_regex(\n                r'(?s)<span class=\"fbPhotosPhotoCaption\".*?id=\"fbPhotoPageCaption\"><span class=\"hasCaption\">(.*?)</span>',\n                webpage, 'alternative title', default=None)\n        if not video_title:\n            video_title = self._html_search_meta(\n                'description', webpage, 'title', default=None)\n        if video_title:\n            video_title = limit_length(video_title, 80)\n        else:\n            video_title = 'Facebook video #%s' % video_id\n        uploader = clean_html(get_element_by_id(\n            'fbPhotoPageAuthorName', webpage)) or self._search_regex(\n            r'ownerName\\s*:\\s*\"([^\"]+)\"', webpage, 'uploader',\n            default=None) or self._og_search_title(webpage, fatal=False)\n        timestamp = int_or_none(self._search_regex(\n            r'<abbr[^>]+data-utime=[\"\\'](\\d+)', webpage,\n            'timestamp', default=None))\n        thumbnail = self._html_search_meta(['og:image', 'twitter:image'], webpage)\n\n        view_count = parse_count(self._search_regex(\n            r'\\bviewCount\\s*:\\s*[\"\\']([\\d,.]+)', webpage, 'view count',\n            default=None))\n\n        info_dict = {\n            'id': video_id,\n            'title': video_title,\n            'formats': formats,\n            'uploader': uploader,\n            'timestamp': timestamp,\n            'thumbnail': thumbnail,\n            'view_count': view_count,\n            'subtitles': subtitles,\n        }\n\n        return webpage, info_dict",
        "begin_line": 308,
        "end_line": 457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004363001745200698,
            "pseudo_dstar_susp": 0.000444247001332741,
            "pseudo_tarantula_susp": 0.0006377551020408163,
            "pseudo_op2_susp": 0.000444247001332741,
            "pseudo_barinel_susp": 0.0006377551020408163
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDMediathekBaseIE._parse_media_info#34",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDMediathekBaseIE",
        "signature": "youtube_dl.extractor.ard.ARDMediathekBaseIE._parse_media_info(self, media_info, video_id, fsk)",
        "snippet": "    def _parse_media_info(self, media_info, video_id, fsk):\n        formats = self._extract_formats(media_info, video_id)\n\n        if not formats:\n            if fsk:\n                raise ExtractorError(\n                    'This video is only available after 20:00', expected=True)\n            elif media_info.get('_geoblocked'):\n                self.raise_geo_restricted(\n                    'This video is not available due to geoblocking',\n                    countries=self._GEO_COUNTRIES)\n\n        self._sort_formats(formats)\n\n        subtitles = {}\n        subtitle_url = media_info.get('_subtitleUrl')\n        if subtitle_url:\n            subtitles['de'] = [{\n                'ext': 'ttml',\n                'url': subtitle_url,\n            }]\n\n        return {\n            'id': video_id,\n            'duration': int_or_none(media_info.get('_duration')),\n            'thumbnail': media_info.get('_previewImage'),\n            'is_live': media_info.get('_isLive') is True,\n            'formats': formats,\n            'subtitles': subtitles,\n        }",
        "begin_line": 34,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021691973969631237,
            "pseudo_dstar_susp": 0.002061855670103093,
            "pseudo_tarantula_susp": 0.0006071645415907711,
            "pseudo_op2_susp": 0.002061855670103093,
            "pseudo_barinel_susp": 0.0006071645415907711
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDMediathekBaseIE._extract_formats#65",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDMediathekBaseIE",
        "signature": "youtube_dl.extractor.ard.ARDMediathekBaseIE._extract_formats(self, media_info, video_id)",
        "snippet": "    def _extract_formats(self, media_info, video_id):\n        type_ = media_info.get('_type')\n        media_array = media_info.get('_mediaArray', [])\n        formats = []\n        for num, media in enumerate(media_array):\n            for stream in media.get('_mediaStreamArray', []):\n                stream_urls = stream.get('_stream')\n                if not stream_urls:\n                    continue\n                if not isinstance(stream_urls, list):\n                    stream_urls = [stream_urls]\n                quality = stream.get('_quality')\n                server = stream.get('_server')\n                for stream_url in stream_urls:\n                    if not url_or_none(stream_url):\n                        continue\n                    ext = determine_ext(stream_url)\n                    if quality != 'auto' and ext in ('f4m', 'm3u8'):\n                        continue\n                    if ext == 'f4m':\n                        formats.extend(self._extract_f4m_formats(\n                            update_url_query(stream_url, {\n                                'hdcore': '3.1.1',\n                                'plugin': 'aasp-3.1.1.69.124'\n                            }), video_id, f4m_id='hds', fatal=False))\n                    elif ext == 'm3u8':\n                        formats.extend(self._extract_m3u8_formats(\n                            stream_url, video_id, 'mp4', 'm3u8_native',\n                            m3u8_id='hls', fatal=False))\n                    else:\n                        if server and server.startswith('rtmp'):\n                            f = {\n                                'url': server,\n                                'play_path': stream_url,\n                                'format_id': 'a%s-rtmp-%s' % (num, quality),\n                            }\n                        else:\n                            f = {\n                                'url': stream_url,\n                                'format_id': 'a%s-%s-%s' % (num, ext, quality)\n                            }\n                        m = re.search(\n                            r'_(?P<width>\\d+)x(?P<height>\\d+)\\.mp4$',\n                            stream_url)\n                        if m:\n                            f.update({\n                                'width': int(m.group('width')),\n                                'height': int(m.group('height')),\n                            })\n                        if type_ == 'audio':\n                            f['vcodec'] = 'none'\n                        formats.append(f)\n        return formats",
        "begin_line": 65,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0018587360594795538,
            "pseudo_dstar_susp": 0.001869158878504673,
            "pseudo_tarantula_susp": 0.0006385696040868455,
            "pseudo_op2_susp": 0.001869158878504673,
            "pseudo_barinel_susp": 0.0006385696040868455
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDMediathekIE.suitable#158",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDMediathekIE",
        "signature": "youtube_dl.extractor.ard.ARDMediathekIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if ARDBetaMediathekIE.suitable(url) else super(ARDMediathekIE, cls).suitable(url)",
        "begin_line": 158,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDMediathekIE._real_extract#161",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDMediathekIE",
        "signature": "youtube_dl.extractor.ard.ARDMediathekIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        # determine video id from url\n        m = re.match(self._VALID_URL, url)\n\n        document_id = None\n\n        numid = re.search(r'documentId=([0-9]+)', url)\n        if numid:\n            document_id = video_id = numid.group(1)\n        else:\n            video_id = m.group('video_id')\n\n        webpage = self._download_webpage(url, video_id)\n\n        ERRORS = (\n            ('>Leider liegt eine St\u00f6rung vor.', 'Video %s is unavailable'),\n            ('>Der gew\u00fcnschte Beitrag ist nicht mehr verf\u00fcgbar.<',\n             'Video %s is no longer available'),\n        )\n\n        for pattern, message in ERRORS:\n            if pattern in webpage:\n                raise ExtractorError(message % video_id, expected=True)\n\n        if re.search(r'[\\?&]rss($|[=&])', url):\n            doc = compat_etree_fromstring(webpage.encode('utf-8'))\n            if doc.tag == 'rss':\n                return GenericIE()._extract_rss(url, video_id, doc)\n\n        title = self._html_search_regex(\n            [r'<h1(?:\\s+class=\"boxTopHeadline\")?>(.*?)</h1>',\n             r'<meta name=\"dcterms\\.title\" content=\"(.*?)\"/>',\n             r'<h4 class=\"headline\">(.*?)</h4>',\n             r'<title[^>]*>(.*?)</title>'],\n            webpage, 'title')\n        description = self._html_search_meta(\n            'dcterms.abstract', webpage, 'description', default=None)\n        if description is None:\n            description = self._html_search_meta(\n                'description', webpage, 'meta description', default=None)\n        if description is None:\n            description = self._html_search_regex(\n                r'<p\\s+class=\"teasertext\">(.+?)</p>',\n                webpage, 'teaser text', default=None)\n\n        # Thumbnail is sometimes not present.\n        # It is in the mobile version, but that seems to use a different URL\n        # structure altogether.\n        thumbnail = self._og_search_thumbnail(webpage, default=None)\n\n        media_streams = re.findall(r'''(?x)\n            mediaCollection\\.addMediaStream\\([0-9]+,\\s*[0-9]+,\\s*\"[^\"]*\",\\s*\n            \"([^\"]+)\"''', webpage)\n\n        if media_streams:\n            QUALITIES = qualities(['lo', 'hi', 'hq'])\n            formats = []\n            for furl in set(media_streams):\n                if furl.endswith('.f4m'):\n                    fid = 'f4m'\n                else:\n                    fid_m = re.match(r'.*\\.([^.]+)\\.[^.]+$', furl)\n                    fid = fid_m.group(1) if fid_m else None\n                formats.append({\n                    'quality': QUALITIES(fid),\n                    'format_id': fid,\n                    'url': furl,\n                })\n            self._sort_formats(formats)\n            info = {\n                'formats': formats,\n            }\n        else:  # request JSON file\n            if not document_id:\n                video_id = self._search_regex(\n                    r'/play/(?:config|media)/(\\d+)', webpage, 'media id')\n            info = self._extract_media_info(\n                'http://www.ardmediathek.de/play/media/%s' % video_id,\n                webpage, video_id)\n\n        info.update({\n            'id': video_id,\n            'title': self._live_title(title) if info.get('is_live') else title,\n            'description': description,\n            'thumbnail': thumbnail,\n        })\n\n        return info",
        "begin_line": 161,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.14285714285714285,
            "pseudo_dstar_susp": 0.125,
            "pseudo_tarantula_susp": 0.0008952551477170994,
            "pseudo_op2_susp": 0.125,
            "pseudo_barinel_susp": 0.0008952551477170994
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDIE._real_extract#274",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDIE",
        "signature": "youtube_dl.extractor.ard.ARDIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        display_id = mobj.group('display_id')\n\n        player_url = mobj.group('mainurl') + '~playerXml.xml'\n        doc = self._download_xml(player_url, display_id)\n        video_node = doc.find('./video')\n        upload_date = unified_strdate(xpath_text(\n            video_node, './broadcastDate'))\n        thumbnail = xpath_text(video_node, './/teaserImage//variant/url')\n\n        formats = []\n        for a in video_node.findall('.//asset'):\n            f = {\n                'format_id': a.attrib['type'],\n                'width': int_or_none(a.find('./frameWidth').text),\n                'height': int_or_none(a.find('./frameHeight').text),\n                'vbr': int_or_none(a.find('./bitrateVideo').text),\n                'abr': int_or_none(a.find('./bitrateAudio').text),\n                'vcodec': a.find('./codecVideo').text,\n                'tbr': int_or_none(a.find('./totalBitrate').text),\n            }\n            if a.find('./serverPrefix').text:\n                f['url'] = a.find('./serverPrefix').text\n                f['playpath'] = a.find('./fileName').text\n            else:\n                f['url'] = a.find('./fileName').text\n            formats.append(f)\n        self._sort_formats(formats)\n\n        return {\n            'id': mobj.group('id'),\n            'formats': formats,\n            'display_id': display_id,\n            'title': video_node.find('./title').text,\n            'duration': parse_duration(video_node.find('./duration').text),\n            'upload_date': upload_date,\n            'thumbnail': thumbnail,\n        }",
        "begin_line": 274,
        "end_line": 312,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDBetaMediathekIE._real_extract#348",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDBetaMediathekIE",
        "signature": "youtube_dl.extractor.ard.ARDBetaMediathekIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('video_id')\n        display_id = mobj.group('display_id')\n        if display_id:\n            display_id = display_id.rstrip('/')\n        if not display_id:\n            display_id = video_id\n\n        player_page = self._download_json(\n            'https://api.ardmediathek.de/public-gateway',\n            display_id, data=json.dumps({\n                'query': '''{\n  playerPage(client:\"%s\", clipId: \"%s\") {\n    blockedByFsk\n    broadcastedOn\n    maturityContentRating\n    mediaCollection {\n      _duration\n      _geoblocked\n      _isLive\n      _mediaArray {\n        _mediaStreamArray {\n          _quality\n          _server\n          _stream\n        }\n      }\n      _previewImage\n      _subtitleUrl\n      _type\n    }\n    show {\n      title\n    }\n    synopsis\n    title\n    tracking {\n      atiCustomVars {\n        contentId\n      }\n    }\n  }\n}''' % (mobj.group('client'), video_id),\n            }).encode(), headers={\n                'Content-Type': 'application/json'\n            })['data']['playerPage']\n        title = player_page['title']\n        content_id = str_or_none(try_get(\n            player_page, lambda x: x['tracking']['atiCustomVars']['contentId']))\n        media_collection = player_page.get('mediaCollection') or {}\n        if not media_collection and content_id:\n            media_collection = self._download_json(\n                'https://www.ardmediathek.de/play/media/' + content_id,\n                content_id, fatal=False) or {}\n        info = self._parse_media_info(\n            media_collection, content_id or video_id,\n            player_page.get('blockedByFsk'))\n        age_limit = None\n        description = player_page.get('synopsis')\n        maturity_content_rating = player_page.get('maturityContentRating')\n        if maturity_content_rating:\n            age_limit = int_or_none(maturity_content_rating.lstrip('FSK'))\n        if not age_limit and description:\n            age_limit = int_or_none(self._search_regex(\n                r'\\(FSK\\s*(\\d+)\\)\\s*$', description, 'age limit', default=None))\n        info.update({\n            'age_limit': age_limit,\n            'display_id': display_id,\n            'title': title,\n            'description': description,\n            'timestamp': unified_timestamp(player_page.get('broadcastedOn')),\n            'series': try_get(player_page, lambda x: x['show']['title']),\n        })\n        return info",
        "begin_line": 348,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.downloader.rtmp.rtmpdump_version#18",
        "src_path": "youtube_dl/downloader/rtmp.py",
        "class_name": "youtube_dl.downloader.rtmp",
        "signature": "youtube_dl.downloader.rtmp.rtmpdump_version()",
        "snippet": "def rtmpdump_version():\n    return get_exe_version(\n        'rtmpdump', ['--help'], r'(?i)RTMPDump\\s*v?([0-9a-zA-Z._-]+)')",
        "begin_line": 18,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.__init__.main#472",
        "src_path": "youtube_dl/__init__.py",
        "class_name": "youtube_dl.__init__",
        "signature": "youtube_dl.__init__.main(argv=None)",
        "snippet": "def main(argv=None):\n    try:\n        _real_main(argv)\n    except DownloadError:\n        sys.exit(1)\n    except SameFileError:\n        sys.exit('ERROR: fixed output name but more than one file to download')\n    except KeyboardInterrupt:\n        sys.exit('\\nERROR: Interrupted by user')",
        "begin_line": 472,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.000177210703526493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.compat._TreeBuilder.doctype#2518",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat._TreeBuilder",
        "signature": "youtube_dl.compat._TreeBuilder.doctype(self, name, pubid, system)",
        "snippet": "    def doctype(self, name, pubid, system):\n        pass",
        "begin_line": 2518,
        "end_line": 2519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.0002801905295601009,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.0002801905295601009,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.compat.compat_etree_fromstring#2532",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat.compat_etree_fromstring(text)",
        "snippet": "    def compat_etree_fromstring(text):\n        return etree.XML(text, parser=etree.XMLParser(target=_TreeBuilder()))",
        "begin_line": 2532,
        "end_line": 2533,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009852216748768472,
            "pseudo_dstar_susp": 0.0010395010395010396,
            "pseudo_tarantula_susp": 0.0004149377593360996,
            "pseudo_op2_susp": 0.0010395010395010396,
            "pseudo_barinel_susp": 0.0004149377593360996
        }
    },
    {
        "name": "youtube_dl.compat.compat_ord#2676",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat.compat_ord(c)",
        "snippet": "def compat_ord(c):\n    if type(c) is int:\n        return c\n    else:\n        return ord(c)",
        "begin_line": 2676,
        "end_line": 2680,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.0001858736059479554,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.compat.compat_setenv#2687",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat.compat_setenv(key, value, env=os.environ)",
        "snippet": "    def compat_setenv(key, value, env=os.environ):\n        env[key] = value",
        "begin_line": 2687,
        "end_line": 2688,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018294914013904133,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.compat.compat_print#2784",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat.compat_print(s)",
        "snippet": "    def compat_print(s):\n        assert isinstance(s, compat_str)\n        print(s)",
        "begin_line": 2784,
        "end_line": 2786,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.compat._testfunc#2805",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat._testfunc(x)",
        "snippet": "    def _testfunc(x):\n        pass",
        "begin_line": 2805,
        "end_line": 2806,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.compat.workaround_optparse_bug9161#2855",
        "src_path": "youtube_dl/compat.py",
        "class_name": "youtube_dl.compat",
        "signature": "youtube_dl.compat.workaround_optparse_bug9161()",
        "snippet": "def workaround_optparse_bug9161():\n    op = optparse.OptionParser()\n    og = optparse.OptionGroup(op, 'foo')\n    try:\n        og.add_option('-t')\n    except TypeError:\n        real_add_option = optparse.OptionGroup.add_option\n\n        def _compat_add_option(self, *args, **kwargs):\n            enc = lambda v: (\n                v.encode('ascii', 'replace') if isinstance(v, compat_str)\n                else v)\n            bargs = [enc(a) for a in args]\n            bkwargs = dict(\n                (k, enc(v)) for k, v in kwargs.items())\n            return real_add_option(self, *bargs, **bkwargs)\n        optparse.OptionGroup.add_option = _compat_add_option",
        "begin_line": 2855,
        "end_line": 2871,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.0001858736059479554,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.svt.SVTSeriesIE.suitable#255",
        "src_path": "youtube_dl/extractor/svt.py",
        "class_name": "youtube_dl.extractor.svt.SVTSeriesIE",
        "signature": "youtube_dl.extractor.svt.SVTSeriesIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if SVTIE.suitable(url) or SVTPlayIE.suitable(url) else super(SVTSeriesIE, cls).suitable(url)",
        "begin_line": 255,
        "end_line": 256,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    },
    {
        "name": "youtube_dl.extractor.svt.SVTPageIE.suitable#354",
        "src_path": "youtube_dl/extractor/svt.py",
        "class_name": "youtube_dl.extractor.svt.SVTPageIE",
        "signature": "youtube_dl.extractor.svt.SVTPageIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if SVTIE.suitable(url) else super(SVTPageIE, cls).suitable(url)",
        "begin_line": 354,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002994908655286014,
            "pseudo_dstar_susp": 0.0003333333333333333,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.0003333333333333333,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.limelight.LimelightBaseIE._extract_urls#23",
        "src_path": "youtube_dl/extractor/limelight.py",
        "class_name": "youtube_dl.extractor.limelight.LimelightBaseIE",
        "signature": "youtube_dl.extractor.limelight.LimelightBaseIE._extract_urls(cls, webpage, source_url)",
        "snippet": "    def _extract_urls(cls, webpage, source_url):\n        lm = {\n            'Media': 'media',\n            'Channel': 'channel',\n            'ChannelList': 'channel_list',\n        }\n\n        def smuggle(url):\n            return smuggle_url(url, {'source_url': source_url})\n\n        entries = []\n        for kind, video_id in re.findall(\n                r'LimelightPlayer\\.doLoad(Media|Channel|ChannelList)\\([\"\\'](?P<id>[a-z0-9]{32})',\n                webpage):\n            entries.append(cls.url_result(\n                smuggle('limelight:%s:%s' % (lm[kind], video_id)),\n                'Limelight%s' % kind, video_id))\n        for mobj in re.finditer(\n                # As per [1] class attribute should be exactly equal to\n                # LimelightEmbeddedPlayerFlash but numerous examples seen\n                # that don't exactly match it (e.g. [2]).\n                # 1. http://support.3playmedia.com/hc/en-us/articles/227732408-Limelight-Embedding-the-Captions-Plugin-with-the-Limelight-Player-on-Your-Webpage\n                # 2. http://www.sedona.com/FacilitatorTraining2017\n                r'''(?sx)\n                    <object[^>]+class=([\"\\'])(?:(?!\\1).)*\\bLimelightEmbeddedPlayerFlash\\b(?:(?!\\1).)*\\1[^>]*>.*?\n                        <param[^>]+\n                            name=([\"\\'])flashVars\\2[^>]+\n                            value=([\"\\'])(?:(?!\\3).)*(?P<kind>media|channel(?:List)?)Id=(?P<id>[a-z0-9]{32})\n                ''', webpage):\n            kind, video_id = mobj.group('kind'), mobj.group('id')\n            entries.append(cls.url_result(\n                smuggle('limelight:%s:%s' % (kind, video_id)),\n                'Limelight%s' % kind.capitalize(), video_id))\n        # http://support.3playmedia.com/hc/en-us/articles/115009517327-Limelight-Embedding-the-Audio-Description-Plugin-with-the-Limelight-Player-on-Your-Web-Page)\n        for video_id in re.findall(\n                r'(?s)LimelightPlayerUtil\\.embed\\s*\\(\\s*{.*?\\bmediaId[\"\\']\\s*:\\s*[\"\\'](?P<id>[a-z0-9]{32})',\n                webpage):\n            entries.append(cls.url_result(\n                smuggle('limelight:media:%s' % video_id),\n                LimelightMediaIE.ie_key(), video_id))\n        return entries",
        "begin_line": 23,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00019334880123743234,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.limelight.LimelightChannelIE._real_extract#302",
        "src_path": "youtube_dl/extractor/limelight.py",
        "class_name": "youtube_dl.extractor.limelight.LimelightChannelIE",
        "signature": "youtube_dl.extractor.limelight.LimelightChannelIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n        channel_id = self._match_id(url)\n        source_url = smuggled_data.get('source_url')\n\n        pc, mobile = self._extract(\n            channel_id, 'getPlaylistByChannelId',\n            'getMobilePlaylistWithNItemsByChannelId?begin=0&count=-1',\n            source_url)\n\n        entries = [\n            self._extract_info(pc, mobile, i, source_url)\n            for i in range(len(pc['playlistItems']))]\n\n        return self.playlist_result(\n            entries, channel_id, pc.get('title'), mobile.get('description'))",
        "begin_line": 302,
        "end_line": 317,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._set_language#73",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._set_language(self)",
        "snippet": "    def _set_language(self):\n        self._set_cookie(\n            '.youtube.com', 'PREF', 'f1=50000000&hl=en',\n            # YouTube sets the expire time to about two months\n            expire_time=time.time() + 2 * 30 * 24 * 3600)",
        "begin_line": 73,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005073566717402334,
            "pseudo_dstar_susp": 0.00044802867383512545,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00044802867383512545,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._ids_to_results#79",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._ids_to_results(self, ids)",
        "snippet": "    def _ids_to_results(self, ids):\n        return [\n            self.url_result(vid_id, 'Youtube', video_id=vid_id)\n            for vid_id in ids]",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._login#84",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._login(self)",
        "snippet": "    def _login(self):\n        \"\"\"\n        Attempt to log in to YouTube.\n        True is returned if successful or skipped.\n        False is returned if login failed.\n\n        If _LOGIN_REQUIRED is set and no authentication was provided, an error is raised.\n        \"\"\"\n        username, password = self._get_login_info()\n        # No authentication to be performed\n        if username is None:\n            if self._LOGIN_REQUIRED and self._downloader.params.get('cookiefile') is None:\n                raise ExtractorError('No login info available, needed for using %s.' % self.IE_NAME, expected=True)\n            return True\n\n        login_page = self._download_webpage(\n            self._LOGIN_URL, None,\n            note='Downloading login page',\n            errnote='unable to fetch login page', fatal=False)\n        if login_page is False:\n            return\n\n        login_form = self._hidden_inputs(login_page)\n\n        def req(url, f_req, note, errnote):\n            data = login_form.copy()\n            data.update({\n                'pstMsg': 1,\n                'checkConnection': 'youtube',\n                'checkedDomains': 'youtube',\n                'hl': 'en',\n                'deviceinfo': '[null,null,null,[],null,\"US\",null,null,[],\"GlifWebSignIn\",null,[null,null,[]]]',\n                'f.req': json.dumps(f_req),\n                'flowName': 'GlifWebSignIn',\n                'flowEntry': 'ServiceLogin',\n                # TODO: reverse actual botguard identifier generation algo\n                'bgRequest': '[\"identifier\",\"\"]',\n            })\n            return self._download_json(\n                url, None, note=note, errnote=errnote,\n                transform_source=lambda s: re.sub(r'^[^[]*', '', s),\n                fatal=False,\n                data=urlencode_postdata(data), headers={\n                    'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8',\n                    'Google-Accounts-XSRF': 1,\n                })\n\n        def warn(message):\n            self._downloader.report_warning(message)\n\n        lookup_req = [\n            username,\n            None, [], None, 'US', None, None, 2, False, True,\n            [\n                None, None,\n                [2, 1, None, 1,\n                 'https://accounts.google.com/ServiceLogin?passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252F%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3Dsign_in_button&hl=en&service=youtube&uilel=3&requestPath=%2FServiceLogin&Page=PasswordSeparationSignIn',\n                 None, [], 4],\n                1, [None, None, []], None, None, None, True\n            ],\n            username,\n        ]\n\n        lookup_results = req(\n            self._LOOKUP_URL, lookup_req,\n            'Looking up account info', 'Unable to look up account info')\n\n        if lookup_results is False:\n            return False\n\n        user_hash = try_get(lookup_results, lambda x: x[0][2], compat_str)\n        if not user_hash:\n            warn('Unable to extract user hash')\n            return False\n\n        challenge_req = [\n            user_hash,\n            None, 1, None, [1, None, None, None, [password, None, True]],\n            [\n                None, None, [2, 1, None, 1, 'https://accounts.google.com/ServiceLogin?passive=true&continue=https%3A%2F%2Fwww.youtube.com%2Fsignin%3Fnext%3D%252F%26action_handle_signin%3Dtrue%26hl%3Den%26app%3Ddesktop%26feature%3Dsign_in_button&hl=en&service=youtube&uilel=3&requestPath=%2FServiceLogin&Page=PasswordSeparationSignIn', None, [], 4],\n                1, [None, None, []], None, None, None, True\n            ]]\n\n        challenge_results = req(\n            self._CHALLENGE_URL, challenge_req,\n            'Logging in', 'Unable to log in')\n\n        if challenge_results is False:\n            return\n\n        login_res = try_get(challenge_results, lambda x: x[0][5], list)\n        if login_res:\n            login_msg = try_get(login_res, lambda x: x[5], compat_str)\n            warn(\n                'Unable to login: %s' % 'Invalid password'\n                if login_msg == 'INCORRECT_ANSWER_ENTERED' else login_msg)\n            return False\n\n        res = try_get(challenge_results, lambda x: x[0][-1], list)\n        if not res:\n            warn('Unable to extract result entry')\n            return False\n\n        login_challenge = try_get(res, lambda x: x[0][0], list)\n        if login_challenge:\n            challenge_str = try_get(login_challenge, lambda x: x[2], compat_str)\n            if challenge_str == 'TWO_STEP_VERIFICATION':\n                # SEND_SUCCESS - TFA code has been successfully sent to phone\n                # QUOTA_EXCEEDED - reached the limit of TFA codes\n                status = try_get(login_challenge, lambda x: x[5], compat_str)\n                if status == 'QUOTA_EXCEEDED':\n                    warn('Exceeded the limit of TFA codes, try later')\n                    return False\n\n                tl = try_get(challenge_results, lambda x: x[1][2], compat_str)\n                if not tl:\n                    warn('Unable to extract TL')\n                    return False\n\n                tfa_code = self._get_tfa_info('2-step verification code')\n\n                if not tfa_code:\n                    warn(\n                        'Two-factor authentication required. Provide it either interactively or with --twofactor <code>'\n                        '(Note that only TOTP (Google Authenticator App) codes work at this time.)')\n                    return False\n\n                tfa_code = remove_start(tfa_code, 'G-')\n\n                tfa_req = [\n                    user_hash, None, 2, None,\n                    [\n                        9, None, None, None, None, None, None, None,\n                        [None, tfa_code, True, 2]\n                    ]]\n\n                tfa_results = req(\n                    self._TFA_URL.format(tl), tfa_req,\n                    'Submitting TFA code', 'Unable to submit TFA code')\n\n                if tfa_results is False:\n                    return False\n\n                tfa_res = try_get(tfa_results, lambda x: x[0][5], list)\n                if tfa_res:\n                    tfa_msg = try_get(tfa_res, lambda x: x[5], compat_str)\n                    warn(\n                        'Unable to finish TFA: %s' % 'Invalid TFA code'\n                        if tfa_msg == 'INCORRECT_ANSWER_ENTERED' else tfa_msg)\n                    return False\n\n                check_cookie_url = try_get(\n                    tfa_results, lambda x: x[0][-1][2], compat_str)\n            else:\n                CHALLENGES = {\n                    'LOGIN_CHALLENGE': \"This device isn't recognized. For your security, Google wants to make sure it's really you.\",\n                    'USERNAME_RECOVERY': 'Please provide additional information to aid in the recovery process.',\n                    'REAUTH': \"There is something unusual about your activity. For your security, Google wants to make sure it's really you.\",\n                }\n                challenge = CHALLENGES.get(\n                    challenge_str,\n                    '%s returned error %s.' % (self.IE_NAME, challenge_str))\n                warn('%s\\nGo to https://accounts.google.com/, login and solve a challenge.' % challenge)\n                return False\n        else:\n            check_cookie_url = try_get(res, lambda x: x[2], compat_str)\n\n        if not check_cookie_url:\n            warn('Unable to extract CheckCookie URL')\n            return False\n\n        check_cookie_results = self._download_webpage(\n            check_cookie_url, None, 'Checking cookie', fatal=False)\n\n        if check_cookie_results is False:\n            return False\n\n        if 'https://myaccount.google.com/' not in check_cookie_results:\n            warn('Unable to log in')\n            return False\n\n        return True",
        "begin_line": 84,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005379236148466917,
            "pseudo_dstar_susp": 0.0004807692307692308,
            "pseudo_tarantula_susp": 0.0032679738562091504,
            "pseudo_op2_susp": 0.0004807692307692308,
            "pseudo_barinel_susp": 0.0032679738562091504
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._download_webpage_handle#267",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._download_webpage_handle(self, *args, **kwargs)",
        "snippet": "    def _download_webpage_handle(self, *args, **kwargs):\n        query = kwargs.get('query', {}).copy()\n        query['disable_polymer'] = 'true'\n        kwargs['query'] = query\n        return super(YoutubeBaseInfoExtractor, self)._download_webpage_handle(\n            *args, **compat_kwargs(kwargs))",
        "begin_line": 267,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004952947003467063,
            "pseudo_dstar_susp": 0.0004464285714285714,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004464285714285714,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._real_initialize#274",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        if self._downloader is None:\n            return\n        self._set_language()\n        if not self._login():\n            return",
        "begin_line": 274,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000484027105517909,
            "pseudo_dstar_susp": 0.0004329004329004329,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeEntryListBaseInfoExtractor._entries#284",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeEntryListBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeEntryListBaseInfoExtractor._entries(self, page, playlist_id)",
        "snippet": "    def _entries(self, page, playlist_id):\n        more_widget_html = content_html = page\n        for page_num in itertools.count(1):\n            for entry in self._process_page(content_html):\n                yield entry\n\n            mobj = re.search(r'data-uix-load-more-href=\"/?(?P<more>[^\"]+)\"', more_widget_html)\n            if not mobj:\n                break\n\n            count = 0\n            retries = 3\n            while count <= retries:\n                try:\n                    # Downloading page may result in intermittent 5xx HTTP error\n                    # that is usually worked around with a retry\n                    more = self._download_json(\n                        'https://youtube.com/%s' % mobj.group('more'), playlist_id,\n                        'Downloading page #%s%s'\n                        % (page_num, ' (retry #%d)' % count if count else ''),\n                        transform_source=uppercase_escape)\n                    break\n                except ExtractorError as e:\n                    if isinstance(e.cause, compat_HTTPError) and e.cause.code in (500, 503):\n                        count += 1\n                        if count <= retries:\n                            continue\n                    raise\n\n            content_html = more['content_html']\n            if not content_html.strip():\n                # Some webpages show a \"Load more\" button but they don't\n                # have more videos\n                break\n            more_widget_html = more['load_more_widget_html']",
        "begin_line": 284,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor._process_page#322",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor._process_page(self, content)",
        "snippet": "    def _process_page(self, content):\n        for video_id, video_title in self.extract_videos_from_page(content):\n            yield self.url_result(video_id, 'Youtube', video_id, video_title)",
        "begin_line": 322,
        "end_line": 324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor.extract_videos_from_page_impl#326",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistBaseInfoExtractor.extract_videos_from_page_impl(self, video_re, page, ids_in_page, titles_in_page)",
        "snippet": "    def extract_videos_from_page_impl(self, video_re, page, ids_in_page, titles_in_page):\n        for mobj in re.finditer(video_re, page):\n            # The link with index 0 is not the first video of the playlist (not sure if still actual)\n            if 'index' in mobj.groupdict() and mobj.group('id') == '0':\n                continue\n            video_id = mobj.group('id')\n            video_title = unescapeHTML(\n                mobj.group('title')) if 'title' in mobj.groupdict() else None\n            if video_title:\n                video_title = video_title.strip()\n            if video_title == '\u25ba Play all':\n                video_title = None\n            try:\n                idx = ids_in_page.index(video_id)\n                if video_title and not titles_in_page[idx]:\n                    titles_in_page[idx] = video_title\n            except ValueError:\n                ids_in_page.append(video_id)\n                titles_in_page.append(video_title)",
        "begin_line": 326,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.__init__#1264",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(YoutubeIE, self).__init__(*args, **kwargs)\n        self._player_cache = {}",
        "begin_line": 1264,
        "end_line": 1266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004008016032064128,
            "pseudo_dstar_susp": 0.00044365572315882877,
            "pseudo_tarantula_susp": 0.0009174311926605505,
            "pseudo_op2_susp": 0.00044365572315882877,
            "pseudo_barinel_susp": 0.0009165902841429881
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._extract_player_info#1289",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._extract_player_info(cls, player_url)",
        "snippet": "    def _extract_player_info(cls, player_url):\n        for player_re in cls._PLAYER_INFO_RE:\n            id_m = re.search(player_re, player_url)\n            if id_m:\n                break\n        else:\n            raise ExtractorError('Cannot identify player %r' % player_url)\n        return id_m.group('ext'), id_m.group('id')",
        "begin_line": 1289,
        "end_line": 1296,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._extract_signature_function#1298",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._extract_signature_function(self, video_id, player_url, example_sig)",
        "snippet": "    def _extract_signature_function(self, video_id, player_url, example_sig):\n        player_type, player_id = self._extract_player_info(player_url)\n\n        # Read from filesystem cache\n        func_id = '%s_%s_%s' % (\n            player_type, player_id, self._signature_cache_id(example_sig))\n        assert os.path.basename(func_id) == func_id\n\n        cache_spec = self._downloader.cache.load('youtube-sigfuncs', func_id)\n        if cache_spec is not None:\n            return lambda s: ''.join(s[i] for i in cache_spec)\n\n        download_note = (\n            'Downloading player %s' % player_url\n            if self._downloader.params.get('verbose') else\n            'Downloading %s player %s' % (player_type, player_id)\n        )\n        if player_type == 'js':\n            code = self._download_webpage(\n                player_url, video_id,\n                note=download_note,\n                errnote='Download of %s failed' % player_url)\n            res = self._parse_sig_js(code)\n        elif player_type == 'swf':\n            urlh = self._request_webpage(\n                player_url, video_id,\n                note=download_note,\n                errnote='Download of %s failed' % player_url)\n            code = urlh.read()\n            res = self._parse_sig_swf(code)\n        else:\n            assert False, 'Invalid player type %r' % player_type\n\n        test_string = ''.join(map(compat_chr, range(len(example_sig))))\n        cache_res = res(test_string)\n        cache_spec = [ord(c) for c in cache_res]\n\n        self._downloader.cache.store('youtube-sigfuncs', func_id, cache_spec)\n        return res",
        "begin_line": 1298,
        "end_line": 1336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._parse_sig_js#1377",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._parse_sig_js(self, jscode)",
        "snippet": "    def _parse_sig_js(self, jscode):\n        funcname = self._search_regex(\n            (r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*encodeURIComponent\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\b(?P<sig>[a-zA-Z0-9$]{2})\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n             r'(?P<sig>[a-zA-Z0-9$]+)\\s*=\\s*function\\(\\s*a\\s*\\)\\s*{\\s*a\\s*=\\s*a\\.split\\(\\s*\"\"\\s*\\)',\n             # Obsolete patterns\n             r'([\"\\'])signature\\1\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\.sig\\|\\|(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'yt\\.akamaized\\.net/\\)\\s*\\|\\|\\s*.*?\\s*[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?:encodeURIComponent\\s*\\()?\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\b[cs]\\s*&&\\s*[adf]\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\b[a-zA-Z0-9]+\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\bc\\s*&&\\s*a\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\(',\n             r'\\bc\\s*&&\\s*[a-zA-Z0-9]+\\.set\\([^,]+\\s*,\\s*\\([^)]*\\)\\s*\\(\\s*(?P<sig>[a-zA-Z0-9$]+)\\('),\n            jscode, 'Initial JS player signature function name', group='sig')\n\n        jsi = JSInterpreter(jscode)\n        initial_function = jsi.extract_function(funcname)\n        return lambda s: initial_function([s])",
        "begin_line": 1377,
        "end_line": 1396,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._decrypt_signature#1405",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._decrypt_signature(self, s, video_id, player_url, age_gate=False)",
        "snippet": "    def _decrypt_signature(self, s, video_id, player_url, age_gate=False):\n        \"\"\"Turn the encrypted s field into a working signature\"\"\"\n\n        if player_url is None:\n            raise ExtractorError('Cannot decrypt signature without player_url')\n\n        if player_url.startswith('//'):\n            player_url = 'https:' + player_url\n        elif not re.match(r'https?://', player_url):\n            player_url = compat_urlparse.urljoin(\n                'https://www.youtube.com', player_url)\n        try:\n            player_id = (player_url, self._signature_cache_id(s))\n            if player_id not in self._player_cache:\n                func = self._extract_signature_function(\n                    video_id, player_url, s\n                )\n                self._player_cache[player_id] = func\n            func = self._player_cache[player_id]\n            if self._downloader.params.get('youtube_print_sig_code'):\n                self._print_sig_code(func, s)\n            return func(s)\n        except Exception as e:\n            tb = traceback.format_exc()\n            raise ExtractorError(\n                'Signature extraction failed: ' + tb, cause=e)",
        "begin_line": 1405,
        "end_line": 1430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._get_ytplayer_config#1464",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._get_ytplayer_config(self, video_id, webpage)",
        "snippet": "    def _get_ytplayer_config(self, video_id, webpage):\n        patterns = (\n            # User data may contain arbitrary character sequences that may affect\n            # JSON extraction with regex, e.g. when '};' is contained the second\n            # regex won't capture the whole JSON. Yet working around by trying more\n            # concrete regex first keeping in mind proper quoted string handling\n            # to be implemented in future that will replace this workaround (see\n            # https://github.com/ytdl-org/youtube-dl/issues/7468,\n            # https://github.com/ytdl-org/youtube-dl/pull/7599)\n            r';ytplayer\\.config\\s*=\\s*({.+?});ytplayer',\n            r';ytplayer\\.config\\s*=\\s*({.+?});',\n        )\n        config = self._search_regex(\n            patterns, webpage, 'ytplayer.config', default=None)\n        if config:\n            return self._parse_json(\n                uppercase_escape(config), video_id, fatal=False)",
        "begin_line": 1464,
        "end_line": 1480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.0007513148009015778,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0007513148009015778,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._mark_watched#1584",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._mark_watched(self, video_id, video_info, player_response)",
        "snippet": "    def _mark_watched(self, video_id, video_info, player_response):\n        playback_url = url_or_none(try_get(\n            player_response,\n            lambda x: x['playbackTracking']['videostatsPlaybackUrl']['baseUrl']) or try_get(\n            video_info, lambda x: x['videostats_playback_base_url'][0]))\n        if not playback_url:\n            return\n        parsed_playback_url = compat_urlparse.urlparse(playback_url)\n        qs = compat_urlparse.parse_qs(parsed_playback_url.query)\n\n        # cpn generation algorithm is reverse engineered from base.js.\n        # In fact it works even with dummy cpn.\n        CPN_ALPHABET = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_'\n        cpn = ''.join((CPN_ALPHABET[random.randint(0, 256) & 63] for _ in range(0, 16)))\n\n        qs.update({\n            'ver': ['2'],\n            'cpn': [cpn],\n        })\n        playback_url = compat_urlparse.urlunparse(\n            parsed_playback_url._replace(query=compat_urllib_parse_urlencode(qs, True)))\n\n        self._download_webpage(\n            playback_url, video_id, 'Marking watched',\n            'Unable to mark watched', fatal=False)",
        "begin_line": 1584,
        "end_line": 1608,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._extract_urls#1611",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        # Embedded YouTube player\n        entries = [\n            unescapeHTML(mobj.group('url'))\n            for mobj in re.finditer(r'''(?x)\n            (?:\n                <iframe[^>]+?src=|\n                data-video-url=|\n                <embed[^>]+?src=|\n                embedSWF\\(?:\\s*|\n                <object[^>]+data=|\n                new\\s+SWFObject\\(\n            )\n            ([\"\\'])\n                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n                (?:embed|v|p)/[0-9A-Za-z_-]{11}.*?)\n            \\1''', webpage)]\n\n        # lazyYT YouTube embed\n        entries.extend(list(map(\n            unescapeHTML,\n            re.findall(r'class=\"lazyYT\" data-youtube-id=\"([^\"]+)\"', webpage))))\n\n        # Wordpress \"YouTube Video Importer\" plugin\n        matches = re.findall(r'''(?x)<div[^>]+\n            class=(?P<q1>[\\'\"])[^\\'\"]*\\byvii_single_video_player\\b[^\\'\"]*(?P=q1)[^>]+\n            data-video_id=(?P<q2>[\\'\"])([^\\'\"]+)(?P=q2)''', webpage)\n        entries.extend(m[-1] for m in matches)\n\n        return entries",
        "begin_line": 1611,
        "end_line": 1640,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007911392405063291,
            "pseudo_dstar_susp": 0.0008058017727639,
            "pseudo_tarantula_susp": 0.0005027652086475615,
            "pseudo_op2_susp": 0.0008058017727639,
            "pseudo_barinel_susp": 0.0005027652086475615
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._extract_url#1643",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._extract_url(webpage)",
        "snippet": "    def _extract_url(webpage):\n        urls = YoutubeIE._extract_urls(webpage)\n        return urls[0] if urls else None",
        "begin_line": 1643,
        "end_line": 1645,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005081300813008131,
            "pseudo_dstar_susp": 0.00047938638542665386,
            "pseudo_tarantula_susp": 0.0015384615384615385,
            "pseudo_op2_susp": 0.00047938638542665386,
            "pseudo_barinel_susp": 0.0015384615384615385
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.extract_id#1648",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.extract_id(cls, url)",
        "snippet": "    def extract_id(cls, url):\n        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n        video_id = mobj.group(2)\n        return video_id",
        "begin_line": 1648,
        "end_line": 1653,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010183299389002036,
            "pseudo_dstar_susp": 0.0007462686567164179,
            "pseudo_tarantula_susp": 0.00273224043715847,
            "pseudo_op2_susp": 0.0007462686567164179,
            "pseudo_barinel_susp": 0.00273224043715847
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._real_extract#1690",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, smuggled_data = unsmuggle_url(url, {})\n\n        proto = (\n            'http' if self._downloader.params.get('prefer_insecure', False)\n            else 'https')\n\n        start_time = None\n        end_time = None\n        parsed_url = compat_urllib_parse_urlparse(url)\n        for component in [parsed_url.fragment, parsed_url.query]:\n            query = compat_parse_qs(component)\n            if start_time is None and 't' in query:\n                start_time = parse_duration(query['t'][0])\n            if start_time is None and 'start' in query:\n                start_time = parse_duration(query['start'][0])\n            if end_time is None and 'end' in query:\n                end_time = parse_duration(query['end'][0])\n\n        # Extract original video URL from URL with redirection, like age verification, using next_url parameter\n        mobj = re.search(self._NEXT_URL_RE, url)\n        if mobj:\n            url = proto + '://www.youtube.com/' + compat_urllib_parse_unquote(mobj.group(1)).lstrip('/')\n        video_id = self.extract_id(url)\n\n        # Get video webpage\n        url = proto + '://www.youtube.com/watch?v=%s&gl=US&hl=en&has_verified=1&bpctr=9999999999' % video_id\n        video_webpage, urlh = self._download_webpage_handle(url, video_id)\n\n        qs = compat_parse_qs(compat_urllib_parse_urlparse(urlh.geturl()).query)\n        video_id = qs.get('v', [None])[0] or video_id\n\n        # Attempt to extract SWF player URL\n        mobj = re.search(r'swfConfig.*?\"(https?:\\\\/\\\\/.*?watch.*?-.*?\\.swf)\"', video_webpage)\n        if mobj is not None:\n            player_url = re.sub(r'\\\\(.)', r'\\1', mobj.group(1))\n        else:\n            player_url = None\n\n        dash_mpds = []\n\n        def add_dash_mpd(video_info):\n            dash_mpd = video_info.get('dashmpd')\n            if dash_mpd and dash_mpd[0] not in dash_mpds:\n                dash_mpds.append(dash_mpd[0])\n\n        def add_dash_mpd_pr(pl_response):\n            dash_mpd = url_or_none(try_get(\n                pl_response, lambda x: x['streamingData']['dashManifestUrl'],\n                compat_str))\n            if dash_mpd and dash_mpd not in dash_mpds:\n                dash_mpds.append(dash_mpd)\n\n        is_live = None\n        view_count = None\n\n        def extract_view_count(v_info):\n            return int_or_none(try_get(v_info, lambda x: x['view_count'][0]))\n\n        def extract_player_response(player_response, video_id):\n            pl_response = str_or_none(player_response)\n            if not pl_response:\n                return\n            pl_response = self._parse_json(pl_response, video_id, fatal=False)\n            if isinstance(pl_response, dict):\n                add_dash_mpd_pr(pl_response)\n                return pl_response\n\n        player_response = {}\n\n        # Get video info\n        video_info = {}\n        embed_webpage = None\n        if re.search(r'player-age-gate-content\">', video_webpage) is not None:\n            age_gate = True\n            # We simulate the access to the video from www.youtube.com/v/{video_id}\n            # this can be viewed without login into Youtube\n            url = proto + '://www.youtube.com/embed/%s' % video_id\n            embed_webpage = self._download_webpage(url, video_id, 'Downloading embed webpage')\n            data = compat_urllib_parse_urlencode({\n                'video_id': video_id,\n                'eurl': 'https://youtube.googleapis.com/v/' + video_id,\n                'sts': self._search_regex(\n                    r'\"sts\"\\s*:\\s*(\\d+)', embed_webpage, 'sts', default=''),\n            })\n            video_info_url = proto + '://www.youtube.com/get_video_info?' + data\n            try:\n                video_info_webpage = self._download_webpage(\n                    video_info_url, video_id,\n                    note='Refetching age-gated info webpage',\n                    errnote='unable to download video info webpage')\n            except ExtractorError:\n                video_info_webpage = None\n            if video_info_webpage:\n                video_info = compat_parse_qs(video_info_webpage)\n                pl_response = video_info.get('player_response', [None])[0]\n                player_response = extract_player_response(pl_response, video_id)\n                add_dash_mpd(video_info)\n                view_count = extract_view_count(video_info)\n        else:\n            age_gate = False\n            # Try looking directly into the video webpage\n            ytplayer_config = self._get_ytplayer_config(video_id, video_webpage)\n            if ytplayer_config:\n                args = ytplayer_config['args']\n                if args.get('url_encoded_fmt_stream_map') or args.get('hlsvp'):\n                    # Convert to the same format returned by compat_parse_qs\n                    video_info = dict((k, [v]) for k, v in args.items())\n                    add_dash_mpd(video_info)\n                # Rental video is not rented but preview is available (e.g.\n                # https://www.youtube.com/watch?v=yYr8q0y5Jfg,\n                # https://github.com/ytdl-org/youtube-dl/issues/10532)\n                if not video_info and args.get('ypc_vid'):\n                    return self.url_result(\n                        args['ypc_vid'], YoutubeIE.ie_key(), video_id=args['ypc_vid'])\n                if args.get('livestream') == '1' or args.get('live_playback') == 1:\n                    is_live = True\n                if not player_response:\n                    player_response = extract_player_response(args.get('player_response'), video_id)\n            if not video_info or self._downloader.params.get('youtube_include_dash_manifest', True):\n                add_dash_mpd_pr(player_response)\n\n        def extract_unavailable_message():\n            messages = []\n            for tag, kind in (('h1', 'message'), ('div', 'submessage')):\n                msg = self._html_search_regex(\n                    r'(?s)<{tag}[^>]+id=[\"\\']unavailable-{kind}[\"\\'][^>]*>(.+?)</{tag}>'.format(tag=tag, kind=kind),\n                    video_webpage, 'unavailable %s' % kind, default=None)\n                if msg:\n                    messages.append(msg)\n            if messages:\n                return '\\n'.join(messages)\n\n        if not video_info and not player_response:\n            unavailable_message = extract_unavailable_message()\n            if not unavailable_message:\n                unavailable_message = 'Unable to extract video data'\n            raise ExtractorError(\n                'YouTube said: %s' % unavailable_message, expected=True, video_id=video_id)\n\n        if not isinstance(video_info, dict):\n            video_info = {}\n\n        video_details = try_get(\n            player_response, lambda x: x['videoDetails'], dict) or {}\n\n        video_title = video_info.get('title', [None])[0] or video_details.get('title')\n        if not video_title:\n            self._downloader.report_warning('Unable to extract video title')\n            video_title = '_'\n\n        description_original = video_description = get_element_by_id(\"eow-description\", video_webpage)\n        if video_description:\n\n            def replace_url(m):\n                redir_url = compat_urlparse.urljoin(url, m.group(1))\n                parsed_redir_url = compat_urllib_parse_urlparse(redir_url)\n                if re.search(r'^(?:www\\.)?(?:youtube(?:-nocookie)?\\.com|youtu\\.be)$', parsed_redir_url.netloc) and parsed_redir_url.path == '/redirect':\n                    qs = compat_parse_qs(parsed_redir_url.query)\n                    q = qs.get('q')\n                    if q and q[0]:\n                        return q[0]\n                return redir_url\n\n            description_original = video_description = re.sub(r'''(?x)\n                <a\\s+\n                    (?:[a-zA-Z-]+=\"[^\"]*\"\\s+)*?\n                    (?:title|href)=\"([^\"]+)\"\\s+\n                    (?:[a-zA-Z-]+=\"[^\"]*\"\\s+)*?\n                    class=\"[^\"]*\"[^>]*>\n                [^<]+\\.{3}\\s*\n                </a>\n            ''', replace_url, video_description)\n            video_description = clean_html(video_description)\n        else:\n            video_description = self._html_search_meta('description', video_webpage) or video_details.get('shortDescription')\n\n        if not smuggled_data.get('force_singlefeed', False):\n            if not self._downloader.params.get('noplaylist'):\n                multifeed_metadata_list = try_get(\n                    player_response,\n                    lambda x: x['multicamera']['playerLegacyMulticameraRenderer']['metadataList'],\n                    compat_str) or try_get(\n                    video_info, lambda x: x['multifeed_metadata_list'][0], compat_str)\n                if multifeed_metadata_list:\n                    entries = []\n                    feed_ids = []\n                    for feed in multifeed_metadata_list.split(','):\n                        # Unquote should take place before split on comma (,) since textual\n                        # fields may contain comma as well (see\n                        # https://github.com/ytdl-org/youtube-dl/issues/8536)\n                        feed_data = compat_parse_qs(compat_urllib_parse_unquote_plus(feed))\n\n                        def feed_entry(name):\n                            return try_get(feed_data, lambda x: x[name][0], compat_str)\n\n                        feed_id = feed_entry('id')\n                        if not feed_id:\n                            continue\n                        feed_title = feed_entry('title')\n                        title = video_title\n                        if feed_title:\n                            title += ' (%s)' % feed_title\n                        entries.append({\n                            '_type': 'url_transparent',\n                            'ie_key': 'Youtube',\n                            'url': smuggle_url(\n                                '%s://www.youtube.com/watch?v=%s' % (proto, feed_data['id'][0]),\n                                {'force_singlefeed': True}),\n                            'title': title,\n                        })\n                        feed_ids.append(feed_id)\n                    self.to_screen(\n                        'Downloading multifeed video (%s) - add --no-playlist to just download video %s'\n                        % (', '.join(feed_ids), video_id))\n                    return self.playlist_result(entries, video_id, video_title, video_description)\n            else:\n                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n\n        if view_count is None:\n            view_count = extract_view_count(video_info)\n        if view_count is None and video_details:\n            view_count = int_or_none(video_details.get('viewCount'))\n\n        if is_live is None:\n            is_live = bool_or_none(video_details.get('isLive'))\n\n        # Check for \"rental\" videos\n        if 'ypc_video_rental_bar_text' in video_info and 'author' not in video_info:\n            raise ExtractorError('\"rental\" videos not supported. See https://github.com/ytdl-org/youtube-dl/issues/359 for more information.', expected=True)\n\n        def _extract_filesize(media_url):\n            return int_or_none(self._search_regex(\n                r'\\bclen[=/](\\d+)', media_url, 'filesize', default=None))\n\n        streaming_formats = try_get(player_response, lambda x: x['streamingData']['formats'], list) or []\n        streaming_formats.extend(try_get(player_response, lambda x: x['streamingData']['adaptiveFormats'], list) or [])\n\n        if 'conn' in video_info and video_info['conn'][0].startswith('rtmp'):\n            self.report_rtmp_download()\n            formats = [{\n                'format_id': '_rtmp',\n                'protocol': 'rtmp',\n                'url': video_info['conn'][0],\n                'player_url': player_url,\n            }]\n        elif not is_live and (streaming_formats or len(video_info.get('url_encoded_fmt_stream_map', [''])[0]) >= 1 or len(video_info.get('adaptive_fmts', [''])[0]) >= 1):\n            encoded_url_map = video_info.get('url_encoded_fmt_stream_map', [''])[0] + ',' + video_info.get('adaptive_fmts', [''])[0]\n            if 'rtmpe%3Dyes' in encoded_url_map:\n                raise ExtractorError('rtmpe downloads are not supported, see https://github.com/ytdl-org/youtube-dl/issues/343 for more information.', expected=True)\n            formats = []\n            formats_spec = {}\n            fmt_list = video_info.get('fmt_list', [''])[0]\n            if fmt_list:\n                for fmt in fmt_list.split(','):\n                    spec = fmt.split('/')\n                    if len(spec) > 1:\n                        width_height = spec[1].split('x')\n                        if len(width_height) == 2:\n                            formats_spec[spec[0]] = {\n                                'resolution': spec[1],\n                                'width': int_or_none(width_height[0]),\n                                'height': int_or_none(width_height[1]),\n                            }\n            for fmt in streaming_formats:\n                itag = str_or_none(fmt.get('itag'))\n                if not itag:\n                    continue\n                quality = fmt.get('quality')\n                quality_label = fmt.get('qualityLabel') or quality\n                formats_spec[itag] = {\n                    'asr': int_or_none(fmt.get('audioSampleRate')),\n                    'filesize': int_or_none(fmt.get('contentLength')),\n                    'format_note': quality_label,\n                    'fps': int_or_none(fmt.get('fps')),\n                    'height': int_or_none(fmt.get('height')),\n                    # bitrate for itag 43 is always 2147483647\n                    'tbr': float_or_none(fmt.get('averageBitrate') or fmt.get('bitrate'), 1000) if itag != '43' else None,\n                    'width': int_or_none(fmt.get('width')),\n                }\n\n            for fmt in streaming_formats:\n                if fmt.get('drmFamilies') or fmt.get('drm_families'):\n                    continue\n                url = url_or_none(fmt.get('url'))\n\n                if not url:\n                    cipher = fmt.get('cipher') or fmt.get('signatureCipher')\n                    if not cipher:\n                        continue\n                    url_data = compat_parse_qs(cipher)\n                    url = url_or_none(try_get(url_data, lambda x: x['url'][0], compat_str))\n                    if not url:\n                        continue\n                else:\n                    cipher = None\n                    url_data = compat_parse_qs(compat_urllib_parse_urlparse(url).query)\n\n                stream_type = int_or_none(try_get(url_data, lambda x: x['stream_type'][0]))\n                # Unsupported FORMAT_STREAM_TYPE_OTF\n                if stream_type == 3:\n                    continue\n\n                format_id = fmt.get('itag') or url_data['itag'][0]\n                if not format_id:\n                    continue\n                format_id = compat_str(format_id)\n\n                if cipher:\n                    if 's' in url_data or self._downloader.params.get('youtube_include_dash_manifest', True):\n                        ASSETS_RE = r'\"assets\":.+?\"js\":\\s*(\"[^\"]+\")'\n                        jsplayer_url_json = self._search_regex(\n                            ASSETS_RE,\n                            embed_webpage if age_gate else video_webpage,\n                            'JS player URL (1)', default=None)\n                        if not jsplayer_url_json and not age_gate:\n                            # We need the embed website after all\n                            if embed_webpage is None:\n                                embed_url = proto + '://www.youtube.com/embed/%s' % video_id\n                                embed_webpage = self._download_webpage(\n                                    embed_url, video_id, 'Downloading embed webpage')\n                            jsplayer_url_json = self._search_regex(\n                                ASSETS_RE, embed_webpage, 'JS player URL')\n\n                        player_url = json.loads(jsplayer_url_json)\n                        if player_url is None:\n                            player_url_json = self._search_regex(\n                                r'ytplayer\\.config.*?\"url\"\\s*:\\s*(\"[^\"]+\")',\n                                video_webpage, 'age gate player URL')\n                            player_url = json.loads(player_url_json)\n\n                    if 'sig' in url_data:\n                        url += '&signature=' + url_data['sig'][0]\n                    elif 's' in url_data:\n                        encrypted_sig = url_data['s'][0]\n\n                        if self._downloader.params.get('verbose'):\n                            if player_url is None:\n                                player_desc = 'unknown'\n                            else:\n                                player_type, player_version = self._extract_player_info(player_url)\n                                player_desc = '%s player %s' % ('flash' if player_type == 'swf' else 'html5', player_version)\n                            parts_sizes = self._signature_cache_id(encrypted_sig)\n                            self.to_screen('{%s} signature length %s, %s' %\n                                           (format_id, parts_sizes, player_desc))\n\n                        signature = self._decrypt_signature(\n                            encrypted_sig, video_id, player_url, age_gate)\n                        sp = try_get(url_data, lambda x: x['sp'][0], compat_str) or 'signature'\n                        url += '&%s=%s' % (sp, signature)\n                if 'ratebypass' not in url:\n                    url += '&ratebypass=yes'\n\n                dct = {\n                    'format_id': format_id,\n                    'url': url,\n                    'player_url': player_url,\n                }\n                if format_id in self._formats:\n                    dct.update(self._formats[format_id])\n                if format_id in formats_spec:\n                    dct.update(formats_spec[format_id])\n\n                # Some itags are not included in DASH manifest thus corresponding formats will\n                # lack metadata (see https://github.com/ytdl-org/youtube-dl/pull/5993).\n                # Trying to extract metadata from url_encoded_fmt_stream_map entry.\n                mobj = re.search(r'^(?P<width>\\d+)[xX](?P<height>\\d+)$', url_data.get('size', [''])[0])\n                width, height = (int(mobj.group('width')), int(mobj.group('height'))) if mobj else (None, None)\n\n                if width is None:\n                    width = int_or_none(fmt.get('width'))\n                if height is None:\n                    height = int_or_none(fmt.get('height'))\n\n                filesize = int_or_none(url_data.get(\n                    'clen', [None])[0]) or _extract_filesize(url)\n\n                quality = url_data.get('quality', [None])[0] or fmt.get('quality')\n                quality_label = url_data.get('quality_label', [None])[0] or fmt.get('qualityLabel')\n\n                tbr = (float_or_none(url_data.get('bitrate', [None])[0], 1000)\n                       or float_or_none(fmt.get('bitrate'), 1000)) if format_id != '43' else None\n                fps = int_or_none(url_data.get('fps', [None])[0]) or int_or_none(fmt.get('fps'))\n\n                more_fields = {\n                    'filesize': filesize,\n                    'tbr': tbr,\n                    'width': width,\n                    'height': height,\n                    'fps': fps,\n                    'format_note': quality_label or quality,\n                }\n                for key, value in more_fields.items():\n                    if value:\n                        dct[key] = value\n                type_ = url_data.get('type', [None])[0] or fmt.get('mimeType')\n                if type_:\n                    type_split = type_.split(';')\n                    kind_ext = type_split[0].split('/')\n                    if len(kind_ext) == 2:\n                        kind, _ = kind_ext\n                        dct['ext'] = mimetype2ext(type_split[0])\n                        if kind in ('audio', 'video'):\n                            codecs = None\n                            for mobj in re.finditer(\n                                    r'(?P<key>[a-zA-Z_-]+)=(?P<quote>[\"\\']?)(?P<val>.+?)(?P=quote)(?:;|$)', type_):\n                                if mobj.group('key') == 'codecs':\n                                    codecs = mobj.group('val')\n                                    break\n                            if codecs:\n                                dct.update(parse_codecs(codecs))\n                if dct.get('acodec') == 'none' or dct.get('vcodec') == 'none':\n                    dct['downloader_options'] = {\n                        # Youtube throttles chunks >~10M\n                        'http_chunk_size': 10485760,\n                    }\n                formats.append(dct)\n        else:\n            manifest_url = (\n                url_or_none(try_get(\n                    player_response,\n                    lambda x: x['streamingData']['hlsManifestUrl'],\n                    compat_str))\n                or url_or_none(try_get(\n                    video_info, lambda x: x['hlsvp'][0], compat_str)))\n            if manifest_url:\n                formats = []\n                m3u8_formats = self._extract_m3u8_formats(\n                    manifest_url, video_id, 'mp4', fatal=False)\n                for a_format in m3u8_formats:\n                    itag = self._search_regex(\n                        r'/itag/(\\d+)/', a_format['url'], 'itag', default=None)\n                    if itag:\n                        a_format['format_id'] = itag\n                        if itag in self._formats:\n                            dct = self._formats[itag].copy()\n                            dct.update(a_format)\n                            a_format = dct\n                    a_format['player_url'] = player_url\n                    # Accept-Encoding header causes failures in live streams on Youtube and Youtube Gaming\n                    a_format.setdefault('http_headers', {})['Youtubedl-no-compression'] = 'True'\n                    formats.append(a_format)\n            else:\n                error_message = extract_unavailable_message()\n                if not error_message:\n                    error_message = clean_html(try_get(\n                        player_response, lambda x: x['playabilityStatus']['reason'],\n                        compat_str))\n                if not error_message:\n                    error_message = clean_html(\n                        try_get(video_info, lambda x: x['reason'][0], compat_str))\n                if error_message:\n                    raise ExtractorError(error_message, expected=True)\n                raise ExtractorError('no conn, hlsvp, hlsManifestUrl or url_encoded_fmt_stream_map information found in video info')\n\n        # uploader\n        video_uploader = try_get(\n            video_info, lambda x: x['author'][0],\n            compat_str) or str_or_none(video_details.get('author'))\n        if video_uploader:\n            video_uploader = compat_urllib_parse_unquote_plus(video_uploader)\n        else:\n            self._downloader.report_warning('unable to extract uploader name')\n\n        # uploader_id\n        video_uploader_id = None\n        video_uploader_url = None\n        mobj = re.search(\n            r'<link itemprop=\"url\" href=\"(?P<uploader_url>https?://www\\.youtube\\.com/(?:user|channel)/(?P<uploader_id>[^\"]+))\">',\n            video_webpage)\n        if mobj is not None:\n            video_uploader_id = mobj.group('uploader_id')\n            video_uploader_url = mobj.group('uploader_url')\n        else:\n            self._downloader.report_warning('unable to extract uploader nickname')\n\n        channel_id = (\n            str_or_none(video_details.get('channelId'))\n            or self._html_search_meta(\n                'channelId', video_webpage, 'channel id', default=None)\n            or self._search_regex(\n                r'data-channel-external-id=([\"\\'])(?P<id>(?:(?!\\1).)+)\\1',\n                video_webpage, 'channel id', default=None, group='id'))\n        channel_url = 'http://www.youtube.com/channel/%s' % channel_id if channel_id else None\n\n        # thumbnail image\n        # We try first to get a high quality image:\n        m_thumb = re.search(r'<span itemprop=\"thumbnail\".*?href=\"(.*?)\">',\n                            video_webpage, re.DOTALL)\n        if m_thumb is not None:\n            video_thumbnail = m_thumb.group(1)\n        elif 'thumbnail_url' not in video_info:\n            self._downloader.report_warning('unable to extract video thumbnail')\n            video_thumbnail = None\n        else:   # don't panic if we can't find it\n            video_thumbnail = compat_urllib_parse_unquote_plus(video_info['thumbnail_url'][0])\n\n        # upload date\n        upload_date = self._html_search_meta(\n            'datePublished', video_webpage, 'upload date', default=None)\n        if not upload_date:\n            upload_date = self._search_regex(\n                [r'(?s)id=\"eow-date.*?>(.*?)</span>',\n                 r'(?:id=\"watch-uploader-info\".*?>.*?|[\"\\']simpleText[\"\\']\\s*:\\s*[\"\\'])(?:Published|Uploaded|Streamed live|Started) on (.+?)[<\"\\']'],\n                video_webpage, 'upload date', default=None)\n        upload_date = unified_strdate(upload_date)\n\n        video_license = self._html_search_regex(\n            r'<h4[^>]+class=\"title\"[^>]*>\\s*License\\s*</h4>\\s*<ul[^>]*>\\s*<li>(.+?)</li',\n            video_webpage, 'license', default=None)\n\n        m_music = re.search(\n            r'''(?x)\n                <h4[^>]+class=\"title\"[^>]*>\\s*Music\\s*</h4>\\s*\n                <ul[^>]*>\\s*\n                <li>(?P<title>.+?)\n                by (?P<creator>.+?)\n                (?:\n                    \\(.+?\\)|\n                    <a[^>]*\n                        (?:\n                            \\bhref=[\"\\']/red[^>]*>|             # drop possible\n                            >\\s*Listen ad-free with YouTube Red # YouTube Red ad\n                        )\n                    .*?\n                )?</li\n            ''',\n            video_webpage)\n        if m_music:\n            video_alt_title = remove_quotes(unescapeHTML(m_music.group('title')))\n            video_creator = clean_html(m_music.group('creator'))\n        else:\n            video_alt_title = video_creator = None\n\n        def extract_meta(field):\n            return self._html_search_regex(\n                r'<h4[^>]+class=\"title\"[^>]*>\\s*%s\\s*</h4>\\s*<ul[^>]*>\\s*<li>(.+?)</li>\\s*' % field,\n                video_webpage, field, default=None)\n\n        track = extract_meta('Song')\n        artist = extract_meta('Artist')\n        album = extract_meta('Album')\n\n        # Youtube Music Auto-generated description\n        release_date = release_year = None\n        if video_description:\n            mobj = re.search(r'(?s)Provided to YouTube by [^\\n]+\\n+(?P<track>[^\u00b7]+)\u00b7(?P<artist>[^\\n]+)\\n+(?P<album>[^\\n]+)(?:.+?\u2117\\s*(?P<release_year>\\d{4})(?!\\d))?(?:.+?Released on\\s*:\\s*(?P<release_date>\\d{4}-\\d{2}-\\d{2}))?(.+?\\nArtist\\s*:\\s*(?P<clean_artist>[^\\n]+))?', video_description)\n            if mobj:\n                if not track:\n                    track = mobj.group('track').strip()\n                if not artist:\n                    artist = mobj.group('clean_artist') or ', '.join(a.strip() for a in mobj.group('artist').split('\u00b7'))\n                if not album:\n                    album = mobj.group('album'.strip())\n                release_year = mobj.group('release_year')\n                release_date = mobj.group('release_date')\n                if release_date:\n                    release_date = release_date.replace('-', '')\n                    if not release_year:\n                        release_year = int(release_date[:4])\n                if release_year:\n                    release_year = int(release_year)\n\n        m_episode = re.search(\n            r'<div[^>]+id=\"watch7-headline\"[^>]*>\\s*<span[^>]*>.*?>(?P<series>[^<]+)</a></b>\\s*S(?P<season>\\d+)\\s*\u2022\\s*E(?P<episode>\\d+)</span>',\n            video_webpage)\n        if m_episode:\n            series = unescapeHTML(m_episode.group('series'))\n            season_number = int(m_episode.group('season'))\n            episode_number = int(m_episode.group('episode'))\n        else:\n            series = season_number = episode_number = None\n\n        m_cat_container = self._search_regex(\n            r'(?s)<h4[^>]*>\\s*Category\\s*</h4>\\s*<ul[^>]*>(.*?)</ul>',\n            video_webpage, 'categories', default=None)\n        if m_cat_container:\n            category = self._html_search_regex(\n                r'(?s)<a[^<]+>(.*?)</a>', m_cat_container, 'category',\n                default=None)\n            video_categories = None if category is None else [category]\n        else:\n            video_categories = None\n\n        video_tags = [\n            unescapeHTML(m.group('content'))\n            for m in re.finditer(self._meta_regex('og:video:tag'), video_webpage)]\n\n        def _extract_count(count_name):\n            return str_to_int(self._search_regex(\n                r'-%s-button[^>]+><span[^>]+class=\"yt-uix-button-content\"[^>]*>([\\d,]+)</span>'\n                % re.escape(count_name),\n                video_webpage, count_name, default=None))\n\n        like_count = _extract_count('like')\n        dislike_count = _extract_count('dislike')\n\n        if view_count is None:\n            view_count = str_to_int(self._search_regex(\n                r'<[^>]+class=[\"\\']watch-view-count[^>]+>\\s*([\\d,\\s]+)', video_webpage,\n                'view count', default=None))\n\n        average_rating = (\n            float_or_none(video_details.get('averageRating'))\n            or try_get(video_info, lambda x: float_or_none(x['avg_rating'][0])))\n\n        # subtitles\n        video_subtitles = self.extract_subtitles(video_id, video_webpage)\n        automatic_captions = self.extract_automatic_captions(video_id, video_webpage)\n\n        video_duration = try_get(\n            video_info, lambda x: int_or_none(x['length_seconds'][0]))\n        if not video_duration:\n            video_duration = int_or_none(video_details.get('lengthSeconds'))\n        if not video_duration:\n            video_duration = parse_duration(self._html_search_meta(\n                'duration', video_webpage, 'video duration'))\n\n        # annotations\n        video_annotations = None\n        if self._downloader.params.get('writeannotations', False):\n            xsrf_token = self._search_regex(\n                r'([\\'\"])XSRF_TOKEN\\1\\s*:\\s*([\\'\"])(?P<xsrf_token>[A-Za-z0-9+/=]+)\\2',\n                video_webpage, 'xsrf token', group='xsrf_token', fatal=False)\n            invideo_url = try_get(\n                player_response, lambda x: x['annotations'][0]['playerAnnotationsUrlsRenderer']['invideoUrl'], compat_str)\n            if xsrf_token and invideo_url:\n                xsrf_field_name = self._search_regex(\n                    r'([\\'\"])XSRF_FIELD_NAME\\1\\s*:\\s*([\\'\"])(?P<xsrf_field_name>\\w+)\\2',\n                    video_webpage, 'xsrf field name',\n                    group='xsrf_field_name', default='session_token')\n                video_annotations = self._download_webpage(\n                    self._proto_relative_url(invideo_url),\n                    video_id, note='Downloading annotations',\n                    errnote='Unable to download video annotations', fatal=False,\n                    data=urlencode_postdata({xsrf_field_name: xsrf_token}))\n\n        chapters = self._extract_chapters(description_original, video_duration)\n\n        # Look for the DASH manifest\n        if self._downloader.params.get('youtube_include_dash_manifest', True):\n            dash_mpd_fatal = True\n            for mpd_url in dash_mpds:\n                dash_formats = {}\n                try:\n                    def decrypt_sig(mobj):\n                        s = mobj.group(1)\n                        dec_s = self._decrypt_signature(s, video_id, player_url, age_gate)\n                        return '/signature/%s' % dec_s\n\n                    mpd_url = re.sub(r'/s/([a-fA-F0-9\\.]+)', decrypt_sig, mpd_url)\n\n                    for df in self._extract_mpd_formats(\n                            mpd_url, video_id, fatal=dash_mpd_fatal,\n                            formats_dict=self._formats):\n                        if not df.get('filesize'):\n                            df['filesize'] = _extract_filesize(df['url'])\n                        # Do not overwrite DASH format found in some previous DASH manifest\n                        if df['format_id'] not in dash_formats:\n                            dash_formats[df['format_id']] = df\n                        # Additional DASH manifests may end up in HTTP Error 403 therefore\n                        # allow them to fail without bug report message if we already have\n                        # some DASH manifest succeeded. This is temporary workaround to reduce\n                        # burst of bug reports until we figure out the reason and whether it\n                        # can be fixed at all.\n                        dash_mpd_fatal = False\n                except (ExtractorError, KeyError) as e:\n                    self.report_warning(\n                        'Skipping DASH manifest: %r' % e, video_id)\n                if dash_formats:\n                    # Remove the formats we found through non-DASH, they\n                    # contain less info and it can be wrong, because we use\n                    # fixed values (for example the resolution). See\n                    # https://github.com/ytdl-org/youtube-dl/issues/5774 for an\n                    # example.\n                    formats = [f for f in formats if f['format_id'] not in dash_formats.keys()]\n                    formats.extend(dash_formats.values())\n\n        # Check for malformed aspect ratio\n        stretched_m = re.search(\n            r'<meta\\s+property=\"og:video:tag\".*?content=\"yt:stretch=(?P<w>[0-9]+):(?P<h>[0-9]+)\">',\n            video_webpage)\n        if stretched_m:\n            w = float(stretched_m.group('w'))\n            h = float(stretched_m.group('h'))\n            # yt:stretch may hold invalid ratio data (e.g. for Q39EVAstoRM ratio is 17:0).\n            # We will only process correct ratios.\n            if w > 0 and h > 0:\n                ratio = w / h\n                for f in formats:\n                    if f.get('vcodec') != 'none':\n                        f['stretched_ratio'] = ratio\n\n        if not formats:\n            if 'reason' in video_info:\n                if 'The uploader has not made this video available in your country.' in video_info['reason']:\n                    regions_allowed = self._html_search_meta(\n                        'regionsAllowed', video_webpage, default=None)\n                    countries = regions_allowed.split(',') if regions_allowed else None\n                    self.raise_geo_restricted(\n                        msg=video_info['reason'][0], countries=countries)\n                reason = video_info['reason'][0]\n                if 'Invalid parameters' in reason:\n                    unavailable_message = extract_unavailable_message()\n                    if unavailable_message:\n                        reason = unavailable_message\n                raise ExtractorError(\n                    'YouTube said: %s' % reason,\n                    expected=True, video_id=video_id)\n            if video_info.get('license_info') or try_get(player_response, lambda x: x['streamingData']['licenseInfos']):\n                raise ExtractorError('This video is DRM protected.', expected=True)\n\n        self._sort_formats(formats)\n\n        self.mark_watched(video_id, video_info, player_response)\n\n        return {\n            'id': video_id,\n            'uploader': video_uploader,\n            'uploader_id': video_uploader_id,\n            'uploader_url': video_uploader_url,\n            'channel_id': channel_id,\n            'channel_url': channel_url,\n            'upload_date': upload_date,\n            'license': video_license,\n            'creator': video_creator or artist,\n            'title': video_title,\n            'alt_title': video_alt_title or track,\n            'thumbnail': video_thumbnail,\n            'description': video_description,\n            'categories': video_categories,\n            'tags': video_tags,\n            'subtitles': video_subtitles,\n            'automatic_captions': automatic_captions,\n            'duration': video_duration,\n            'age_limit': 18 if age_gate else 0,\n            'annotations': video_annotations,\n            'chapters': chapters,\n            'webpage_url': proto + '://www.youtube.com/watch?v=%s' % video_id,\n            'view_count': view_count,\n            'like_count': like_count,\n            'dislike_count': dislike_count,\n            'average_rating': average_rating,\n            'formats': formats,\n            'is_live': is_live,\n            'start_time': start_time,\n            'end_time': end_time,\n            'series': series,\n            'season_number': season_number,\n            'episode_number': episode_number,\n            'track': track,\n            'artist': artist,\n            'album': album,\n            'release_date': release_date,\n            'release_year': release_year,\n        }",
        "begin_line": 1690,
        "end_line": 2445,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.0007513148009015778,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0007513148009015778,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.add_dash_mpd#1731",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.add_dash_mpd(video_info)",
        "snippet": "        def add_dash_mpd(video_info):\n            dash_mpd = video_info.get('dashmpd')\n            if dash_mpd and dash_mpd[0] not in dash_mpds:\n                dash_mpds.append(dash_mpd[0])",
        "begin_line": 1731,
        "end_line": 1734,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000484027105517909,
            "pseudo_dstar_susp": 0.0004329004329004329,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.add_dash_mpd_pr#1736",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.add_dash_mpd_pr(pl_response)",
        "snippet": "        def add_dash_mpd_pr(pl_response):\n            dash_mpd = url_or_none(try_get(\n                pl_response, lambda x: x['streamingData']['dashManifestUrl'],\n                compat_str))\n            if dash_mpd and dash_mpd not in dash_mpds:\n                dash_mpds.append(dash_mpd)",
        "begin_line": 1736,
        "end_line": 1741,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000484027105517909,
            "pseudo_dstar_susp": 0.0004329004329004329,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.extract_view_count#1746",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.extract_view_count(v_info)",
        "snippet": "        def extract_view_count(v_info):\n            return int_or_none(try_get(v_info, lambda x: x['view_count'][0]))",
        "begin_line": 1746,
        "end_line": 1747,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000484027105517909,
            "pseudo_dstar_susp": 0.0004329004329004329,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.extract_player_response#1749",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.extract_player_response(player_response, video_id)",
        "snippet": "        def extract_player_response(player_response, video_id):\n            pl_response = str_or_none(player_response)\n            if not pl_response:\n                return\n            pl_response = self._parse_json(pl_response, video_id, fatal=False)\n            if isinstance(pl_response, dict):\n                add_dash_mpd_pr(pl_response)\n                return pl_response",
        "begin_line": 1749,
        "end_line": 1756,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000484027105517909,
            "pseudo_dstar_susp": 0.0004329004329004329,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.extract_unavailable_message#1812",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.extract_unavailable_message()",
        "snippet": "        def extract_unavailable_message():\n            messages = []\n            for tag, kind in (('h1', 'message'), ('div', 'submessage')):\n                msg = self._html_search_regex(\n                    r'(?s)<{tag}[^>]+id=[\"\\']unavailable-{kind}[\"\\'][^>]*>(.+?)</{tag}>'.format(tag=tag, kind=kind),\n                    video_webpage, 'unavailable %s' % kind, default=None)\n                if msg:\n                    messages.append(msg)\n            if messages:\n                return '\\n'.join(messages)",
        "begin_line": 1812,
        "end_line": 1821,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.0007513148009015778,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0007513148009015778,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_initialize#2645",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 2645,
        "end_line": 2646,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00031201248049921997,
            "pseudo_tarantula_susp": 0.0007326007326007326,
            "pseudo_op2_susp": 0.00031201248049921997,
            "pseudo_barinel_susp": 0.000744047619047619
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE.extract_videos_from_page#2648",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE.extract_videos_from_page(self, page)",
        "snippet": "    def extract_videos_from_page(self, page):\n        ids_in_page = []\n        titles_in_page = []\n\n        for item in re.findall(\n                r'(<[^>]*\\bdata-video-id\\s*=\\s*[\"\\'][0-9A-Za-z_-]{11}[^>]+>)', page):\n            attrs = extract_attributes(item)\n            video_id = attrs['data-video-id']\n            video_title = unescapeHTML(attrs.get('data-title'))\n            if video_title:\n                video_title = video_title.strip()\n            ids_in_page.append(video_id)\n            titles_in_page.append(video_title)\n\n        # Fallback with old _VIDEO_RE\n        self.extract_videos_from_page_impl(\n            self._VIDEO_RE, page, ids_in_page, titles_in_page)\n\n        # Relaxed fallbacks\n        self.extract_videos_from_page_impl(\n            r'href=\"\\s*/watch\\?v\\s*=\\s*(?P<id>[0-9A-Za-z_-]{11})', page,\n            ids_in_page, titles_in_page)\n        self.extract_videos_from_page_impl(\n            r'data-video-ids\\s*=\\s*[\"\\'](?P<id>[0-9A-Za-z_-]{11})', page,\n            ids_in_page, titles_in_page)\n\n        return zip(ids_in_page, titles_in_page)",
        "begin_line": 2648,
        "end_line": 2674,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007097232079489,
            "pseudo_dstar_susp": 0.0005530973451327434,
            "pseudo_tarantula_susp": 0.0035335689045936395,
            "pseudo_op2_susp": 0.0005530973451327434,
            "pseudo_barinel_susp": 0.0035335689045936395
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_mix#2676",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_mix(self, playlist_id)",
        "snippet": "    def _extract_mix(self, playlist_id):\n        # The mixes are generated from a single video\n        # the id of the playlist is just 'RD' + video_id\n        ids = []\n        last_id = playlist_id[-11:]\n        for n in itertools.count(1):\n            url = 'https://youtube.com/watch?v=%s&list=%s' % (last_id, playlist_id)\n            webpage = self._download_webpage(\n                url, playlist_id, 'Downloading page {0} of Youtube mix'.format(n))\n            new_ids = orderedSet(re.findall(\n                r'''(?xs)data-video-username=\".*?\".*?\n                           href=\"/watch\\?v=([0-9A-Za-z_-]{11})&amp;[^\"]*?list=%s''' % re.escape(playlist_id),\n                webpage))\n            # Fetch new pages until all the videos are repeated, it seems that\n            # there are always 51 unique videos.\n            new_ids = [_id for _id in new_ids if _id not in ids]\n            if not new_ids:\n                break\n            ids.extend(new_ids)\n            last_id = ids[-1]\n\n        url_results = self._ids_to_results(ids)\n\n        search_title = lambda class_name: get_element_by_attribute('class', class_name, webpage)\n        title_span = (\n            search_title('playlist-title')\n            or search_title('title long-title')\n            or search_title('title'))\n        title = clean_html(title_span)\n\n        return self.playlist_result(url_results, playlist_id, title)",
        "begin_line": 2676,
        "end_line": 2706,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_playlist#2708",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_playlist(self, playlist_id)",
        "snippet": "    def _extract_playlist(self, playlist_id):\n        url = self._TEMPLATE_URL % playlist_id\n        page = self._download_webpage(url, playlist_id)\n\n        # the yt-alert-message now has tabindex attribute (see https://github.com/ytdl-org/youtube-dl/issues/11604)\n        for match in re.findall(r'<div class=\"yt-alert-message\"[^>]*>([^<]+)</div>', page):\n            match = match.strip()\n            # Check if the playlist exists or is private\n            mobj = re.match(r'[^<]*(?:The|This) playlist (?P<reason>does not exist|is private)[^<]*', match)\n            if mobj:\n                reason = mobj.group('reason')\n                message = 'This playlist %s' % reason\n                if 'private' in reason:\n                    message += ', use --username or --netrc to access it'\n                message += '.'\n                raise ExtractorError(message, expected=True)\n            elif re.match(r'[^<]*Invalid parameters[^<]*', match):\n                raise ExtractorError(\n                    'Invalid parameters. Maybe URL is incorrect.',\n                    expected=True)\n            elif re.match(r'[^<]*Choose your language[^<]*', match):\n                continue\n            else:\n                self.report_warning('Youtube gives an alert message: ' + match)\n\n        playlist_title = self._html_search_regex(\n            r'(?s)<h1 class=\"pl-header-title[^\"]*\"[^>]*>\\s*(.*?)\\s*</h1>',\n            page, 'title', default=None)\n\n        _UPLOADER_BASE = r'class=[\"\\']pl-header-details[^>]+>\\s*<li>\\s*<a[^>]+\\bhref='\n        uploader = self._html_search_regex(\n            r'%s[\"\\']/(?:user|channel)/[^>]+>([^<]+)' % _UPLOADER_BASE,\n            page, 'uploader', default=None)\n        mobj = re.search(\n            r'%s([\"\\'])(?P<path>/(?:user|channel)/(?P<uploader_id>.+?))\\1' % _UPLOADER_BASE,\n            page)\n        if mobj:\n            uploader_id = mobj.group('uploader_id')\n            uploader_url = compat_urlparse.urljoin(url, mobj.group('path'))\n        else:\n            uploader_id = uploader_url = None\n\n        has_videos = True\n\n        if not playlist_title:\n            try:\n                # Some playlist URLs don't actually serve a playlist (e.g.\n                # https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4)\n                next(self._entries(page, playlist_id))\n            except StopIteration:\n                has_videos = False\n\n        playlist = self.playlist_result(\n            self._entries(page, playlist_id), playlist_id, playlist_title)\n        playlist.update({\n            'uploader': uploader,\n            'uploader_id': uploader_id,\n            'uploader_url': uploader_url,\n        })\n\n        return has_videos, playlist",
        "begin_line": 2708,
        "end_line": 2768,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007097232079489,
            "pseudo_dstar_susp": 0.0005530973451327434,
            "pseudo_tarantula_susp": 0.0035335689045936395,
            "pseudo_op2_susp": 0.0005530973451327434,
            "pseudo_barinel_susp": 0.0035335689045936395
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._check_download_just_video#2770",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._check_download_just_video(self, url, playlist_id)",
        "snippet": "    def _check_download_just_video(self, url, playlist_id):\n        # Check if it's a video-specific URL\n        query_dict = compat_urlparse.parse_qs(compat_urlparse.urlparse(url).query)\n        video_id = query_dict.get('v', [None])[0] or self._search_regex(\n            r'(?:(?:^|//)youtu\\.be/|youtube\\.com/embed/(?!videoseries))([0-9A-Za-z_-]{11})', url,\n            'video id', default=None)\n        if video_id:\n            if self._downloader.params.get('noplaylist'):\n                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n                return video_id, self.url_result(video_id, 'Youtube', video_id=video_id)\n            else:\n                self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n                return video_id, None\n        return None, None",
        "begin_line": 2770,
        "end_line": 2783,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007309941520467836,
            "pseudo_dstar_susp": 0.0005624296962879641,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005624296962879641,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_extract#2785",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        # Extract playlist id\n        mobj = re.match(self._VALID_URL, url)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n        playlist_id = mobj.group(1) or mobj.group(2)\n\n        video_id, video = self._check_download_just_video(url, playlist_id)\n        if video:\n            return video\n\n        if playlist_id.startswith(('RD', 'UL', 'PU')):\n            # Mixes require a custom extraction process\n            return self._extract_mix(playlist_id)\n\n        has_videos, playlist = self._extract_playlist(playlist_id)\n        if has_videos or not video_id:\n            return playlist\n\n        # Some playlist URLs don't actually serve a playlist (see\n        # https://github.com/ytdl-org/youtube-dl/issues/10537).\n        # Fallback to plain video extraction if there is a video id\n        # along with playlist id.\n        return self.url_result(video_id, 'Youtube', video_id=video_id)",
        "begin_line": 2785,
        "end_line": 2808,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007309941520467836,
            "pseudo_dstar_susp": 0.0005624296962879641,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0005624296962879641,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeChannelIE.suitable#2847",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeChannelIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeChannelIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if YoutubePlaylistsIE.suitable(url) or YoutubeLiveIE.suitable(url)\n                else super(YoutubeChannelIE, cls).suitable(url))",
        "begin_line": 2847,
        "end_line": 2849,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011788282447247436,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeChannelIE._build_template_url#2851",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeChannelIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeChannelIE._build_template_url(self, url, channel_id)",
        "snippet": "    def _build_template_url(self, url, channel_id):\n        return self._TEMPLATE_URL % channel_id",
        "begin_line": 2851,
        "end_line": 2852,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeChannelIE._real_extract#2854",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeChannelIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeChannelIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        channel_id = self._match_id(url)\n\n        url = self._build_template_url(url, channel_id)\n\n        # Channel by page listing is restricted to 35 pages of 30 items, i.e. 1050 videos total (see #5778)\n        # Workaround by extracting as a playlist if managed to obtain channel playlist URL\n        # otherwise fallback on channel by page extraction\n        channel_page = self._download_webpage(\n            url + '?view=57', channel_id,\n            'Downloading channel page', fatal=False)\n        if channel_page is False:\n            channel_playlist_id = False\n        else:\n            channel_playlist_id = self._html_search_meta(\n                'channelId', channel_page, 'channel id', default=None)\n            if not channel_playlist_id:\n                channel_url = self._html_search_meta(\n                    ('al:ios:url', 'twitter:app:url:iphone', 'twitter:app:url:ipad'),\n                    channel_page, 'channel url', default=None)\n                if channel_url:\n                    channel_playlist_id = self._search_regex(\n                        r'vnd\\.youtube://user/([0-9A-Za-z_-]+)',\n                        channel_url, 'channel id', default=None)\n        if channel_playlist_id and channel_playlist_id.startswith('UC'):\n            playlist_id = 'UU' + channel_playlist_id[2:]\n            return self.url_result(\n                compat_urlparse.urljoin(url, '/playlist?list=%s' % playlist_id), 'YoutubePlaylist')\n\n        channel_page = self._download_webpage(url, channel_id, 'Downloading page #1')\n        autogenerated = re.search(r'''(?x)\n                class=\"[^\"]*?(?:\n                    channel-header-autogenerated-label|\n                    yt-channel-title-autogenerated\n                )[^\"]*\"''', channel_page) is not None\n\n        if autogenerated:\n            # The videos are contained in a single page\n            # the ajax pages can't be used, they are empty\n            entries = [\n                self.url_result(\n                    video_id, 'Youtube', video_id=video_id,\n                    video_title=video_title)\n                for video_id, video_title in self.extract_videos_from_page(channel_page)]\n            return self.playlist_result(entries, channel_id)\n\n        try:\n            next(self._entries(channel_page, channel_id))\n        except StopIteration:\n            alert_message = self._html_search_regex(\n                r'(?s)<div[^>]+class=([\"\\']).*?\\byt-alert-message\\b.*?\\1[^>]*>(?P<alert>[^<]+)</div>',\n                channel_page, 'alert', default=None, group='alert')\n            if alert_message:\n                raise ExtractorError('Youtube said: %s' % alert_message, expected=True)\n\n        return self.playlist_result(self._entries(channel_page, channel_id), channel_id)",
        "begin_line": 2854,
        "end_line": 2909,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0005073566717402334,
            "pseudo_dstar_susp": 0.00044802867383512545,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00044802867383512545,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeUserIE.suitable#2954",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeUserIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeUserIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        # Don't return True if the url can be extracted with other youtube\n        # extractor, the regex would is too permissive and it would match.\n        other_yt_ies = iter(klass for (name, klass) in globals().items() if name.startswith('Youtube') and name.endswith('IE') and klass is not cls)\n        if any(ie.suitable(url) for ie in other_yt_ies):\n            return False\n        else:\n            return super(YoutubeUserIE, cls).suitable(url)",
        "begin_line": 2954,
        "end_line": 2961,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeUserIE._build_template_url#2963",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeUserIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeUserIE._build_template_url(self, url, channel_id)",
        "snippet": "    def _build_template_url(self, url, channel_id):\n        mobj = re.match(self._VALID_URL, url)\n        return self._TEMPLATE_URL % (mobj.group('user') or 'user', mobj.group('id'))",
        "begin_line": 2963,
        "end_line": 2965,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00043975373790677223,
            "pseudo_dstar_susp": 0.0003944773175542406,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003944773175542406,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeLiveIE._real_extract#3004",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeLiveIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeLiveIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        channel_id = mobj.group('id')\n        base_url = mobj.group('base_url')\n        webpage = self._download_webpage(url, channel_id, fatal=False)\n        if webpage:\n            page_type = self._og_search_property(\n                'type', webpage, 'page type', default='')\n            video_id = self._html_search_meta(\n                'videoId', webpage, 'video id', default=None)\n            if page_type.startswith('video') and video_id and re.match(\n                    r'^[0-9A-Za-z_-]{11}$', video_id):\n                return self.url_result(video_id, YoutubeIE.ie_key())\n        return self.url_result(base_url)",
        "begin_line": 3004,
        "end_line": 3017,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeSearchIE._get_n_results#3065",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeSearchIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeSearchIE._get_n_results(self, query, n)",
        "snippet": "    def _get_n_results(self, query, n):\n        \"\"\"Get a specified number of results for a query\"\"\"\n\n        videos = []\n        limit = n\n\n        url_query = {\n            'search_query': query.encode('utf-8'),\n        }\n        url_query.update(self._EXTRA_QUERY_ARGS)\n        result_url = 'https://www.youtube.com/results?' + compat_urllib_parse_urlencode(url_query)\n\n        for pagenum in itertools.count(1):\n            data = self._download_json(\n                result_url, video_id='query \"%s\"' % query,\n                note='Downloading page %s' % pagenum,\n                errnote='Unable to download API page',\n                query={'spf': 'navigate'})\n            html_content = data[1]['body']['content']\n\n            if 'class=\"search-message' in html_content:\n                raise ExtractorError(\n                    '[youtube] No video results', expected=True)\n\n            new_videos = list(self._process_page(html_content))\n            videos += new_videos\n            if not new_videos or len(videos) > limit:\n                break\n            next_link = self._html_search_regex(\n                r'href=\"(/results\\?[^\"]*\\bsp=[^\"]+)\"[^>]*>\\s*<span[^>]+class=\"[^\"]*\\byt-uix-button-content\\b[^\"]*\"[^>]*>Next',\n                html_content, 'next link', default=None)\n            if next_link is None:\n                break\n            result_url = compat_urlparse.urljoin('https://www.youtube.com/', next_link)\n\n        if len(videos) > n:\n            videos = videos[:n]\n        return self.playlist_result(videos, query)",
        "begin_line": 3065,
        "end_line": 3102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeSearchURLIE._real_extract#3127",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeSearchURLIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeSearchURLIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        query = compat_urllib_parse_unquote_plus(mobj.group('query'))\n        webpage = self._download_webpage(url, query)\n        return self.playlist_result(self._process_page(webpage), playlist_title=query)",
        "begin_line": 3127,
        "end_line": 3131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeShowIE._real_extract#3147",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeShowIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeShowIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n        return super(YoutubeShowIE, self)._real_extract(\n            'https://www.youtube.com/show/%s/playlists' % playlist_id)",
        "begin_line": 3147,
        "end_line": 3150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor.IE_NAME#3161",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor.IE_NAME(self)",
        "snippet": "    def IE_NAME(self):\n        return 'youtube:%s' % self._FEED_NAME",
        "begin_line": 3161,
        "end_line": 3162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018294914013904133,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor._entries#3167",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor._entries(self, page)",
        "snippet": "    def _entries(self, page):\n        # The extraction process is the same as for playlists, but the regex\n        # for the video ids doesn't contain an index\n        ids = []\n        more_widget_html = content_html = page\n        for page_num in itertools.count(1):\n            matches = re.findall(r'href=\"\\s*/watch\\?v=([0-9A-Za-z_-]{11})', content_html)\n\n            # 'recommended' feed has infinite 'load more' and each new portion spins\n            # the same videos in (sometimes) slightly different order, so we'll check\n            # for unicity and break when portion has no new videos\n            new_ids = list(filter(lambda video_id: video_id not in ids, orderedSet(matches)))\n            if not new_ids:\n                break\n\n            ids.extend(new_ids)\n\n            for entry in self._ids_to_results(new_ids):\n                yield entry\n\n            mobj = re.search(r'data-uix-load-more-href=\"/?(?P<more>[^\"]+)\"', more_widget_html)\n            if not mobj:\n                break\n\n            more = self._download_json(\n                'https://youtube.com/%s' % mobj.group('more'), self._PLAYLIST_TITLE,\n                'Downloading page #%s' % page_num,\n                transform_source=uppercase_escape)\n            content_html = more['content_html']\n            more_widget_html = more['load_more_widget_html']",
        "begin_line": 3167,
        "end_line": 3196,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00018294914013904133,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor._real_extract#3198",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        page = self._download_webpage(\n            'https://www.youtube.com/feed/%s' % self._FEED_NAME,\n            self._PLAYLIST_TITLE)\n        return self.playlist_result(\n            self._entries(page), playlist_title=self._PLAYLIST_TITLE)",
        "begin_line": 3198,
        "end_line": 3203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeWatchLaterIE._real_extract#3219",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeWatchLaterIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeWatchLaterIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        _, video = self._check_download_just_video(url, 'WL')\n        if video:\n            return video\n        _, playlist = self._extract_playlist('WL')\n        return playlist",
        "begin_line": 3219,
        "end_line": 3224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeFavouritesIE._real_extract#3233",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeFavouritesIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeFavouritesIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        webpage = self._download_webpage('https://www.youtube.com/my_favorites', 'Youtube Favourites videos')\n        playlist_id = self._search_regex(r'list=(.+?)[\"&]', webpage, 'favourites playlist id')\n        return self.url_result(playlist_id, 'YoutubePlaylist')",
        "begin_line": 3233,
        "end_line": 3236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeTruncatedURLIE._real_extract#3299",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeTruncatedURLIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeTruncatedURLIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        raise ExtractorError(\n            'Did you forget to quote the URL? Remember that & is a meta '\n            'character in most shells, so you want to put the URL in quotes, '\n            'like  youtube-dl '\n            '\"https://www.youtube.com/watch?feature=foo&v=BaW_jenozKc\" '\n            ' or simply  youtube-dl BaW_jenozKc  .',\n            expected=True)",
        "begin_line": 3299,
        "end_line": 3306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeTruncatedIDIE._real_extract#3319",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeTruncatedIDIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeTruncatedIDIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        raise ExtractorError(\n            'Incomplete YouTube ID %s. URL %s looks truncated.' % (video_id, url),\n            expected=True)",
        "begin_line": 3319,
        "end_line": 3323,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00014259232853272493,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.dailymotion.DailymotionIE._extract_urls#194",
        "src_path": "youtube_dl/extractor/dailymotion.py",
        "class_name": "youtube_dl.extractor.dailymotion.DailymotionIE",
        "signature": "youtube_dl.extractor.dailymotion.DailymotionIE._extract_urls(webpage)",
        "snippet": "    def _extract_urls(webpage):\n        urls = []\n        # Look for embedded Dailymotion player\n        # https://developer.dailymotion.com/player#player-parameters\n        for mobj in re.finditer(\n                r'<(?:(?:embed|iframe)[^>]+?src=|input[^>]+id=[\\'\"]dmcloudUrlEmissionSelect[\\'\"][^>]+value=)([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?dailymotion\\.com/(?:embed|swf)/video/.+?)\\1', webpage):\n            urls.append(unescapeHTML(mobj.group('url')))\n        for mobj in re.finditer(\n                r'(?s)DM\\.player\\([^,]+,\\s*{.*?video[\\'\"]?\\s*:\\s*[\"\\']?(?P<id>[0-9a-zA-Z]+).+?}\\s*\\);', webpage):\n            urls.append('https://www.dailymotion.com/embed/video/' + mobj.group('id'))\n        return urls",
        "begin_line": 194,
        "end_line": 204,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.dailymotion.DailymotionIE._real_extract#206",
        "src_path": "youtube_dl/extractor/dailymotion.py",
        "class_name": "youtube_dl.extractor.dailymotion.DailymotionIE",
        "signature": "youtube_dl.extractor.dailymotion.DailymotionIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id, playlist_id = re.match(self._VALID_URL, url).groups()\n\n        if playlist_id:\n            if not self._downloader.params.get('noplaylist'):\n                self.to_screen('Downloading playlist %s - add --no-playlist to just download video' % playlist_id)\n                return self.url_result(\n                    'http://www.dailymotion.com/playlist/' + playlist_id,\n                    'DailymotionPlaylist', playlist_id)\n            self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n\n        password = self._downloader.params.get('videopassword')\n        media = self._call_api(\n            'media', video_id, '''... on Video {\n      %s\n      stats {\n        likes {\n          total\n        }\n        views {\n          total\n        }\n      }\n    }\n    ... on Live {\n      %s\n      audienceCount\n      isOnAir\n    }''' % (self._COMMON_MEDIA_FIELDS, self._COMMON_MEDIA_FIELDS), 'Downloading media JSON metadata',\n            'password: \"%s\"' % self._downloader.params.get('videopassword') if password else None)\n        xid = media['xid']\n\n        metadata = self._download_json(\n            'https://www.dailymotion.com/player/metadata/video/' + xid,\n            xid, 'Downloading metadata JSON',\n            query={'app': 'com.dailymotion.neon'})\n\n        error = metadata.get('error')\n        if error:\n            title = error.get('title') or error['raw_message']\n            # See https://developer.dailymotion.com/api#access-error\n            if error.get('code') == 'DM007':\n                allowed_countries = try_get(media, lambda x: x['geoblockedCountries']['allowed'], list)\n                self.raise_geo_restricted(msg=title, countries=allowed_countries)\n            raise ExtractorError(\n                '%s said: %s' % (self.IE_NAME, title), expected=True)\n\n        title = metadata['title']\n        is_live = media.get('isOnAir')\n        formats = []\n        for quality, media_list in metadata['qualities'].items():\n            for m in media_list:\n                media_url = m.get('url')\n                media_type = m.get('type')\n                if not media_url or media_type == 'application/vnd.lumberjack.manifest':\n                    continue\n                if media_type == 'application/x-mpegURL':\n                    formats.extend(self._extract_m3u8_formats(\n                        media_url, video_id, 'mp4',\n                        'm3u8' if is_live else 'm3u8_native',\n                        m3u8_id='hls', fatal=False))\n                else:\n                    f = {\n                        'url': media_url,\n                        'format_id': 'http-' + quality,\n                    }\n                    m = re.search(r'/H264-(\\d+)x(\\d+)(?:-(60)/)?', media_url)\n                    if m:\n                        width, height, fps = map(int_or_none, m.groups())\n                        f.update({\n                            'fps': fps,\n                            'height': height,\n                            'width': width,\n                        })\n                    formats.append(f)\n        for f in formats:\n            f['url'] = f['url'].split('#')[0]\n            if not f.get('fps') and f['format_id'].endswith('@60'):\n                f['fps'] = 60\n        self._sort_formats(formats)\n\n        subtitles = {}\n        subtitles_data = try_get(metadata, lambda x: x['subtitles']['data'], dict) or {}\n        for subtitle_lang, subtitle in subtitles_data.items():\n            subtitles[subtitle_lang] = [{\n                'url': subtitle_url,\n            } for subtitle_url in subtitle.get('urls', [])]\n\n        thumbnails = []\n        for height, poster_url in metadata.get('posters', {}).items():\n            thumbnails.append({\n                'height': int_or_none(height),\n                'id': height,\n                'url': poster_url,\n            })\n\n        owner = metadata.get('owner') or {}\n        stats = media.get('stats') or {}\n        get_count = lambda x: int_or_none(try_get(stats, lambda y: y[x + 's']['total']))\n\n        return {\n            'id': video_id,\n            'title': self._live_title(title) if is_live else title,\n            'description': clean_html(media.get('description')),\n            'thumbnails': thumbnails,\n            'duration': int_or_none(metadata.get('duration')) or None,\n            'timestamp': int_or_none(metadata.get('created_time')),\n            'uploader': owner.get('screenname'),\n            'uploader_id': owner.get('id') or metadata.get('screenname'),\n            'age_limit': 18 if metadata.get('explicit') else 0,\n            'tags': metadata.get('tags'),\n            'view_count': get_count('view') or int_or_none(media.get('audienceCount')),\n            'like_count': get_count('like'),\n            'formats': formats,\n            'subtitles': subtitles,\n            'is_live': is_live,\n        }",
        "begin_line": 206,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.markiza.MarkizaPageIE.suitable#107",
        "src_path": "youtube_dl/extractor/markiza.py",
        "class_name": "youtube_dl.extractor.markiza.MarkizaPageIE",
        "signature": "youtube_dl.extractor.markiza.MarkizaPageIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if MarkizaIE.suitable(url) else super(MarkizaPageIE, cls).suitable(url)",
        "begin_line": 107,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003104625892579944,
            "pseudo_dstar_susp": 0.00036036036036036037,
            "pseudo_tarantula_susp": 0.0002864508736751647,
            "pseudo_op2_susp": 0.00036036036036036037,
            "pseudo_barinel_susp": 0.0002864508736751647
        }
    },
    {
        "name": "youtube_dl.extractor.linkedin.LinkedInLearningCourseIE.suitable#153",
        "src_path": "youtube_dl/extractor/linkedin.py",
        "class_name": "youtube_dl.extractor.linkedin.LinkedInLearningCourseIE",
        "signature": "youtube_dl.extractor.linkedin.LinkedInLearningCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if LinkedInLearningIE.suitable(url) else super(LinkedInLearningCourseIE, cls).suitable(url)",
        "begin_line": 153,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0002897710808461316,
            "pseudo_dstar_susp": 0.00033090668431502316,
            "pseudo_tarantula_susp": 0.0002752546105147261,
            "pseudo_op2_susp": 0.00033090668431502316,
            "pseudo_barinel_susp": 0.0002729257641921397
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPOIE.suitable#177",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPOIE",
        "signature": "youtube_dl.extractor.npo.NPOIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return (False if any(ie.suitable(url)\n                for ie in (NPOLiveIE, NPORadioIE, NPORadioFragmentIE))\n                else super(NPOIE, cls).suitable(url))",
        "begin_line": 177,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000333000333000333,
            "pseudo_dstar_susp": 0.000395882818685669,
            "pseudo_tarantula_susp": 0.0002774694783573807,
            "pseudo_op2_susp": 0.000395882818685669,
            "pseudo_barinel_susp": 0.0002774694783573807
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPOIE._get_old_info#299",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPOIE",
        "signature": "youtube_dl.extractor.npo.NPOIE._get_old_info(self, video_id)",
        "snippet": "    def _get_old_info(self, video_id):\n        metadata = self._download_json(\n            'http://e.omroep.nl/metadata/%s' % video_id,\n            video_id,\n            # We have to remove the javascript callback\n            transform_source=strip_jsonp,\n        )\n\n        error = metadata.get('error')\n        if error:\n            raise ExtractorError(error, expected=True)\n\n        # For some videos actual video id (prid) is different (e.g. for\n        # http://www.omroepwnl.nl/video/fragment/vandaag-de-dag-verkiezingen__POMS_WNL_853698\n        # video id is POMS_WNL_853698 but prid is POW_00996502)\n        video_id = metadata.get('prid') or video_id\n\n        # titel is too generic in some cases so utilize aflevering_titel as well\n        # when available (e.g. http://tegenlicht.vpro.nl/afleveringen/2014-2015/access-to-africa.html)\n        title = metadata['titel']\n        sub_title = metadata.get('aflevering_titel')\n        if sub_title and sub_title != title:\n            title += ': %s' % sub_title\n\n        token = self._get_token(video_id)\n\n        formats = []\n        urls = set()\n\n        def is_legal_url(format_url):\n            return format_url and format_url not in urls and re.match(\n                r'^(?:https?:)?//', format_url)\n\n        QUALITY_LABELS = ('Laag', 'Normaal', 'Hoog')\n        QUALITY_FORMATS = ('adaptive', 'wmv_sb', 'h264_sb', 'wmv_bb', 'h264_bb', 'wvc1_std', 'h264_std')\n\n        quality_from_label = qualities(QUALITY_LABELS)\n        quality_from_format_id = qualities(QUALITY_FORMATS)\n        items = self._download_json(\n            'http://ida.omroep.nl/app.php/%s' % video_id, video_id,\n            'Downloading formats JSON', query={\n                'adaptive': 'yes',\n                'token': token,\n            })['items'][0]\n        for num, item in enumerate(items):\n            item_url = item.get('url')\n            if not is_legal_url(item_url):\n                continue\n            urls.add(item_url)\n            format_id = self._search_regex(\n                r'video/ida/([^/]+)', item_url, 'format id',\n                default=None)\n\n            item_label = item.get('label')\n\n            def add_format_url(format_url):\n                width = int_or_none(self._search_regex(\n                    r'(\\d+)[xX]\\d+', format_url, 'width', default=None))\n                height = int_or_none(self._search_regex(\n                    r'\\d+[xX](\\d+)', format_url, 'height', default=None))\n                if item_label in QUALITY_LABELS:\n                    quality = quality_from_label(item_label)\n                    f_id = item_label\n                elif item_label in QUALITY_FORMATS:\n                    quality = quality_from_format_id(format_id)\n                    f_id = format_id\n                else:\n                    quality, f_id = [None] * 2\n                formats.append({\n                    'url': format_url,\n                    'format_id': f_id,\n                    'width': width,\n                    'height': height,\n                    'quality': quality,\n                })\n\n            # Example: http://www.npo.nl/de-nieuwe-mens-deel-1/21-07-2010/WO_VPRO_043706\n            if item.get('contentType') in ('url', 'audio'):\n                add_format_url(item_url)\n                continue\n\n            try:\n                stream_info = self._download_json(\n                    item_url + '&type=json', video_id,\n                    'Downloading %s stream JSON'\n                    % item_label or item.get('format') or format_id or num)\n            except ExtractorError as ee:\n                if isinstance(ee.cause, compat_HTTPError) and ee.cause.code == 404:\n                    error = (self._parse_json(\n                        ee.cause.read().decode(), video_id,\n                        fatal=False) or {}).get('errorstring')\n                    if error:\n                        raise ExtractorError(error, expected=True)\n                raise\n            # Stream URL instead of JSON, example: npo:LI_NL1_4188102\n            if isinstance(stream_info, compat_str):\n                if not stream_info.startswith('http'):\n                    continue\n                video_url = stream_info\n            # JSON\n            else:\n                video_url = stream_info.get('url')\n            if not video_url or 'vodnotavailable.' in video_url or video_url in urls:\n                continue\n            urls.add(video_url)\n            if determine_ext(video_url) == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    video_url, video_id, ext='mp4',\n                    entry_protocol='m3u8_native', m3u8_id='hls', fatal=False))\n            else:\n                add_format_url(video_url)\n\n        is_live = metadata.get('medium') == 'live'\n\n        if not is_live:\n            for num, stream in enumerate(metadata.get('streams', [])):\n                stream_url = stream.get('url')\n                if not is_legal_url(stream_url):\n                    continue\n                urls.add(stream_url)\n                # smooth streaming is not supported\n                stream_type = stream.get('type', '').lower()\n                if stream_type in ['ss', 'ms']:\n                    continue\n                if stream_type == 'hds':\n                    f4m_formats = self._extract_f4m_formats(\n                        stream_url, video_id, fatal=False)\n                    # f4m downloader downloads only piece of live stream\n                    for f4m_format in f4m_formats:\n                        f4m_format['preference'] = -1\n                    formats.extend(f4m_formats)\n                elif stream_type == 'hls':\n                    formats.extend(self._extract_m3u8_formats(\n                        stream_url, video_id, ext='mp4', fatal=False))\n                # Example: http://www.npo.nl/de-nieuwe-mens-deel-1/21-07-2010/WO_VPRO_043706\n                elif '.asf' in stream_url:\n                    asx = self._download_xml(\n                        stream_url, video_id,\n                        'Downloading stream %d ASX playlist' % num,\n                        transform_source=fix_xml_ampersands, fatal=False)\n                    if not asx:\n                        continue\n                    ref = asx.find('./ENTRY/Ref')\n                    if ref is None:\n                        continue\n                    video_url = ref.get('href')\n                    if not video_url or video_url in urls:\n                        continue\n                    urls.add(video_url)\n                    formats.append({\n                        'url': video_url,\n                        'ext': stream.get('formaat', 'asf'),\n                        'quality': stream.get('kwaliteit'),\n                        'preference': -10,\n                    })\n                else:\n                    formats.append({\n                        'url': stream_url,\n                        'quality': stream.get('kwaliteit'),\n                    })\n\n        self._sort_formats(formats)\n\n        subtitles = {}\n        if metadata.get('tt888') == 'ja':\n            subtitles['nl'] = [{\n                'ext': 'vtt',\n                'url': 'http://tt888.omroep.nl/tt888/%s' % video_id,\n            }]\n\n        return {\n            'id': video_id,\n            'title': self._live_title(title) if is_live else title,\n            'description': metadata.get('info'),\n            'thumbnail': metadata.get('images', [{'url': None}])[-1]['url'],\n            'upload_date': unified_strdate(metadata.get('gidsdatum')),\n            'duration': parse_duration(metadata.get('tijdsduur')),\n            'formats': formats,\n            'subtitles': subtitles,\n            'is_live': is_live,\n        }",
        "begin_line": 299,
        "end_line": 479,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPOLiveIE._real_extract#506",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPOLiveIE",
        "signature": "youtube_dl.extractor.npo.NPOLiveIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url) or 'npo-1'\n\n        webpage = self._download_webpage(url, display_id)\n\n        live_id = self._search_regex(\n            [r'media-id=\"([^\"]+)\"', r'data-prid=\"([^\"]+)\"'], webpage, 'live id')\n\n        return {\n            '_type': 'url_transparent',\n            'url': 'npo:%s' % live_id,\n            'ie_key': NPOIE.ie_key(),\n            'id': live_id,\n            'display_id': display_id,\n        }",
        "begin_line": 506,
        "end_line": 520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPORadioIE.suitable#541",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPORadioIE",
        "signature": "youtube_dl.extractor.npo.NPORadioIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if NPORadioFragmentIE.suitable(url) else super(NPORadioIE, cls).suitable(url)",
        "begin_line": 541,
        "end_line": 542,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003104625892579944,
            "pseudo_dstar_susp": 0.00036036036036036037,
            "pseudo_tarantula_susp": 0.0002864508736751647,
            "pseudo_op2_susp": 0.00036036036036036037,
            "pseudo_barinel_susp": 0.0002864508736751647
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPODataMidEmbedIE._real_extract#606",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPODataMidEmbedIE",
        "signature": "youtube_dl.extractor.npo.NPODataMidEmbedIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url)\n        webpage = self._download_webpage(url, display_id)\n        video_id = self._search_regex(\n            r'data-mid=([\"\\'])(?P<id>(?:(?!\\1).)+)\\1', webpage, 'video_id', group='id')\n        return {\n            '_type': 'url_transparent',\n            'ie_key': 'NPO',\n            'url': 'npo:%s' % video_id,\n            'display_id': display_id\n        }",
        "begin_line": 606,
        "end_line": 616,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003736920777279522,
            "pseudo_dstar_susp": 0.0003224766204450177,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.0003224766204450177,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.npo.NPOPlaylistBaseIE._real_extract#660",
        "src_path": "youtube_dl/extractor/npo.py",
        "class_name": "youtube_dl.extractor.npo.NPOPlaylistBaseIE",
        "signature": "youtube_dl.extractor.npo.NPOPlaylistBaseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        playlist_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, playlist_id)\n\n        entries = [\n            self.url_result('npo:%s' % video_id if not video_id.startswith('http') else video_id)\n            for video_id in orderedSet(re.findall(self._PLAYLIST_ENTRY_RE, webpage))\n        ]\n\n        playlist_title = self._html_search_regex(\n            self._PLAYLIST_TITLE_RE, webpage, 'playlist title',\n            default=None) or self._og_search_title(webpage)\n\n        return self.playlist_result(entries, playlist_id, playlist_title)",
        "begin_line": 660,
        "end_line": 674,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0004248088360237893,
            "pseudo_dstar_susp": 0.0004086636697997548,
            "pseudo_tarantula_susp": 0.0016750418760469012,
            "pseudo_op2_susp": 0.0004086636697997548,
            "pseudo_barinel_susp": 0.0016863406408094434
        }
    },
    {
        "name": "youtube_dl.extractor.kuwo.KuwoAlbumIE._real_extract#153",
        "src_path": "youtube_dl/extractor/kuwo.py",
        "class_name": "youtube_dl.extractor.kuwo.KuwoAlbumIE",
        "signature": "youtube_dl.extractor.kuwo.KuwoAlbumIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        album_id = self._match_id(url)\n\n        webpage = self._download_webpage(\n            url, album_id, note='Download album info',\n            errnote='Unable to get album info')\n\n        album_name = self._html_search_regex(\n            r'<div[^>]+class=\"comm\"[^<]+<h1[^>]+title=\"([^\"]+)\"', webpage,\n            'album name')\n        album_intro = remove_start(\n            clean_html(get_element_by_id('intro', webpage)),\n            '%s\u7b80\u4ecb\uff1a' % album_name)\n\n        entries = [\n            self.url_result(song_url, 'Kuwo') for song_url in re.findall(\n                r'<p[^>]+class=\"listen\"><a[^>]+href=\"(http://www\\.kuwo\\.cn/yinyue/\\d+/)\"',\n                webpage)\n        ]\n        return self.playlist_result(entries, album_id, album_name, album_intro)",
        "begin_line": 153,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011499540018399264,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.kuwo.KuwoChartIE._real_extract#187",
        "src_path": "youtube_dl/extractor/kuwo.py",
        "class_name": "youtube_dl.extractor.kuwo.KuwoChartIE",
        "signature": "youtube_dl.extractor.kuwo.KuwoChartIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        chart_id = self._match_id(url)\n        webpage = self._download_webpage(\n            url, chart_id, note='Download chart info',\n            errnote='Unable to get chart info')\n\n        entries = [\n            self.url_result(song_url, 'Kuwo') for song_url in re.findall(\n                r'<a[^>]+href=\"(http://www\\.kuwo\\.cn/yinyue/\\d+)', webpage)\n        ]\n        return self.playlist_result(entries, chart_id)",
        "begin_line": 187,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011532695190866105,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.kuwo.KuwoSingerIE._real_extract#223",
        "src_path": "youtube_dl/extractor/kuwo.py",
        "class_name": "youtube_dl.extractor.kuwo.KuwoSingerIE",
        "signature": "youtube_dl.extractor.kuwo.KuwoSingerIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        singer_id = self._match_id(url)\n        webpage = self._download_webpage(\n            url, singer_id, note='Download singer info',\n            errnote='Unable to get singer info')\n\n        singer_name = self._html_search_regex(\n            r'<h1>([^<]+)</h1>', webpage, 'singer name')\n\n        artist_id = self._html_search_regex(\n            r'data-artistid=\"(\\d+)\"', webpage, 'artist id')\n\n        page_count = int(self._html_search_regex(\n            r'data-page=\"(\\d+)\"', webpage, 'page count'))\n\n        def page_func(page_num):\n            webpage = self._download_webpage(\n                'http://www.kuwo.cn/artist/contentMusicsAjax',\n                singer_id, note='Download song list page #%d' % (page_num + 1),\n                errnote='Unable to get song list page #%d' % (page_num + 1),\n                query={'artistId': artist_id, 'pn': page_num, 'rn': self.PAGE_SIZE})\n\n            return [\n                self.url_result(compat_urlparse.urljoin(url, song_url), 'Kuwo')\n                for song_url in re.findall(\n                    r'<div[^>]+class=\"name\"><a[^>]+href=\"(/yinyue/\\d+)',\n                    webpage)\n            ]\n\n        entries = InAdvancePagedList(page_func, page_count, self.PAGE_SIZE)\n\n        return self.playlist_result(entries, singer_id, singer_name)",
        "begin_line": 223,
        "end_line": 254,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00011615750958299454,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.animeondemand.AnimeOnDemandIE._real_extract#98",
        "src_path": "youtube_dl/extractor/animeondemand.py",
        "class_name": "youtube_dl.extractor.animeondemand.AnimeOnDemandIE",
        "signature": "youtube_dl.extractor.animeondemand.AnimeOnDemandIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        anime_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, anime_id)\n\n        if 'data-playlist=' not in webpage:\n            self._download_webpage(\n                self._APPLY_HTML5_URL, anime_id,\n                'Activating HTML5 beta', 'Unable to apply HTML5 beta')\n            webpage = self._download_webpage(url, anime_id)\n\n        csrf_token = self._html_search_meta(\n            'csrf-token', webpage, 'csrf token', fatal=True)\n\n        anime_title = self._html_search_regex(\n            r'(?s)<h1[^>]+itemprop=\"name\"[^>]*>(.+?)</h1>',\n            webpage, 'anime name')\n        anime_description = self._html_search_regex(\n            r'(?s)<div[^>]+itemprop=\"description\"[^>]*>(.+?)</div>',\n            webpage, 'anime description', default=None)\n\n        entries = []\n\n        def extract_info(html, video_id, num=None):\n            title, description = [None] * 2\n            formats = []\n\n            for input_ in re.findall(\n                    r'<input[^>]+class=[\"\\'].*?streamstarter[^>]+>', html):\n                attributes = extract_attributes(input_)\n                title = attributes.get('data-dialog-header')\n                playlist_urls = []\n                for playlist_key in ('data-playlist', 'data-otherplaylist', 'data-stream'):\n                    playlist_url = attributes.get(playlist_key)\n                    if isinstance(playlist_url, compat_str) and re.match(\n                            r'/?[\\da-zA-Z]+', playlist_url):\n                        playlist_urls.append(attributes[playlist_key])\n                if not playlist_urls:\n                    continue\n\n                lang = attributes.get('data-lang')\n                lang_note = attributes.get('value')\n\n                for playlist_url in playlist_urls:\n                    kind = self._search_regex(\n                        r'videomaterialurl/\\d+/([^/]+)/',\n                        playlist_url, 'media kind', default=None)\n                    format_id_list = []\n                    if lang:\n                        format_id_list.append(lang)\n                    if kind:\n                        format_id_list.append(kind)\n                    if not format_id_list and num is not None:\n                        format_id_list.append(compat_str(num))\n                    format_id = '-'.join(format_id_list)\n                    format_note = ', '.join(filter(None, (kind, lang_note)))\n                    item_id_list = []\n                    if format_id:\n                        item_id_list.append(format_id)\n                    item_id_list.append('videomaterial')\n                    playlist = self._download_json(\n                        urljoin(url, playlist_url), video_id,\n                        'Downloading %s JSON' % ' '.join(item_id_list),\n                        headers={\n                            'X-Requested-With': 'XMLHttpRequest',\n                            'X-CSRF-Token': csrf_token,\n                            'Referer': url,\n                            'Accept': 'application/json, text/javascript, */*; q=0.01',\n                        }, fatal=False)\n                    if not playlist:\n                        continue\n                    stream_url = url_or_none(playlist.get('streamurl'))\n                    if stream_url:\n                        rtmp = re.search(\n                            r'^(?P<url>rtmpe?://(?P<host>[^/]+)/(?P<app>.+/))(?P<playpath>mp[34]:.+)',\n                            stream_url)\n                        if rtmp:\n                            formats.append({\n                                'url': rtmp.group('url'),\n                                'app': rtmp.group('app'),\n                                'play_path': rtmp.group('playpath'),\n                                'page_url': url,\n                                'player_url': 'https://www.anime-on-demand.de/assets/jwplayer.flash-55abfb34080700304d49125ce9ffb4a6.swf',\n                                'rtmp_real_time': True,\n                                'format_id': 'rtmp',\n                                'ext': 'flv',\n                            })\n                            continue\n                    start_video = playlist.get('startvideo', 0)\n                    playlist = playlist.get('playlist')\n                    if not playlist or not isinstance(playlist, list):\n                        continue\n                    playlist = playlist[start_video]\n                    title = playlist.get('title')\n                    if not title:\n                        continue\n                    description = playlist.get('description')\n                    for source in playlist.get('sources', []):\n                        file_ = source.get('file')\n                        if not file_:\n                            continue\n                        ext = determine_ext(file_)\n                        format_id_list = [lang, kind]\n                        if ext == 'm3u8':\n                            format_id_list.append('hls')\n                        elif source.get('type') == 'video/dash' or ext == 'mpd':\n                            format_id_list.append('dash')\n                        format_id = '-'.join(filter(None, format_id_list))\n                        if ext == 'm3u8':\n                            file_formats = self._extract_m3u8_formats(\n                                file_, video_id, 'mp4',\n                                entry_protocol='m3u8_native', m3u8_id=format_id, fatal=False)\n                        elif source.get('type') == 'video/dash' or ext == 'mpd':\n                            continue\n                            file_formats = self._extract_mpd_formats(\n                                file_, video_id, mpd_id=format_id, fatal=False)\n                        else:\n                            continue\n                        for f in file_formats:\n                            f.update({\n                                'language': lang,\n                                'format_note': format_note,\n                            })\n                        formats.extend(file_formats)\n\n            return {\n                'title': title,\n                'description': description,\n                'formats': formats,\n            }\n\n        def extract_entries(html, video_id, common_info, num=None):\n            info = extract_info(html, video_id, num)\n\n            if info['formats']:\n                self._sort_formats(info['formats'])\n                f = common_info.copy()\n                f.update(info)\n                entries.append(f)\n\n            # Extract teaser/trailer only when full episode is not available\n            if not info['formats']:\n                m = re.search(\n                    r'data-dialog-header=([\"\\'])(?P<title>.+?)\\1[^>]+href=([\"\\'])(?P<href>.+?)\\3[^>]*>(?P<kind>Teaser|Trailer)<',\n                    html)\n                if m:\n                    f = common_info.copy()\n                    f.update({\n                        'id': '%s-%s' % (f['id'], m.group('kind').lower()),\n                        'title': m.group('title'),\n                        'url': urljoin(url, m.group('href')),\n                    })\n                    entries.append(f)\n\n        def extract_episodes(html):\n            for num, episode_html in enumerate(re.findall(\n                    r'(?s)<h3[^>]+class=\"episodebox-title\".+?>Episodeninhalt<', html), 1):\n                episodebox_title = self._search_regex(\n                    (r'class=\"episodebox-title\"[^>]+title=([\"\\'])(?P<title>.+?)\\1',\n                     r'class=\"episodebox-title\"[^>]+>(?P<title>.+?)<'),\n                    episode_html, 'episodebox title', default=None, group='title')\n                if not episodebox_title:\n                    continue\n\n                episode_number = int(self._search_regex(\n                    r'(?:Episode|Film)\\s*(\\d+)',\n                    episodebox_title, 'episode number', default=num))\n                episode_title = self._search_regex(\n                    r'(?:Episode|Film)\\s*\\d+\\s*-\\s*(.+)',\n                    episodebox_title, 'episode title', default=None)\n\n                video_id = 'episode-%d' % episode_number\n\n                common_info = {\n                    'id': video_id,\n                    'series': anime_title,\n                    'episode': episode_title,\n                    'episode_number': episode_number,\n                }\n\n                extract_entries(episode_html, video_id, common_info)\n\n        def extract_film(html, video_id):\n            common_info = {\n                'id': anime_id,\n                'title': anime_title,\n                'description': anime_description,\n            }\n            extract_entries(html, video_id, common_info)\n\n        extract_episodes(webpage)\n\n        if not entries:\n            extract_film(webpage, anime_id)\n\n        return self.playlist_result(entries, anime_id, anime_title, anime_description)",
        "begin_line": 98,
        "end_line": 293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003215434083601286,
            "pseudo_dstar_susp": 0.00029342723004694836,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00029342723004694836,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "youtube_dl.extractor.peertube.PeerTubeIE._extract_peertube_url#472",
        "src_path": "youtube_dl/extractor/peertube.py",
        "class_name": "youtube_dl.extractor.peertube.PeerTubeIE",
        "signature": "youtube_dl.extractor.peertube.PeerTubeIE._extract_peertube_url(webpage, source_url)",
        "snippet": "    def _extract_peertube_url(webpage, source_url):\n        mobj = re.match(\n            r'https?://(?P<host>[^/]+)/videos/(?:watch|embed)/(?P<id>%s)'\n            % PeerTubeIE._UUID_RE, source_url)\n        if mobj and any(p in webpage for p in (\n                '<title>PeerTube<',\n                'There will be other non JS-based clients to access PeerTube',\n                '>We are sorry but it seems that PeerTube is not compatible with your web browser.<')):\n            return 'peertube:%s:%s' % mobj.group('host', 'id')",
        "begin_line": 472,
        "end_line": 480,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.peertube.PeerTubeIE._extract_urls#483",
        "src_path": "youtube_dl/extractor/peertube.py",
        "class_name": "youtube_dl.extractor.peertube.PeerTubeIE",
        "signature": "youtube_dl.extractor.peertube.PeerTubeIE._extract_urls(webpage, source_url)",
        "snippet": "    def _extract_urls(webpage, source_url):\n        entries = re.findall(\n            r'''(?x)<iframe[^>]+\\bsrc=[\"\\'](?P<url>(?:https?:)?//%s/videos/embed/%s)'''\n            % (PeerTubeIE._INSTANCES_RE, PeerTubeIE._UUID_RE), webpage)\n        if not entries:\n            peertube_url = PeerTubeIE._extract_peertube_url(webpage, source_url)\n            if peertube_url:\n                entries = [peertube_url]\n        return entries",
        "begin_line": 483,
        "end_line": 491,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00015617679212868969,
            "pseudo_dstar_susp": 0.00015617679212868969,
            "pseudo_tarantula_susp": 0.00015617679212868969,
            "pseudo_op2_susp": 0.00022256843979523704,
            "pseudo_barinel_susp": 0.00015617679212868969
        }
    },
    {
        "name": "youtube_dl.extractor.polskieradio.PolskieRadioCategoryIE.suitable#147",
        "src_path": "youtube_dl/extractor/polskieradio.py",
        "class_name": "youtube_dl.extractor.polskieradio.PolskieRadioCategoryIE",
        "signature": "youtube_dl.extractor.polskieradio.PolskieRadioCategoryIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if PolskieRadioIE.suitable(url) else super(PolskieRadioCategoryIE, cls).suitable(url)",
        "begin_line": 147,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003026634382566586,
            "pseudo_dstar_susp": 0.00033715441672285906,
            "pseudo_tarantula_susp": 0.00028121484814398203,
            "pseudo_op2_susp": 0.00033715441672285906,
            "pseudo_barinel_susp": 0.00028121484814398203
        }
    }
]