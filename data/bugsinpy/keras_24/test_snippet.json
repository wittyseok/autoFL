[
    {
        "name": "tests.keras.test_callbacks.test_TerminateOnNaN#35",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TerminateOnNaN()",
        "snippet": "def test_TerminateOnNaN():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    cbks = [callbacks.TerminateOnNaN()]\n    model = Sequential()\n    initializer = initializers.Constant(value=1e5)\n    for _ in range(5):\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu',\n                        kernel_initializer=initializer))\n    model.add(Dense(num_classes, activation='linear'))\n    model.compile(loss='mean_squared_error',\n                  optimizer='rmsprop')\n\n    # case 1 fit\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n    loss = history.history['loss']\n    assert len(loss) == 1\n    assert loss[0] == np.inf\n\n    # case 2 fit_generator\n    def data_generator():\n        max_batch_index = len(X_train) // batch_size\n        i = 0\n        while 1:\n            yield (X_train[i * batch_size: (i + 1) * batch_size],\n                   y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n    history = model.fit_generator(data_generator(),\n                                  len(X_train),\n                                  validation_data=(X_test, y_test),\n                                  callbacks=cbks,\n                                  epochs=20)\n    loss = history.history['loss']\n    assert len(loss) == 1\n    assert loss[0] == np.inf or np.isnan(loss[0])",
        "begin_line": 35,
        "end_line": 78,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#63",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator()",
        "snippet": "    def data_generator():\n        max_batch_index = len(X_train) // batch_size\n        i = 0\n        while 1:\n            yield (X_train[i * batch_size: (i + 1) * batch_size],\n                   y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 63,
        "end_line": 70,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_stop_training_csv#82",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_stop_training_csv(tmpdir)",
        "snippet": "def test_stop_training_csv(tmpdir):\n    np.random.seed(1337)\n    fp = str(tmpdir / 'test.csv')\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    cbks = [callbacks.TerminateOnNaN(), callbacks.CSVLogger(fp)]\n    model = Sequential()\n    for _ in range(5):\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='linear'))\n    model.compile(loss='mean_squared_error',\n                  optimizer='rmsprop')\n\n    def data_generator():\n        i = 0\n        max_batch_index = len(X_train) // batch_size\n        tot = 0\n        while 1:\n            if tot > 3 * len(X_train):\n                yield np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan\n            else:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            tot += 1\n            i = i % max_batch_index\n\n    history = model.fit_generator(data_generator(),\n                                  len(X_train) // batch_size,\n                                  validation_data=(X_test, y_test),\n                                  callbacks=cbks,\n                                  epochs=20)\n    loss = history.history['loss']\n    assert len(loss) > 1\n    assert loss[-1] == np.inf or np.isnan(loss[-1])\n\n    values = []\n    with open(fp) as f:\n        for x in reader(f):\n            values.append(x)\n\n    assert 'nan' in values[-1], 'The last epoch was not logged.'\n    os.remove(fp)",
        "begin_line": 82,
        "end_line": 130,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#101",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator()",
        "snippet": "    def data_generator():\n        i = 0\n        max_batch_index = len(X_train) // batch_size\n        tot = 0\n        while 1:\n            if tot > 3 * len(X_train):\n                yield np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan\n            else:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            tot += 1\n            i = i % max_batch_index",
        "begin_line": 101,
        "end_line": 113,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ModelCheckpoint#134",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ModelCheckpoint(tmpdir)",
        "snippet": "def test_ModelCheckpoint(tmpdir):\n    np.random.seed(1337)\n    filepath = str(tmpdir / 'checkpoint.h5')\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    # case 1\n    monitor = 'val_loss'\n    save_best_only = False\n    mode = 'auto'\n\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 2\n    mode = 'min'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 3\n    mode = 'max'\n    monitor = 'val_acc'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 4\n    save_best_only = True\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 5\n    save_best_only = False\n    period = 2\n    mode = 'auto'\n    filepath = 'checkpoint.{epoch:02d}.h5'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode,\n                                      period=period)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n    assert os.path.isfile(filepath.format(epoch=2))\n    assert os.path.isfile(filepath.format(epoch=4))\n    assert not os.path.exists(filepath.format(epoch=1))\n    assert not os.path.exists(filepath.format(epoch=3))\n    os.remove(filepath.format(epoch=2))\n    os.remove(filepath.format(epoch=4))\n    assert not tmpdir.listdir()",
        "begin_line": 134,
        "end_line": 207,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping#211",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping()",
        "snippet": "def test_EarlyStopping():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    mode = 'max'\n    monitor = 'val_acc'\n    patience = 0\n    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n\n    mode = 'auto'\n    monitor = 'val_acc'\n    patience = 2\n    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)",
        "begin_line": 211,
        "end_line": 238,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_reuse#242",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_reuse()",
        "snippet": "def test_EarlyStopping_reuse():\n    np.random.seed(1337)\n    patience = 3\n    data = np.random.random((100, 1))\n    labels = np.where(data > 0.5, 1, 0)\n    model = Sequential((\n        Dense(1, input_dim=1, activation='relu'),\n        Dense(1, activation='sigmoid'),\n    ))\n    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n    stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n    weights = model.get_weights()\n\n    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n    assert len(hist.epoch) >= patience\n\n    # This should allow training to go for at least `patience` epochs\n    model.set_weights(weights)\n    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n    assert len(hist.epoch) >= patience",
        "begin_line": 242,
        "end_line": 261,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_patience#265",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_patience()",
        "snippet": "def test_EarlyStopping_patience():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n\n    # Should stop after epoch 3, as the loss has not improved after patience=2 epochs.\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    assert epochs_trained == 3",
        "begin_line": 265,
        "end_line": 286,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_patience#265",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_patience()",
        "snippet": "def test_EarlyStopping_patience():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n\n    # Should stop after epoch 3, as the loss has not improved after patience=2 epochs.\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    assert epochs_trained == 3",
        "begin_line": 265,
        "end_line": 286,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#267",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False",
        "begin_line": 267,
        "end_line": 268,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_baseline#290",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_baseline()",
        "snippet": "def test_EarlyStopping_baseline():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75, patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained\n\n    acc_levels = [0.55, 0.76, 0.81, 0.81]\n    baseline_met = baseline_tester(acc_levels)\n    acc_levels = [0.55, 0.74, 0.81, 0.81]\n    baseline_not_met = baseline_tester(acc_levels)\n\n    # All epochs should run because baseline was met in second epoch\n    assert baseline_met == 4\n    # Baseline was not met by second epoch and should stop\n    assert baseline_not_met == 2",
        "begin_line": 290,
        "end_line": 315,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_baseline#290",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_baseline()",
        "snippet": "def test_EarlyStopping_baseline():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75, patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained\n\n    acc_levels = [0.55, 0.76, 0.81, 0.81]\n    baseline_met = baseline_tester(acc_levels)\n    acc_levels = [0.55, 0.74, 0.81, 0.81]\n    baseline_not_met = baseline_tester(acc_levels)\n\n    # All epochs should run because baseline was met in second epoch\n    assert baseline_met == 4\n    # Baseline was not met by second epoch and should stop\n    assert baseline_not_met == 2",
        "begin_line": 290,
        "end_line": 315,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#292",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False",
        "begin_line": 292,
        "end_line": 293,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.baseline_tester#295",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.baseline_tester(acc_levels)",
        "snippet": "    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75, patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained",
        "begin_line": 295,
        "end_line": 305,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_LearningRateScheduler#319",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_LearningRateScheduler()",
        "snippet": "def test_LearningRateScheduler():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n    assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()",
        "begin_line": 319,
        "end_line": 338,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau#342",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau()",
        "snippet": "def test_ReduceLROnPlateau():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model\n\n    model = make_model()\n\n    # This should reduce the LR after the first epoch (due to high epsilon).\n    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_delta=10, patience=1, cooldown=5)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n    assert np.allclose(float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n\n    model = make_model()\n    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_delta=0, patience=1, cooldown=5)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n    assert np.allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())",
        "begin_line": 342,
        "end_line": 375,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.make_model#352",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.make_model()",
        "snippet": "    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model",
        "begin_line": 352,
        "end_line": 361,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau_patience#379",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 379,
        "end_line": 400,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyOptimizer.test_ReduceLROnPlateau_patience#379",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyOptimizer",
        "signature": "tests.keras.test_callbacks.DummyOptimizer.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 379,
        "end_line": 400,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyOptimizer.__init__#381",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyOptimizer",
        "signature": "tests.keras.test_callbacks.DummyOptimizer.__init__(self)",
        "snippet": "        def __init__(self):\n            self.lr = K.variable(1.0)",
        "begin_line": 381,
        "end_line": 382,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_ReduceLROnPlateau_patience#379",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 379,
        "end_line": 400,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#385",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.optimizer = DummyOptimizer()",
        "begin_line": 385,
        "end_line": 386,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau_backwards_compatibility#404",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau_backwards_compatibility()",
        "snippet": "def test_ReduceLROnPlateau_backwards_compatibility():\n    import warnings\n    with warnings.catch_warnings(record=True) as ws:\n        reduce_on_plateau = callbacks.ReduceLROnPlateau(epsilon=1e-13)\n        # Check if warnings are disabled\n        if os.environ.get(\"PYTHONWARNINGS\") != \"ignore\":\n            assert \"`epsilon` argument is deprecated\" in str(ws[0].message)\n    assert not hasattr(reduce_on_plateau, 'epsilon')\n    assert hasattr(reduce_on_plateau, 'min_delta')\n    assert reduce_on_plateau.min_delta == 1e-13",
        "begin_line": 404,
        "end_line": 413,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_CSVLogger#417",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_CSVLogger(tmpdir)",
        "snippet": "def test_CSVLogger(tmpdir):\n    np.random.seed(1337)\n    filepath = str(tmpdir / 'log.tsv')\n    sep = '\\t'\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model\n\n    # case 1, create new file with defined separator\n    model = make_model()\n    cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n\n    assert os.path.isfile(filepath)\n    with open(filepath) as csvfile:\n        dialect = Sniffer().sniff(csvfile.read())\n    assert dialect.delimiter == sep\n    del model\n    del cbks\n\n    # case 2, append data to existing file, skip header\n    model = make_model()\n    cbks = [callbacks.CSVLogger(filepath, separator=sep, append=True)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n\n    # case 3, reuse of CSVLogger object\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n\n    import re\n    with open(filepath) as csvfile:\n        output = \" \".join(csvfile.readlines())\n        assert len(re.findall('epoch', output)) == 1\n\n    os.remove(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 417,
        "end_line": 469,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.make_model#429",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.make_model()",
        "snippet": "    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model",
        "begin_line": 429,
        "end_line": 438,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard#473",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard(tmpdir)",
        "snippet": "def test_TensorBoard(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim,),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit(X_train, y_train, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test),\n              callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=(X_test, y_test),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 473,
        "end_line": 541,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#486",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 486,
        "end_line": 501,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#513",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq)",
        "snippet": "    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]",
        "begin_line": 513,
        "end_line": 519,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_histogram_freq_must_have_validation_data#547",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir)",
        "snippet": "def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim,),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]\n\n    # fit without validation data should raise ValueError if histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n    assert 'validation_data must be provided' in str(raised_exception.value)\n\n    # fit generator without validation data should raise ValueError if\n    # histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                            callbacks=callbacks_factory(histogram_freq=1))\n    assert 'validation_data must be provided' in str(raised_exception.value)\n\n    # fit generator with validation data generator should raise ValueError if\n    # histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                            validation_data=data_generator(False),\n                            validation_steps=1,\n                            callbacks=callbacks_factory(histogram_freq=1))\n    assert 'validation_data must be provided' in str(raised_exception.value)",
        "begin_line": 547,
        "end_line": 615,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#560",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 560,
        "end_line": 575,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#587",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq)",
        "snippet": "    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]",
        "begin_line": 587,
        "end_line": 593,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_multi_input_output#619",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_multi_input_output(tmpdir)",
        "snippet": "def test_TensorBoard_multi_input_output(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim, input_dim),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n            else:\n                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n            i += 1\n            i = i % max_batch_index\n\n    inp1 = Input((input_dim, input_dim))\n    inp2 = Input((input_dim, input_dim))\n    inp_3d = add([inp1, inp2])\n    inp_2d = GlobalAveragePooling1D()(inp_3d)\n    inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])  # test a layer with a list of output tensors\n    hidden = dot(inp_pair, axes=-1)\n    hidden = Dense(num_hidden, activation='relu')(hidden)\n    hidden = Dropout(0.1)(hidden)\n    output1 = Dense(num_classes, activation='softmax')(hidden)\n    output2 = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0), epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              validation_data=([X_test] * 2, [y_test] * 2),\n              callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=([X_test] * 2, [y_test] * 2),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 619,
        "end_line": 693,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#632",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n            else:\n                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 632,
        "end_line": 647,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#665",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq)",
        "snippet": "    def callbacks_factory(histogram_freq):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=1,\n                                      embeddings_layer_names=['dense_1'],\n                                      batch_size=5)]",
        "begin_line": 665,
        "end_line": 671,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_convnet#697",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_convnet(tmpdir)",
        "snippet": "def test_TensorBoard_convnet(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    input_shape = (16, 16, 3)\n    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n                                                         num_test=200,\n                                                         input_shape=input_shape,\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_train = np_utils.to_categorical(y_train)\n    y_test = np_utils.to_categorical(y_test)\n\n    model = Sequential([\n        Conv2D(filters=8, kernel_size=3,\n               activation='relu',\n               input_shape=input_shape),\n        MaxPooling2D(pool_size=2),\n        Conv2D(filters=4, kernel_size=(3, 3),\n               activation='relu', padding='same'),\n        GlobalAveragePooling2D(),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n                                write_images=True, write_grads=True,\n                                batch_size=16)\n    cbks = [tsb]\n    model.summary()\n    history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n                        validation_data=(x_test, y_test),\n                        callbacks=cbks,\n                        verbose=0)\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 697,
        "end_line": 734,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_CallbackValData#738",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_CallbackValData()",
        "snippet": "def test_CallbackValData():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbk = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=[cbk], epochs=1)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    cbk2 = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n    model.fit_generator(data_generator(True), len(X_train), epochs=1,\n                        validation_data=(X_test, y_test),\n                        callbacks=[cbk2])\n\n    # callback validation data should always have x, y, and sample weights\n    assert len(cbk.validation_data) == len(cbk2.validation_data) == 3\n    assert cbk.validation_data[0] is cbk2.validation_data[0]\n    assert cbk.validation_data[1] is cbk2.validation_data[1]\n    assert cbk.validation_data[2].shape == cbk2.validation_data[2].shape",
        "begin_line": 738,
        "end_line": 783,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#758",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 758,
        "end_line": 772,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_LambdaCallback#787",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_LambdaCallback()",
        "snippet": "def test_LambdaCallback():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # Start an arbitrary process that should run during model training and be terminated after training has completed.\n    def f():\n        while True:\n            pass\n\n    p = multiprocessing.Process(target=f)\n    p.start()\n    cleanup_callback = callbacks.LambdaCallback(on_train_end=lambda logs: p.terminate())\n\n    cbks = [cleanup_callback]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n    p.join()\n    assert not p.is_alive()",
        "begin_line": 787,
        "end_line": 816,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.f#804",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.f()",
        "snippet": "    def f():\n        while True:\n            pass",
        "begin_line": 804,
        "end_line": 806,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_with_ReduceLROnPlateau#820",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_with_ReduceLROnPlateau(tmpdir)",
        "snippet": "def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n    import shutil\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbks = [\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=4,\n            verbose=1),\n        callbacks.TensorBoard(\n            log_dir=filepath)]\n\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 820,
        "end_line": 854,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.tests_RemoteMonitor#858",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.tests_RemoteMonitor()",
        "snippet": "def tests_RemoteMonitor():\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    cbks = [callbacks.RemoteMonitor()]\n\n    with patch('requests.post'):\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)",
        "begin_line": 858,
        "end_line": 876,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.tests_RemoteMonitorWithJsonPayload#880",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.tests_RemoteMonitorWithJsonPayload()",
        "snippet": "def tests_RemoteMonitorWithJsonPayload():\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    cbks = [callbacks.RemoteMonitor(send_as_json=True)]\n\n    with patch('requests.post'):\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)",
        "begin_line": 880,
        "end_line": 898,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.wrapper#11",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.wrapper(*args)",
        "snippet": "    def wrapper(*args):\n        x = args[0]\n        w = args[1]\n        if x.ndim == 3:\n            w = np.flipud(w)\n            w = np.transpose(w, (1, 2, 0))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            w = np.fliplr(np.flipud(w))\n            w = np.transpose(w, (2, 3, 0, 1))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            w = np.flip(np.fliplr(np.flipud(w)), axis=2)\n            w = np.transpose(w, (3, 4, 0, 1, 2))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 4, 1, 2, 3))\n\n        y = func(x, w, args[2], args[3])\n\n        if args[3] == 'channels_last':\n            if y.ndim == 3:\n                y = np.transpose(y, (0, 2, 1))\n            elif y.ndim == 4:\n                y = np.transpose(y, (0, 2, 3, 1))\n            else:\n                y = np.transpose(y, (0, 2, 3, 4, 1))\n\n        return y",
        "begin_line": 11,
        "end_line": 40,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.conv#46",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.conv(x, w, padding, data_format)",
        "snippet": "def conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[1]):\n            __y = []\n            for k in range(w.shape[0]):\n                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))\n            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))\n        y.append(_y)\n    y = np.array(y)\n    return y",
        "begin_line": 46,
        "end_line": 57,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.depthwise_conv#61",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.depthwise_conv(x, w, padding, data_format)",
        "snippet": "def depthwise_conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[0]):\n            __y = []\n            for k in range(w.shape[1]):\n                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))\n            _y.append(np.stack(__y, axis=0))\n        y.append(np.concatenate(_y, axis=0))\n    y = np.array(y)\n    return y",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.separable_conv#75",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.separable_conv(x, w1, w2, padding, data_format)",
        "snippet": "def separable_conv(x, w1, w2, padding, data_format):\n    x2 = depthwise_conv(x, w1, padding, data_format)\n    return conv(x2, w2, padding, data_format)",
        "begin_line": 75,
        "end_line": 77,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.pool#80",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.pool(x, pool_size, strides, padding, data_format, pool_mode)",
        "snippet": "def pool(x, pool_size, strides, padding, data_format, pool_mode):\n    if data_format == 'channels_last':\n        if x.ndim == 3:\n            x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            x = np.transpose(x, (0, 4, 1, 2, 3))\n\n    if padding == 'same':\n        pad = [(0, 0), (0, 0)] + [(s // 2, s // 2) for s in pool_size]\n        x = np.pad(x, pad, 'constant', constant_values=-np.inf)\n\n    # indexing trick\n    x = np.pad(x, [(0, 0), (0, 0)] + [(0, 1) for _ in pool_size],\n               'constant', constant_values=0)\n\n    if x.ndim == 3:\n        y = [x[:, :, k:k1:strides[0]]\n             for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0))]\n    elif x.ndim == 4:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                y.append(x[:, :, k:k1:strides[0], l:l1:strides[1]])\n    else:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                for (m, m1) in zip(range(pool_size[2]), range(-pool_size[2], 0)):\n                    y.append(x[:, :, k:k1:strides[0], l:l1:strides[1], m:m1:strides[2]])\n    y = np.stack(y, axis=-1)\n    if pool_mode == 'avg':\n        y = np.mean(np.ma.masked_invalid(y), axis=-1).data\n    elif pool_mode == 'max':\n        y = np.max(y, axis=-1)\n\n    if data_format == 'channels_last':\n        if y.ndim == 3:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.ndim == 4:\n            y = np.transpose(y, (0, 2, 3, 1))\n        else:\n            y = np.transpose(y, (0, 2, 3, 4, 1))\n\n    return y",
        "begin_line": 80,
        "end_line": 125,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.rnn#128",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None)",
        "snippet": "def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n    w_i, w_h, w_o = w\n    h = []\n    o = []\n\n    if go_backwards:\n        t_list = range(x.shape[1] - 1, -1, -1)\n    else:\n        t_list = range(x.shape[1])\n\n    if mask is not None:\n        from keras import backend as K\n        np_mask = K.eval(mask)\n    else:\n        np_mask = None\n\n    for (i, t) in enumerate(t_list):\n        h_t = np.dot(x[:, t], w_i)\n\n        if w_h is not None:\n            prev = h[i - 1] if i > 0 else init\n            h_t1 = np.dot(prev, w_h)\n            if np_mask is not None:\n                h_t1[np_mask[:, t] == 0] = prev[np_mask[:, t] == 0]\n        else:\n            h_t1 = 0\n\n        o_t = h_t + h_t1\n        if w_o is not None:\n            o_t = np.dot(o_t, w_o)\n        o.append(o_t)\n\n        if np_mask is not None:\n            h_t = h_t * np_mask[:, t].reshape(-1, 1)\n        h.append(h_t + h_t1)\n\n    return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)",
        "begin_line": 128,
        "end_line": 164,
        "comment": "",
        "is_bug": false
    }
]