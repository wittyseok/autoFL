[
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed#13",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed()",
        "snippet": "def test_TimeDistributed():\n    # first, test with Dense layer\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)),\n              epochs=1,\n              batch_size=10)\n\n    # test config\n    model.get_config()\n\n    # test when specifying a batch_input_shape\n    test_input = np.random.random((1, 3, 4))\n    test_output = model.predict(test_input)\n    weights = model.layers[0].get_weights()\n\n    reference = Sequential()\n    reference.add(wrappers.TimeDistributed(layers.Dense(2),\n                                           batch_input_shape=(1, 3, 4)))\n    reference.add(layers.Activation('relu'))\n    reference.compile(optimizer='rmsprop', loss='mse')\n    reference.layers[0].set_weights(weights)\n\n    reference_output = reference.predict(test_input)\n    assert_allclose(test_output, reference_output, atol=1e-05)\n\n    # test with Embedding\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n                                       batch_input_shape=(10, 3, 4),\n                                       dtype='int32'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.fit(np.random.randint(5, size=(10, 3, 4), dtype='int32'),\n              np.random.random((10, 3, 4, 6)), epochs=1, batch_size=10)\n\n    # compare to not using batch_input_shape\n    test_input = np.random.randint(5, size=(10, 3, 4), dtype='int32')\n    test_output = model.predict(test_input)\n    weights = model.layers[0].get_weights()\n\n    reference = Sequential()\n    reference.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n                                           input_shape=(3, 4), dtype='int32'))\n    reference.compile(optimizer='rmsprop', loss='mse')\n    reference.layers[0].set_weights(weights)\n\n    reference_output = reference.predict(test_input)\n    assert_allclose(test_output, reference_output, atol=1e-05)\n\n    # test with Conv2D\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Conv2D(5, (2, 2),\n                                                     padding='same'),\n                                       input_shape=(2, 4, 4, 3)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(np.random.random((1, 2, 4, 4, 3)),\n                         np.random.random((1, 2, 4, 4, 5)))\n\n    model = model_from_json(model.to_json())\n    model.summary()\n\n    # test stacked layers\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n    model.add(wrappers.TimeDistributed(layers.Dense(3)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n\n    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)),\n              epochs=1, batch_size=10)\n\n    # test wrapping Sequential model\n    model = Sequential()\n    model.add(layers.Dense(3, input_dim=2))\n    outer_model = Sequential()\n    outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n    outer_model.compile(optimizer='rmsprop', loss='mse')\n    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n                    epochs=1, batch_size=10)\n\n    # test with functional API\n    x = Input(shape=(3, 2))\n    y = wrappers.TimeDistributed(model)(x)\n    outer_model = Model(x, y)\n    outer_model.compile(optimizer='rmsprop', loss='mse')\n    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n                    epochs=1, batch_size=10)\n\n    # test with BatchNormalization\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.BatchNormalization(center=True, scale=True),\n        name='bn', input_shape=(10, 2)))\n    model.compile(optimizer='rmsprop', loss='mse')\n    # Assert that mean and variance are 0 and 1.\n    td = model.layers[0]\n    assert np.array_equal(td.get_weights()[2], np.array([0, 0]))\n    assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n    # Train\n    model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n                         np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n    # Assert that mean and variance changed.\n    assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n    assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n    # Verify input_map has one mapping from inputs to reshaped inputs.\n    uid = _object_list_uid(model.inputs)\n    assert len(td._input_map.keys()) == 1\n    assert uid in td._input_map\n    assert K.int_shape(td._input_map[uid]) == (None, 2)",
        "begin_line": 13,
        "end_line": 124,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed_learning_phase#130",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed_learning_phase()",
        "snippet": "def test_TimeDistributed_learning_phase():\n    # test layers that need learning_phase to be set\n    np.random.seed(1234)\n    x = Input(shape=(3, 2))\n    y = wrappers.TimeDistributed(layers.Dropout(.999))(x, training=True)\n    model = Model(x, y)\n    y = model.predict(np.random.random((10, 3, 2)))\n    assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)",
        "begin_line": 130,
        "end_line": 137,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed_trainable#141",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed_trainable()",
        "snippet": "def test_TimeDistributed_trainable():\n    # test layers that need learning_phase to be set\n    x = Input(shape=(3, 2))\n    layer = wrappers.TimeDistributed(layers.BatchNormalization())\n    _ = layer(x)\n    assert len(layer.updates) == 2\n    assert len(layer.trainable_weights) == 2\n    layer.trainable = False\n    assert len(layer.updates) == 0\n    assert len(layer.trainable_weights) == 0\n    layer.trainable = True\n    assert len(layer.updates) == 2\n    assert len(layer.trainable_weights) == 2",
        "begin_line": 141,
        "end_line": 153,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_regularizers#157",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_regularizers()",
        "snippet": "def test_regularizers():\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.Dense(2, kernel_regularizer='l1'), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    assert len(model.layers[0].layer.losses) == 1\n    assert len(model.layers[0].losses) == 1\n    assert len(model.layers[0].get_losses_for(None)) == 1\n    assert len(model.losses) == 1\n\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.Dense(2, activity_regularizer='l1'), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    assert len(model.losses) == 1",
        "begin_line": 157,
        "end_line": 173,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional#177",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional()",
        "snippet": "def test_Bidirectional():\n    rnn = layers.SimpleRNN\n    samples = 2\n    dim = 2\n    timesteps = 2\n    output_dim = 2\n    dropout_rate = 0.2\n    for mode in ['sum', 'concat']:\n        x = np.random.random((samples, timesteps, dim))\n        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n        y = np.random.random((samples, target_dim))\n\n        # test with Sequential model\n        model = Sequential()\n        model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                             recurrent_dropout=dropout_rate),\n                                         merge_mode=mode,\n                                         input_shape=(timesteps, dim)))\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # test config\n        model.get_config()\n        model = model_from_json(model.to_json())\n        model.summary()\n\n        # test stacked bidirectional layers\n        model = Sequential()\n        model.add(wrappers.Bidirectional(rnn(output_dim,\n                                             return_sequences=True),\n                                         merge_mode=mode,\n                                         input_shape=(timesteps, dim)))\n        model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # test with functional API\n        inputs = Input((timesteps, dim))\n        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                             recurrent_dropout=dropout_rate),\n                                         merge_mode=mode)(inputs)\n        model = Model(inputs, outputs)\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # Bidirectional and stateful\n        inputs = Input(batch_shape=(1, timesteps, dim))\n        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n                                         merge_mode=mode)(inputs)\n        model = Model(inputs, outputs)\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)",
        "begin_line": 177,
        "end_line": 228,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_merged_value#233",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_merged_value(merge_mode)",
        "snippet": "def test_Bidirectional_merged_value(merge_mode):\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n    X = [np.random.rand(samples, timesteps, dim)]\n\n    if merge_mode == 'sum':\n        merge_func = lambda y, y_rev: y + y_rev\n    elif merge_mode == 'mul':\n        merge_func = lambda y, y_rev: y * y_rev\n    elif merge_mode == 'ave':\n        merge_func = lambda y, y_rev: (y + y_rev) / 2\n    elif merge_mode == 'concat':\n        merge_func = lambda y, y_rev: np.concatenate((y, y_rev), axis=-1)\n    else:\n        merge_func = lambda y, y_rev: [y, y_rev]\n\n    # basic case\n    inputs = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_sequences=True), merge_mode=merge_mode)\n    f_merged = K.function([inputs], _to_list(layer(inputs)))\n    f_forward = K.function([inputs], [layer.forward_layer.call(inputs)])\n    f_backward = K.function([inputs], [K.reverse(layer.backward_layer.call(inputs), 1)])\n\n    y_merged = f_merged(X)\n    y_expected = _to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n    assert len(y_merged) == len(y_expected)\n    for x1, x2 in zip(y_merged, y_expected):\n        assert_allclose(x1, x2, atol=1e-5)\n\n    # test return_state\n    inputs = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_state=True), merge_mode=merge_mode)\n    f_merged = K.function([inputs], layer(inputs))\n    f_forward = K.function([inputs], layer.forward_layer.call(inputs))\n    f_backward = K.function([inputs], layer.backward_layer.call(inputs))\n    n_states = len(layer.layer.states)\n\n    y_merged = f_merged(X)\n    y_forward = f_forward(X)\n    y_backward = f_backward(X)\n    y_expected = _to_list(merge_func(y_forward[0], y_backward[0]))\n    assert len(y_merged) == len(y_expected) + n_states * 2\n    for x1, x2 in zip(y_merged, y_expected):\n        assert_allclose(x1, x2, atol=1e-5)\n\n    # test if the state of a BiRNN is the concatenation of the underlying RNNs\n    y_merged = y_merged[-n_states * 2:]\n    y_forward = y_forward[-n_states:]\n    y_backward = y_backward[-n_states:]\n    for state_birnn, state_inner in zip(y_merged, y_forward + y_backward):\n        assert_allclose(state_birnn, state_inner, atol=1e-5)",
        "begin_line": 233,
        "end_line": 286,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_dropout#292",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_dropout(merge_mode)",
        "snippet": "def test_Bidirectional_dropout(merge_mode):\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n    X = [np.random.rand(samples, timesteps, dim)]\n\n    inputs = Input((timesteps, dim))\n    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, recurrent_dropout=0.2),\n                                     merge_mode=merge_mode)\n    outputs = _to_list(wrapped(inputs, training=True))\n    assert all(not getattr(x, '_uses_learning_phase') for x in outputs)\n\n    inputs = Input((timesteps, dim))\n    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, return_state=True),\n                                     merge_mode=merge_mode)\n    outputs = _to_list(wrapped(inputs))\n    assert all(x._uses_learning_phase for x in outputs)\n\n    model = Model(inputs, outputs)\n    assert model.uses_learning_phase\n    y1 = _to_list(model.predict(X))\n    y2 = _to_list(model.predict(X))\n    for x1, x2 in zip(y1, y2):\n        assert_allclose(x1, x2, atol=1e-5)",
        "begin_line": 292,
        "end_line": 317,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_state_reuse#321",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_state_reuse()",
        "snippet": "def test_Bidirectional_state_reuse():\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n\n    input1 = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_state=True, return_sequences=True))\n    state = layer(input1)[1:]\n\n    # test passing invalid initial_state: passing a tensor\n    input2 = Input((timesteps, dim))\n    with pytest.raises(ValueError):\n        output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state[0])\n\n    # test valid usage: passing a list\n    output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state)\n    model = Model([input1, input2], output)\n    assert len(model.layers) == 4\n    assert isinstance(model.layers[-1].input, list)\n    inputs = [np.random.rand(samples, timesteps, dim),\n              np.random.rand(samples, timesteps, dim)]\n    outputs = model.predict(inputs)",
        "begin_line": 321,
        "end_line": 344,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_trainable#348",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_trainable()",
        "snippet": "def test_Bidirectional_trainable():\n    # test layers that need learning_phase to be set\n    x = Input(shape=(3, 2))\n    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n    _ = layer(x)\n    assert len(layer.trainable_weights) == 6\n    layer.trainable = False\n    assert len(layer.trainable_weights) == 0\n    layer.trainable = True\n    assert len(layer.trainable_weights) == 6",
        "begin_line": 348,
        "end_line": 357,
        "comment": "",
        "is_bug": false
    }
]