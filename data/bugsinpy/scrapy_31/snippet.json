[
    {
        "name": "scrapy.utils.deprecate.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005941770647653001,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__new__#55",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__new__(metacls, name, bases, clsdict_)",
        "snippet": "        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005941770647653001,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__init__#61",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__init__(cls, name, bases, clsdict_)",
        "snippet": "        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__#77",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__(cls, inst)",
        "snippet": "        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__#81",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__(cls, sub)",
        "snippet": "        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)",
        "begin_line": 81,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__call__#95",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__call__(cls, *args, **kwargs)",
        "snippet": "        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate._clspath#120",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate._clspath(cls, forced=None)",
        "snippet": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "begin_line": 120,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.deprecate.update_classpath#149",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.update_classpath(path)",
        "snippet": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path",
        "begin_line": 149,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.__init__#17",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.spider_modules = settings.getlist('SPIDER_MODULES')\n        self._spiders = {}\n        for name in self.spider_modules:\n            for module in walk_modules(name):\n                self._load_spiders(module)",
        "begin_line": 17,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_spiders#24",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_spiders(self, module)",
        "snippet": "    def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._spiders[spcls.name] = spcls",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.from_settings#29",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings)",
        "begin_line": 29,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.load#32",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.load(self, spider_name)",
        "snippet": "    def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(\"Spider not found: {}\".format(spider_name))",
        "begin_line": 32,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.find_by_request#42",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.find_by_request(self, request)",
        "snippet": "    def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [name for name, cls in self._spiders.items()\n                if cls.handles_request(request)]",
        "begin_line": 42,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.list#49",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.list(self)",
        "snippet": "    def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.default.UrlContract.adjust_request_args#16",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.UrlContract",
        "signature": "scrapy.contracts.default.UrlContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.__init__#42",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ReturnsContract, self).__init__(*args, **kwargs)\n\n        assert len(self.args) in [1, 2, 3]\n        self.obj_name = self.args[0] or None\n        self.obj_type = self.objects[self.obj_name]\n\n        try:\n            self.min_bound = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound = int(self.args[2])\n        except IndexError:\n            self.max_bound = float('inf')",
        "begin_line": 42,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.post_process#59",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if isinstance(x, self.obj_type):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = '%s..%s' % (self.min_bound, self.max_bound)\n\n            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n                (occurrences, self.obj_name, expected))",
        "begin_line": 59,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.default.ScrapesContract.post_process#84",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ScrapesContract",
        "signature": "scrapy.contracts.default.ScrapesContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        for x in output:\n            if isinstance(x, (BaseItem, dict)):\n                for arg in self.args:\n                    if not arg in x:\n                        raise ContractFail(\"'%s' field is missing\" % arg)",
        "begin_line": 84,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_mimetype#42",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_mimetype(self, mimetype)",
        "snippet": "    def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = \"%s/*\" % mimetype.split('/')[0]\n            return self.classes.get(basetype, Response)",
        "begin_line": 42,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_type#52",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_type(self, content_type, content_encoding=None)",
        "snippet": "    def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_disposition#60",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_disposition(self, content_disposition)",
        "snippet": "    def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n            filename = filename.strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response",
        "begin_line": 60,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_headers#68",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_headers(self, headers)",
        "snippet": "    def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls",
        "begin_line": 68,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_filename#81",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_filename(self, filename)",
        "snippet": "    def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response",
        "begin_line": 81,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_body#89",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_body(self, body)",
        "snippet": "    def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if isbinarytext(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')",
        "begin_line": 89,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_args#105",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_args(self, headers=None, url=None, filename=None, body=None)",
        "snippet": "    def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls",
        "begin_line": 105,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.defer_fail#10",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_fail(_failure)",
        "snippet": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d",
        "begin_line": 10,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.defer_succeed#21",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_succeed(result)",
        "snippet": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d",
        "begin_line": 21,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.defer_result#32",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_result(result)",
        "snippet": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)",
        "begin_line": 32,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.mustbe_deferred#40",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.mustbe_deferred(f, *args, **kw)",
        "snippet": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)",
        "begin_line": 40,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006172839506172839,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain#66",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain(callbacks, input, *a, **kw)",
        "snippet": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d",
        "begin_line": 66,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain_both#74",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain_both(callbacks, errbacks, input, *a, **kw)",
        "snippet": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d",
        "begin_line": 74,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.process_parallel#86",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_parallel(callbacks, input, *a, **kw)",
        "snippet": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d",
        "begin_line": 86,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.defer.iter_errback#95",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.iter_errback(iterable, errback, *a, **kw)",
        "snippet": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except:\n            errback(failure.Failure(), *a, **kw)",
        "begin_line": 95,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.ItemMeta.__new__#27",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.ItemMeta",
        "signature": "scrapy.item.ItemMeta.__new__(mcs, class_name, bases, attrs)",
        "snippet": "    def __new__(mcs, class_name, bases, attrs):\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)",
        "begin_line": 27,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005861664712778429,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__init__#49",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._values = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in six.iteritems(dict(*args, **kwargs)):\n                self[k] = v",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005889281507656066,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__getitem__#55",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._values[key]",
        "begin_line": 55,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__setitem__#58",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(\"%s does not support field: %s\" %\n                (self.__class__.__name__, key))",
        "begin_line": 58,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__getattr__#68",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(\"Use item[%r] to get field value\" % name)\n        raise AttributeError(name)",
        "begin_line": 68,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__setattr__#73",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                (name, value))\n        super(DictItem, self).__setattr__(name, value)",
        "begin_line": 73,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__len__#79",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._values)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__iter__#82",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self._values)",
        "begin_line": 82,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.keys#87",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.keys(self)",
        "snippet": "    def keys(self):\n        return self._values.keys()",
        "begin_line": 87,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.__repr__#90",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return pformat(dict(self))",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.item.DictItem.copy#93",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.copy(self)",
        "snippet": "    def copy(self):\n        return self.__class__(self)",
        "begin_line": 93,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.misc.arg_to_iter#17",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.arg_to_iter(arg)",
        "snippet": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]",
        "begin_line": 17,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.misc.load_object#31",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.load_object(path)",
        "snippet": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    object can be a class, function, variable o instance.\n    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n    \"\"\"\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n\n    module, name = path[:dot], path[dot+1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n\n    return obj",
        "begin_line": 31,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.misc.walk_modules#54",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.walk_modules(path)",
        "snippet": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from a the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods",
        "begin_line": 54,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.misc.extract_regex#76",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.extract_regex(regex, text, encoding='utf-8')",
        "snippet": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n\n    if isinstance(regex, six.string_types):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, six.text_type):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]",
        "begin_line": 76,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__init__#19",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)",
        "snippet": "    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None",
        "begin_line": 19,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007633587786259542,
            "pseudo_dstar_susp": 0.007633587786259542,
            "pseudo_tarantula_susp": 0.007692307692307693,
            "pseudo_op2_susp": 0.007633587786259542,
            "pseudo_barinel_susp": 0.007692307692307693
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.meta#41",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.meta(self)",
        "snippet": "    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta",
        "begin_line": 41,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011111111111111112,
            "pseudo_dstar_susp": 0.010869565217391304,
            "pseudo_tarantula_susp": 0.011111111111111112,
            "pseudo_op2_susp": 0.010869565217391304,
            "pseudo_barinel_susp": 0.011111111111111112
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_url#46",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 46,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009345794392523364,
            "pseudo_dstar_susp": 0.009259259259259259,
            "pseudo_tarantula_susp": 0.009433962264150943,
            "pseudo_op2_susp": 0.009259259259259259,
            "pseudo_barinel_susp": 0.009433962264150943
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_url#49",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        url = to_native_str(url, self.encoding)\n        self._url = escape_ajax(safe_url_string(url))\n\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)",
        "begin_line": 49,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006578947368421052,
            "pseudo_dstar_susp": 0.006578947368421052,
            "pseudo_tarantula_susp": 0.006622516556291391,
            "pseudo_op2_susp": 0.006578947368421052,
            "pseudo_barinel_susp": 0.006622516556291391
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_body#61",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.000555247084952804,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_body#64",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)",
        "begin_line": 64,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007633587786259542,
            "pseudo_dstar_susp": 0.007633587786259542,
            "pseudo_tarantula_susp": 0.007692307692307693,
            "pseudo_op2_susp": 0.007633587786259542,
            "pseudo_barinel_susp": 0.007692307692307693
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.encoding#73",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._encoding",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006578947368421052,
            "pseudo_dstar_susp": 0.006578947368421052,
            "pseudo_tarantula_susp": 0.006622516556291391,
            "pseudo_op2_susp": 0.006578947368421052,
            "pseudo_barinel_susp": 0.006622516556291391
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.copy#81",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.replace#85",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 85,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006172839506172839,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.SpiderInfo.__init__#21",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.SpiderInfo",
        "signature": "scrapy.pipelines.media.SpiderInfo.__init__(self, spider)",
        "snippet": "        def __init__(self, spider):\n            self.spider = spider\n            self.downloading = set()\n            self.downloaded = {}\n            self.waiting = defaultdict(list)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.__init__#27",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.__init__(self, download_func=None)",
        "snippet": "    def __init__(self, download_func=None):\n        self.download_func = download_func",
        "begin_line": 27,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.open_spider#39",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.process_item#42",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=1)\n        return dfd.addCallback(self.item_completed, item, info)",
        "begin_line": 42,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006172839506172839,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._process_request#49",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._process_request(self, request, info)",
        "snippet": "    def _process_request(self, request, info):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info)\n        dfd.addCallback(self._check_media_to_download, request, info)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)  # it must return wad at last",
        "begin_line": 49,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._check_media_to_download#78",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._check_media_to_download(self, result, request, info)",
        "snippet": "    def _check_media_to_download(self, result, request, info):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd",
        "begin_line": 78,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters#95",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters(self, result, fp, info)",
        "snippet": "    def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)",
        "begin_line": 95,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_to_download#107",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        \"\"\"Check request before starting download\"\"\"\n        pass",
        "begin_line": 107,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.get_media_requests#111",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass",
        "begin_line": 111,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_downloaded#115",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        \"\"\"Handler for success downloads\"\"\"\n        return response",
        "begin_line": 115,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006485084306095979,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_failed#119",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_failed(self, failure, request, info)",
        "snippet": "    def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure",
        "begin_line": 119,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.item_completed#123",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item",
        "begin_line": 123,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.rpc.XmlRpcRequest.__init__#18",
        "src_path": "scrapy/http/request/rpc.py",
        "class_name": "scrapy.http.request.rpc.XmlRpcRequest",
        "signature": "scrapy.http.request.rpc.XmlRpcRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        encoding = kwargs.get('encoding', None)\n        if 'body' not in kwargs and 'params' in kwargs:\n            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n            kwargs['body'] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault('method', 'POST')\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault('dont_filter', True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs['encoding'] = encoding\n\n        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n        self.headers.setdefault('Content-Type', 'text/xml')",
        "begin_line": 18,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.link.Link.__init__#15",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__init__(self, url, text='', fragment='', nofollow=False)",
        "snippet": "    def __init__(self, url, text='', fragment='', nofollow=False):\n        if isinstance(url, six.text_type):\n            import warnings\n            warnings.warn(\"Do not instantiate Link objects with unicode urls. \"\n                \"Assuming utf-8 encoding (which could be wrong)\")\n            url = url.encode('utf-8')\n        self.url = url\n        self.text = text\n        self.fragment = fragment\n        self.nofollow = nofollow",
        "begin_line": 15,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.link.Link.__eq__#26",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self.url == other.url and self.text == other.text and \\\n            self.fragment == other.fragment and self.nofollow == other.nofollow",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.link.Link.__hash__#30",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.link.Link.__repr__#33",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n            (self.url, self.text, self.fragment, self.nofollow)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__#22",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n            raise NotConfigured\n\n        self.crawler = crawler\n        self._useragent = crawler.settings.get('USER_AGENT')\n        self._parsers = {}",
        "begin_line": 22,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request#34",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        rp = self.robot_parser(request, spider)\n        if rp and not rp.can_fetch(self._useragent, request.url):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest",
        "begin_line": 34,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser#43",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser(self, request, spider)",
        "snippet": "    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n        if netloc not in self._parsers:\n            self._parsers[netloc] = None\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n        return self._parsers[netloc]",
        "begin_line": 43,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots#66",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots(self, response)",
        "snippet": "    def _parse_robots(self, response):\n        rp = robotparser.RobotFileParser(response.url)\n        body = ''\n        if hasattr(response, 'body_as_unicode'):\n            body = response.body_as_unicode()\n        else: # last effort try\n            try:\n                body = response.body.decode('utf-8')\n            except UnicodeDecodeError:\n                # If we found garbage, disregard it:,\n                # but keep the lookup cached (in self._parsers)\n                # Running rp.parse() will set rp state from\n                # 'disallow all' to 'allow any'.\n                pass\n        rp.parse(body.splitlines())\n        self._parsers[urlparse_cached(response).netloc] = rp",
        "begin_line": 66,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.spider.iterate_spider_output#12",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iterate_spider_output(result)",
        "snippet": "def iterate_spider_output(result):\n    return arg_to_iter(result)",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.spider.iter_spider_classes#16",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iter_spider_classes(module)",
        "snippet": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (ie. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in six.itervalues(vars(module)):\n        if inspect.isclass(obj) and \\\n           issubclass(obj, Spider) and \\\n           obj.__module__ == module.__name__ and \\\n           getattr(obj, 'name', None):\n            yield obj",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.__init__#10",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.__init__(self, policy=None, check_expired_frequency=10000)",
        "snippet": "    def __init__(self, policy=None, check_expired_frequency=10000):\n        self.policy = policy or DefaultCookiePolicy()\n        self.jar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()\n        self.check_expired_frequency = check_expired_frequency\n        self.processed = 0",
        "begin_line": 10,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.extract_cookies#17",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.extract_cookies(self, response, request)",
        "snippet": "    def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1111111111111111,
            "pseudo_dstar_susp": 0.09090909090909091,
            "pseudo_tarantula_susp": 0.2,
            "pseudo_op2_susp": 0.09090909090909091,
            "pseudo_barinel_susp": 0.2
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.add_cookie_header#22",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.add_cookie_header(self, request)",
        "snippet": "    def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()",
        "begin_line": 22,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.make_cookies#73",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.make_cookies(self, response, request)",
        "snippet": "    def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.set_cookie_if_ok#81",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.set_cookie_if_ok(self, cookie, request)",
        "snippet": "    def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.potential_domain_matches#85",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies",
        "signature": "scrapy.http.cookies.potential_domain_matches(domain)",
        "snippet": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]",
        "begin_line": 85,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.5,
            "pseudo_dstar_susp": 0.5,
            "pseudo_tarantula_susp": 0.1,
            "pseudo_op2_susp": 0.5,
            "pseudo_barinel_susp": 0.1
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.acquire#105",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.acquire(self)",
        "snippet": "    def acquire(self):\n        pass",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.release#108",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.release(self)",
        "snippet": "    def release(self):\n        pass",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.__init__#118",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.__init__(self, request)",
        "snippet": "    def __init__(self, request):\n        self.request = request",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011764705882352941,
            "pseudo_dstar_susp": 0.011494252873563218,
            "pseudo_tarantula_susp": 0.011764705882352941,
            "pseudo_op2_susp": 0.011494252873563218,
            "pseudo_barinel_susp": 0.011764705882352941
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_full_url#121",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_full_url(self)",
        "snippet": "    def get_full_url(self):\n        return self.request.url",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_host#124",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_host(self)",
        "snippet": "    def get_host(self):\n        return urlparse_cached(self.request).netloc",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_type#127",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_type(self)",
        "snippet": "    def get_type(self):\n        return urlparse_cached(self.request).scheme",
        "begin_line": 127,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.is_unverifiable#130",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.is_unverifiable(self)",
        "snippet": "    def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)",
        "begin_line": 130,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006485084306095979,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.unverifiable#142",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.unverifiable(self)",
        "snippet": "    def unverifiable(self):\n        return self.is_unverifiable()",
        "begin_line": 142,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006485084306095979,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_origin_req_host#145",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_origin_req_host(self)",
        "snippet": "    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.has_header#148",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.has_header(self, name)",
        "snippet": "    def has_header(self, name):\n        return name in self.request.headers",
        "begin_line": 148,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_header#151",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_header(self, name, default=None)",
        "snippet": "    def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default))",
        "begin_line": 151,
        "end_line": 152,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.header_items#154",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.header_items(self)",
        "snippet": "    def header_items(self):\n        return [\n            (to_native_str(k), [to_native_str(x) for x in v])\n            for k, v in self.request.headers.items()\n        ]",
        "begin_line": 154,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.add_unredirected_header#160",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.add_unredirected_header(self, name, value)",
        "snippet": "    def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)",
        "begin_line": 160,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.__init__#166",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.__init__(self, response)",
        "snippet": "    def __init__(self, response):\n        self.response = response",
        "begin_line": 166,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013157894736842105,
            "pseudo_dstar_susp": 0.01282051282051282,
            "pseudo_tarantula_susp": 0.013157894736842105,
            "pseudo_op2_susp": 0.01282051282051282,
            "pseudo_barinel_susp": 0.013157894736842105
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.info#169",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.info(self)",
        "snippet": "    def info(self):\n        return self",
        "begin_line": 169,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014285714285714285,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.014492753623188406,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.014492753623188406
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.get_all#173",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.get_all(self, name, default=None)",
        "snippet": "    def get_all(self, name, default=None):\n        return [to_native_str(v) for v in self.response.headers.getlist(name)]",
        "begin_line": 173,
        "end_line": 174,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.25,
            "pseudo_tarantula_susp": 0.013888888888888888,
            "pseudo_op2_susp": 0.25,
            "pseudo_barinel_susp": 0.013888888888888888
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__init__#15",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__init__(self, url, status=200, headers=None, body=b'', flags=None, request=None)",
        "snippet": "    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 15,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007042253521126761,
            "pseudo_dstar_susp": 0.007042253521126761,
            "pseudo_tarantula_susp": 0.0070921985815602835,
            "pseudo_op2_susp": 0.007042253521126761,
            "pseudo_barinel_susp": 0.0070921985815602835
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.meta#24",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.meta(self)",
        "snippet": "    def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_url#33",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005344735435595938,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_url#36",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n                type(url).__name__))",
        "begin_line": 36,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011363636363636364,
            "pseudo_dstar_susp": 0.011111111111111112,
            "pseudo_tarantula_susp": 0.011363636363636364,
            "pseudo_op2_susp": 0.011111111111111112,
            "pseudo_barinel_susp": 0.011363636363636364
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_body#45",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005260389268805891,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_body#48",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body",
        "begin_line": 48,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00819672131147541,
            "pseudo_dstar_susp": 0.00819672131147541,
            "pseudo_tarantula_susp": 0.008264462809917356,
            "pseudo_op2_susp": 0.00819672131147541,
            "pseudo_barinel_susp": 0.008264462809917356
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__str__#61",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%d %s>\" % (self.status, self.url)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.copy#66",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()",
        "begin_line": 66,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006485084306095979,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.replace#70",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 70,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005820721769499418,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.urljoin#79",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.csstranslator.ScrapyXPathExpr.from_xpath#12",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.ScrapyXPathExpr",
        "signature": "scrapy.selector.csstranslator.ScrapyXPathExpr.from_xpath(cls, xpath, textnode=False, attribute=None)",
        "snippet": "    def from_xpath(cls, xpath, textnode=False, attribute=None):\n        x = cls(path=xpath.path, element=xpath.element, condition=xpath.condition)\n        x.textnode = textnode\n        x.attribute = attribute\n        return x",
        "begin_line": 12,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.csstranslator.ScrapyXPathExpr.__str__#18",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.ScrapyXPathExpr",
        "signature": "scrapy.selector.csstranslator.ScrapyXPathExpr.__str__(self)",
        "snippet": "    def __str__(self):\n        path = super(ScrapyXPathExpr, self).__str__()\n        if self.textnode:\n            if path == '*':\n                path = 'text()'\n            elif path.endswith('::*/*'):\n                path = path[:-3] + 'text()'\n            else:\n                path += '/text()'\n\n        if self.attribute is not None:\n            if path.endswith('::*/*'):\n                path = path[:-2]\n            path += '/@%s' % self.attribute\n\n        return path",
        "begin_line": 18,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_element#44",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_element(self, selector)",
        "snippet": "    def xpath_element(self, selector):\n        xpath = super(TranslatorMixin, self).xpath_element(selector)\n        return ScrapyXPathExpr.from_xpath(xpath)",
        "begin_line": 44,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_pseudo_element#48",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_pseudo_element(self, xpath, pseudo_element)",
        "snippet": "    def xpath_pseudo_element(self, xpath, pseudo_element):\n        if isinstance(pseudo_element, FunctionalPseudoElement):\n            method = 'xpath_%s_functional_pseudo_element' % (\n                pseudo_element.name.replace('-', '_'))\n            method = _unicode_safe_getattr(self, method, None)\n            if not method:\n                raise ExpressionError(\n                    \"The functional pseudo-element ::%s() is unknown\"\n                % pseudo_element.name)\n            xpath = method(xpath, pseudo_element)\n        else:\n            method = 'xpath_%s_simple_pseudo_element' % (\n                pseudo_element.replace('-', '_'))\n            method = _unicode_safe_getattr(self, method, None)\n            if not method:\n                raise ExpressionError(\n                    \"The pseudo-element ::%s is unknown\"\n                    % pseudo_element)\n            xpath = method(xpath)\n        return xpath",
        "begin_line": 48,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_text_simple_pseudo_element#77",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_text_simple_pseudo_element(self, xpath)",
        "snippet": "    def xpath_text_simple_pseudo_element(self, xpath):\n        \"\"\"Support selecting text nodes using ::text pseudo-element\"\"\"\n        return ScrapyXPathExpr.from_xpath(xpath, textnode=True)",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.flatten#15",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.flatten(x)",
        "snippet": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))",
        "begin_line": 15,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.iflatten#35",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.iflatten(x)",
        "snippet": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in flatten(el):\n                yield el_\n        else:\n            yield el",
        "begin_line": 35,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.is_listlike#47",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.is_listlike(x)",
        "snippet": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(six.moves.xrange(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))",
        "begin_line": 47,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.to_unicode#97",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_unicode(text, encoding=None, errors='strict')",
        "snippet": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object `text`. If `text`\n    is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, six.text_type):\n        return text\n    if not isinstance(text, (bytes, six.text_type)):\n        raise TypeError('to_unicode must receive a bytes, str or unicode '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)",
        "begin_line": 97,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00909090909090909,
            "pseudo_dstar_susp": 0.00909090909090909,
            "pseudo_tarantula_susp": 0.009259259259259259,
            "pseudo_op2_susp": 0.00909090909090909,
            "pseudo_barinel_susp": 0.009259259259259259
        }
    },
    {
        "name": "scrapy.utils.python.to_bytes#110",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_bytes(text, encoding=None, errors='strict')",
        "snippet": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of `text`. If `text`\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, six.string_types):\n        raise TypeError('to_bytes must receive a unicode, str or bytes '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)",
        "begin_line": 110,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.to_native_str#123",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_native_str(text, encoding=None, errors='strict')",
        "snippet": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of `text`\n    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n    if six.PY2:\n        return to_bytes(text, encoding, errors)\n    else:\n        return to_unicode(text, encoding, errors)",
        "begin_line": 123,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0058823529411764705,
            "pseudo_dstar_susp": 0.0058823529411764705,
            "pseudo_tarantula_susp": 0.0058823529411764705,
            "pseudo_op2_susp": 0.0058823529411764705,
            "pseudo_barinel_susp": 0.0058823529411764705
        }
    },
    {
        "name": "scrapy.utils.python.memoizemethod_noargs#165",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.memoizemethod_noargs(method)",
        "snippet": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n    return new_method",
        "begin_line": 165,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.new_method#171",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.new_method(self, *args, **kwargs)",
        "snippet": "    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]",
        "begin_line": 171,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.isbinarytext#181",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.isbinarytext(text)",
        "snippet": "def isbinarytext(text):\n    \"\"\"Return True if the given text is considered binary, or False\n    otherwise, by looking for binary bytes at their chars\n    \"\"\"\n    if not isinstance(text, bytes):\n        raise TypeError(\"text must be bytes, got '%s'\" % type(text).__name__)\n    return any(c in _BINARYCHARS for c in text)",
        "begin_line": 181,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.get_func_args#190",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_func_args(func, stripself=False)",
        "snippet": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        func_args, _, _, _ = inspect.getargspec(func)\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n    if stripself:\n        func_args.pop(0)\n    return func_args",
        "begin_line": 190,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.get_spec#217",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_spec(func)",
        "snippet": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test(object):\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getargspec(func)\n    elif hasattr(func, '__call__'):\n        spec = inspect.getargspec(func.__call__)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs",
        "begin_line": 217,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.equal_attributes#254",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.equal_attributes(obj1, obj2, attributes)",
        "snippet": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if not attr(obj1) == attr(obj2):\n                return False\n        else:\n            # check that objects has attribute\n            if not hasattr(obj1, attr):\n                return False\n            if not hasattr(obj2, attr):\n                return False\n            # compare object attributes\n            if not getattr(obj1, attr) == getattr(obj2, attr):\n                return False\n    # all attributes equal\n    return True",
        "begin_line": 254,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__init__#280",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__init__(self, default_factory)",
        "snippet": "    def __init__(self, default_factory):\n        self.default_factory = default_factory\n        self._weakdict = weakref.WeakKeyDictionary()",
        "begin_line": 280,
        "end_line": 282,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__getitem__#284",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]",
        "begin_line": 284,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.__init__#27",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.__init__(self, item=None, selector=None, response=None, **context)",
        "snippet": "    def __init__(self, item=None, selector=None, response=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n        self.selector = selector\n        context.update(selector=selector, response=response)\n        if item is None:\n            item = self.default_item_class()\n        self.item = context['item'] = item\n        self.context = context\n        self._values = defaultdict(list)",
        "begin_line": 27,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.httpobj.urlparse_cached#8",
        "src_path": "scrapy/utils/httpobj.py",
        "class_name": "scrapy.utils.httpobj",
        "signature": "scrapy.utils.httpobj.urlparse_cached(request_or_response)",
        "snippet": "def urlparse_cached(request_or_response):\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]",
        "begin_line": 8,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012195121951219513,
            "pseudo_dstar_susp": 0.011904761904761904,
            "pseudo_tarantula_susp": 0.012195121951219513,
            "pseudo_op2_susp": 0.011904761904761904,
            "pseudo_barinel_susp": 0.012195121951219513
        }
    },
    {
        "name": "scrapy.selector.lxmldocument._factory#11",
        "src_path": "scrapy/selector/lxmldocument.py",
        "class_name": "scrapy.selector.lxmldocument",
        "signature": "scrapy.selector.lxmldocument._factory(response, parser_cls)",
        "snippet": "def _factory(response, parser_cls):\n    url = response.url\n    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'\n    parser = parser_cls(recover=True, encoding='utf8')\n    return etree.fromstring(body, parser=parser, base_url=url)",
        "begin_line": 11,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005820721769499418,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.lxmldocument.LxmlDocument.__new__#23",
        "src_path": "scrapy/selector/lxmldocument.py",
        "class_name": "scrapy.selector.lxmldocument.LxmlDocument",
        "signature": "scrapy.selector.lxmldocument.LxmlDocument.__new__(cls, response, parser=etree.HTMLParser)",
        "snippet": "    def __new__(cls, response, parser=etree.HTMLParser):\n        cache = cls.cache.setdefault(response, {})\n        if parser not in cache:\n            obj = object_ref.__new__(cls)\n            cache[parser] = _factory(response, parser)\n        return cache[parser]",
        "begin_line": 23,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005820721769499418,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__#16",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__(self, debug=False)",
        "snippet": "    def __init__(self, debug=False):\n        self.jars = defaultdict(CookieJar)\n        self.debug = debug",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request#26",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        for cookie in cookies:\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)",
        "begin_line": 26,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response#41",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response",
        "begin_line": 41,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1111111111111111,
            "pseudo_dstar_susp": 0.09090909090909091,
            "pseudo_tarantula_susp": 0.2,
            "pseudo_op2_susp": 0.09090909090909091,
            "pseudo_barinel_susp": 0.2
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie#53",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie(self, request, spider)",
        "snippet": "    def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = request.headers.getlist('Cookie')\n            if cl:\n                msg = \"Sending cookies to: %s\" % request + os.linesep\n                msg += os.linesep.join(\"Cookie: %s\" % c for c in cl)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 53,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.023809523809523808,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie#61",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie(self, response, spider)",
        "snippet": "    def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = response.headers.getlist('Set-Cookie')\n            if cl:\n                msg = \"Received cookies from: %s\" % response + os.linesep\n                msg += os.linesep.join(\"Set-Cookie: %s\" % c for c in cl)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 61,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie#69",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie(self, cookie)",
        "snippet": "    def _format_cookie(self, cookie):\n        # build cookie string\n        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n\n        if cookie.get('path', None):\n            cookie_str += '; Path=%s' % cookie['path']\n        if cookie.get('domain', None):\n            cookie_str += '; Domain=%s' % cookie['domain']\n\n        return cookie_str",
        "begin_line": 69,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies#80",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies(self, jar, request)",
        "snippet": "    def _get_request_cookies(self, jar, request):\n        if isinstance(request.cookies, dict):\n            cookie_list = [{'name': k, 'value': v} for k, v in \\\n                    six.iteritems(request.cookies)]\n        else:\n            cookie_list = request.cookies\n\n        cookies = [self._format_cookie(x) for x in cookie_list]\n        headers = {'Set-Cookie': cookies}\n        response = Response(request.url, headers=headers)\n\n        return jar.make_cookies(response, request)",
        "begin_line": 80,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.5,
            "pseudo_dstar_susp": 0.5,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.5,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "scrapy.http.common.newsetter#2",
        "src_path": "scrapy/http/common.py",
        "class_name": "scrapy.http.common",
        "signature": "scrapy.http.common.newsetter(self, value)",
        "snippet": "    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n        raise AttributeError(msg)",
        "begin_line": 2,
        "end_line": 5,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.__init__#22",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._encoding = kwargs.pop('encoding', None)\n        self._cached_benc = None\n        self._cached_ubody = None\n        self._cached_selector = None\n        super(TextResponse, self).__init__(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005285412262156448,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_url#29",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, six.text_type):\n            if six.PY2 and self.encoding is None:\n                raise TypeError(\"Cannot convert unicode url - %s \"\n                                \"has no encoding\" % type(self).__name__)\n            self._url = to_native_str(url, self.encoding)\n        else:\n            super(TextResponse, self)._set_url(url)",
        "begin_line": 29,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005285412262156448,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_body#38",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, six.text_type):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                    type(self).__name__)\n            self._body = body.encode(self._encoding)\n        else:\n            super(TextResponse, self)._set_body(body)",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.replace#48",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006006006006006006,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.encoding#53",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005285412262156448,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._declared_encoding#56",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._declared_encoding(self)",
        "snippet": "    def _declared_encoding(self):\n        return self._encoding or self._headers_encoding() \\\n            or self._body_declared_encoding()",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005361930294906167,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.body_as_unicode#60",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.body_as_unicode(self)",
        "snippet": "    def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        # check for self.encoding before _cached_ubody just in\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = 'charset=%s' % benc\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody",
        "begin_line": 60,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005452562704471102,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.urljoin#70",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._headers_encoding#76",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._headers_encoding(self)",
        "snippet": "    def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_native_str(content_type))",
        "begin_line": 76,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005361930294906167,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_inferred_encoding#80",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_inferred_encoding(self)",
        "snippet": "    def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                    auto_detect_fun=self._auto_detect_fun,\n                    default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc",
        "begin_line": 80,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005396654074473826,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._auto_detect_fun#90",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._auto_detect_fun(self, text)",
        "snippet": "    def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)",
        "begin_line": 90,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_declared_encoding#99",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_declared_encoding(self)",
        "snippet": "    def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)",
        "begin_line": 99,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005361930294906167,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.selector#103",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.selector(self)",
        "snippet": "    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector",
        "begin_line": 103,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.xpath#109",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.xpath(self, query)",
        "snippet": "    def xpath(self, query):\n        return self.selector.xpath(query)",
        "begin_line": 109,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.css#112",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.selector.css(query)",
        "begin_line": 112,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.__init__#10",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.__init__(self, jobdir=None)",
        "snippet": "    def __init__(self, jobdir=None):\n        self.jobdir = jobdir",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_closed#20",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=2)",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_opened#25",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}",
        "begin_line": 25,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.statefn#33",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.statefn(self)",
        "snippet": "    def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.serialize.ScrapyJSONEncoder.default#16",
        "src_path": "scrapy/utils/serialize.py",
        "class_name": "scrapy.utils.serialize.ScrapyJSONEncoder",
        "signature": "scrapy.utils.serialize.ScrapyJSONEncoder.default(self, o)",
        "snippet": "    def default(self, o):\n        if isinstance(o, datetime.datetime):\n            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif isinstance(o, BaseItem):\n            return dict(o)\n        elif isinstance(o, Request):\n            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n        elif isinstance(o, Response):\n            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n        else:\n            return super(ScrapyJSONEncoder, self).default(o)",
        "begin_line": 16,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.request.request_fingerprint#20",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_fingerprint(request, include_headers=None)",
        "snippet": "def request_fingerprint(request, include_headers=None):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (ie. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accesible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    \"\"\"\n    if include_headers:\n        include_headers = tuple([to_bytes(h.lower())\n                                 for h in sorted(include_headers)])\n    cache = _fingerprint_cache.setdefault(request, {})\n    if include_headers not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url)))\n        fp.update(request.body or b'')\n        if include_headers:\n            for hdr in include_headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[include_headers] = fp.hexdigest()\n    return cache[include_headers]",
        "begin_line": 20,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.request.request_authenticate#66",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_authenticate(request, username, password)",
        "snippet": "def request_authenticate(request, username, password):\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)",
        "begin_line": 66,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.request.request_httprepr#73",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_httprepr(request)",
        "snippet": "def request_httprepr(request):\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s",
        "begin_line": 73,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.http.decode_chunked_transfer#9",
        "src_path": "scrapy/utils/http.py",
        "class_name": "scrapy.utils.http",
        "signature": "scrapy.utils.http.decode_chunked_transfer(chunked_body)",
        "snippet": "def decode_chunked_transfer(chunked_body):\n    \"\"\"Parsed body received with chunked transfer encoding, and return the\n    decoded body.\n\n    For more info see:\n    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n    \"\"\"\n    body, h, t = '', '', chunked_body\n    while t:\n        h, t = t.split('\\r\\n', 1)\n        if h == '0':\n            break\n        size = int(h, 16)\n        body += t[:size]\n        t = t[size+2:]\n    return body",
        "begin_line": 9,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.response.get_base_url#26",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_base_url(response)",
        "snippet": "def get_base_url(response):\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.body_as_unicode()[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\n            response.encoding)\n    return _baseurl_cache[response]",
        "begin_line": 26,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.response.get_meta_refresh#38",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_meta_refresh(response)",
        "snippet": "def get_meta_refresh(response):\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.body_as_unicode()[0:4096]\n        text = _noscript_re.sub(u'', text)\n        text = _script_re.sub(u'', text)\n        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n            response.encoding)\n    return _metaref_cache[response]",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.response.response_httprepr#61",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_httprepr(response)",
        "snippet": "def response_httprepr(response):\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n        to_bytes(RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n    if response.headers:\n        s += response.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += response.body\n    return s",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.response.open_in_browser#75",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.open_in_browser(response, _openfunc=webbrowser.open)",
        "snippet": "def open_in_browser(response, _openfunc=webbrowser.open):\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = '<head><base href=\"%s\">' % response.url\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: %s\" %\n                        response.__class__.__name__)\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(\"file://%s\" % fname)",
        "begin_line": 75,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.test.assert_samelines#49",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.assert_samelines(testcase, text1, text2, msg=None)",
        "snippet": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log#20",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop('dont_log', _IgnoredException)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender,\n                *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses",
        "begin_line": 20,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log_deferred#47",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d",
        "begin_line": 47,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.signal.logerror#52",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.logerror(failure, recv)",
        "snippet": "    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.signal.disconnect_all#74",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.disconnect_all(signal=Any, sender=Any)",
        "snippet": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)",
        "begin_line": 74,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005652911249293386,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.trackref.object_ref.__new__#28",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref.object_ref",
        "signature": "scrapy.utils.trackref.object_ref.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0055248618784530384,
            "pseudo_dstar_susp": 0.0055248618784530384,
            "pseudo_tarantula_susp": 0.0055248618784530384,
            "pseudo_op2_susp": 0.0055248618784530384,
            "pseudo_barinel_susp": 0.0055248618784530384
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__init__#31",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__init__(self, value, priority)",
        "snippet": "    def __init__(self, value, priority):\n        self.value = value\n        self.priority = priority",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.000543773790103317,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.set#35",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.set(self, value, priority)",
        "snippet": "    def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            self.value = value\n            self.priority = priority",
        "begin_line": 35,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005691519635742744,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__init__#50",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        self.frozen = False\n        self.attributes = {}\n        self.setmodule(default_settings, priority='default')\n        if values is not None:\n            self.setdict(values, priority)",
        "begin_line": 50,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__getitem__#57",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        value = None\n        if opt_name in self.attributes:\n            value = self.attributes[opt_name].value\n        return value",
        "begin_line": 57,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005757052389176742,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.get#63",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.get(self, name, default=None)",
        "snippet": "    def get(self, name, default=None):\n        return self[name] if self[name] is not None else default",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005757052389176742,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getbool#66",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getbool(self, name, default=False)",
        "snippet": "    def getbool(self, name, default=False):\n        \"\"\"\n        True is: 1, '1', True\n        False is: 0, '0', False, None\n        \"\"\"\n        return bool(int(self.get(name, default)))",
        "begin_line": 66,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006680026720106881,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getint#73",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getint(self, name, default=0)",
        "snippet": "    def getint(self, name, default=0):\n        return int(self.get(name, default))",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getfloat#76",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getfloat(self, name, default=0.0)",
        "snippet": "    def getfloat(self, name, default=0.0):\n        return float(self.get(name, default))",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getlist#79",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getlist(self, name, default=None)",
        "snippet": "    def getlist(self, name, default=None):\n        value = self.get(name, default or [])\n        if isinstance(value, six.string_types):\n            value = value.split(',')\n        return list(value)",
        "begin_line": 79,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getdict#85",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getdict(self, name, default=None)",
        "snippet": "    def getdict(self, name, default=None):\n        value = self.get(name, default or {})\n        if isinstance(value, six.string_types):\n            value = json.loads(value)\n        return dict(value)",
        "begin_line": 85,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.set#91",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.set(self, name, value, priority='project')",
        "snippet": "    def set(self, name, value, priority='project'):\n        self._assert_mutability()\n        if isinstance(priority, six.string_types):\n            priority = SETTINGS_PRIORITIES[priority]\n        if name not in self.attributes:\n            self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)",
        "begin_line": 91,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005714285714285715,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.setdict#100",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.setdict(self, values, priority='project')",
        "snippet": "    def setdict(self, values, priority='project'):\n        self._assert_mutability()\n        for name, value in six.iteritems(values):\n            self.set(name, value, priority)",
        "begin_line": 100,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.setmodule#105",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.setmodule(self, module, priority='project')",
        "snippet": "    def setmodule(self, module, priority='project'):\n        self._assert_mutability()\n        if isinstance(module, six.string_types):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)",
        "begin_line": 105,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings._assert_mutability#113",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings._assert_mutability(self)",
        "snippet": "    def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")",
        "begin_line": 113,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.copy#117",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.copy(self)",
        "snippet": "    def copy(self):\n        return copy.deepcopy(self)",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.freeze#120",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.freeze(self)",
        "snippet": "    def freeze(self):\n        self.frozen = True",
        "begin_line": 120,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.frozencopy#123",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.frozencopy(self)",
        "snippet": "    def frozencopy(self):\n        copy = self.copy()\n        copy.freeze()\n        return copy",
        "begin_line": 123,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.overrides#129",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.overrides(self)",
        "snippet": "    def overrides(self):\n        warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='cmdline')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._overrides\n        except AttributeError:\n            self._overrides = o = _DictProxy(self, 'cmdline')\n        return o",
        "begin_line": 129,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.defaults#141",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.defaults(self)",
        "snippet": "    def defaults(self):\n        warnings.warn(\"`Settings.defaults` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='default')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._defaults\n        except AttributeError:\n            self._defaults = o = _DictProxy(self, 'default')\n        return o",
        "begin_line": 141,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__init__#155",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__init__(self, settings, priority)",
        "snippet": "    def __init__(self, settings, priority):\n        self.o = {}\n        self.settings = settings\n        self.priority = priority",
        "begin_line": 155,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__getitem__#163",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__getitem__(self, k)",
        "snippet": "    def __getitem__(self, k):\n        return self.o[k]",
        "begin_line": 163,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__setitem__#166",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__setitem__(self, k, v)",
        "snippet": "    def __setitem__(self, k, v):\n        self.settings.set(k, v, priority=self.priority)\n        self.o[k] = v",
        "begin_line": 166,
        "end_line": 168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__init__#179",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__init__(self, settings_module=None, **kw)",
        "snippet": "    def __init__(self, settings_module=None, **kw):\n        Settings.__init__(self, **kw)\n        self.settings_module = settings_module",
        "begin_line": 179,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__getitem__#183",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        if opt_name in self.overrides:\n            return self.overrides[opt_name]\n        if self.settings_module and hasattr(self.settings_module, opt_name):\n            return getattr(self.settings_module, opt_name)\n        if opt_name in self.defaults:\n            return self.defaults[opt_name]\n        return Settings.__getitem__(self, opt_name)",
        "begin_line": 183,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.gz.gunzip#10",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gunzip(data)",
        "snippet": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = f.read(8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or f.extrabuf:\n                output += f.extrabuf\n                break\n            else:\n                raise\n    return output",
        "begin_line": 10,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.__init__#16",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.__init__(self, *middlewares)",
        "snippet": "    def __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self.methods = defaultdict(list)\n        for mw in middlewares:\n            self._add_middleware(mw)",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_settings#27",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_settings(cls, settings, crawler=None)",
        "snippet": "    def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler and hasattr(mwcls, 'from_crawler'):\n                    mw = mwcls.from_crawler(crawler)\n                elif hasattr(mwcls, 'from_settings'):\n                    mw = mwcls.from_settings(settings)\n                else:\n                    mw = mwcls()\n                middlewares.append(mw)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        enabled = [x.__class__.__name__ for x in middlewares]\n        logger.info(\"Enabled %(componentname)ss: %(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': ', '.join(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)",
        "begin_line": 27,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._add_middleware#58",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].insert(0, mw.close_spider)",
        "begin_line": 58,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_any_domain#20",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_any_domain(url, domains)",
        "snippet": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)",
        "begin_line": 20,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_spider#29",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_spider(url, spider)",
        "snippet": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url,\n        [spider.name] + list(getattr(spider, 'allowed_domains', [])))",
        "begin_line": 29,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url.canonicalize_url#39",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.canonicalize_url(url, keep_blank_values=True, keep_fragments=False, encoding=None)",
        "snippet": "def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                     encoding=None):\n    \"\"\"Canonicalize the given url by applying the following procedures:\n\n    - sort query arguments, first by key, then by value\n    - percent encode paths and query arguments. non-ASCII characters are\n      percent-encoded using UTF-8 (RFC-3986)\n    - normalize all spaces (in query arguments) '+' (plus symbol)\n    - normalize percent encodings case (%2f -> %2F)\n    - remove query arguments with blank values (unless keep_blank_values is True)\n    - remove fragments (unless keep_fragments is True)\n\n    The url passed can be a str or unicode, while the url returned is always a\n    str.\n\n    For examples see the tests in tests/test_utils_url.py\n    \"\"\"\n\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n    keyvals.sort()\n    query = urlencode(keyvals)\n\n    # XXX: copied from w3lib.url.safe_url_string to add encoding argument\n    # path = to_native_str(path, encoding)\n    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'\n\n    path = safe_url_string(_unquotepath(path)) or '/'\n    fragment = '' if not keep_fragments else fragment\n    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))",
        "begin_line": 39,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005583472920156337,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url._unquotepath#71",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url._unquotepath(path)",
        "snippet": "def _unquotepath(path):\n    for reserved in ('2f', '2F', '3f', '3F'):\n        path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)",
        "begin_line": 71,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005583472920156337,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url.parse_url#77",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.parse_url(url, encoding=None)",
        "snippet": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_native_str(url, encoding))",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005452562704471102,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.url.escape_ajax#86",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.escape_ajax(url)",
        "snippet": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])",
        "begin_line": 86,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007633587786259542,
            "pseudo_dstar_susp": 0.007633587786259542,
            "pseudo_tarantula_susp": 0.007692307692307693,
            "pseudo_op2_susp": 0.007633587786259542,
            "pseudo_barinel_susp": 0.007692307692307693
        }
    },
    {
        "name": "scrapy.utils.log.failure_to_exc_info#18",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.failure_to_exc_info(failure)",
        "snippet": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__init__#25",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__init__(self, name=None, **kwargs)",
        "snippet": "    def __init__(self, name=None, **kwargs):\n        if name is not None:\n            self.name = name\n        elif not getattr(self, 'name', None):\n            raise ValueError(\"%s must have a name\" % type(self).__name__)\n        self.__dict__.update(kwargs)\n        if not hasattr(self, 'start_urls'):\n            self.start_urls = []",
        "begin_line": 25,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010526315789473684,
            "pseudo_dstar_susp": 0.010416666666666666,
            "pseudo_tarantula_susp": 0.010638297872340425,
            "pseudo_op2_susp": 0.010416666666666666,
            "pseudo_barinel_susp": 0.010638297872340425
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.handles_request#83",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.handles_request(cls, request)",
        "snippet": "    def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.__init__#17",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop('formdata', None)\n        if formdata and kwargs.get('method') is None:\n            kwargs['method'] = 'POST'\n\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            querystr = _urlencode(items, self.encoding)\n            if self.method == 'POST':\n                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                self._set_body(querystr)\n            else:\n                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)",
        "begin_line": 17,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.from_response#34",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None, clickdata=None, dont_click=False, formxpath=None, **kwargs)",
        "snippet": "    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, **kwargs):\n        kwargs.setdefault('encoding', response.encoding)\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n        method = kwargs.pop('method', form.method)\n        return cls(url=url, method=method, formdata=formdata, **kwargs)",
        "begin_line": 34,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._get_form_url#44",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form_url(form, url)",
        "snippet": "def _get_form_url(form, url):\n    if url is None:\n        return form.action or form.base_url\n    return urljoin(form.base_url, url)",
        "begin_line": 44,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._urlencode#50",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._urlencode(seq, enc)",
        "snippet": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)",
        "begin_line": 50,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._get_form#57",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form(response, formname, formid, formnumber, formxpath)",
        "snippet": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    from scrapy.selector.lxmldocument import LxmlDocument\n    root = LxmlDocument(response, lxml.html.HTMLParser)\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(\"No <form> element found in %s\" % response)\n\n    if formname is not None:\n        f = root.xpath('//form[@name=\"%s\"]' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath('//form[@id=\"%s\"]' % formid)\n        if f:\n            return f[0]\n            \n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError('No <form> element found with %s' % formxpath)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(\"Form number %d not found in %s\" %\n                             (formnumber, response))\n        else:\n            return form",
        "begin_line": 57,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._get_inputs#100",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_inputs(form, formdata, dont_click, clickdata, response)",
        "snippet": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[@type!=\"submit\" and @type!=\"image\" and @type!=\"reset\"'\n                        'and ((@type!=\"checkbox\" and @type!=\"radio\") or @checked)]')\n    values = [(k, u'' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values",
        "begin_line": 100,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._value#123",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._value(ele)",
        "snippet": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v",
        "begin_line": 123,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.request.form._get_clickable#146",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_clickable(clickdata, form)",
        "snippet": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = [el for el in form.xpath('.//input[@type=\"submit\"]')]\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.name, el.value)\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.name, el.value)\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u'.//*' + \\\n            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].name, el[0].value)\n    elif len(el) > 1:\n        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n                         \"in clickdata: %r\" % (el, clickdata))\n    else:\n        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))",
        "begin_line": 146,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__init__#14",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__init__(self, xmltext)",
        "snippet": "    def __init__(self, xmltext):\n        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006313131313131314,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__iter__#20",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__iter__(self)",
        "snippet": "    def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d",
        "begin_line": 20,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.sitemap.sitemap_urls_from_robots#37",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap",
        "signature": "scrapy.utils.sitemap.sitemap_urls_from_robots(robots_text)",
        "snippet": "def sitemap_urls_from_robots(robots_text):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().startswith('Sitemap:'):\n            yield line.split(':', 1)[1].strip()",
        "begin_line": 37,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.__init__#14",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.__init__(self, contracts)",
        "snippet": "    def __init__(self, contracts):\n        for contract in contracts:\n            self.contracts[contract.name] = contract",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.extract_contracts#27",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.extract_contracts(self, method)",
        "snippet": "    def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts",
        "begin_line": 27,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_method#48",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_method(self, method, results)",
        "snippet": "    def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            # calculate request args\n            args, kwargs = get_spec(Request.__init__)\n            kwargs['callback'] = method\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            # create and prepare request\n            args.remove('self')\n            if set(args).issubset(set(kwargs)):\n                request = Request(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request",
        "begin_line": 48,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager._clean_req#71",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager._clean_req(self, request, method, results)",
        "snippet": "    def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper",
        "begin_line": 71,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.cb_wrapper#77",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.cb_wrapper(response)",
        "snippet": "        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.eb_wrapper#85",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.eb_wrapper(failure)",
        "snippet": "        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)",
        "begin_line": 85,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.__init__#97",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.__init__(self, method, *args)",
        "snippet": "    def __init__(self, method, *args):\n        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n        self.args = args",
        "begin_line": 97,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_pre_hook#102",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_pre_hook(self, request, results)",
        "snippet": "    def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response)))\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 102,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_post_hook#125",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_post_hook(self, request, results)",
        "snippet": "    def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 125,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.wrapper#130",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.wrapper(response)",
        "snippet": "            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output",
        "begin_line": 130,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.adjust_request_args#149",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        return args",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__",
        "signature": "scrapy.contracts.__init__._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase.__str__#157",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase.__str__(_self)",
        "snippet": "        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.open#18",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.open(self)",
        "snippet": "    def open(self):  # can return deferred\n        pass",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.__init__#31",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.__init__(self, path=None, debug=False)",
        "snippet": "    def __init__(self, path=None, debug=False):\n        self.file = None\n        self.fingerprints = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.fingerprints.update(x.rstrip() for x in self.file)",
        "begin_line": 31,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_seen#46",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_seen(self, request)",
        "snippet": "    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)",
        "begin_line": 46,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint#54",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint(self, request)",
        "snippet": "    def request_fingerprint(self, request):\n        return request_fingerprint(request)",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.close#57",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.file:\n            self.file.close()",
        "begin_line": 57,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__init__#9",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__init__(self, seq=None, encoding='utf-8')",
        "snippet": "    def __init__(self, seq=None, encoding='utf-8'):\n        self.encoding = encoding\n        super(Headers, self).__init__(seq)",
        "begin_line": 9,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005780346820809248,
            "pseudo_dstar_susp": 0.005780346820809248,
            "pseudo_tarantula_susp": 0.005780346820809248,
            "pseudo_op2_susp": 0.005780346820809248,
            "pseudo_barinel_susp": 0.005780346820809248
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normkey#13",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())",
        "begin_line": 13,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006097560975609756,
            "pseudo_dstar_susp": 0.006097560975609756,
            "pseudo_tarantula_susp": 0.006134969325153374,
            "pseudo_op2_susp": 0.006097560975609756,
            "pseudo_barinel_susp": 0.006134969325153374
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normvalue#17",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (six.text_type, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]",
        "begin_line": 17,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009615384615384616,
            "pseudo_dstar_susp": 0.16666666666666666,
            "pseudo_tarantula_susp": 0.009708737864077669,
            "pseudo_op2_susp": 0.16666666666666666,
            "pseudo_barinel_susp": 0.009708737864077669
        }
    },
    {
        "name": "scrapy.http.headers.Headers._tobytes#28",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers._tobytes(self, x)",
        "snippet": "    def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, six.text_type):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return six.text_type(x).encode(self.encoding)\n        else:\n            raise TypeError('Unsupported value type: {}'.format(type(x)))",
        "begin_line": 28,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00847457627118644,
            "pseudo_dstar_susp": 0.00847457627118644,
            "pseudo_tarantula_susp": 0.008547008547008548,
            "pseudo_op2_susp": 0.00847457627118644,
            "pseudo_barinel_susp": 0.008547008547008548
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__getitem__#38",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        try:\n            return super(Headers, self).__getitem__(key)[-1]\n        except IndexError:\n            return None",
        "begin_line": 38,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.get#44",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        try:\n            return super(Headers, self).get(key, def_val)[-1]\n        except IndexError:\n            return None",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.getlist#50",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.getlist(self, key, def_val=None)",
        "snippet": "    def getlist(self, key, def_val=None):\n        try:\n            return super(Headers, self).__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []",
        "begin_line": 50,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013157894736842105,
            "pseudo_dstar_susp": 0.01282051282051282,
            "pseudo_tarantula_susp": 0.013157894736842105,
            "pseudo_op2_susp": 0.01282051282051282,
            "pseudo_barinel_susp": 0.013157894736842105
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlist#58",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlist(self, key, list_)",
        "snippet": "    def setlist(self, key, list_):\n        self[key] = list_",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlistdefault#61",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlistdefault(self, key, default_list=())",
        "snippet": "    def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.appendlist#64",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.appendlist(self, key, value)",
        "snippet": "    def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst",
        "begin_line": 64,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006485084306095979,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.items#69",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.items(self)",
        "snippet": "    def items(self):\n        return list(self.iteritems())",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005608524957936063,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.iteritems#72",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.iteritems(self)",
        "snippet": "    def iteritems(self):\n        return ((k, self.getlist(k)) for k in self.keys())",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005608524957936063,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.values#75",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.values(self)",
        "snippet": "    def values(self):\n        return [self[k] for k in self.keys()]",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_string#78",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_string(self)",
        "snippet": "    def to_string(self):\n        return headers_dict_to_raw(self)",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__copy__#81",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__#17",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__(self, maxlength)",
        "snippet": "    def __init__(self, maxlength):\n        self.maxlength = maxlength",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output#27",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter#28",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.conf.build_component_list#11",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.build_component_list(base, custom, convert=update_classpath)",
        "snippet": "def build_component_list(base, custom, convert=update_classpath):\n    \"\"\"Compose a component list based on a custom and base dict of components\n    (typically middlewares or extensions), unless custom is already a list, in\n    which case it's returned.\n    \"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))\n\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    def _map_keys(compdict):\n        _check_components(compdict)\n        return {convert(k): v for k, v in six.iteritems(compdict)}\n\n    compdict = _map_keys(base)\n    compdict.update(_map_keys(custom))\n    items = (x for x in six.iteritems(compdict) if x[1] is not None)\n    return [x[0] for x in sorted(items, key=itemgetter(1))]",
        "begin_line": 11,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.conf._check_components#17",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._check_components(complist)",
        "snippet": "    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.conf._map_keys#26",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._map_keys(compdict)",
        "snippet": "    def _map_keys(compdict):\n        _check_components(compdict)\n        return {convert(k): v for k, v in six.iteritems(compdict)}",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.conf.arglist_to_dict#36",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.arglist_to_dict(arglist)",
        "snippet": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__init__#167",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__init__(self, seq=None)",
        "snippet": "    def __init__(self, seq=None):\n        super(CaselessDict, self).__init__()\n        if seq:\n            self.update(seq)",
        "begin_line": 167,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008771929824561403,
            "pseudo_dstar_susp": 0.008771929824561403,
            "pseudo_tarantula_susp": 0.008928571428571428,
            "pseudo_op2_susp": 0.008771929824561403,
            "pseudo_barinel_susp": 0.008928571428571428
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__getitem__#172",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))",
        "begin_line": 172,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.009523809523809525,
            "pseudo_dstar_susp": 0.009433962264150943,
            "pseudo_tarantula_susp": 0.009615384615384616,
            "pseudo_op2_susp": 0.009433962264150943,
            "pseudo_barinel_susp": 0.009615384615384616
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__setitem__#175",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))",
        "begin_line": 175,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005506607929515419,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__delitem__#178",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))",
        "begin_line": 178,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__contains__#181",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))",
        "begin_line": 181,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012658227848101266,
            "pseudo_dstar_susp": 0.012345679012345678,
            "pseudo_tarantula_susp": 0.012658227848101266,
            "pseudo_op2_susp": 0.012345679012345678,
            "pseudo_barinel_susp": 0.012658227848101266
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__copy__#185",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 185,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normkey#189",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()",
        "begin_line": 189,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006172839506172839,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normvalue#193",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value",
        "begin_line": 193,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0006172839506172839,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.get#197",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 197,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005313496280552603,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.setdefault#200",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.setdefault(self, key, def_val=None)",
        "snippet": "    def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 200,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005608524957936063,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.update#203",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.update(self, seq)",
        "snippet": "    def update(self, seq):\n        seq = seq.items() if isinstance(seq, dict) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super(CaselessDict, self).update(iseq)",
        "begin_line": 203,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01098901098901099,
            "pseudo_dstar_susp": 0.2,
            "pseudo_tarantula_susp": 0.008771929824561403,
            "pseudo_op2_susp": 0.2,
            "pseudo_barinel_susp": 0.008771929824561403
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.fromkeys#209",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.fromkeys(cls, keys, value=None)",
        "snippet": "    def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)",
        "begin_line": 209,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.pop#212",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)",
        "begin_line": 212,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014285714285714285,
            "pseudo_dstar_susp": 0.013888888888888888,
            "pseudo_tarantula_susp": 0.014492753623188406,
            "pseudo_op2_susp": 0.013888888888888888,
            "pseudo_barinel_susp": 0.014492753623188406
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.push#14",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.push(self, obj)",
        "snippet": "        def push(self, obj):\n            s = serialize(obj)\n            super(SerializableQueue, self).push(s)",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005254860746190226,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.pop#18",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.pop(self)",
        "snippet": "        def pop(self):\n            s = super(SerializableQueue, self).pop()\n            if s:\n                return deserialize(s)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005254860746190226,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.squeues._pickle_serialize#25",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues",
        "signature": "scrapy.squeues._pickle_serialize(obj)",
        "snippet": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=2)\n    except pickle.PicklingError as e:\n        raise ValueError(str(e))",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0005330490405117271,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__#28",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__(self)",
        "snippet": "    def __init__(self):\n        self._formats = {\n            'tar': self._is_tar,\n            'zip': self._is_zip,\n            'gz': self._is_gzip,\n            'bz2': self._is_bzip2\n        }",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0008718395815170009,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar#36",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar(self, response)",
        "snippet": "    def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip#47",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip(self, response)",
        "snippet": "    def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip#59",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip(self, response)",
        "snippet": "    def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 59,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2#69",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2(self, response)",
        "snippet": "    def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 69,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response#78",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in six.iteritems(self._formats):\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response",
        "begin_line": 78,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "conftest.chdir#38",
        "src_path": "conftest.py",
        "class_name": "conftest",
        "signature": "conftest.chdir(tmpdir)",
        "snippet": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()",
        "begin_line": 38,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005434782608695652,
            "pseudo_dstar_susp": 0.005434782608695652,
            "pseudo_tarantula_susp": 0.005434782608695652,
            "pseudo_op2_susp": 0.005434782608695652,
            "pseudo_barinel_susp": 0.005434782608695652
        }
    },
    {
        "name": "scrapy.selector.unified.SafeXMLParser.__init__#21",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SafeXMLParser",
        "signature": "scrapy.selector.unified.SafeXMLParser.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        kwargs.setdefault('resolve_entities', False)\n        super(SafeXMLParser, self).__init__(*args, **kwargs)",
        "begin_line": 21,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified._st#35",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._st(response, st)",
        "snippet": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    elif st in ('xml', 'html'):\n        return st\n    else:\n        raise ValueError('Invalid type: %s' % st)",
        "begin_line": 35,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.__init__#69",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.__init__(self, response=None, text=None, type=None, namespaces=None, _root=None, _expr=None)",
        "snippet": "    def __init__(self, response=None, text=None, type=None, namespaces=None,\n                 _root=None, _expr=None):\n        self.type = st = _st(response, type or self._default_type)\n        self._parser = _ctgroup[st]['_parser']\n        self._csstranslator = _ctgroup[st]['_csstranslator']\n        self._tostring_method = _ctgroup[st]['_tostring_method']\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            _root = LxmlDocument(response, self._parser)\n\n        self.response = response\n        self.namespaces = dict(self._default_namespaces)\n        if namespaces is not None:\n            self.namespaces.update(namespaces)\n        self._root = _root\n        self._expr = _expr",
        "begin_line": 69,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.xpath#89",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.xpath(self, query)",
        "snippet": "    def xpath(self, query):\n        try:\n            xpathev = self._root.xpath\n        except AttributeError:\n            return SelectorList([])\n\n        try:\n            result = xpathev(query, namespaces=self.namespaces,\n                             smart_strings=self._lxml_smart_strings)\n        except etree.XPathError:\n            msg = u\"Invalid XPath: %s\" % query\n            raise ValueError(msg if six.PY3 else msg.encode(\"unicode_escape\"))\n\n        if type(result) is not list:\n            result = [result]\n\n        result = [self.__class__(_root=x, _expr=query,\n                                 namespaces=self.namespaces,\n                                 type=self.type)\n                  for x in result]\n        return SelectorList(result)",
        "begin_line": 89,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.css#111",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.xpath(self._css2xpath(query))",
        "begin_line": 111,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector._css2xpath#114",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector._css2xpath(self, query)",
        "snippet": "    def _css2xpath(self, query):\n        return self._csstranslator.css_to_xpath(query)",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.re#117",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.re(self, regex)",
        "snippet": "    def re(self, regex):\n        return extract_regex(regex, self.extract())",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.001076426264800861,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.extract#120",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.extract(self)",
        "snippet": "    def extract(self):\n        try:\n            return etree.tostring(self._root,\n                                  method=self._tostring_method,\n                                  encoding=\"unicode\",\n                                  with_tail=False)\n        except (AttributeError, TypeError):\n            if self._root is True:\n                return u'1'\n            elif self._root is False:\n                return u'0'\n            else:\n                return six.text_type(self._root)",
        "begin_line": 120,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.selector.unified.SelectorList.extract#184",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SelectorList",
        "signature": "scrapy.selector.unified.SelectorList.extract(self)",
        "snippet": "    def extract(self):\n        return [x.extract() for x in self]",
        "begin_line": 184,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007027406886858749,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__#59",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__(self, uri, _stdout=sys.stdout)",
        "snippet": "    def __init__(self, uri, _stdout=sys.stdout):\n        self._stdout = _stdout",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.open#62",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        return self._stdout",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.store#65",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        pass",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.__init__#72",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.0007401924500370096,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.open#75",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')",
        "begin_line": 75,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.002036659877800407,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.store#81",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        file.close()",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009551098376313276,
            "pseudo_dstar_susp": 0.0009551098376313276,
            "pseudo_tarantula_susp": 0.0009551098376313276,
            "pseudo_op2_susp": 0.00078003120124805,
            "pseudo_barinel_susp": 0.0009551098376313276
        }
    }
]