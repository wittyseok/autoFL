[
    {
        "name": "spacy.tokens._serialize.DocBin.__init__#45",
        "src_path": "spacy/tokens/_serialize.py",
        "class_name": "spacy.tokens._serialize.DocBin",
        "signature": "spacy.tokens._serialize.DocBin.__init__(self, attrs=None, store_user_data=False)",
        "snippet": "    def __init__(self, attrs=None, store_user_data=False):\n        \"\"\"Create a DocBin object to hold serialized annotations.\n\n        attrs (list): List of attributes to serialize. 'orth' and 'spacy' are\n            always serialized, so they're not required. Defaults to None.\n        store_user_data (bool): Whether to include the `Doc.user_data`.\n        RETURNS (DocBin): The newly constructed object.\n\n        DOCS: https://spacy.io/api/docbin#init\n        \"\"\"\n        attrs = attrs or []\n        attrs = sorted([intify_attr(attr) for attr in attrs])\n        self.attrs = [attr for attr in attrs if attr != ORTH and attr != SPACY]\n        self.attrs.insert(0, ORTH)  # Ensure ORTH is always attrs[0]\n        self.tokens = []\n        self.spaces = []\n        self.cats = []\n        self.user_data = []\n        self.strings = set()\n        self.store_user_data = store_user_data",
        "begin_line": 45,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens._serialize.DocBin.add#70",
        "src_path": "spacy/tokens/_serialize.py",
        "class_name": "spacy.tokens._serialize.DocBin",
        "signature": "spacy.tokens._serialize.DocBin.add(self, doc)",
        "snippet": "    def add(self, doc):\n        \"\"\"Add a Doc's annotations to the DocBin for serialization.\n\n        doc (Doc): The Doc object to add.\n\n        DOCS: https://spacy.io/api/docbin#add\n        \"\"\"\n        array = doc.to_array(self.attrs)\n        if len(array.shape) == 1:\n            array = array.reshape((array.shape[0], 1))\n        self.tokens.append(array)\n        spaces = doc.to_array(SPACY)\n        assert array.shape[0] == spaces.shape[0]  # this should never happen\n        spaces = spaces.reshape((spaces.shape[0], 1))\n        self.spaces.append(numpy.asarray(spaces, dtype=bool))\n        self.strings.update(w.text for w in doc)\n        self.cats.append(doc.cats)\n        if self.store_user_data:\n            self.user_data.append(srsly.msgpack_dumps(doc.user_data))",
        "begin_line": 70,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens._serialize.DocBin.get_docs#90",
        "src_path": "spacy/tokens/_serialize.py",
        "class_name": "spacy.tokens._serialize.DocBin",
        "signature": "spacy.tokens._serialize.DocBin.get_docs(self, vocab)",
        "snippet": "    def get_docs(self, vocab):\n        \"\"\"Recover Doc objects from the annotations, using the given vocab.\n\n        vocab (Vocab): The shared vocab.\n        YIELDS (Doc): The Doc objects.\n\n        DOCS: https://spacy.io/api/docbin#get_docs\n        \"\"\"\n        for string in self.strings:\n            vocab[string]\n        orth_col = self.attrs.index(ORTH)\n        for i in range(len(self.tokens)):\n            tokens = self.tokens[i]\n            spaces = self.spaces[i]\n            words = [vocab.strings[orth] for orth in tokens[:, orth_col]]\n            doc = Doc(vocab, words=words, spaces=spaces)\n            doc = doc.from_array(self.attrs, tokens)\n            doc.cats = self.cats[i]\n            if self.store_user_data:\n                user_data = srsly.msgpack_loads(self.user_data[i], use_list=False)\n                doc.user_data.update(user_data)\n            yield doc",
        "begin_line": 90,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens._serialize.DocBin.to_bytes#131",
        "src_path": "spacy/tokens/_serialize.py",
        "class_name": "spacy.tokens._serialize.DocBin",
        "signature": "spacy.tokens._serialize.DocBin.to_bytes(self)",
        "snippet": "    def to_bytes(self):\n        \"\"\"Serialize the DocBin's annotations to a bytestring.\n\n        RETURNS (bytes): The serialized DocBin.\n\n        DOCS: https://spacy.io/api/docbin#to_bytes\n        \"\"\"\n        for tokens in self.tokens:\n            assert len(tokens.shape) == 2, tokens.shape  # this should never happen\n        lengths = [len(tokens) for tokens in self.tokens]\n        msg = {\n            \"attrs\": self.attrs,\n            \"tokens\": numpy.vstack(self.tokens).tobytes(\"C\"),\n            \"spaces\": numpy.vstack(self.spaces).tobytes(\"C\"),\n            \"lengths\": numpy.asarray(lengths, dtype=\"int32\").tobytes(\"C\"),\n            \"strings\": list(self.strings),\n            \"cats\": self.cats,\n        }\n        if self.store_user_data:\n            msg[\"user_data\"] = self.user_data\n        return zlib.compress(srsly.msgpack_dumps(msg))",
        "begin_line": 131,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens._serialize.DocBin.from_bytes#153",
        "src_path": "spacy/tokens/_serialize.py",
        "class_name": "spacy.tokens._serialize.DocBin",
        "signature": "spacy.tokens._serialize.DocBin.from_bytes(self, bytes_data)",
        "snippet": "    def from_bytes(self, bytes_data):\n        \"\"\"Deserialize the DocBin's annotations from a bytestring.\n\n        bytes_data (bytes): The data to load from.\n        RETURNS (DocBin): The loaded DocBin.\n\n        DOCS: https://spacy.io/api/docbin#from_bytes\n        \"\"\"\n        msg = srsly.msgpack_loads(zlib.decompress(bytes_data))\n        self.attrs = msg[\"attrs\"]\n        self.strings = set(msg[\"strings\"])\n        lengths = numpy.frombuffer(msg[\"lengths\"], dtype=\"int32\")\n        flat_spaces = numpy.frombuffer(msg[\"spaces\"], dtype=bool)\n        flat_tokens = numpy.frombuffer(msg[\"tokens\"], dtype=\"uint64\")\n        shape = (flat_tokens.size // len(self.attrs), len(self.attrs))\n        flat_tokens = flat_tokens.reshape(shape)\n        flat_spaces = flat_spaces.reshape((flat_spaces.size, 1))\n        self.tokens = NumpyOps().unflatten(flat_tokens, lengths)\n        self.spaces = NumpyOps().unflatten(flat_spaces, lengths)\n        self.cats = msg[\"cats\"]\n        if self.store_user_data and \"user_data\" in msg:\n            self.user_data = list(msg[\"user_data\"])\n        for tokens in self.tokens:\n            assert len(tokens.shape) == 2, tokens.shape  # this should never happen\n        return self",
        "begin_line": 153,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.yo.lex_attrs.strip_accents_text#90",
        "src_path": "spacy/lang/yo/lex_attrs.py",
        "class_name": "spacy.lang.yo.lex_attrs",
        "signature": "spacy.lang.yo.lex_attrs.strip_accents_text(text)",
        "snippet": "def strip_accents_text(text):\n    \"\"\"\n    Converts the string to NFD, separates & returns only the base characters\n    :param text:\n    :return: input string without diacritic adornments on base characters\n    \"\"\"\n    return \"\".join(\n        c for c in unicodedata.normalize(\"NFD\", text) if unicodedata.category(c) != \"Mn\"\n    )",
        "begin_line": 90,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5442152140325365e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.yo.lex_attrs.like_num#101",
        "src_path": "spacy/lang/yo/lex_attrs.py",
        "class_name": "spacy.lang.yo.lex_attrs",
        "signature": "spacy.lang.yo.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    num_markers = [\"d\u00ed\", \"d\u1ecd\", \"l\u00e9\", \"d\u00edn\", \"di\", \"din\", \"le\", \"do\"]\n    if any(mark in text for mark in num_markers):\n        return True\n    text = strip_accents_text(text)\n    _num_words_stripped = [strip_accents_text(num) for num in _num_words]\n    if text.isdigit():\n        return True\n    if text in _num_words_stripped or text.lower() in _num_words_stripped:\n        return True\n    return False",
        "begin_line": 101,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.en.__init__._return_en#19",
        "src_path": "spacy/lang/en/__init__.py",
        "class_name": "spacy.lang.en.__init__",
        "signature": "spacy.lang.en.__init__._return_en(_)",
        "snippet": "def _return_en(_):\n    return \"en\"",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003215434083601286,
            "pseudo_dstar_susp": 0.003215434083601286,
            "pseudo_tarantula_susp": 0.003215434083601286,
            "pseudo_op2_susp": 0.003215434083601286,
            "pseudo_barinel_susp": 0.003215434083601286
        }
    },
    {
        "name": "spacy.pipeline.functions.merge_noun_chunks#14",
        "src_path": "spacy/pipeline/functions.py",
        "class_name": "spacy.pipeline.functions",
        "signature": "spacy.pipeline.functions.merge_noun_chunks(doc)",
        "snippet": "def merge_noun_chunks(doc):\n    \"\"\"Merge noun chunks into a single token.\n\n    doc (Doc): The Doc object.\n    RETURNS (Doc): The Doc object with merged noun chunks.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_noun_chunks\n    \"\"\"\n    if not doc.is_parsed:\n        return doc\n    with doc.retokenize() as retokenizer:\n        for np in doc.noun_chunks:\n            attrs = {\"tag\": np.root.tag, \"dep\": np.root.dep}\n            retokenizer.merge(np, attrs=attrs)\n    return doc",
        "begin_line": 14,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.functions.merge_entities#36",
        "src_path": "spacy/pipeline/functions.py",
        "class_name": "spacy.pipeline.functions",
        "signature": "spacy.pipeline.functions.merge_entities(doc)",
        "snippet": "def merge_entities(doc):\n    \"\"\"Merge entities into a single token.\n\n    doc (Doc): The Doc object.\n    RETURNS (Doc): The Doc object with merged entities.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_entities\n    \"\"\"\n    with doc.retokenize() as retokenizer:\n        for ent in doc.ents:\n            attrs = {\"tag\": ent.root.tag, \"dep\": ent.root.dep, \"ent_type\": ent.label}\n            retokenizer.merge(ent, attrs=attrs)\n    return doc",
        "begin_line": 36,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.functions.merge_subtokens#52",
        "src_path": "spacy/pipeline/functions.py",
        "class_name": "spacy.pipeline.functions",
        "signature": "spacy.pipeline.functions.merge_subtokens(doc, label='subtok')",
        "snippet": "def merge_subtokens(doc, label=\"subtok\"):\n    \"\"\"Merge subtokens into a single token.\n\n    doc (Doc): The Doc object.\n    label (unicode): The subtoken dependency label.\n    RETURNS (Doc): The Doc object with merged subtokens.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_subtokens\n    \"\"\"\n    merger = Matcher(doc.vocab)\n    merger.add(\"SUBTOK\", None, [{\"DEP\": label, \"op\": \"+\"}])\n    matches = merger(doc)\n    spans = filter_spans([doc[start : end + 1] for _, start, end in matches])\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)\n    return doc",
        "begin_line": 52,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.fr.lex_attrs.like_num#28",
        "src_path": "spacy/lang/fr/lex_attrs.py",
        "class_name": "spacy.lang.fr.lex_attrs",
        "signature": "spacy.lang.fr.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    # Might require more work?\n    # See this discussion: https://github.com/explosion/spaCy/pull/1161\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    if text.lower() in _ordinal_words:\n        return True\n    return False",
        "begin_line": 28,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.cosine#37",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.cosine(vec1, vec2)",
        "snippet": "def cosine(vec1, vec2):\n    xp = get_array_module(vec1)\n    norm1 = xp.linalg.norm(vec1)\n    norm2 = xp.linalg.norm(vec2)\n    if norm1 == 0.0 or norm2 == 0.0:\n        return 0\n    else:\n        return vec1.dot(vec2) / (norm1 * norm2)",
        "begin_line": 37,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.create_default_optimizer#47",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.create_default_optimizer(ops, **cfg)",
        "snippet": "def create_default_optimizer(ops, **cfg):\n    learn_rate = util.env_opt(\"learn_rate\", 0.001)\n    beta1 = util.env_opt(\"optimizer_B1\", 0.9)\n    beta2 = util.env_opt(\"optimizer_B2\", 0.999)\n    eps = util.env_opt(\"optimizer_eps\", 1e-8)\n    L2 = util.env_opt(\"L2_penalty\", 1e-6)\n    max_grad_norm = util.env_opt(\"grad_norm_clip\", 1.0)\n    optimizer = Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)\n    optimizer.max_grad_norm = max_grad_norm\n    optimizer.device = ops.device\n    return optimizer",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.446816079686944e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.with_cpu#82",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.with_cpu(ops, model)",
        "snippet": "def with_cpu(ops, model):\n    \"\"\"Wrap a model that should run on CPU, transferring inputs and outputs\n    as necessary.\"\"\"\n    model.to_cpu()\n\n    def with_cpu_forward(inputs, drop=0.0):\n        cpu_outputs, backprop = model.begin_update(_to_cpu(inputs), drop=drop)\n        gpu_outputs = _to_device(ops, cpu_outputs)\n\n        def with_cpu_backprop(d_outputs, sgd=None):\n            cpu_d_outputs = _to_cpu(d_outputs)\n            return backprop(cpu_d_outputs, sgd=sgd)\n\n        return gpu_outputs, with_cpu_backprop\n\n    return wrap(with_cpu_forward, model)",
        "begin_line": 82,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.with_cpu_forward#87",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.with_cpu_forward(inputs, drop=0.0)",
        "snippet": "    def with_cpu_forward(inputs, drop=0.0):\n        cpu_outputs, backprop = model.begin_update(_to_cpu(inputs), drop=drop)\n        gpu_outputs = _to_device(ops, cpu_outputs)\n\n        def with_cpu_backprop(d_outputs, sgd=None):\n            cpu_d_outputs = _to_cpu(d_outputs)\n            return backprop(cpu_d_outputs, sgd=sgd)\n\n        return gpu_outputs, with_cpu_backprop",
        "begin_line": 87,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.575402635431918e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.with_cpu_backprop#91",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.with_cpu_backprop(d_outputs, sgd=None)",
        "snippet": "        def with_cpu_backprop(d_outputs, sgd=None):\n            cpu_d_outputs = _to_cpu(d_outputs)\n            return backprop(cpu_d_outputs, sgd=sgd)",
        "begin_line": 91,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml._to_cpu#100",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml._to_cpu(X)",
        "snippet": "def _to_cpu(X):\n    if isinstance(X, numpy.ndarray):\n        return X\n    elif isinstance(X, tuple):\n        return tuple([_to_cpu(x) for x in X])\n    elif isinstance(X, list):\n        return [_to_cpu(x) for x in X]\n    elif hasattr(X, \"get\"):\n        return X.get()\n    else:\n        return X",
        "begin_line": 100,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml._to_device#113",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml._to_device(ops, X)",
        "snippet": "def _to_device(ops, X):\n    if isinstance(X, tuple):\n        return tuple([_to_device(ops, x) for x in X])\n    elif isinstance(X, list):\n        return [_to_device(ops, x) for x in X]\n    else:\n        return ops.asarray(X)",
        "begin_line": 113,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.575402635431918e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.extract_ngrams.__init__#123",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.extract_ngrams",
        "signature": "spacy._ml.extract_ngrams.__init__(self, ngram_size, attr=LOWER)",
        "snippet": "    def __init__(self, ngram_size, attr=LOWER):\n        Model.__init__(self)\n        self.ngram_size = ngram_size\n        self.attr = attr",
        "begin_line": 123,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.extract_ngrams.begin_update#128",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.extract_ngrams",
        "signature": "spacy._ml.extract_ngrams.begin_update(self, docs, drop=0.0)",
        "snippet": "    def begin_update(self, docs, drop=0.0):\n        batch_keys = []\n        batch_vals = []\n        for doc in docs:\n            unigrams = doc.to_array([self.attr])\n            ngrams = [unigrams]\n            for n in range(2, self.ngram_size + 1):\n                ngrams.append(self.ops.ngrams(n, unigrams))\n            keys = self.ops.xp.concatenate(ngrams)\n            keys, vals = self.ops.xp.unique(keys, return_counts=True)\n            batch_keys.append(keys)\n            batch_vals.append(vals)\n        # The dtype here matches what thinc is expecting -- which differs per\n        # platform (by int definition). This should be fixed once the problem\n        # is fixed on Thinc's side.\n        lengths = self.ops.asarray(\n            [arr.shape[0] for arr in batch_keys], dtype=numpy.int_\n        )\n        batch_keys = self.ops.xp.concatenate(batch_keys)\n        batch_vals = self.ops.asarray(self.ops.xp.concatenate(batch_vals), dtype=\"f\")\n        return (batch_keys, batch_vals, lengths), None",
        "begin_line": 128,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine.__init__#171",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine.__init__(self, nO=None, nI=None, nF=None, nP=None, **kwargs)",
        "snippet": "    def __init__(self, nO=None, nI=None, nF=None, nP=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nO = nO\n        self.nP = nP\n        self.nI = nI\n        self.nF = nF",
        "begin_line": 171,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.454938299104557e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine.begin_update#178",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine.begin_update(self, X, drop=0.0)",
        "snippet": "    def begin_update(self, X, drop=0.0):\n        Yf = self.ops.gemm(\n            X, self.W.reshape((self.nF * self.nO * self.nP, self.nI)), trans2=True\n        )\n        Yf = Yf.reshape((Yf.shape[0], self.nF, self.nO, self.nP))\n        Yf = self._add_padding(Yf)\n\n        def backward(dY_ids, sgd=None):\n            dY, ids = dY_ids\n            dY, ids = self._backprop_padding(dY, ids)\n            Xf = X[ids]\n            Xf = Xf.reshape((Xf.shape[0], self.nF * self.nI))\n\n            self.d_b += dY.sum(axis=0)\n            dY = dY.reshape((dY.shape[0], self.nO * self.nP))\n\n            Wopfi = self.W.transpose((1, 2, 0, 3))\n            Wopfi = self.ops.xp.ascontiguousarray(Wopfi)\n            Wopfi = Wopfi.reshape((self.nO * self.nP, self.nF * self.nI))\n            dXf = self.ops.gemm(dY.reshape((dY.shape[0], self.nO * self.nP)), Wopfi)\n\n            # Reuse the buffer\n            dWopfi = Wopfi\n            dWopfi.fill(0.0)\n            self.ops.gemm(dY, Xf, out=dWopfi, trans1=True)\n            dWopfi = dWopfi.reshape((self.nO, self.nP, self.nF, self.nI))\n            # (o, p, f, i) --> (f, o, p, i)\n            self.d_W += dWopfi.transpose((2, 0, 1, 3))\n\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dXf.reshape((dXf.shape[0], self.nF, self.nI))\n\n        return Yf, backward",
        "begin_line": 178,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.46149727848666e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine.backward#185",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine.backward(dY_ids, sgd=None)",
        "snippet": "        def backward(dY_ids, sgd=None):\n            dY, ids = dY_ids\n            dY, ids = self._backprop_padding(dY, ids)\n            Xf = X[ids]\n            Xf = Xf.reshape((Xf.shape[0], self.nF * self.nI))\n\n            self.d_b += dY.sum(axis=0)\n            dY = dY.reshape((dY.shape[0], self.nO * self.nP))\n\n            Wopfi = self.W.transpose((1, 2, 0, 3))\n            Wopfi = self.ops.xp.ascontiguousarray(Wopfi)\n            Wopfi = Wopfi.reshape((self.nO * self.nP, self.nF * self.nI))\n            dXf = self.ops.gemm(dY.reshape((dY.shape[0], self.nO * self.nP)), Wopfi)\n\n            # Reuse the buffer\n            dWopfi = Wopfi\n            dWopfi.fill(0.0)\n            self.ops.gemm(dY, Xf, out=dWopfi, trans1=True)\n            dWopfi = dWopfi.reshape((self.nO, self.nP, self.nF, self.nI))\n            # (o, p, f, i) --> (f, o, p, i)\n            self.d_W += dWopfi.transpose((2, 0, 1, 3))\n\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return dXf.reshape((dXf.shape[0], self.nF, self.nI))",
        "begin_line": 185,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine._add_padding#213",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine._add_padding(self, Yf)",
        "snippet": "    def _add_padding(self, Yf):\n        Yf_padded = self.ops.xp.vstack((self.pad, Yf))\n        return Yf_padded",
        "begin_line": 213,
        "end_line": 215,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.46149727848666e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine._backprop_padding#217",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine._backprop_padding(self, dY, ids)",
        "snippet": "    def _backprop_padding(self, dY, ids):\n        # (1, nF, nO, nP) += (nN, nF, nO, nP) where IDs (nN, nF) < 0\n        mask = ids < 0.0\n        mask = mask.sum(axis=1)\n        d_pad = dY * mask.reshape((ids.shape[0], 1, 1))\n        self.d_pad += d_pad.sum(axis=0)\n        return dY, ids",
        "begin_line": 217,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.575402635431918e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine.init_weights#226",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine.init_weights(model)",
        "snippet": "    def init_weights(model):\n        \"\"\"This is like the 'layer sequential unit variance', but instead\n        of taking the actual inputs, we randomly generate whitened data.\n\n        Why's this all so complicated? We have a huge number of inputs,\n        and the maxout unit makes guessing the dynamics tricky. Instead\n        we set the maxout weights to values that empirically result in\n        whitened outputs given whitened inputs.\n        \"\"\"\n        if (model.W ** 2).sum() != 0.0:\n            return\n        ops = model.ops\n        xp = ops.xp\n        ops.normal_init(model.W, model.nF * model.nI, inplace=True)\n\n        ids = ops.allocate((5000, model.nF), dtype=\"f\")\n        ids += xp.random.uniform(0, 1000, ids.shape)\n        ids = ops.asarray(ids, dtype=\"i\")\n        tokvecs = ops.allocate((5000, model.nI), dtype=\"f\")\n        tokvecs += xp.random.normal(loc=0.0, scale=1.0, size=tokvecs.size).reshape(\n            tokvecs.shape\n        )\n\n        def predict(ids, tokvecs):\n            # nS ids. nW tokvecs. Exclude the padding array.\n            hiddens = model(tokvecs[:-1])  # (nW, f, o, p)\n            vectors = model.ops.allocate((ids.shape[0], model.nO * model.nP), dtype=\"f\")\n            # need nS vectors\n            hiddens = hiddens.reshape(\n                (hiddens.shape[0] * model.nF, model.nO * model.nP)\n            )\n            model.ops.scatter_add(vectors, ids.flatten(), hiddens)\n            vectors = vectors.reshape((vectors.shape[0], model.nO, model.nP))\n            vectors += model.b\n            vectors = model.ops.asarray(vectors)\n            if model.nP >= 2:\n                return model.ops.maxout(vectors)[0]\n            else:\n                return vectors * (vectors >= 0)\n\n        tol_var = 0.01\n        tol_mean = 0.01\n        t_max = 10\n        t_i = 0\n        for t_i in range(t_max):\n            acts1 = predict(ids, tokvecs)\n            var = model.ops.xp.var(acts1)\n            mean = model.ops.xp.mean(acts1)\n            if abs(var - 1.0) >= tol_var:\n                model.W /= model.ops.xp.sqrt(var)\n            elif abs(mean) >= tol_mean:\n                model.b -= mean\n            else:\n                break",
        "begin_line": 226,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.481491440351349e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.PrecomputableAffine.predict#249",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.PrecomputableAffine",
        "signature": "spacy._ml.PrecomputableAffine.predict(ids, tokvecs)",
        "snippet": "        def predict(ids, tokvecs):\n            # nS ids. nW tokvecs. Exclude the padding array.\n            hiddens = model(tokvecs[:-1])  # (nW, f, o, p)\n            vectors = model.ops.allocate((ids.shape[0], model.nO * model.nP), dtype=\"f\")\n            # need nS vectors\n            hiddens = hiddens.reshape(\n                (hiddens.shape[0] * model.nF, model.nO * model.nP)\n            )\n            model.ops.scatter_add(vectors, ids.flatten(), hiddens)\n            vectors = vectors.reshape((vectors.shape[0], model.nO, model.nP))\n            vectors += model.b\n            vectors = model.ops.asarray(vectors)\n            if model.nP >= 2:\n                return model.ops.maxout(vectors)[0]\n            else:\n                return vectors * (vectors >= 0)",
        "begin_line": 249,
        "end_line": 264,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.481491440351349e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.link_vectors_to_models#282",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.link_vectors_to_models(vocab)",
        "snippet": "def link_vectors_to_models(vocab):\n    vectors = vocab.vectors\n    if vectors.name is None:\n        vectors.name = VECTORS_KEY\n        if vectors.data.size != 0:\n            warnings.warn(Warnings.W020.format(shape=vectors.data.shape))\n    ops = Model.ops\n    for word in vocab:\n        if word.orth in vectors.key2row:\n            word.rank = vectors.key2row[word.orth]\n        else:\n            word.rank = util.OOV_RANK\n    data = ops.asarray(vectors.data)\n    # Set an entry here, so that vectors are accessed by StaticVectors\n    # (unideal, I know)\n    key = (ops.device, vectors.name)\n    if key in thinc.extra.load_nlp.VECTORS:\n        if thinc.extra.load_nlp.VECTORS[key].shape != data.shape:\n            # This is a hack to avoid the problem in #3853.\n            old_name = vectors.name\n            new_name = vectors.name + \"_%d\" % data.shape[0]\n            warnings.warn(Warnings.W019.format(old=old_name, new=new_name))\n            vectors.name = new_name\n            key = (ops.device, vectors.name)\n    thinc.extra.load_nlp.VECTORS[key] = data",
        "begin_line": 282,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.Tok2Vec#320",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.Tok2Vec(width, embed_size, **kwargs)",
        "snippet": "def Tok2Vec(width, embed_size, **kwargs):\n    if not USE_MODEL_REGISTRY_TOK2VEC:\n        # Preserve prior tok2vec for backwards compat, in v2.2.2\n        return _legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)\n    pretrained_vectors = kwargs.get(\"pretrained_vectors\", None)\n    cnn_maxout_pieces = kwargs.get(\"cnn_maxout_pieces\", 3)\n    subword_features = kwargs.get(\"subword_features\", True)\n    char_embed = kwargs.get(\"char_embed\", False)\n    conv_depth = kwargs.get(\"conv_depth\", 4)\n    bilstm_depth = kwargs.get(\"bilstm_depth\", 0)\n    conv_window = kwargs.get(\"conv_window\", 1)\n\n    cols = [\"ID\", \"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\", \"ORTH\"]\n\n    doc2feats_cfg = {\"arch\": \"spacy.Doc2Feats.v1\", \"config\": {\"columns\": cols}}\n    if char_embed:\n        embed_cfg = {\n            \"arch\": \"spacy.CharacterEmbed.v1\",\n            \"config\": {\n                \"width\": 64,\n                \"chars\": 6,\n                \"@mix\": {\n                    \"arch\": \"spacy.LayerNormalizedMaxout.v1\",\n                    \"config\": {\"width\": width, \"pieces\": 3},\n                },\n                \"@embed_features\": None,\n            },\n        }\n    else:\n        embed_cfg = {\n            \"arch\": \"spacy.MultiHashEmbed.v1\",\n            \"config\": {\n                \"width\": width,\n                \"rows\": embed_size,\n                \"columns\": cols,\n                \"use_subwords\": subword_features,\n                \"@pretrained_vectors\": None,\n                \"@mix\": {\n                    \"arch\": \"spacy.LayerNormalizedMaxout.v1\",\n                    \"config\": {\"width\": width, \"pieces\": 3},\n                },\n            },\n        }\n        if pretrained_vectors:\n            embed_cfg[\"config\"][\"@pretrained_vectors\"] = {\n                \"arch\": \"spacy.PretrainedVectors.v1\",\n                \"config\": {\n                    \"vectors_name\": pretrained_vectors,\n                    \"width\": width,\n                    \"column\": cols.index(\"ID\"),\n                },\n            }\n    if cnn_maxout_pieces >= 2:\n        cnn_cfg = {\n            \"arch\": \"spacy.MaxoutWindowEncoder.v1\",\n            \"config\": {\n                \"width\": width,\n                \"window_size\": conv_window,\n                \"pieces\": cnn_maxout_pieces,\n                \"depth\": conv_depth,\n            },\n        }\n    else:\n        cnn_cfg = {\n            \"arch\": \"spacy.MishWindowEncoder.v1\",\n            \"config\": {\"width\": width, \"window_size\": conv_window, \"depth\": conv_depth},\n        }\n    bilstm_cfg = {\n        \"arch\": \"spacy.TorchBiLSTMEncoder.v1\",\n        \"config\": {\"width\": width, \"depth\": bilstm_depth},\n    }\n    if conv_depth == 0 and bilstm_depth == 0:\n        encode_cfg = {}\n    elif conv_depth >= 1 and bilstm_depth >= 1:\n        encode_cfg = {\n            \"arch\": \"thinc.FeedForward.v1\",\n            \"config\": {\"children\": [cnn_cfg, bilstm_cfg]},\n        }\n    elif conv_depth >= 1:\n        encode_cfg = cnn_cfg\n    else:\n        encode_cfg = bilstm_cfg\n    config = {\"@doc2feats\": doc2feats_cfg, \"@embed\": embed_cfg, \"@encode\": encode_cfg}\n    return new_ml.Tok2Vec(config)",
        "begin_line": 320,
        "end_line": 403,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.435770049680625e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.logistic#499",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.logistic(X, drop=0.0)",
        "snippet": "def logistic(X, drop=0.0):\n    xp = get_array_module(X)\n    if not isinstance(X, xp.ndarray):\n        X = xp.asarray(X)\n    # Clip to range (-10, 10)\n    X = xp.minimum(X, 10.0, X)\n    X = xp.maximum(X, -10.0, X)\n    Y = 1.0 / (1.0 + xp.exp(-X))\n\n    def logistic_bwd(dY, sgd=None):\n        dX = dY * (Y * (1 - Y))\n        return dX\n\n    return Y, logistic_bwd",
        "begin_line": 499,
        "end_line": 512,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.616592031762153e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.logistic_bwd#508",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.logistic_bwd(dY, sgd=None)",
        "snippet": "    def logistic_bwd(dY, sgd=None):\n        dX = dY * (Y * (1 - Y))\n        return dX",
        "begin_line": 508,
        "end_line": 510,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.zero_init#515",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.zero_init(model)",
        "snippet": "def zero_init(model):\n    def _zero_init_impl(self, X, y):\n        self.W.fill(0)\n\n    model.on_data_hooks.append(_zero_init_impl)\n    return model",
        "begin_line": 515,
        "end_line": 520,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5442152140325365e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml._zero_init_impl#516",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml._zero_init_impl(self, X, y)",
        "snippet": "    def _zero_init_impl(self, X, y):\n        self.W.fill(0)",
        "begin_line": 516,
        "end_line": 517,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5442152140325365e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.build_tagger_model#570",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.build_tagger_model(nr_class, **cfg)",
        "snippet": "def build_tagger_model(nr_class, **cfg):\n    embed_size = util.env_opt(\"embed_size\", 2000)\n    if \"token_vector_width\" in cfg:\n        token_vector_width = cfg[\"token_vector_width\"]\n    else:\n        token_vector_width = util.env_opt(\"token_vector_width\", 96)\n    pretrained_vectors = cfg.get(\"pretrained_vectors\")\n    subword_features = cfg.get(\"subword_features\", True)\n    with Model.define_operators({\">>\": chain, \"+\": add}):\n        if \"tok2vec\" in cfg:\n            tok2vec = cfg[\"tok2vec\"]\n        else:\n            tok2vec = Tok2Vec(\n                token_vector_width,\n                embed_size,\n                subword_features=subword_features,\n                pretrained_vectors=pretrained_vectors,\n            )\n        softmax = with_flatten(Softmax(nr_class, token_vector_width))\n        model = tok2vec >> softmax\n    model.nI = None\n    model.tok2vec = tok2vec\n    model.softmax = softmax\n    return model",
        "begin_line": 570,
        "end_line": 593,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.575402635431918e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.build_text_classifier#638",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.build_text_classifier(nr_class, width=64, **cfg)",
        "snippet": "def build_text_classifier(nr_class, width=64, **cfg):\n    depth = cfg.get(\"depth\", 2)\n    nr_vector = cfg.get(\"nr_vector\", 5000)\n    pretrained_dims = cfg.get(\"pretrained_dims\", 0)\n    with Model.define_operators({\">>\": chain, \"+\": add, \"|\": concatenate, \"**\": clone}):\n        if cfg.get(\"low_data\") and pretrained_dims:\n            model = (\n                SpacyVectors\n                >> flatten_add_lengths\n                >> with_getitem(0, Affine(width, pretrained_dims))\n                >> ParametricAttention(width)\n                >> Pooling(sum_pool)\n                >> Residual(ReLu(width, width)) ** 2\n                >> zero_init(Affine(nr_class, width, drop_factor=0.0))\n                >> logistic\n            )\n            return model\n\n        lower = HashEmbed(width, nr_vector, column=1)\n        prefix = HashEmbed(width // 2, nr_vector, column=2)\n        suffix = HashEmbed(width // 2, nr_vector, column=3)\n        shape = HashEmbed(width // 2, nr_vector, column=4)\n\n        trained_vectors = FeatureExtracter(\n            [ORTH, LOWER, PREFIX, SUFFIX, SHAPE, ID]\n        ) >> with_flatten(\n            uniqued(\n                (lower | prefix | suffix | shape)\n                >> LN(Maxout(width, width + (width // 2) * 3)),\n                column=0,\n            )\n        )\n\n        if pretrained_dims:\n            static_vectors = SpacyVectors >> with_flatten(\n                Affine(width, pretrained_dims)\n            )\n            # TODO Make concatenate support lists\n            vectors = concatenate_lists(trained_vectors, static_vectors)\n            vectors_width = width * 2\n        else:\n            vectors = trained_vectors\n            vectors_width = width\n            static_vectors = None\n        tok2vec = vectors >> with_flatten(\n            LN(Maxout(width, vectors_width))\n            >> Residual((ExtractWindow(nW=1) >> LN(Maxout(width, width * 3)))) ** depth,\n            pad=depth,\n        )\n        cnn_model = (\n            tok2vec\n            >> flatten_add_lengths\n            >> ParametricAttention(width)\n            >> Pooling(sum_pool)\n            >> Residual(zero_init(Maxout(width, width)))\n            >> zero_init(Affine(nr_class, width, drop_factor=0.0))\n        )\n\n        linear_model = build_bow_text_classifier(\n            nr_class,\n            ngram_size=cfg.get(\"ngram_size\", 1),\n            exclusive_classes=cfg.get(\"exclusive_classes\", False),\n        )\n        if cfg.get(\"exclusive_classes\", False):\n            output_layer = Softmax(nr_class, nr_class * 2)\n        else:\n            output_layer = (\n                zero_init(Affine(nr_class, nr_class * 2, drop_factor=0.0)) >> logistic\n            )\n        model = (linear_model | cnn_model) >> output_layer\n        model.tok2vec = chain(tok2vec, flatten)\n    model.nO = nr_class\n    model.lsuv = False\n    return model",
        "begin_line": 638,
        "end_line": 711,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.build_bow_text_classifier#714",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.build_bow_text_classifier(nr_class, ngram_size=1, exclusive_classes=False, no_output_layer=False, **cfg)",
        "snippet": "def build_bow_text_classifier(\n    nr_class, ngram_size=1, exclusive_classes=False, no_output_layer=False, **cfg\n):\n    with Model.define_operators({\">>\": chain}):\n        model = with_cpu(\n            Model.ops, extract_ngrams(ngram_size, attr=ORTH) >> LinearModel(nr_class)\n        )\n        if not no_output_layer:\n            model = model >> (cpu_softmax if exclusive_classes else logistic)\n    model.nO = nr_class\n    return model",
        "begin_line": 714,
        "end_line": 724,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.cpu_softmax#728",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.cpu_softmax(X, drop=0.0)",
        "snippet": "def cpu_softmax(X, drop=0.0):\n    ops = NumpyOps()\n\n    def cpu_softmax_backward(dY, sgd=None):\n        return dY\n\n    return ops.softmax(X), cpu_softmax_backward",
        "begin_line": 728,
        "end_line": 734,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.cpu_softmax_backward#731",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.cpu_softmax_backward(dY, sgd=None)",
        "snippet": "    def cpu_softmax_backward(dY, sgd=None):\n        return dY",
        "begin_line": 731,
        "end_line": 732,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.build_nel_encoder#757",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.build_nel_encoder(embed_width, hidden_width, ner_types, **cfg)",
        "snippet": "def build_nel_encoder(embed_width, hidden_width, ner_types, **cfg):\n    if \"entity_width\" not in cfg:\n        raise ValueError(Errors.E144.format(param=\"entity_width\"))\n\n    conv_depth = cfg.get(\"conv_depth\", 2)\n    cnn_maxout_pieces = cfg.get(\"cnn_maxout_pieces\", 3)\n    pretrained_vectors = cfg.get(\"pretrained_vectors\", None)\n    context_width = cfg.get(\"entity_width\")\n\n    with Model.define_operators({\">>\": chain, \"**\": clone}):\n        # context encoder\n        tok2vec = Tok2Vec(\n            width=hidden_width,\n            embed_size=embed_width,\n            pretrained_vectors=pretrained_vectors,\n            cnn_maxout_pieces=cnn_maxout_pieces,\n            subword_features=True,\n            conv_depth=conv_depth,\n            bilstm_depth=0,\n        )\n\n        model = (\n            tok2vec\n            >> flatten_add_lengths\n            >> Pooling(mean_pool)\n            >> Residual(zero_init(Maxout(hidden_width, hidden_width)))\n            >> zero_init(Affine(context_width, hidden_width, drop_factor=0.0))\n        )\n\n        model.tok2vec = tok2vec\n        model.nO = context_width\n    return model",
        "begin_line": 757,
        "end_line": 788,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.flatten#792",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.flatten(seqs, drop=0.0)",
        "snippet": "def flatten(seqs, drop=0.0):\n    ops = Model.ops\n    lengths = ops.asarray([len(seq) for seq in seqs], dtype=\"i\")\n\n    def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=0)\n\n    X = ops.flatten(seqs, pad=0)\n    return X, finish_update",
        "begin_line": 792,
        "end_line": 800,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.481491440351349e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.finish_update#796",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.finish_update(d_X, sgd=None)",
        "snippet": "    def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=0)",
        "begin_line": 796,
        "end_line": 797,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.wrapped#907",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml",
        "signature": "spacy._ml.wrapped(W, ops)",
        "snippet": "    def wrapped(W, ops):\n        copy_array(W, ops.xp.random.uniform(lo, hi, W.shape))",
        "begin_line": 907,
        "end_line": 908,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.CharacterEmbed.__init__#922",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.CharacterEmbed",
        "signature": "spacy._ml.CharacterEmbed.__init__(self, nM=None, nC=None, **kwargs)",
        "snippet": "    def __init__(self, nM=None, nC=None, **kwargs):\n        Model.__init__(self, **kwargs)\n        self.nM = nM\n        self.nC = nC",
        "begin_line": 922,
        "end_line": 925,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.CharacterEmbed.nO#928",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.CharacterEmbed",
        "signature": "spacy._ml.CharacterEmbed.nO(self)",
        "snippet": "    def nO(self):\n        return self.nM * self.nC",
        "begin_line": 928,
        "end_line": 929,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.CharacterEmbed.nV#932",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.CharacterEmbed",
        "signature": "spacy._ml.CharacterEmbed.nV(self)",
        "snippet": "    def nV(self):\n        return 256",
        "begin_line": 932,
        "end_line": 933,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.CharacterEmbed.begin_update#935",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.CharacterEmbed",
        "signature": "spacy._ml.CharacterEmbed.begin_update(self, docs, drop=0.0)",
        "snippet": "    def begin_update(self, docs, drop=0.0):\n        if not docs:\n            return []\n        ids = []\n        output = []\n        weights = self.vectors\n        # This assists in indexing; it's like looping over this dimension.\n        # Still consider this weird witch craft...But thanks to Mark Neumann\n        # for the tip.\n        nCv = self.ops.xp.arange(self.nC)\n        for doc in docs:\n            doc_ids = doc.to_utf8_array(nr_char=self.nC)\n            doc_vectors = self.ops.allocate((len(doc), self.nC, self.nM))\n            # Let's say I have a 2d array of indices, and a 3d table of data. What numpy\n            # incantation do I chant to get\n            # output[i, j, k] == data[j, ids[i, j], k]?\n            doc_vectors[:, nCv] = weights[nCv, doc_ids[:, nCv]]\n            output.append(doc_vectors.reshape((len(doc), self.nO)))\n            ids.append(doc_ids)\n\n        def backprop_character_embed(d_vectors, sgd=None):\n            gradient = self.d_vectors\n            for doc_ids, d_doc_vectors in zip(ids, d_vectors):\n                d_doc_vectors = d_doc_vectors.reshape((len(doc_ids), self.nC, self.nM))\n                gradient[nCv, doc_ids[:, nCv]] += d_doc_vectors[:, nCv]\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None\n\n        return output, backprop_character_embed",
        "begin_line": 935,
        "end_line": 964,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy._ml.CharacterEmbed.backprop_character_embed#955",
        "src_path": "spacy/_ml.py",
        "class_name": "spacy._ml.CharacterEmbed",
        "signature": "spacy._ml.CharacterEmbed.backprop_character_embed(d_vectors, sgd=None)",
        "snippet": "        def backprop_character_embed(d_vectors, sgd=None):\n            gradient = self.d_vectors\n            for doc_ids, d_doc_vectors in zip(ids, d_vectors):\n                d_doc_vectors = d_doc_vectors.reshape((len(doc_ids), self.nC, self.nM))\n                gradient[nCv, doc_ids[:, nCv]] += d_doc_vectors[:, nCv]\n            if sgd is not None:\n                sgd(self._mem.weights, self._mem.gradient, key=self.id)\n            return None",
        "begin_line": 955,
        "end_line": 962,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.compat.symlink_to#89",
        "src_path": "spacy/compat.py",
        "class_name": "spacy.compat",
        "signature": "spacy.compat.symlink_to(orig, dest)",
        "snippet": "def symlink_to(orig, dest):\n    \"\"\"Create a symlink. Used for model shortcut links.\n\n    orig (unicode / Path): The origin path.\n    dest (unicode / Path): The destination path of the symlink.\n    \"\"\"\n    if is_windows:\n        import subprocess\n\n        subprocess.check_call(\n            [\"mklink\", \"/d\", path2str(orig), path2str(dest)], shell=True\n        )\n    else:\n        orig.symlink_to(dest)",
        "begin_line": 89,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.compat.symlink_remove#105",
        "src_path": "spacy/compat.py",
        "class_name": "spacy.compat",
        "signature": "spacy.compat.symlink_remove(link)",
        "snippet": "def symlink_remove(link):\n    \"\"\"Remove a symlink. Used for model shortcut links.\n\n    link (unicode / Path): The path to the symlink.\n    \"\"\"\n    # https://stackoverflow.com/q/26554135/6400719\n    if os.path.isdir(path2str(link)) and is_windows:\n        # this should only be on Py2.7 and windows\n        os.rmdir(path2str(link))\n    else:\n        os.unlink(path2str(link))",
        "begin_line": 105,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.compat.is_config#118",
        "src_path": "spacy/compat.py",
        "class_name": "spacy.compat",
        "signature": "spacy.compat.is_config(python2=None, python3=None, windows=None, linux=None, osx=None)",
        "snippet": "def is_config(python2=None, python3=None, windows=None, linux=None, osx=None):\n    \"\"\"Check if a specific configuration of Python version and operating system\n    matches the user's setup. Mostly used to display targeted error messages.\n\n    python2 (bool): spaCy is executed with Python 2.x.\n    python3 (bool): spaCy is executed with Python 3.x.\n    windows (bool): spaCy is executed on Windows.\n    linux (bool): spaCy is executed on Linux.\n    osx (bool): spaCy is executed on OS X or macOS.\n    RETURNS (bool): Whether the configuration matches the user's platform.\n\n    DOCS: https://spacy.io/api/top-level#compat.is_config\n    \"\"\"\n    return (\n        python2 in (None, is_python2)\n        and python3 in (None, is_python3)\n        and windows in (None, is_windows)\n        and linux in (None, is_linux)\n        and osx in (None, is_osx)\n    )",
        "begin_line": 118,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.compat.unescape_unicode#161",
        "src_path": "spacy/compat.py",
        "class_name": "spacy.compat",
        "signature": "spacy.compat.unescape_unicode(string)",
        "snippet": "def unescape_unicode(string):\n    \"\"\"Python2.7's re module chokes when compiling patterns that have ranges\n    between escaped unicode codepoints if the two codepoints are unrecognised\n    in the unicode database. For instance:\n\n        re.compile('[\\\\uAA77-\\\\uAA79]').findall(\"hello\")\n\n    Ends up matching every character (on Python 2). This problem doesn't occur\n    if we're dealing with unicode literals.\n    \"\"\"\n    if string is None:\n        return string\n    # We only want to unescape the unicode, so we first must protect the other\n    # backslashes.\n    string = string.replace(\"\\\\\", \"\\\\\\\\\")\n    # Now we remove that protection for the unicode.\n    string = string.replace(\"\\\\\\\\u\", \"\\\\u\")\n    string = string.replace(\"\\\\\\\\U\", \"\\\\U\")\n    # Now we unescape by evaling the string with the AST. This can't execute\n    # code -- it only does the representational level.\n    return ast.literal_eval(\"u'''\" + string + \"'''\")",
        "begin_line": 161,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010869565217391304,
            "pseudo_dstar_susp": 0.010101010101010102,
            "pseudo_tarantula_susp": 0.010869565217391304,
            "pseudo_op2_susp": 0.010101010101010102,
            "pseudo_barinel_susp": 0.010869565217391304
        }
    },
    {
        "name": "spacy.lang.nl.lex_attrs.like_num#25",
        "src_path": "spacy/lang/nl/lex_attrs.py",
        "class_name": "spacy.lang.nl.lex_attrs",
        "signature": "spacy.lang.nl.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    # This only does the most basic check for whether a token is a digit\n    # or matches one of the number words. In order to handle numbers like\n    # \"drie\u00ebntwintig\", more work is required.\n    # See this discussion: https://github.com/explosion/spaCy/pull/1177\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    if text.lower() in _ordinal_words:\n        return True\n    return False",
        "begin_line": 25,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.fr.lemmatizer.FrenchLemmatizer.__call__#20",
        "src_path": "spacy/lang/fr/lemmatizer.py",
        "class_name": "spacy.lang.fr.lemmatizer.FrenchLemmatizer",
        "signature": "spacy.lang.fr.lemmatizer.FrenchLemmatizer.__call__(self, string, univ_pos, morphology=None)",
        "snippet": "    def __call__(self, string, univ_pos, morphology=None):\n        lookup_table = self.lookups.get_table(\"lemma_lookup\", {})\n        if \"lemma_rules\" not in self.lookups:\n            return [lookup_table.get(string, string)]\n        if univ_pos in (NOUN, \"NOUN\", \"noun\"):\n            univ_pos = \"noun\"\n        elif univ_pos in (VERB, \"VERB\", \"verb\"):\n            univ_pos = \"verb\"\n        elif univ_pos in (ADJ, \"ADJ\", \"adj\"):\n            univ_pos = \"adj\"\n        elif univ_pos in (ADP, \"ADP\", \"adp\"):\n            univ_pos = \"adp\"\n        elif univ_pos in (ADV, \"ADV\", \"adv\"):\n            univ_pos = \"adv\"\n        elif univ_pos in (AUX, \"AUX\", \"aux\"):\n            univ_pos = \"aux\"\n        elif univ_pos in (CCONJ, \"CCONJ\", \"cconj\"):\n            univ_pos = \"cconj\"\n        elif univ_pos in (DET, \"DET\", \"det\"):\n            univ_pos = \"det\"\n        elif univ_pos in (PRON, \"PRON\", \"pron\"):\n            univ_pos = \"pron\"\n        elif univ_pos in (PUNCT, \"PUNCT\", \"punct\"):\n            univ_pos = \"punct\"\n        elif univ_pos in (SCONJ, \"SCONJ\", \"sconj\"):\n            univ_pos = \"sconj\"\n        else:\n            return [self.lookup(string)]\n        # See Issue #435 for example of where this logic is requied.\n        if self.is_base_form(univ_pos, morphology):\n            return list(set([string.lower()]))\n        index_table = self.lookups.get_table(\"lemma_index\", {})\n        exc_table = self.lookups.get_table(\"lemma_exc\", {})\n        rules_table = self.lookups.get_table(\"lemma_rules\", {})\n        lemmas = self.lemmatize(\n            string,\n            index_table.get(univ_pos, {}),\n            exc_table.get(univ_pos, {}),\n            rules_table.get(univ_pos, []),\n        )\n        return lemmas",
        "begin_line": 20,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.__init__#29",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.__init__(self, nlp, phrase_matcher_attr=None, validate=False, **cfg)",
        "snippet": "    def __init__(self, nlp, phrase_matcher_attr=None, validate=False, **cfg):\n        \"\"\"Initialize the entitiy ruler. If patterns are supplied here, they\n        need to be a list of dictionaries with a `\"label\"` and `\"pattern\"`\n        key. A pattern can either be a token pattern (list) or a phrase pattern\n        (string). For example: `{'label': 'ORG', 'pattern': 'Apple'}`.\n\n        nlp (Language): The shared nlp object to pass the vocab to the matchers\n            and process phrase patterns.\n        phrase_matcher_attr (int / unicode): Token attribute to match on, passed\n            to the internal PhraseMatcher as `attr`\n        validate (bool): Whether patterns should be validated, passed to\n            Matcher and PhraseMatcher as `validate`\n        patterns (iterable): Optional patterns to load in.\n        overwrite_ents (bool): If existing entities are present, e.g. entities\n            added by the model, overwrite them by matches if necessary.\n        **cfg: Other config parameters. If pipeline component is loaded as part\n            of a model pipeline, this will include all keyword arguments passed\n            to `spacy.load`.\n        RETURNS (EntityRuler): The newly constructed object.\n\n        DOCS: https://spacy.io/api/entityruler#init\n        \"\"\"\n        self.nlp = nlp\n        self.overwrite = cfg.get(\"overwrite_ents\", False)\n        self.token_patterns = defaultdict(list)\n        self.phrase_patterns = defaultdict(list)\n        self.matcher = Matcher(nlp.vocab, validate=validate)\n        if phrase_matcher_attr is not None:\n            if phrase_matcher_attr.upper() == \"TEXT\":\n                phrase_matcher_attr = \"ORTH\"\n            self.phrase_matcher_attr = phrase_matcher_attr\n            self.phrase_matcher = PhraseMatcher(\n                nlp.vocab, attr=self.phrase_matcher_attr, validate=validate\n            )\n        else:\n            self.phrase_matcher_attr = None\n            self.phrase_matcher = PhraseMatcher(nlp.vocab, validate=validate)\n        self.ent_id_sep = cfg.get(\"ent_id_sep\", DEFAULT_ENT_ID_SEP)\n        self._ent_ids = defaultdict(dict)\n        patterns = cfg.get(\"patterns\")\n        if patterns is not None:\n            self.add_patterns(patterns)",
        "begin_line": 29,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.from_nlp#73",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.from_nlp(cls, nlp, **cfg)",
        "snippet": "    def from_nlp(cls, nlp, **cfg):\n        return cls(nlp, **cfg)",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.__len__#76",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.__len__(self)",
        "snippet": "    def __len__(self):\n        \"\"\"The number of all patterns added to the entity ruler.\"\"\"\n        n_token_patterns = sum(len(p) for p in self.token_patterns.values())\n        n_phrase_patterns = sum(len(p) for p in self.phrase_patterns.values())\n        return n_token_patterns + n_phrase_patterns",
        "begin_line": 76,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.616592031762153e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.__contains__#82",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.__contains__(self, label)",
        "snippet": "    def __contains__(self, label):\n        \"\"\"Whether a label is present in the patterns.\"\"\"\n        return label in self.token_patterns or label in self.phrase_patterns",
        "begin_line": 82,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.__call__#86",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.__call__(self, doc)",
        "snippet": "    def __call__(self, doc):\n        \"\"\"Find matches in document and add them as entities.\n\n        doc (Doc): The Doc object in the pipeline.\n        RETURNS (Doc): The Doc with added entities, if available.\n\n        DOCS: https://spacy.io/api/entityruler#call\n        \"\"\"\n        matches = list(self.matcher(doc)) + list(self.phrase_matcher(doc))\n        matches = set(\n            [(m_id, start, end) for m_id, start, end in matches if start != end]\n        )\n        get_sort_key = lambda m: (m[2] - m[1], m[1])\n        matches = sorted(matches, key=get_sort_key, reverse=True)\n        entities = list(doc.ents)\n        new_entities = []\n        seen_tokens = set()\n        for match_id, start, end in matches:\n            if any(t.ent_type for t in doc[start:end]) and not self.overwrite:\n                continue\n            # check for end - 1 here because boundaries are inclusive\n            if start not in seen_tokens and end - 1 not in seen_tokens:\n                if match_id in self._ent_ids:\n                    label, ent_id = self._ent_ids[match_id]\n                    span = Span(doc, start, end, label=label)\n                    if ent_id:\n                        for token in span:\n                            token.ent_id_ = ent_id\n                else:\n                    span = Span(doc, start, end, label=match_id)\n                new_entities.append(span)\n                entities = [\n                    e for e in entities if not (e.start < end and e.end > start)\n                ]\n                seen_tokens.update(range(start, end))\n        doc.ents = entities + new_entities\n        return doc",
        "begin_line": 86,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.labels#125",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.labels(self)",
        "snippet": "    def labels(self):\n        \"\"\"All labels present in the match patterns.\n\n        RETURNS (set): The string labels.\n\n        DOCS: https://spacy.io/api/entityruler#labels\n        \"\"\"\n        keys = set(self.token_patterns.keys())\n        keys.update(self.phrase_patterns.keys())\n        all_labels = set()\n\n        for l in keys:\n            if self.ent_id_sep in l:\n                label, _ = self._split_label(l)\n                all_labels.add(label)\n            else:\n                all_labels.add(l)\n        return tuple(all_labels)",
        "begin_line": 125,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.ent_ids#145",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.ent_ids(self)",
        "snippet": "    def ent_ids(self):\n        \"\"\"All entity ids present in the match patterns `id` properties\n\n        RETURNS (set): The string entity ids.\n\n        DOCS: https://spacy.io/api/entityruler#ent_ids\n        \"\"\"\n        keys = set(self.token_patterns.keys())\n        keys.update(self.phrase_patterns.keys())\n        all_ent_ids = set()\n\n        for l in keys:\n            if self.ent_id_sep in l:\n                _, ent_id = self._split_label(l)\n                all_ent_ids.add(ent_id)\n        return tuple(all_ent_ids)",
        "begin_line": 145,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.patterns#163",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.patterns(self)",
        "snippet": "    def patterns(self):\n        \"\"\"Get all patterns that were added to the entity ruler.\n\n        RETURNS (list): The original patterns, one dictionary per pattern.\n\n        DOCS: https://spacy.io/api/entityruler#patterns\n        \"\"\"\n        all_patterns = []\n        for label, patterns in self.token_patterns.items():\n            for pattern in patterns:\n                ent_label, ent_id = self._split_label(label)\n                p = {\"label\": ent_label, \"pattern\": pattern}\n                if ent_id:\n                    p[\"id\"] = ent_id\n                all_patterns.append(p)\n        for label, patterns in self.phrase_patterns.items():\n            for pattern in patterns:\n                ent_label, ent_id = self._split_label(label)\n                p = {\"label\": ent_label, \"pattern\": pattern.text}\n                if ent_id:\n                    p[\"id\"] = ent_id\n                all_patterns.append(p)\n\n        return all_patterns",
        "begin_line": 163,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.616592031762153e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.add_patterns#188",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.add_patterns(self, patterns)",
        "snippet": "    def add_patterns(self, patterns):\n        \"\"\"Add patterns to the entitiy ruler. A pattern can either be a token\n        pattern (list of dicts) or a phrase pattern (string). For example:\n        {'label': 'ORG', 'pattern': 'Apple'}\n        {'label': 'GPE', 'pattern': [{'lower': 'san'}, {'lower': 'francisco'}]}\n\n        patterns (list): The patterns to add.\n\n        DOCS: https://spacy.io/api/entityruler#add_patterns\n        \"\"\"\n\n        # disable the nlp components after this one in case they hadn't been initialized / deserialised yet\n        try:\n            current_index = self.nlp.pipe_names.index(self.name)\n            subsequent_pipes = [\n                pipe for pipe in self.nlp.pipe_names[current_index + 1 :]\n            ]\n        except ValueError:\n            subsequent_pipes = []\n        with self.nlp.disable_pipes(subsequent_pipes):\n            token_patterns = []\n            phrase_pattern_labels = []\n            phrase_pattern_texts = []\n            phrase_pattern_ids = []\n\n            for entry in patterns:\n                if isinstance(entry[\"pattern\"], basestring_):\n                    phrase_pattern_labels.append(entry[\"label\"])\n                    phrase_pattern_texts.append(entry[\"pattern\"])\n                    phrase_pattern_ids.append(entry.get(\"id\"))\n                elif isinstance(entry[\"pattern\"], list):\n                    token_patterns.append(entry)\n\n            phrase_patterns = []\n            for label, pattern, ent_id in zip(\n                phrase_pattern_labels,\n                self.nlp.pipe(phrase_pattern_texts),\n                phrase_pattern_ids,\n            ):\n                phrase_pattern = {\"label\": label, \"pattern\": pattern, \"id\": ent_id}\n                if ent_id:\n                    phrase_pattern[\"id\"] = ent_id\n                phrase_patterns.append(phrase_pattern)\n\n            for entry in token_patterns + phrase_patterns:\n                label = entry[\"label\"]\n                if \"id\" in entry:\n                    ent_label = label\n                    label = self._create_label(label, entry[\"id\"])\n                    key = self.matcher._normalize_key(label)\n                    self._ent_ids[key] = (ent_label, entry[\"id\"])\n\n                pattern = entry[\"pattern\"]\n                if isinstance(pattern, Doc):\n                    self.phrase_patterns[label].append(pattern)\n                elif isinstance(pattern, list):\n                    self.token_patterns[label].append(pattern)\n                else:\n                    raise ValueError(Errors.E097.format(pattern=pattern))\n            for label, patterns in self.token_patterns.items():\n                self.matcher.add(label, patterns)\n            for label, patterns in self.phrase_patterns.items():\n                self.phrase_matcher.add(label, patterns)",
        "begin_line": 188,
        "end_line": 250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler._split_label#252",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler._split_label(self, label)",
        "snippet": "    def _split_label(self, label):\n        \"\"\"Split Entity label into ent_label and ent_id if it contains self.ent_id_sep\n\n        label (str): The value of label in a pattern entry\n\n        RETURNS (tuple): ent_label, ent_id\n        \"\"\"\n        if self.ent_id_sep in label:\n            ent_label, ent_id = label.rsplit(self.ent_id_sep, 1)\n        else:\n            ent_label = label\n            ent_id = None\n\n        return ent_label, ent_id",
        "begin_line": 252,
        "end_line": 265,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler._create_label#267",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler._create_label(self, label, ent_id)",
        "snippet": "    def _create_label(self, label, ent_id):\n        \"\"\"Join Entity label with ent_id if the pattern has an `id` attribute\n\n        label (str): The label to set for ent.label_\n        ent_id (str): The label\n\n        RETURNS (str): The ent_label joined with configured `ent_id_sep`\n        \"\"\"\n        if isinstance(ent_id, basestring_):\n            label = \"{}{}{}\".format(label, self.ent_id_sep, ent_id)\n        return label",
        "begin_line": 267,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.518752824220515e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.from_bytes#279",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.from_bytes(self, patterns_bytes, **kwargs)",
        "snippet": "    def from_bytes(self, patterns_bytes, **kwargs):\n        \"\"\"Load the entity ruler from a bytestring.\n\n        patterns_bytes (bytes): The bytestring to load.\n        **kwargs: Other config paramters, mostly for consistency.\n\n        RETURNS (EntityRuler): The loaded entity ruler.\n\n        DOCS: https://spacy.io/api/entityruler#from_bytes\n        \"\"\"\n        cfg = srsly.msgpack_loads(patterns_bytes)\n        if isinstance(cfg, dict):\n            self.add_patterns(cfg.get(\"patterns\", cfg))\n            self.overwrite = cfg.get(\"overwrite\", False)\n            self.phrase_matcher_attr = cfg.get(\"phrase_matcher_attr\", None)\n            if self.phrase_matcher_attr is not None:\n                self.phrase_matcher = PhraseMatcher(\n                    self.nlp.vocab, attr=self.phrase_matcher_attr\n                )\n            self.ent_id_sep = cfg.get(\"ent_id_sep\", DEFAULT_ENT_ID_SEP)\n        else:\n            self.add_patterns(cfg)\n        return self",
        "begin_line": 279,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.to_bytes#303",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.to_bytes(self, **kwargs)",
        "snippet": "    def to_bytes(self, **kwargs):\n        \"\"\"Serialize the entity ruler patterns to a bytestring.\n\n        RETURNS (bytes): The serialized patterns.\n\n        DOCS: https://spacy.io/api/entityruler#to_bytes\n        \"\"\"\n\n        serial = OrderedDict(\n            (\n                (\"overwrite\", self.overwrite),\n                (\"ent_id_sep\", self.ent_id_sep),\n                (\"phrase_matcher_attr\", self.phrase_matcher_attr),\n                (\"patterns\", self.patterns),\n            )\n        )\n        return srsly.msgpack_dumps(serial)",
        "begin_line": 303,
        "end_line": 319,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.from_disk#321",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.from_disk(self, path, **kwargs)",
        "snippet": "    def from_disk(self, path, **kwargs):\n        \"\"\"Load the entity ruler from a file. Expects a file containing\n        newline-delimited JSON (JSONL) with one entry per line.\n\n        path (unicode / Path): The JSONL file to load.\n        **kwargs: Other config paramters, mostly for consistency.\n\n        RETURNS (EntityRuler): The loaded entity ruler.\n\n        DOCS: https://spacy.io/api/entityruler#from_disk\n        \"\"\"\n        path = ensure_path(path)\n        depr_patterns_path = path.with_suffix(\".jsonl\")\n        if depr_patterns_path.is_file():\n            patterns = srsly.read_jsonl(depr_patterns_path)\n            self.add_patterns(patterns)\n        else:\n            cfg = {}\n            deserializers_patterns = {\n                \"patterns\": lambda p: self.add_patterns(\n                    srsly.read_jsonl(p.with_suffix(\".jsonl\"))\n                )\n            }\n            deserializers_cfg = {\"cfg\": lambda p: cfg.update(srsly.read_json(p))}\n            from_disk(path, deserializers_cfg, {})\n            self.overwrite = cfg.get(\"overwrite\", False)\n            self.phrase_matcher_attr = cfg.get(\"phrase_matcher_attr\")\n            self.ent_id_sep = cfg.get(\"ent_id_sep\", DEFAULT_ENT_ID_SEP)\n\n            if self.phrase_matcher_attr is not None:\n                self.phrase_matcher = PhraseMatcher(\n                    self.nlp.vocab, attr=self.phrase_matcher_attr\n                )\n            from_disk(path, deserializers_patterns, {})\n        return self",
        "begin_line": 321,
        "end_line": 355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.pipeline.entityruler.EntityRuler.to_disk#357",
        "src_path": "spacy/pipeline/entityruler.py",
        "class_name": "spacy.pipeline.entityruler.EntityRuler",
        "signature": "spacy.pipeline.entityruler.EntityRuler.to_disk(self, path, **kwargs)",
        "snippet": "    def to_disk(self, path, **kwargs):\n        \"\"\"Save the entity ruler patterns to a directory. The patterns will be\n        saved as newline-delimited JSON (JSONL).\n\n        path (unicode / Path): The JSONL file to save.\n        **kwargs: Other config paramters, mostly for consistency.\n\n        DOCS: https://spacy.io/api/entityruler#to_disk\n        \"\"\"\n        path = ensure_path(path)\n        cfg = {\n            \"overwrite\": self.overwrite,\n            \"phrase_matcher_attr\": self.phrase_matcher_attr,\n            \"ent_id_sep\": self.ent_id_sep,\n        }\n        serializers = {\n            \"patterns\": lambda p: srsly.write_jsonl(\n                p.with_suffix(\".jsonl\"), self.patterns\n            ),\n            \"cfg\": lambda p: srsly.write_json(p, cfg),\n        }\n        if path.suffix == \".jsonl\":  # user wants to save only JSONL\n            srsly.write_jsonl(path, self.patterns)\n        else:\n            to_disk(path, serializers, {})",
        "begin_line": 357,
        "end_line": 381,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.tl.__init__._return_tl#14",
        "src_path": "spacy/lang/tl/__init__.py",
        "class_name": "spacy.lang.tl.__init__",
        "signature": "spacy.lang.tl.__init__._return_tl(_)",
        "snippet": "def _return_tl(_):\n    return \"tl\"",
        "begin_line": 14,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.hy.lex_attrs.like_num#43",
        "src_path": "spacy/lang/hy/lex_attrs.py",
        "class_name": "spacy.lang.hy.lex_attrs",
        "signature": "spacy.lang.hy.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 43,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.nb.syntax_iterators.noun_chunks#7",
        "src_path": "spacy/lang/nb/syntax_iterators.py",
        "class_name": "spacy.lang.nb.syntax_iterators",
        "signature": "spacy.lang.nb.syntax_iterators.noun_chunks(obj)",
        "snippet": "def noun_chunks(obj):\n    \"\"\"\n    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n    \"\"\"\n    labels = [\n        \"nsubj\",\n        \"nsubj:pass\",\n        \"obj\",\n        \"iobj\",\n        \"ROOT\",\n        \"appos\",\n        \"nmod\",\n        \"nmod:poss\",\n    ]\n    doc = obj.doc  # Ensure works on both Doc and Span.\n    np_deps = [doc.vocab.strings[label] for label in labels]\n    conj = doc.vocab.strings.add(\"conj\")\n    np_label = doc.vocab.strings.add(\"NP\")\n    seen = set()\n    for i, word in enumerate(obj):\n        if word.pos not in (NOUN, PROPN, PRON):\n            continue\n        # Prevent nested chunks from being produced\n        if word.i in seen:\n            continue\n        if word.dep in np_deps:\n            if any(w.i in seen for w in word.subtree):\n                continue\n            seen.update(j for j in range(word.left_edge.i, word.right_edge.i + 1))\n            yield word.left_edge.i, word.right_edge.i + 1, np_label\n        elif word.dep == conj:\n            head = word.head\n            while head.dep == conj and head.head.i < head.i:\n                head = head.head\n            # If the head is an NP, and we're coordinated to it, we're an NP\n            if head.dep in np_deps:\n                if any(w.i in seen for w in word.subtree):\n                    continue\n                seen.update(j for j in range(word.left_edge.i, word.right_edge.i + 1))\n                yield word.left_edge.i, word.right_edge.i + 1, np_label",
        "begin_line": 7,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.el.lex_attrs.like_num#82",
        "src_path": "spacy/lang/el/lex_attrs.py",
        "class_name": "spacy.lang.el.lex_attrs",
        "signature": "spacy.lang.el.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.count(\"^\") == 1:\n        num, denom = text.split(\"^\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words or text.lower().split(\" \")[0] in _num_words:\n        return True\n    if text in _num_words:\n        return True\n    return False",
        "begin_line": 82,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.__init__.load#26",
        "src_path": "spacy/__init__.py",
        "class_name": "spacy.__init__",
        "signature": "spacy.__init__.load(name, **overrides)",
        "snippet": "def load(name, **overrides):\n    depr_path = overrides.get(\"path\")\n    if depr_path not in (True, False, None):\n        warnings.warn(Warnings.W001.format(path=depr_path), DeprecationWarning)\n    return util.load_model(name, **overrides)",
        "begin_line": 26,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.06666666666666667,
            "pseudo_dstar_susp": 0.043478260869565216,
            "pseudo_tarantula_susp": 0.1111111111111111,
            "pseudo_op2_susp": 0.043478260869565216,
            "pseudo_barinel_susp": 0.1111111111111111
        }
    },
    {
        "name": "spacy.__init__.blank#33",
        "src_path": "spacy/__init__.py",
        "class_name": "spacy.__init__",
        "signature": "spacy.__init__.blank(name, **kwargs)",
        "snippet": "def blank(name, **kwargs):\n    LangClass = util.get_lang_class(name)\n    return LangClass(**kwargs)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.526935264825713e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.__init__#24",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.__init__(self)",
        "snippet": "    def __init__(self):\n        \"\"\"Initialize the Lookups object.\n\n        RETURNS (Lookups): The newly created object.\n\n        DOCS: https://spacy.io/api/lookups#init\n        \"\"\"\n        self._tables = OrderedDict()",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003257328990228013,
            "pseudo_dstar_susp": 0.003257328990228013,
            "pseudo_tarantula_susp": 0.003257328990228013,
            "pseudo_op2_susp": 0.003257328990228013,
            "pseudo_barinel_susp": 0.003257328990228013
        }
    },
    {
        "name": "spacy.lookups.Lookups.__contains__#33",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.__contains__(self, name)",
        "snippet": "    def __contains__(self, name):\n        \"\"\"Check if the lookups contain a table of a given name. Delegates to\n        Lookups.has_table.\n\n        name (unicode): Name of the table.\n        RETURNS (bool): Whether a table of that name is in the lookups.\n        \"\"\"\n        return self.has_table(name)",
        "begin_line": 33,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00425531914893617,
            "pseudo_dstar_susp": 0.00425531914893617,
            "pseudo_tarantula_susp": 0.004329004329004329,
            "pseudo_op2_susp": 0.00425531914893617,
            "pseudo_barinel_susp": 0.004329004329004329
        }
    },
    {
        "name": "spacy.lookups.Lookups.__len__#42",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.__len__(self)",
        "snippet": "    def __len__(self):\n        \"\"\"RETURNS (int): The number of tables in the lookups.\"\"\"\n        return len(self._tables)",
        "begin_line": 42,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.tables#47",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.tables(self)",
        "snippet": "    def tables(self):\n        \"\"\"RETURNS (list): Names of all tables in the lookups.\"\"\"\n        return list(self._tables.keys())",
        "begin_line": 47,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.add_table#51",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.add_table(self, name, data=SimpleFrozenDict())",
        "snippet": "    def add_table(self, name, data=SimpleFrozenDict()):\n        \"\"\"Add a new table to the lookups. Raises an error if the table exists.\n\n        name (unicode): Unique name of table.\n        data (dict): Optional data to add to the table.\n        RETURNS (Table): The newly added table.\n\n        DOCS: https://spacy.io/api/lookups#add_table\n        \"\"\"\n        if name in self.tables:\n            raise ValueError(Errors.E158.format(name=name))\n        table = Table(name=name, data=data)\n        self._tables[name] = table\n        return table",
        "begin_line": 51,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.get_table#66",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.get_table(self, name, default=UNSET)",
        "snippet": "    def get_table(self, name, default=UNSET):\n        \"\"\"Get a table. Raises an error if the table doesn't exist and no\n        default value is provided.\n\n        name (unicode): Name of the table.\n        default: Optional default value to return if table doesn't exist.\n        RETURNS (Table): The table.\n\n        DOCS: https://spacy.io/api/lookups#get_table\n        \"\"\"\n        if name not in self._tables:\n            if default == UNSET:\n                raise KeyError(Errors.E159.format(name=name, tables=self.tables))\n            return default\n        return self._tables[name]",
        "begin_line": 66,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004201680672268907,
            "pseudo_dstar_susp": 0.004201680672268907,
            "pseudo_tarantula_susp": 0.004273504273504274,
            "pseudo_op2_susp": 0.004201680672268907,
            "pseudo_barinel_susp": 0.004273504273504274
        }
    },
    {
        "name": "spacy.lookups.Lookups.remove_table#82",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.remove_table(self, name)",
        "snippet": "    def remove_table(self, name):\n        \"\"\"Remove a table. Raises an error if the table doesn't exist.\n\n        name (unicode): Name of the table to remove.\n        RETURNS (Table): The removed table.\n\n        DOCS: https://spacy.io/api/lookups#remove_table\n        \"\"\"\n        if name not in self._tables:\n            raise KeyError(Errors.E159.format(name=name, tables=self.tables))\n        return self._tables.pop(name)",
        "begin_line": 82,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.has_table#94",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.has_table(self, name)",
        "snippet": "    def has_table(self, name):\n        \"\"\"Check if the lookups contain a table of a given name.\n\n        name (unicode): Name of the table.\n        RETURNS (bool): Whether a table of that name exists.\n\n        DOCS: https://spacy.io/api/lookups#has_table\n        \"\"\"\n        return name in self._tables",
        "begin_line": 94,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00425531914893617,
            "pseudo_dstar_susp": 0.00425531914893617,
            "pseudo_tarantula_susp": 0.004329004329004329,
            "pseudo_op2_susp": 0.00425531914893617,
            "pseudo_barinel_susp": 0.004329004329004329
        }
    },
    {
        "name": "spacy.lookups.Lookups.to_bytes#104",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.to_bytes(self, **kwargs)",
        "snippet": "    def to_bytes(self, **kwargs):\n        \"\"\"Serialize the lookups to a bytestring.\n\n        RETURNS (bytes): The serialized Lookups.\n\n        DOCS: https://spacy.io/api/lookups#to_bytes\n        \"\"\"\n        return srsly.msgpack_dumps(self._tables)",
        "begin_line": 104,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.513857542655954e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.from_bytes#113",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.from_bytes(self, bytes_data, **kwargs)",
        "snippet": "    def from_bytes(self, bytes_data, **kwargs):\n        \"\"\"Load the lookups from a bytestring.\n\n        bytes_data (bytes): The data to load.\n        RETURNS (Lookups): The loaded Lookups.\n\n        DOCS: https://spacy.io/api/lookups#from_bytes\n        \"\"\"\n        self._tables = OrderedDict()\n        for key, value in srsly.msgpack_loads(bytes_data).items():\n            self._tables[key] = Table(key)\n            self._tables[key].update(value)\n        return self",
        "begin_line": 113,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Lookups.to_disk#127",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.to_disk(self, path, **kwargs)",
        "snippet": "    def to_disk(self, path, **kwargs):\n        \"\"\"Save the lookups to a directory as lookups.bin. Expects a path to a\n        directory, which will be created if it doesn't exist.\n\n        path (unicode / Path): The file path.\n\n        DOCS: https://spacy.io/api/lookups#to_disk\n        \"\"\"\n        if len(self._tables):\n            path = ensure_path(path)\n            if not path.exists():\n                path.mkdir()\n            filepath = path / \"lookups.bin\"\n            with filepath.open(\"wb\") as file_:\n                file_.write(self.to_bytes())",
        "begin_line": 127,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0078125,
            "pseudo_dstar_susp": 0.007407407407407408,
            "pseudo_tarantula_susp": 0.0078125,
            "pseudo_op2_susp": 0.007407407407407408,
            "pseudo_barinel_susp": 0.0078125
        }
    },
    {
        "name": "spacy.lookups.Lookups.from_disk#143",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Lookups",
        "signature": "spacy.lookups.Lookups.from_disk(self, path, **kwargs)",
        "snippet": "    def from_disk(self, path, **kwargs):\n        \"\"\"Load lookups from a directory containing a lookups.bin. Will skip\n        loading if the file doesn't exist.\n\n        path (unicode / Path): The directory path.\n        RETURNS (Lookups): The loaded lookups.\n\n        DOCS: https://spacy.io/api/lookups#from_disk\n        \"\"\"\n        path = ensure_path(path)\n        filepath = path / \"lookups.bin\"\n        if filepath.exists():\n            with filepath.open(\"rb\") as file_:\n                data = file_.read()\n            return self.from_bytes(data)\n        return self",
        "begin_line": 143,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008,
            "pseudo_dstar_susp": 0.007575757575757576,
            "pseudo_tarantula_susp": 0.008,
            "pseudo_op2_susp": 0.007575757575757576,
            "pseudo_barinel_susp": 0.008
        }
    },
    {
        "name": "spacy.lookups.Table.__init__#182",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.__init__(self, name=None, data=None)",
        "snippet": "    def __init__(self, name=None, data=None):\n        \"\"\"Initialize a new table.\n\n        name (unicode): Optional table name for reference.\n        data (dict): Initial data, used to hint Bloom Filter.\n        RETURNS (Table): The newly created object.\n\n        DOCS: https://spacy.io/api/lookups#table.init\n        \"\"\"\n        OrderedDict.__init__(self)\n        self.name = name\n        # Assume a default size of 1M items\n        self.default_size = 1e6\n        size = len(data) if data and len(data) > 0 else self.default_size\n        self.bloom = BloomFilter.from_error_rate(size)\n        if data:\n            self.update(data)",
        "begin_line": 182,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.__setitem__#200",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        \"\"\"Set new key/value pair. String keys will be hashed.\n\n        key (unicode / int): The key to set.\n        value: The value to set.\n        \"\"\"\n        key = get_string_id(key)\n        OrderedDict.__setitem__(self, key, value)\n        self.bloom.add(key)",
        "begin_line": 200,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5442152140325365e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.set#210",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.set(self, key, value)",
        "snippet": "    def set(self, key, value):\n        \"\"\"Set new key/value pair. String keys will be hashed.\n        Same as table[key] = value.\n\n        key (unicode / int): The key to set.\n        value: The value to set.\n        \"\"\"\n        self[key] = value",
        "begin_line": 210,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.__getitem__#219",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        \"\"\"Get the value for a given key. String keys will be hashed.\n\n        key (unicode / int): The key to get.\n        RETURNS: The value.\n        \"\"\"\n        key = get_string_id(key)\n        return OrderedDict.__getitem__(self, key)",
        "begin_line": 219,
        "end_line": 226,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.get#228",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.get(self, key, default=None)",
        "snippet": "    def get(self, key, default=None):\n        \"\"\"Get the value for a given key. String keys will be hashed.\n\n        key (unicode / int): The key to get.\n        default: The default value to return.\n        RETURNS: The value.\n        \"\"\"\n        key = get_string_id(key)\n        return OrderedDict.get(self, key, default)",
        "begin_line": 228,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.__contains__#238",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        \"\"\"Check whether a key is in the table. String keys will be hashed.\n\n        key (unicode / int): The key to check.\n        RETURNS (bool): Whether the key is in the table.\n        \"\"\"\n        key = get_string_id(key)\n        # This can give a false positive, so we need to check it after\n        if key not in self.bloom:\n            return False\n        return OrderedDict.__contains__(self, key)",
        "begin_line": 238,
        "end_line": 248,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.to_bytes#250",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.to_bytes(self)",
        "snippet": "    def to_bytes(self):\n        \"\"\"Serialize table to a bytestring.\n\n        RETURNS (bytes): The serialized table.\n\n        DOCS: https://spacy.io/api/lookups#table.to_bytes\n        \"\"\"\n        data = [\n            (\"name\", self.name),\n            (\"dict\", dict(self.items())),\n            (\"bloom\", self.bloom.to_bytes()),\n        ]\n        return srsly.msgpack_dumps(OrderedDict(data))",
        "begin_line": 250,
        "end_line": 262,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lookups.Table.from_bytes#264",
        "src_path": "spacy/lookups.py",
        "class_name": "spacy.lookups.Table",
        "signature": "spacy.lookups.Table.from_bytes(self, bytes_data)",
        "snippet": "    def from_bytes(self, bytes_data):\n        \"\"\"Load a table from a bytestring.\n\n        bytes_data (bytes): The data to load.\n        RETURNS (Table): The loaded table.\n\n        DOCS: https://spacy.io/api/lookups#table.from_bytes\n        \"\"\"\n        loaded = srsly.msgpack_loads(bytes_data)\n        data = loaded.get(\"dict\", {})\n        self.name = loaded[\"name\"]\n        self.bloom = BloomFilter().from_bytes(loaded[\"bloom\"])\n        self.clear()\n        self.update(data)\n        return self",
        "begin_line": 264,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.en.lex_attrs.like_num#47",
        "src_path": "spacy/lang/en/lex_attrs.py",
        "class_name": "spacy.lang.en.lex_attrs",
        "signature": "spacy.lang.en.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005291005291005291,
            "pseudo_dstar_susp": 0.0051813471502590676,
            "pseudo_tarantula_susp": 0.005319148936170213,
            "pseudo_op2_susp": 0.0051813471502590676,
            "pseudo_barinel_susp": 0.005319148936170213
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.Tok2Vec#15",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.Tok2Vec(width, embed_size, **kwargs)",
        "snippet": "def Tok2Vec(width, embed_size, **kwargs):\n    # Circular imports :(\n    from .._ml import CharacterEmbed\n    from .._ml import PyTorchBiLSTM\n\n    pretrained_vectors = kwargs.get(\"pretrained_vectors\", None)\n    cnn_maxout_pieces = kwargs.get(\"cnn_maxout_pieces\", 3)\n    subword_features = kwargs.get(\"subword_features\", True)\n    char_embed = kwargs.get(\"char_embed\", False)\n    if char_embed:\n        subword_features = False\n    conv_depth = kwargs.get(\"conv_depth\", 4)\n    bilstm_depth = kwargs.get(\"bilstm_depth\", 0)\n    cols = [ID, NORM, PREFIX, SUFFIX, SHAPE, ORTH]\n    with Model.define_operators({\">>\": chain, \"|\": concatenate, \"**\": clone}):\n        norm = HashEmbed(width, embed_size, column=cols.index(NORM), name=\"embed_norm\")\n        if subword_features:\n            prefix = HashEmbed(\n                width, embed_size // 2, column=cols.index(PREFIX), name=\"embed_prefix\"\n            )\n            suffix = HashEmbed(\n                width, embed_size // 2, column=cols.index(SUFFIX), name=\"embed_suffix\"\n            )\n            shape = HashEmbed(\n                width, embed_size // 2, column=cols.index(SHAPE), name=\"embed_shape\"\n            )\n        else:\n            prefix, suffix, shape = (None, None, None)\n        if pretrained_vectors is not None:\n            glove = StaticVectors(pretrained_vectors, width, column=cols.index(ID))\n\n            if subword_features:\n                embed = uniqued(\n                    (glove | norm | prefix | suffix | shape)\n                    >> LN(Maxout(width, width * 5, pieces=3)),\n                    column=cols.index(ORTH),\n                )\n            else:\n                embed = uniqued(\n                    (glove | norm) >> LN(Maxout(width, width * 2, pieces=3)),\n                    column=cols.index(ORTH),\n                )\n        elif subword_features:\n            embed = uniqued(\n                (norm | prefix | suffix | shape)\n                >> LN(Maxout(width, width * 4, pieces=3)),\n                column=cols.index(ORTH),\n            )\n        elif char_embed:\n            embed = concatenate_lists(\n                CharacterEmbed(nM=64, nC=8),\n                FeatureExtracter(cols) >> with_flatten(norm),\n            )\n            reduce_dimensions = LN(\n                Maxout(width, 64 * 8 + width, pieces=cnn_maxout_pieces)\n            )\n        else:\n            embed = norm\n\n        convolution = Residual(\n            ExtractWindow(nW=1)\n            >> LN(Maxout(width, width * 3, pieces=cnn_maxout_pieces))\n        )\n        if char_embed:\n            tok2vec = embed >> with_flatten(\n                reduce_dimensions >> convolution ** conv_depth, pad=conv_depth\n            )\n        else:\n            tok2vec = FeatureExtracter(cols) >> with_flatten(\n                embed >> convolution ** conv_depth, pad=conv_depth\n            )\n\n        if bilstm_depth >= 1:\n            tok2vec = tok2vec >> PyTorchBiLSTM(width, width, bilstm_depth)\n        # Work around thinc API limitations :(. TODO: Revise in Thinc 7\n        tok2vec.nO = width\n        tok2vec.embed = embed\n    return tok2vec",
        "begin_line": 15,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.flatten#96",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.flatten(seqs, drop=0.0)",
        "snippet": "def flatten(seqs, drop=0.0):\n    ops = Model.ops\n    lengths = ops.asarray([len(seq) for seq in seqs], dtype=\"i\")\n\n    def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=0)\n\n    X = ops.flatten(seqs, pad=0)\n    return X, finish_update",
        "begin_line": 96,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.finish_update#100",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.finish_update(d_X, sgd=None)",
        "snippet": "    def finish_update(d_X, sgd=None):\n        return ops.unflatten(d_X, lengths, pad=0)",
        "begin_line": 100,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.concatenate_lists#107",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.concatenate_lists(*layers, **kwargs)",
        "snippet": "def concatenate_lists(*layers, **kwargs):  # pragma: no cover\n    \"\"\"Compose two or more models `f`, `g`, etc, such that their outputs are\n    concatenated, i.e. `concatenate(f, g)(x)` computes `hstack(f(x), g(x))`\n    \"\"\"\n    if not layers:\n        return noop()\n    drop_factor = kwargs.get(\"drop_factor\", 1.0)\n    ops = layers[0].ops\n    layers = [chain(layer, flatten) for layer in layers]\n    concat = concatenate(*layers)\n\n    def concatenate_lists_fwd(Xs, drop=0.0):\n        if drop is not None:\n            drop *= drop_factor\n        lengths = ops.asarray([len(X) for X in Xs], dtype=\"i\")\n        flat_y, bp_flat_y = concat.begin_update(Xs, drop=drop)\n        ys = ops.unflatten(flat_y, lengths)\n\n        def concatenate_lists_bwd(d_ys, sgd=None):\n            return bp_flat_y(ops.flatten(d_ys), sgd=sgd)\n\n        return ys, concatenate_lists_bwd\n\n    model = wrap(concatenate_lists_fwd, concat)\n    return model",
        "begin_line": 107,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.concatenate_lists_fwd#118",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.concatenate_lists_fwd(Xs, drop=0.0)",
        "snippet": "    def concatenate_lists_fwd(Xs, drop=0.0):\n        if drop is not None:\n            drop *= drop_factor\n        lengths = ops.asarray([len(X) for X in Xs], dtype=\"i\")\n        flat_y, bp_flat_y = concat.begin_update(Xs, drop=drop)\n        ys = ops.unflatten(flat_y, lengths)\n\n        def concatenate_lists_bwd(d_ys, sgd=None):\n            return bp_flat_y(ops.flatten(d_ys), sgd=sgd)\n\n        return ys, concatenate_lists_bwd",
        "begin_line": 118,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.ml._legacy_tok2vec.concatenate_lists_bwd#125",
        "src_path": "spacy/ml/_legacy_tok2vec.py",
        "class_name": "spacy.ml._legacy_tok2vec",
        "signature": "spacy.ml._legacy_tok2vec.concatenate_lists_bwd(d_ys, sgd=None)",
        "snippet": "        def concatenate_lists_bwd(d_ys, sgd=None):\n            return bp_flat_y(ops.flatten(d_ys), sgd=sgd)",
        "begin_line": 125,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_punct#27",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_punct(text)",
        "snippet": "def is_punct(text):\n    for char in text:\n        if not unicodedata.category(char).startswith(\"P\"):\n            return False\n    return True",
        "begin_line": 27,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003236245954692557,
            "pseudo_dstar_susp": 0.003236245954692557,
            "pseudo_tarantula_susp": 0.003236245954692557,
            "pseudo_op2_susp": 0.003236245954692557,
            "pseudo_barinel_susp": 0.003236245954692557
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_ascii#34",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_ascii(text)",
        "snippet": "def is_ascii(text):\n    for char in text:\n        if ord(char) >= 128:\n            return False\n    return True",
        "begin_line": 34,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033003300330033004,
            "pseudo_tarantula_susp": 0.0033003300330033004,
            "pseudo_op2_susp": 0.0033003300330033004,
            "pseudo_barinel_susp": 0.0033003300330033004
        }
    },
    {
        "name": "spacy.lang.lex_attrs.like_num#41",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    # can be overwritten by lang with list of number words\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    return False",
        "begin_line": 41,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4271294492650965e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_bracket#55",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_bracket(text)",
        "snippet": "def is_bracket(text):\n    brackets = (\"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\")\n    return text in brackets",
        "begin_line": 55,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_quote#60",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_quote(text)",
        "snippet": "def is_quote(text):\n    quotes = (\n        '\"',\n        \"'\",\n        \"`\",\n        \"\u00ab\",\n        \"\u00bb\",\n        \"\u2018\",\n        \"\u2019\",\n        \"\u201a\",\n        \"\u201b\",\n        \"\u201c\",\n        \"\u201d\",\n        \"\u201e\",\n        \"\u201f\",\n        \"\u2039\",\n        \"\u203a\",\n        \"\u276e\",\n        \"\u276f\",\n        \"''\",\n        \"``\",\n    )\n    return text in quotes",
        "begin_line": 60,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_left_punct#85",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_left_punct(text)",
        "snippet": "def is_left_punct(text):\n    left_punct = (\n        \"(\",\n        \"[\",\n        \"{\",\n        \"<\",\n        '\"',\n        \"'\",\n        \"\u00ab\",\n        \"\u2018\",\n        \"\u201a\",\n        \"\u201b\",\n        \"\u201c\",\n        \"\u201e\",\n        \"\u201f\",\n        \"\u2039\",\n        \"\u276e\",\n        \"``\",\n    )\n    return text in left_punct",
        "begin_line": 85,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_right_punct#107",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_right_punct(text)",
        "snippet": "def is_right_punct(text):\n    right_punct = (\")\", \"]\", \"}\", \">\", '\"', \"'\", \"\u00bb\", \"\u2019\", \"\u201d\", \"\u203a\", \"\u276f\", \"''\")\n    return text in right_punct",
        "begin_line": 107,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_currency#112",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_currency(text)",
        "snippet": "def is_currency(text):\n    # can be overwritten by lang with list of currency words, e.g. dollar, euro\n    for char in text:\n        if unicodedata.category(char) != \"Sc\":\n            return False\n    return True",
        "begin_line": 112,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003076923076923077,
            "pseudo_dstar_susp": 0.003076923076923077,
            "pseudo_tarantula_susp": 0.003076923076923077,
            "pseudo_op2_susp": 0.003076923076923077,
            "pseudo_barinel_susp": 0.003076923076923077
        }
    },
    {
        "name": "spacy.lang.lex_attrs.like_email#120",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.like_email(text)",
        "snippet": "def like_email(text):\n    return bool(_like_email(text))",
        "begin_line": 120,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.like_url#124",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.like_url(text)",
        "snippet": "def like_url(text):\n    # We're looking for things that function in text like URLs. So, valid URL\n    # or not, anything they say http:// is going to be good.\n    if text.startswith(\"http://\") or text.startswith(\"https://\"):\n        return True\n    elif text.startswith(\"www.\") and len(text) >= 5:\n        return True\n    if text[0] == \".\" or text[-1] == \".\":\n        return False\n    if \"@\" in text:\n        # prevent matches on e-mail addresses \u2013 check after splitting the text\n        # to still allow URLs containing an '@' character (see #1715)\n        return False\n    for i in range(len(text)):\n        if text[i] == \".\":\n            break\n    else:\n        return False\n    tld = text.rsplit(\".\", 1)[1].split(\":\", 1)[0]\n    if tld.endswith(\"/\"):\n        return True\n    if tld.isalpha() and tld in _tlds:\n        return True\n    return False",
        "begin_line": 124,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004739336492890996,
            "pseudo_dstar_susp": 0.004739336492890996,
            "pseudo_tarantula_susp": 0.004830917874396135,
            "pseudo_op2_susp": 0.004739336492890996,
            "pseudo_barinel_susp": 0.004830917874396135
        }
    },
    {
        "name": "spacy.lang.lex_attrs.word_shape#150",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.word_shape(text)",
        "snippet": "def word_shape(text):\n    if len(text) >= 100:\n        return \"LONG\"\n    shape = []\n    last = \"\"\n    shape_char = \"\"\n    seq = 0\n    for char in text:\n        if char.isalpha():\n            if char.isupper():\n                shape_char = \"X\"\n            else:\n                shape_char = \"x\"\n        elif char.isdigit():\n            shape_char = \"d\"\n        else:\n            shape_char = char\n        if shape_char == last:\n            seq += 1\n        else:\n            seq = 0\n            last = shape_char\n        if seq < 4:\n            shape.append(shape_char)\n    return \"\".join(shape)",
        "begin_line": 150,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033112582781456954,
            "pseudo_dstar_susp": 0.0033112582781456954,
            "pseudo_tarantula_susp": 0.0033112582781456954,
            "pseudo_op2_susp": 0.0033112582781456954,
            "pseudo_barinel_susp": 0.0033112582781456954
        }
    },
    {
        "name": "spacy.lang.lex_attrs.lower#177",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.lower(string)",
        "snippet": "def lower(string):\n    return string.lower()",
        "begin_line": 177,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.prefix#181",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.prefix(string)",
        "snippet": "def prefix(string):\n    return string[0]",
        "begin_line": 181,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.suffix#185",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.suffix(string)",
        "snippet": "def suffix(string):\n    return string[-3:]",
        "begin_line": 185,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.cluster#189",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.cluster(string)",
        "snippet": "def cluster(string):\n    return 0",
        "begin_line": 189,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_alpha#193",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_alpha(string)",
        "snippet": "def is_alpha(string):\n    return string.isalpha()",
        "begin_line": 193,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_digit#197",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_digit(string)",
        "snippet": "def is_digit(string):\n    return string.isdigit()",
        "begin_line": 197,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0030120481927710845,
            "pseudo_dstar_susp": 0.0030120481927710845,
            "pseudo_tarantula_susp": 0.0030120481927710845,
            "pseudo_op2_susp": 0.0030120481927710845,
            "pseudo_barinel_susp": 0.0030120481927710845
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_lower#201",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_lower(string)",
        "snippet": "def is_lower(string):\n    return string.islower()",
        "begin_line": 201,
        "end_line": 202,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_space#205",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_space(string)",
        "snippet": "def is_space(string):\n    return string.isspace()",
        "begin_line": 205,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_title#209",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_title(string)",
        "snippet": "def is_title(string):\n    return string.istitle()",
        "begin_line": 209,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_upper#213",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_upper(string)",
        "snippet": "def is_upper(string):\n    return string.isupper()",
        "begin_line": 213,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_stop#217",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_stop(string, stops=set())",
        "snippet": "def is_stop(string, stops=set()):\n    return string.lower() in stops",
        "begin_line": 217,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002702702702702703,
            "pseudo_dstar_susp": 0.002702702702702703,
            "pseudo_tarantula_susp": 0.002702702702702703,
            "pseudo_op2_susp": 0.002702702702702703,
            "pseudo_barinel_susp": 0.002702702702702703
        }
    },
    {
        "name": "spacy.lang.lex_attrs.is_oov#221",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.is_oov(string)",
        "snippet": "def is_oov(string):\n    return True",
        "begin_line": 221,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.lex_attrs.get_prob#225",
        "src_path": "spacy/lang/lex_attrs.py",
        "class_name": "spacy.lang.lex_attrs",
        "signature": "spacy.lang.lex_attrs.get_prob(string)",
        "snippet": "def get_prob(string):\n    return -20.0",
        "begin_line": 225,
        "end_line": 226,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002849002849002849,
            "pseudo_dstar_susp": 0.002849002849002849,
            "pseudo_tarantula_susp": 0.002849002849002849,
            "pseudo_op2_susp": 0.002849002849002849,
            "pseudo_barinel_susp": 0.002849002849002849
        }
    },
    {
        "name": "spacy.lang.ca.lex_attrs.like_num#47",
        "src_path": "spacy/lang/ca/lex_attrs.py",
        "class_name": "spacy.lang.ca.lex_attrs",
        "signature": "spacy.lang.ca.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    return False",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ro.punctuation._make_ro_variants#24",
        "src_path": "spacy/lang/ro/punctuation.py",
        "class_name": "spacy.lang.ro.punctuation",
        "signature": "spacy.lang.ro.punctuation._make_ro_variants(tokens)",
        "snippet": "def _make_ro_variants(tokens):\n    variants = []\n    for token in tokens:\n        upper_token = token.upper()\n        upper_char_variants = [_ro_variants.get(c, [c]) for c in upper_token]\n        upper_variants = [\"\".join(x) for x in itertools.product(*upper_char_variants)]\n        for variant in upper_variants:\n            variants.extend([variant, variant.lower(), variant.title()])\n    return sorted(list(set(variants)))",
        "begin_line": 24,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lemmatizer.Lemmatizer.__init__#23",
        "src_path": "spacy/lemmatizer.py",
        "class_name": "spacy.lemmatizer.Lemmatizer",
        "signature": "spacy.lemmatizer.Lemmatizer.__init__(self, lookups, *args, **kwargs)",
        "snippet": "    def __init__(self, lookups, *args, **kwargs):\n        \"\"\"Initialize a Lemmatizer.\n\n        lookups (Lookups): The lookups object containing the (optional) tables\n            \"lemma_rules\", \"lemma_index\", \"lemma_exc\" and \"lemma_lookup\".\n        RETURNS (Lemmatizer): The newly constructed object.\n        \"\"\"\n        if args or kwargs or not isinstance(lookups, Lookups):\n            raise ValueError(Errors.E173)\n        self.lookups = lookups",
        "begin_line": 23,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003278688524590164,
            "pseudo_dstar_susp": 0.003278688524590164,
            "pseudo_tarantula_susp": 0.003278688524590164,
            "pseudo_op2_susp": 0.003278688524590164,
            "pseudo_barinel_susp": 0.003278688524590164
        }
    },
    {
        "name": "spacy.lemmatizer.Lemmatizer.__call__#34",
        "src_path": "spacy/lemmatizer.py",
        "class_name": "spacy.lemmatizer.Lemmatizer",
        "signature": "spacy.lemmatizer.Lemmatizer.__call__(self, string, univ_pos, morphology=None)",
        "snippet": "    def __call__(self, string, univ_pos, morphology=None):\n        \"\"\"Lemmatize a string.\n\n        string (unicode): The string to lemmatize, e.g. the token text.\n        univ_pos (unicode / int): The token's universal part-of-speech tag.\n        morphology (dict): The token's morphological features following the\n            Universal Dependencies scheme.\n        RETURNS (list): The available lemmas for the string.\n        \"\"\"\n        lookup_table = self.lookups.get_table(\"lemma_lookup\", {})\n        if \"lemma_rules\" not in self.lookups:\n            return [lookup_table.get(string, string)]\n        if univ_pos in (NOUN, \"NOUN\", \"noun\"):\n            univ_pos = \"noun\"\n        elif univ_pos in (VERB, \"VERB\", \"verb\"):\n            univ_pos = \"verb\"\n        elif univ_pos in (ADJ, \"ADJ\", \"adj\"):\n            univ_pos = \"adj\"\n        elif univ_pos in (PUNCT, \"PUNCT\", \"punct\"):\n            univ_pos = \"punct\"\n        elif univ_pos in (PROPN, \"PROPN\"):\n            return [string]\n        else:\n            return [string.lower()]\n        # See Issue #435 for example of where this logic is requied.\n        if self.is_base_form(univ_pos, morphology):\n            return [string.lower()]\n        index_table = self.lookups.get_table(\"lemma_index\", {})\n        exc_table = self.lookups.get_table(\"lemma_exc\", {})\n        rules_table = self.lookups.get_table(\"lemma_rules\", {})\n        lemmas = self.lemmatize(\n            string,\n            index_table.get(univ_pos, {}),\n            exc_table.get(univ_pos, {}),\n            rules_table.get(univ_pos, []),\n        )\n        return lemmas",
        "begin_line": 34,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0043859649122807015,
            "pseudo_dstar_susp": 0.0043859649122807015,
            "pseudo_tarantula_susp": 0.004464285714285714,
            "pseudo_op2_susp": 0.0043859649122807015,
            "pseudo_barinel_susp": 0.004464285714285714
        }
    },
    {
        "name": "spacy.lemmatizer.Lemmatizer.is_base_form#72",
        "src_path": "spacy/lemmatizer.py",
        "class_name": "spacy.lemmatizer.Lemmatizer",
        "signature": "spacy.lemmatizer.Lemmatizer.is_base_form(self, univ_pos, morphology=None)",
        "snippet": "    def is_base_form(self, univ_pos, morphology=None):\n        \"\"\"\n        Check whether we're dealing with an uninflected paradigm, so we can\n        avoid lemmatization entirely.\n\n        univ_pos (unicode / int): The token's universal part-of-speech tag.\n        morphology (dict): The token's morphological features following the\n            Universal Dependencies scheme.\n        \"\"\"\n        if morphology is None:\n            morphology = {}\n        if univ_pos == \"noun\" and morphology.get(\"Number\") == \"sing\":\n            return True\n        elif univ_pos == \"verb\" and morphology.get(\"VerbForm\") == \"inf\":\n            return True\n        # This maps 'VBP' to base form -- probably just need 'IS_BASE'\n        # morphology\n        elif univ_pos == \"verb\" and (\n            morphology.get(\"VerbForm\") == \"fin\"\n            and morphology.get(\"Tense\") == \"pres\"\n            and morphology.get(\"Number\") is None\n        ):\n            return True\n        elif univ_pos == \"adj\" and morphology.get(\"Degree\") == \"pos\":\n            return True\n        elif morphology.get(\"VerbForm\") == \"inf\":\n            return True\n        elif morphology.get(\"VerbForm\") == \"none\":\n            return True\n        elif morphology.get(\"Degree\") == \"pos\":\n            return True\n        else:\n            return False",
        "begin_line": 72,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lemmatizer.Lemmatizer.lookup#130",
        "src_path": "spacy/lemmatizer.py",
        "class_name": "spacy.lemmatizer.Lemmatizer",
        "signature": "spacy.lemmatizer.Lemmatizer.lookup(self, string, orth=None)",
        "snippet": "    def lookup(self, string, orth=None):\n        \"\"\"Look up a lemma in the table, if available. If no lemma is found,\n        the original string is returned.\n\n        string (unicode): The original string.\n        orth (int): Optional hash of the string to look up. If not set, the\n            string will be used and hashed.\n        RETURNS (unicode): The lemma if the string was found, otherwise the\n            original string.\n        \"\"\"\n        lookup_table = self.lookups.get_table(\"lemma_lookup\", {})\n        key = orth if orth is not None else string\n        if key in lookup_table:\n            return lookup_table[key]\n        return string",
        "begin_line": 130,
        "end_line": 144,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lemmatizer.Lemmatizer.lemmatize#146",
        "src_path": "spacy/lemmatizer.py",
        "class_name": "spacy.lemmatizer.Lemmatizer",
        "signature": "spacy.lemmatizer.Lemmatizer.lemmatize(self, string, index, exceptions, rules)",
        "snippet": "    def lemmatize(self, string, index, exceptions, rules):\n        orig = string\n        string = string.lower()\n        forms = []\n        oov_forms = []\n        for old, new in rules:\n            if string.endswith(old):\n                form = string[: len(string) - len(old)] + new\n                if not form:\n                    pass\n                elif form in index or not form.isalpha():\n                    forms.append(form)\n                else:\n                    oov_forms.append(form)\n        # Remove duplicates but preserve the ordering of applied \"rules\"\n        forms = list(OrderedDict.fromkeys(forms))\n        # Put exceptions at the front of the list, so they get priority.\n        # This is a dodgy heuristic -- but it's the best we can do until we get\n        # frequencies on this. We can at least prune out problematic exceptions,\n        # if they shadow more frequent analyses.\n        for form in exceptions.get(string, []):\n            if form not in forms:\n                forms.insert(0, form)\n        if not forms:\n            forms.extend(oov_forms)\n        if not forms:\n            forms.append(orig)\n        return forms",
        "begin_line": 146,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.conllu2json#9",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.conllu2json(input_data, n_sents=10, use_morphology=False, lang=None, **_)",
        "snippet": "def conllu2json(input_data, n_sents=10, use_morphology=False, lang=None, **_):\n    \"\"\"\n    Convert conllu files into JSON format for use with train cli.\n    use_morphology parameter enables appending morphology to tags, which is\n    useful for languages such as Spanish, where UD tags are not so rich.\n\n    Extract NER tags if available and convert them so that they follow\n    BILUO and the Wikipedia scheme\n    \"\"\"\n    # by @dvsrepo, via #11 explosion/spacy-dev-resources\n    # by @katarkor\n    docs = []\n    sentences = []\n    conll_tuples = read_conllx(input_data, use_morphology=use_morphology)\n    checked_for_ner = False\n    has_ner_tags = False\n    for i, (raw_text, tokens) in enumerate(conll_tuples):\n        sentence, brackets = tokens[0]\n        if not checked_for_ner:\n            has_ner_tags = is_ner(sentence[5][0])\n            checked_for_ner = True\n        sentences.append(generate_sentence(sentence, has_ner_tags))\n        # Real-sized documents could be extracted using the comments on the\n        # conluu document\n        if len(sentences) % n_sents == 0:\n            doc = create_doc(sentences, i)\n            docs.append(doc)\n            sentences = []\n    if sentences:\n        doc = create_doc(sentences, i)\n        docs.append(doc)\n    return docs",
        "begin_line": 9,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.is_ner#43",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.is_ner(tag)",
        "snippet": "def is_ner(tag):\n    \"\"\"\n    Check the 10th column of the first token to determine if the file contains\n    NER tags\n    \"\"\"\n    tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n    if tag_match:\n        return True\n    elif tag == \"O\":\n        return True\n    else:\n        return False",
        "begin_line": 43,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.read_conllx#57",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.read_conllx(input_data, use_morphology=False, n=0)",
        "snippet": "def read_conllx(input_data, use_morphology=False, n=0):\n    i = 0\n    for sent in input_data.strip().split(\"\\n\\n\"):\n        lines = sent.strip().split(\"\\n\")\n        if lines:\n            while lines[0].startswith(\"#\"):\n                lines.pop(0)\n            tokens = []\n            for line in lines:\n\n                parts = line.split(\"\\t\")\n                id_, word, lemma, pos, tag, morph, head, dep, _1, iob = parts\n                if \"-\" in id_ or \".\" in id_:\n                    continue\n                try:\n                    id_ = int(id_) - 1\n                    head = (int(head) - 1) if head not in [\"0\", \"_\"] else id_\n                    dep = \"ROOT\" if dep == \"root\" else dep\n                    tag = pos if tag == \"_\" else tag\n                    tag = tag + \"__\" + morph if use_morphology else tag\n                    iob = iob if iob else \"O\"\n                    tokens.append((id_, word, tag, head, dep, iob))\n                except:  # noqa: E722\n                    print(line)\n                    raise\n            tuples = [list(t) for t in zip(*tokens)]\n            yield (None, [[tuples, []]])\n            i += 1\n            if n >= 1 and i >= n:\n                break",
        "begin_line": 57,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.simplify_tags#89",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.simplify_tags(iob)",
        "snippet": "def simplify_tags(iob):\n    \"\"\"\n    Simplify tags obtained from the dataset in order to follow Wikipedia\n    scheme (PER, LOC, ORG, MISC). 'PER', 'LOC' and 'ORG' keep their tags, while\n    'GPE_LOC' is simplified to 'LOC', 'GPE_ORG' to 'ORG' and all remaining tags to\n    'MISC'.\n    \"\"\"\n    new_iob = []\n    for tag in iob:\n        tag_match = re.match(\"([A-Z_]+)-([A-Z_]+)\", tag)\n        if tag_match:\n            prefix = tag_match.group(1)\n            suffix = tag_match.group(2)\n            if suffix == \"GPE_LOC\":\n                suffix = \"LOC\"\n            elif suffix == \"GPE_ORG\":\n                suffix = \"ORG\"\n            elif suffix != \"PER\" and suffix != \"LOC\" and suffix != \"ORG\":\n                suffix = \"MISC\"\n            tag = prefix + \"-\" + suffix\n        new_iob.append(tag)\n    return new_iob",
        "begin_line": 89,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.generate_sentence#113",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.generate_sentence(sent, has_ner_tags)",
        "snippet": "def generate_sentence(sent, has_ner_tags):\n    (id_, word, tag, head, dep, iob) = sent\n    sentence = {}\n    tokens = []\n    if has_ner_tags:\n        iob = simplify_tags(iob)\n        biluo = iob_to_biluo(iob)\n    for i, id in enumerate(id_):\n        token = {}\n        token[\"id\"] = id\n        token[\"orth\"] = word[i]\n        token[\"tag\"] = tag[i]\n        token[\"head\"] = head[i] - id\n        token[\"dep\"] = dep[i]\n        if has_ner_tags:\n            token[\"ner\"] = biluo[i]\n        tokens.append(token)\n    sentence[\"tokens\"] = tokens\n    return sentence",
        "begin_line": 113,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conllu2json.create_doc#134",
        "src_path": "spacy/cli/converters/conllu2json.py",
        "class_name": "spacy.cli.converters.conllu2json",
        "signature": "spacy.cli.converters.conllu2json.create_doc(sentences, id)",
        "snippet": "def create_doc(sentences, id):\n    doc = {}\n    paragraph = {}\n    doc[\"id\"] = id\n    doc[\"paragraphs\"] = []\n    paragraph[\"sentences\"] = sentences\n    doc[\"paragraphs\"].append(paragraph)\n    return doc",
        "begin_line": 134,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.try_jieba_import#22",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__",
        "signature": "spacy.lang.zh.__init__.try_jieba_import(use_jieba)",
        "snippet": "def try_jieba_import(use_jieba):\n    try:\n        import jieba\n\n        # segment a short text to have jieba initialize its cache in advance\n        list(jieba.cut(\"\u4f5c\u4e3a\", cut_all=False))\n\n        return jieba\n    except ImportError:\n        if use_jieba:\n            msg = (\n                \"Jieba not installed. Either set the default to False with \"\n                \"`from spacy.lang.zh import ChineseDefaults; ChineseDefaults.use_jieba = False`, \"\n                \"or install it with `pip install jieba` or from \"\n                \"https://github.com/fxsjy/jieba\"\n            )\n            raise ImportError(msg)",
        "begin_line": 22,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.try_pkuseg_import#41",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__",
        "signature": "spacy.lang.zh.__init__.try_pkuseg_import(use_pkuseg, pkuseg_model, pkuseg_user_dict)",
        "snippet": "def try_pkuseg_import(use_pkuseg, pkuseg_model, pkuseg_user_dict):\n    try:\n        import pkuseg\n\n        if pkuseg_model:\n            return pkuseg.pkuseg(pkuseg_model, pkuseg_user_dict)\n        elif use_pkuseg:\n            msg = (\n                \"Chinese.use_pkuseg is True but no pkuseg model was specified. \"\n                \"Please provide the name of a pretrained model \"\n                \"or the path to a model with \"\n                '`Chinese(meta={\"tokenizer\": {\"config\": {\"pkuseg_model\": name_or_path}}}).'\n            )\n            raise ValueError(msg)\n    except ImportError:\n        if use_pkuseg:\n            msg = (\n                \"pkuseg not installed. Either set Chinese.use_pkuseg = False, \"\n                \"or \" + _PKUSEG_INSTALL_MSG\n            )\n            raise ImportError(msg)\n    except FileNotFoundError:\n        if use_pkuseg:\n            msg = \"Unable to load pkuseg model from: \" + pkuseg_model\n            raise FileNotFoundError(msg)",
        "begin_line": 41,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.__init__#69",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.__init__(self, cls, nlp=None, config={})",
        "snippet": "    def __init__(self, cls, nlp=None, config={}):\n        self.use_jieba = config.get(\"use_jieba\", cls.use_jieba)\n        self.use_pkuseg = config.get(\"use_pkuseg\", cls.use_pkuseg)\n        self.require_pkuseg = config.get(\"require_pkuseg\", False)\n        self.vocab = nlp.vocab if nlp is not None else cls.create_vocab(nlp)\n        self.jieba_seg = try_jieba_import(self.use_jieba)\n        self.pkuseg_seg = try_pkuseg_import(\n            self.use_pkuseg,\n            pkuseg_model=config.get(\"pkuseg_model\", None),\n            pkuseg_user_dict=config.get(\"pkuseg_user_dict\", \"default\"),\n        )\n        # remove relevant settings from config so they're not also saved in\n        # Language.meta\n        for key in [\"use_jieba\", \"use_pkuseg\", \"require_pkuseg\", \"pkuseg_model\"]:\n            if key in config:\n                del config[key]\n        self.tokenizer = Language.Defaults().create_tokenizer(nlp)",
        "begin_line": 69,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.__call__#87",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.__call__(self, text)",
        "snippet": "    def __call__(self, text):\n        use_jieba = self.use_jieba\n        use_pkuseg = self.use_pkuseg\n        if self.require_pkuseg:\n            use_jieba = False\n            use_pkuseg = True\n        if use_jieba:\n            words = list([x for x in self.jieba_seg.cut(text, cut_all=False) if x])\n            (words, spaces) = util.get_words_and_spaces(words, text)\n            return Doc(self.vocab, words=words, spaces=spaces)\n        elif use_pkuseg:\n            words = self.pkuseg_seg.cut(text)\n            (words, spaces) = util.get_words_and_spaces(words, text)\n            return Doc(self.vocab, words=words, spaces=spaces)\n        else:\n            # split into individual characters\n            words = list(text)\n            (words, spaces) = util.get_words_and_spaces(words, text)\n            return Doc(self.vocab, words=words, spaces=spaces)",
        "begin_line": 87,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer._get_config#107",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer._get_config(self)",
        "snippet": "    def _get_config(self):\n        config = OrderedDict(\n            (\n                (\"use_jieba\", self.use_jieba),\n                (\"use_pkuseg\", self.use_pkuseg),\n                (\"require_pkuseg\", self.require_pkuseg),\n            )\n        )\n        return config",
        "begin_line": 107,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer._set_config#117",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer._set_config(self, config={})",
        "snippet": "    def _set_config(self, config={}):\n        self.use_jieba = config.get(\"use_jieba\", False)\n        self.use_pkuseg = config.get(\"use_pkuseg\", False)\n        self.require_pkuseg = config.get(\"require_pkuseg\", False)",
        "begin_line": 117,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.to_bytes#122",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.to_bytes(self, **kwargs)",
        "snippet": "    def to_bytes(self, **kwargs):\n        pkuseg_features_b = b\"\"\n        pkuseg_weights_b = b\"\"\n        pkuseg_processors_data = None\n        if self.pkuseg_seg:\n            with tempfile.TemporaryDirectory() as tempdir:\n                self.pkuseg_seg.feature_extractor.save(tempdir)\n                self.pkuseg_seg.model.save(tempdir)\n                tempdir = Path(tempdir)\n                with open(tempdir / \"features.pkl\", \"rb\") as fileh:\n                    pkuseg_features_b = fileh.read()\n                with open(tempdir / \"weights.npz\", \"rb\") as fileh:\n                    pkuseg_weights_b = fileh.read()\n            pkuseg_processors_data = (\n                _get_pkuseg_trie_data(self.pkuseg_seg.preprocesser.trie),\n                self.pkuseg_seg.postprocesser.do_process,\n                sorted(list(self.pkuseg_seg.postprocesser.common_words)),\n                sorted(list(self.pkuseg_seg.postprocesser.other_words)),\n            )\n        serializers = OrderedDict(\n            (\n                (\"cfg\", lambda: srsly.json_dumps(self._get_config())),\n                (\"pkuseg_features\", lambda: pkuseg_features_b),\n                (\"pkuseg_weights\", lambda: pkuseg_weights_b),\n                (\n                    \"pkuseg_processors\",\n                    lambda: srsly.msgpack_dumps(pkuseg_processors_data),\n                ),\n            )\n        )\n        return util.to_bytes(serializers, [])",
        "begin_line": 122,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.from_bytes#154",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.from_bytes(self, data, **kwargs)",
        "snippet": "    def from_bytes(self, data, **kwargs):\n        pkuseg_features_b = b\"\"\n        pkuseg_weights_b = b\"\"\n        pkuseg_processors_data = None\n\n        def deserialize_pkuseg_features(b):\n            nonlocal pkuseg_features_b\n            pkuseg_features_b = b\n\n        def deserialize_pkuseg_weights(b):\n            nonlocal pkuseg_weights_b\n            pkuseg_weights_b = b\n\n        def deserialize_pkuseg_processors(b):\n            nonlocal pkuseg_processors_data\n            pkuseg_processors_data = srsly.msgpack_loads(b)\n\n        deserializers = OrderedDict(\n            (\n                (\"cfg\", lambda b: self._set_config(srsly.json_loads(b))),\n                (\"pkuseg_features\", deserialize_pkuseg_features),\n                (\"pkuseg_weights\", deserialize_pkuseg_weights),\n                (\"pkuseg_processors\", deserialize_pkuseg_processors),\n            )\n        )\n        util.from_bytes(data, deserializers, [])\n\n        if pkuseg_features_b and pkuseg_weights_b:\n            with tempfile.TemporaryDirectory() as tempdir:\n                tempdir = Path(tempdir)\n                with open(tempdir / \"features.pkl\", \"wb\") as fileh:\n                    fileh.write(pkuseg_features_b)\n                with open(tempdir / \"weights.npz\", \"wb\") as fileh:\n                    fileh.write(pkuseg_weights_b)\n                try:\n                    import pkuseg\n                except ImportError:\n                    raise ImportError(\n                        \"pkuseg not installed. To use this model, \"\n                        + _PKUSEG_INSTALL_MSG\n                    )\n                self.pkuseg_seg = pkuseg.pkuseg(str(tempdir))\n            if pkuseg_processors_data:\n                (\n                    user_dict,\n                    do_process,\n                    common_words,\n                    other_words,\n                ) = pkuseg_processors_data\n                self.pkuseg_seg.preprocesser = pkuseg.Preprocesser(user_dict)\n                self.pkuseg_seg.postprocesser.do_process = do_process\n                self.pkuseg_seg.postprocesser.common_words = set(common_words)\n                self.pkuseg_seg.postprocesser.other_words = set(other_words)\n\n        return self",
        "begin_line": 154,
        "end_line": 208,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_features#159",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_features(b)",
        "snippet": "        def deserialize_pkuseg_features(b):\n            nonlocal pkuseg_features_b\n            pkuseg_features_b = b",
        "begin_line": 159,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_weights#163",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_weights(b)",
        "snippet": "        def deserialize_pkuseg_weights(b):\n            nonlocal pkuseg_weights_b\n            pkuseg_weights_b = b",
        "begin_line": 163,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_processors#167",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.deserialize_pkuseg_processors(b)",
        "snippet": "        def deserialize_pkuseg_processors(b):\n            nonlocal pkuseg_processors_data\n            pkuseg_processors_data = srsly.msgpack_loads(b)",
        "begin_line": 167,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.to_disk#210",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.to_disk(self, path, **kwargs)",
        "snippet": "    def to_disk(self, path, **kwargs):\n        path = util.ensure_path(path)\n\n        def save_pkuseg_model(path):\n            if self.pkuseg_seg:\n                if not path.exists():\n                    path.mkdir(parents=True)\n                self.pkuseg_seg.model.save(path)\n                self.pkuseg_seg.feature_extractor.save(path)\n\n        def save_pkuseg_processors(path):\n            if self.pkuseg_seg:\n                data = (\n                    _get_pkuseg_trie_data(self.pkuseg_seg.preprocesser.trie),\n                    self.pkuseg_seg.postprocesser.do_process,\n                    sorted(list(self.pkuseg_seg.postprocesser.common_words)),\n                    sorted(list(self.pkuseg_seg.postprocesser.other_words)),\n                )\n                srsly.write_msgpack(path, data)\n\n        serializers = OrderedDict(\n            (\n                (\"cfg\", lambda p: srsly.write_json(p, self._get_config())),\n                (\"pkuseg_model\", lambda p: save_pkuseg_model(p)),\n                (\"pkuseg_processors\", lambda p: save_pkuseg_processors(p)),\n            )\n        )\n        return util.to_disk(path, serializers, [])",
        "begin_line": 210,
        "end_line": 237,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.save_pkuseg_model#213",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.save_pkuseg_model(path)",
        "snippet": "        def save_pkuseg_model(path):\n            if self.pkuseg_seg:\n                if not path.exists():\n                    path.mkdir(parents=True)\n                self.pkuseg_seg.model.save(path)\n                self.pkuseg_seg.feature_extractor.save(path)",
        "begin_line": 213,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.save_pkuseg_processors#220",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.save_pkuseg_processors(path)",
        "snippet": "        def save_pkuseg_processors(path):\n            if self.pkuseg_seg:\n                data = (\n                    _get_pkuseg_trie_data(self.pkuseg_seg.preprocesser.trie),\n                    self.pkuseg_seg.postprocesser.do_process,\n                    sorted(list(self.pkuseg_seg.postprocesser.common_words)),\n                    sorted(list(self.pkuseg_seg.postprocesser.other_words)),\n                )\n                srsly.write_msgpack(path, data)",
        "begin_line": 220,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.from_disk#239",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.from_disk(self, path, **kwargs)",
        "snippet": "    def from_disk(self, path, **kwargs):\n        path = util.ensure_path(path)\n\n        def load_pkuseg_model(path):\n            try:\n                import pkuseg\n            except ImportError:\n                if self.use_pkuseg:\n                    raise ImportError(\n                        \"pkuseg not installed. To use this model, \"\n                        + _PKUSEG_INSTALL_MSG\n                    )\n            if path.exists():\n                self.pkuseg_seg = pkuseg.pkuseg(path)\n\n        def load_pkuseg_processors(path):\n            try:\n                import pkuseg\n            except ImportError:\n                if self.use_pkuseg:\n                    raise ImportError(self._pkuseg_install_msg)\n            if self.pkuseg_seg:\n                data = srsly.read_msgpack(path)\n                (user_dict, do_process, common_words, other_words) = data\n                self.pkuseg_seg.preprocesser = pkuseg.Preprocesser(user_dict)\n                self.pkuseg_seg.postprocesser.do_process = do_process\n                self.pkuseg_seg.postprocesser.common_words = set(common_words)\n                self.pkuseg_seg.postprocesser.other_words = set(other_words)\n\n        serializers = OrderedDict(\n            (\n                (\"cfg\", lambda p: self._set_config(srsly.read_json(p))),\n                (\"pkuseg_model\", lambda p: load_pkuseg_model(p)),\n                (\"pkuseg_processors\", lambda p: load_pkuseg_processors(p)),\n            )\n        )\n        util.from_disk(path, serializers, [])",
        "begin_line": 239,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.load_pkuseg_model#242",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.load_pkuseg_model(path)",
        "snippet": "        def load_pkuseg_model(path):\n            try:\n                import pkuseg\n            except ImportError:\n                if self.use_pkuseg:\n                    raise ImportError(\n                        \"pkuseg not installed. To use this model, \"\n                        + _PKUSEG_INSTALL_MSG\n                    )\n            if path.exists():\n                self.pkuseg_seg = pkuseg.pkuseg(path)",
        "begin_line": 242,
        "end_line": 252,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseTokenizer.load_pkuseg_processors#254",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseTokenizer",
        "signature": "spacy.lang.zh.__init__.ChineseTokenizer.load_pkuseg_processors(path)",
        "snippet": "        def load_pkuseg_processors(path):\n            try:\n                import pkuseg\n            except ImportError:\n                if self.use_pkuseg:\n                    raise ImportError(self._pkuseg_install_msg)\n            if self.pkuseg_seg:\n                data = srsly.read_msgpack(path)\n                (user_dict, do_process, common_words, other_words) = data\n                self.pkuseg_seg.preprocesser = pkuseg.Preprocesser(user_dict)\n                self.pkuseg_seg.postprocesser.do_process = do_process\n                self.pkuseg_seg.postprocesser.common_words = set(common_words)\n                self.pkuseg_seg.postprocesser.other_words = set(other_words)",
        "begin_line": 254,
        "end_line": 266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.__init__.ChineseDefaults.create_tokenizer#290",
        "src_path": "spacy/lang/zh/__init__.py",
        "class_name": "spacy.lang.zh.__init__.ChineseDefaults",
        "signature": "spacy.lang.zh.__init__.ChineseDefaults.create_tokenizer(cls, nlp=None, config={})",
        "snippet": "    def create_tokenizer(cls, nlp=None, config={}):\n        return ChineseTokenizer(cls, nlp, config=config)",
        "begin_line": 290,
        "end_line": 291,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.BaseDefaults.create_lemmatizer#48",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.BaseDefaults",
        "signature": "spacy.language.BaseDefaults.create_lemmatizer(cls, nlp=None, lookups=None)",
        "snippet": "    def create_lemmatizer(cls, nlp=None, lookups=None):\n        if lookups is None:\n            lookups = cls.create_lookups(nlp=nlp)\n        return Lemmatizer(lookups=lookups)",
        "begin_line": 48,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004048582995951417,
            "pseudo_dstar_susp": 0.004048582995951417,
            "pseudo_tarantula_susp": 0.00411522633744856,
            "pseudo_op2_susp": 0.004048582995951417,
            "pseudo_barinel_susp": 0.00411522633744856
        }
    },
    {
        "name": "spacy.language.BaseDefaults.create_lookups#54",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.BaseDefaults",
        "signature": "spacy.language.BaseDefaults.create_lookups(cls, nlp=None)",
        "snippet": "    def create_lookups(cls, nlp=None):\n        root = util.get_module_path(cls)\n        filenames = {name: root / filename for name, filename in cls.resources}\n        if LANG in cls.lex_attr_getters:\n            lang = cls.lex_attr_getters[LANG](None)\n            if lang in util.registry.lookups:\n                filenames.update(util.registry.lookups.get(lang))\n        lookups = Lookups()\n        for name, filename in filenames.items():\n            data = util.load_language_data(filename)\n            lookups.add_table(name, data)\n        return lookups",
        "begin_line": 54,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0049261083743842365,
            "pseudo_dstar_susp": 0.08333333333333333,
            "pseudo_tarantula_susp": 0.004464285714285714,
            "pseudo_op2_susp": 0.08333333333333333,
            "pseudo_barinel_susp": 0.004464285714285714
        }
    },
    {
        "name": "spacy.language.BaseDefaults.create_vocab#68",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.BaseDefaults",
        "signature": "spacy.language.BaseDefaults.create_vocab(cls, nlp=None)",
        "snippet": "    def create_vocab(cls, nlp=None):\n        lookups = cls.create_lookups(nlp)\n        lemmatizer = cls.create_lemmatizer(nlp, lookups=lookups)\n        lex_attr_getters = dict(cls.lex_attr_getters)\n        # This is messy, but it's the minimal working fix to Issue #639.\n        lex_attr_getters[IS_STOP] = functools.partial(is_stop, stops=cls.stop_words)\n        vocab = Vocab(\n            lex_attr_getters=lex_attr_getters,\n            tag_map=cls.tag_map,\n            lemmatizer=lemmatizer,\n            lookups=lookups,\n        )\n        for tag_str, exc in cls.morph_rules.items():\n            for orth_str, attrs in exc.items():\n                vocab.morphology.add_special_case(tag_str, orth_str, attrs)\n        return vocab",
        "begin_line": 68,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005025125628140704,
            "pseudo_dstar_susp": 0.0049261083743842365,
            "pseudo_tarantula_susp": 0.005050505050505051,
            "pseudo_op2_susp": 0.0049261083743842365,
            "pseudo_barinel_susp": 0.005050505050505051
        }
    },
    {
        "name": "spacy.language.BaseDefaults.create_tokenizer#86",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.BaseDefaults",
        "signature": "spacy.language.BaseDefaults.create_tokenizer(cls, nlp=None)",
        "snippet": "    def create_tokenizer(cls, nlp=None):\n        rules = cls.tokenizer_exceptions\n        token_match = cls.token_match\n        prefix_search = (\n            util.compile_prefix_regex(cls.prefixes).search if cls.prefixes else None\n        )\n        suffix_search = (\n            util.compile_suffix_regex(cls.suffixes).search if cls.suffixes else None\n        )\n        infix_finditer = (\n            util.compile_infix_regex(cls.infixes).finditer if cls.infixes else None\n        )\n        vocab = nlp.vocab if nlp is not None else cls.create_vocab(nlp)\n        return Tokenizer(\n            vocab,\n            rules=rules,\n            prefix_search=prefix_search,\n            suffix_search=suffix_search,\n            infix_finditer=infix_finditer,\n            token_match=token_match,\n        )",
        "begin_line": 86,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0035842293906810036,
            "pseudo_dstar_susp": 0.0035842293906810036,
            "pseudo_tarantula_susp": 0.0036231884057971015,
            "pseudo_op2_susp": 0.0035842293906810036,
            "pseudo_barinel_susp": 0.0036231884057971015
        }
    },
    {
        "name": "spacy.language.Language.__init__#141",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.__init__(self, vocab=True, make_doc=True, max_length=10 ** 6, meta={}, **kwargs)",
        "snippet": "    def __init__(\n        self, vocab=True, make_doc=True, max_length=10 ** 6, meta={}, **kwargs\n    ):\n        \"\"\"Initialise a Language object.\n\n        vocab (Vocab): A `Vocab` object. If `True`, a vocab is created via\n            `Language.Defaults.create_vocab`.\n        make_doc (callable): A function that takes text and returns a `Doc`\n            object. Usually a `Tokenizer`.\n        meta (dict): Custom meta data for the Language class. Is written to by\n            models to add model meta data.\n        max_length (int) :\n            Maximum number of characters in a single text. The current v2 models\n            may run out memory on extremely long texts, due to large internal\n            allocations. You should segment these texts into meaningful units,\n            e.g. paragraphs, subsections etc, before passing them to spaCy.\n            Default maximum length is 1,000,000 characters (1mb). As a rule of\n            thumb, if all pipeline components are enabled, spaCy's default\n            models currently requires roughly 1GB of temporary memory per\n            100,000 characters in one text.\n        RETURNS (Language): The newly constructed object.\n        \"\"\"\n        user_factories = util.registry.factories.get_all()\n        self.factories.update(user_factories)\n        self._meta = dict(meta)\n        self._path = None\n        if vocab is True:\n            factory = self.Defaults.create_vocab\n            vocab = factory(self, **meta.get(\"vocab\", {}))\n            if vocab.vectors.name is None:\n                vocab.vectors.name = meta.get(\"vectors\", {}).get(\"name\")\n        else:\n            if (self.lang and vocab.lang) and (self.lang != vocab.lang):\n                raise ValueError(Errors.E150.format(nlp=self.lang, vocab=vocab.lang))\n        self.vocab = vocab\n        if make_doc is True:\n            factory = self.Defaults.create_tokenizer\n            make_doc = factory(self, **meta.get(\"tokenizer\", {}))\n        self.tokenizer = make_doc\n        self.pipeline = []\n        self.max_length = max_length\n        self._optimizer = None",
        "begin_line": 141,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004807692307692308,
            "pseudo_dstar_susp": 0.004807692307692308,
            "pseudo_tarantula_susp": 0.004901960784313725,
            "pseudo_op2_susp": 0.004807692307692308,
            "pseudo_barinel_susp": 0.004901960784313725
        }
    },
    {
        "name": "spacy.language.Language.meta#189",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.meta(self)",
        "snippet": "    def meta(self):\n        if self.vocab.lang:\n            self._meta.setdefault(\"lang\", self.vocab.lang)\n        else:\n            self._meta.setdefault(\"lang\", self.lang)\n        self._meta.setdefault(\"name\", \"model\")\n        self._meta.setdefault(\"version\", \"0.0.0\")\n        self._meta.setdefault(\"spacy_version\", \">={}\".format(about.__version__))\n        self._meta.setdefault(\"description\", \"\")\n        self._meta.setdefault(\"author\", \"\")\n        self._meta.setdefault(\"email\", \"\")\n        self._meta.setdefault(\"url\", \"\")\n        self._meta.setdefault(\"license\", \"\")\n        self._meta[\"vectors\"] = {\n            \"width\": self.vocab.vectors_length,\n            \"vectors\": len(self.vocab.vectors),\n            \"keys\": self.vocab.vectors.n_keys,\n            \"name\": self.vocab.vectors.name,\n        }\n        self._meta[\"pipeline\"] = self.pipe_names\n        self._meta[\"factories\"] = self.pipe_factories\n        self._meta[\"labels\"] = self.pipe_labels\n        return self._meta",
        "begin_line": 189,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0125,
            "pseudo_dstar_susp": 0.011494252873563218,
            "pseudo_tarantula_susp": 0.0125,
            "pseudo_op2_susp": 0.011494252873563218,
            "pseudo_barinel_susp": 0.0125
        }
    },
    {
        "name": "spacy.language.Language.parser#228",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.parser(self)",
        "snippet": "    def parser(self):\n        return self.get_pipe(\"parser\")",
        "begin_line": 228,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.entity#232",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.entity(self)",
        "snippet": "    def entity(self):\n        return self.get_pipe(\"ner\")",
        "begin_line": 232,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.pipe_names#244",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.pipe_names(self)",
        "snippet": "    def pipe_names(self):\n        \"\"\"Get names of available pipeline components.\n\n        RETURNS (list): List of component name strings, in order.\n        \"\"\"\n        return [pipe_name for pipe_name, _ in self.pipeline]",
        "begin_line": 244,
        "end_line": 249,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0064516129032258064,
            "pseudo_dstar_susp": 0.09090909090909091,
            "pseudo_tarantula_susp": 0.005,
            "pseudo_op2_susp": 0.09090909090909091,
            "pseudo_barinel_susp": 0.005
        }
    },
    {
        "name": "spacy.language.Language.pipe_factories#252",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.pipe_factories(self)",
        "snippet": "    def pipe_factories(self):\n        \"\"\"Get the component factories for the available pipeline components.\n\n        RETURNS (dict): Factory names, keyed by component names.\n        \"\"\"\n        factories = {}\n        for pipe_name, pipe in self.pipeline:\n            factories[pipe_name] = getattr(pipe, \"factory\", pipe_name)\n        return factories",
        "begin_line": 252,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01282051282051282,
            "pseudo_dstar_susp": 0.011764705882352941,
            "pseudo_tarantula_susp": 0.01282051282051282,
            "pseudo_op2_susp": 0.011764705882352941,
            "pseudo_barinel_susp": 0.01282051282051282
        }
    },
    {
        "name": "spacy.language.Language.pipe_labels#263",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.pipe_labels(self)",
        "snippet": "    def pipe_labels(self):\n        \"\"\"Get the labels set by the pipeline components, if available (if\n        the component exposes a labels property).\n\n        RETURNS (dict): Labels keyed by component name.\n        \"\"\"\n        labels = OrderedDict()\n        for name, pipe in self.pipeline:\n            if hasattr(pipe, \"labels\"):\n                labels[name] = list(pipe.labels)\n        return labels",
        "begin_line": 263,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01282051282051282,
            "pseudo_dstar_susp": 0.011764705882352941,
            "pseudo_tarantula_susp": 0.01282051282051282,
            "pseudo_op2_susp": 0.011764705882352941,
            "pseudo_barinel_susp": 0.01282051282051282
        }
    },
    {
        "name": "spacy.language.Language.get_pipe#275",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.get_pipe(self, name)",
        "snippet": "    def get_pipe(self, name):\n        \"\"\"Get a pipeline component for a given component name.\n\n        name (unicode): Name of pipeline component to get.\n        RETURNS (callable): The pipeline component.\n\n        DOCS: https://spacy.io/api/language#get_pipe\n        \"\"\"\n        for pipe_name, component in self.pipeline:\n            if pipe_name == name:\n                return component\n        raise KeyError(Errors.E001.format(name=name, opts=self.pipe_names))",
        "begin_line": 275,
        "end_line": 286,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.011494252873563218,
            "pseudo_dstar_susp": 0.010638297872340425,
            "pseudo_tarantula_susp": 0.011494252873563218,
            "pseudo_op2_susp": 0.010638297872340425,
            "pseudo_barinel_susp": 0.011494252873563218
        }
    },
    {
        "name": "spacy.language.Language.create_pipe#288",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.create_pipe(self, name, config=dict())",
        "snippet": "    def create_pipe(self, name, config=dict()):\n        \"\"\"Create a pipeline component from a factory.\n\n        name (unicode): Factory name to look up in `Language.factories`.\n        config (dict): Configuration parameters to initialise component.\n        RETURNS (callable): Pipeline component.\n\n        DOCS: https://spacy.io/api/language#create_pipe\n        \"\"\"\n        if name not in self.factories:\n            if name == \"sbd\":\n                raise KeyError(Errors.E108.format(name=name))\n            else:\n                raise KeyError(Errors.E002.format(name=name))\n        factory = self.factories[name]\n        return factory(self, **config)",
        "begin_line": 288,
        "end_line": 303,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006369426751592357,
            "pseudo_dstar_susp": 0.006211180124223602,
            "pseudo_tarantula_susp": 0.0064516129032258064,
            "pseudo_op2_susp": 0.006211180124223602,
            "pseudo_barinel_susp": 0.0064516129032258064
        }
    },
    {
        "name": "spacy.language.Language.add_pipe#305",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.add_pipe(self, component, name=None, before=None, after=None, first=None, last=None)",
        "snippet": "    def add_pipe(\n        self, component, name=None, before=None, after=None, first=None, last=None\n    ):\n        \"\"\"Add a component to the processing pipeline. Valid components are\n        callables that take a `Doc` object, modify it and return it. Only one\n        of before/after/first/last can be set. Default behaviour is \"last\".\n\n        component (callable): The pipeline component.\n        name (unicode): Name of pipeline component. Overwrites existing\n            component.name attribute if available. If no name is set and\n            the component exposes no name attribute, component.__name__ is\n            used. An error is raised if a name already exists in the pipeline.\n        before (unicode): Component name to insert component directly before.\n        after (unicode): Component name to insert component directly after.\n        first (bool): Insert component first / not first in the pipeline.\n        last (bool): Insert component last / not last in the pipeline.\n\n        DOCS: https://spacy.io/api/language#add_pipe\n        \"\"\"\n        if not hasattr(component, \"__call__\"):\n            msg = Errors.E003.format(component=repr(component), name=name)\n            if isinstance(component, basestring_) and component in self.factories:\n                msg += Errors.E004.format(component=component)\n            raise ValueError(msg)\n        if name is None:\n            name = util.get_component_name(component)\n        if name in self.pipe_names:\n            raise ValueError(Errors.E007.format(name=name, opts=self.pipe_names))\n        if sum([bool(before), bool(after), bool(first), bool(last)]) >= 2:\n            raise ValueError(Errors.E006)\n        pipe_index = 0\n        pipe = (name, component)\n        if last or not any([first, before, after]):\n            pipe_index = len(self.pipeline)\n            self.pipeline.append(pipe)\n        elif first:\n            self.pipeline.insert(0, pipe)\n        elif before and before in self.pipe_names:\n            pipe_index = self.pipe_names.index(before)\n            self.pipeline.insert(self.pipe_names.index(before), pipe)\n        elif after and after in self.pipe_names:\n            pipe_index = self.pipe_names.index(after) + 1\n            self.pipeline.insert(self.pipe_names.index(after) + 1, pipe)\n        else:\n            raise ValueError(\n                Errors.E001.format(name=before or after, opts=self.pipe_names)\n            )\n        if ENABLE_PIPELINE_ANALYSIS:\n            analyze_pipes(self.pipeline, name, component, pipe_index)",
        "begin_line": 305,
        "end_line": 353,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005649717514124294,
            "pseudo_dstar_susp": 0.0055248618784530384,
            "pseudo_tarantula_susp": 0.005714285714285714,
            "pseudo_op2_susp": 0.0055248618784530384,
            "pseudo_barinel_susp": 0.005714285714285714
        }
    },
    {
        "name": "spacy.language.Language.has_pipe#355",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.has_pipe(self, name)",
        "snippet": "    def has_pipe(self, name):\n        \"\"\"Check if a component name is present in the pipeline. Equivalent to\n        `name in nlp.pipe_names`.\n\n        name (unicode): Name of the component.\n        RETURNS (bool): Whether a component of the name exists in the pipeline.\n\n        DOCS: https://spacy.io/api/language#has_pipe\n        \"\"\"\n        return name in self.pipe_names",
        "begin_line": 355,
        "end_line": 364,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.534941725998821e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.replace_pipe#366",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.replace_pipe(self, name, component)",
        "snippet": "    def replace_pipe(self, name, component):\n        \"\"\"Replace a component in the pipeline.\n\n        name (unicode): Name of the component to replace.\n        component (callable): Pipeline component.\n\n        DOCS: https://spacy.io/api/language#replace_pipe\n        \"\"\"\n        if name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\n        if not hasattr(component, \"__call__\"):\n            msg = Errors.E003.format(component=repr(component), name=name)\n            if isinstance(component, basestring_) and component in self.factories:\n                msg += Errors.E135.format(name=name)\n            raise ValueError(msg)\n        self.pipeline[self.pipe_names.index(name)] = (name, component)\n        if ENABLE_PIPELINE_ANALYSIS:\n            analyze_all_pipes(self.pipeline)",
        "begin_line": 366,
        "end_line": 383,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.rename_pipe#385",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.rename_pipe(self, old_name, new_name)",
        "snippet": "    def rename_pipe(self, old_name, new_name):\n        \"\"\"Rename a pipeline component.\n\n        old_name (unicode): Name of the component to rename.\n        new_name (unicode): New name of the component.\n\n        DOCS: https://spacy.io/api/language#rename_pipe\n        \"\"\"\n        if old_name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=old_name, opts=self.pipe_names))\n        if new_name in self.pipe_names:\n            raise ValueError(Errors.E007.format(name=new_name, opts=self.pipe_names))\n        i = self.pipe_names.index(old_name)\n        self.pipeline[i] = (new_name, self.pipeline[i][1])",
        "begin_line": 385,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.remove_pipe#400",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.remove_pipe(self, name)",
        "snippet": "    def remove_pipe(self, name):\n        \"\"\"Remove a component from the pipeline.\n\n        name (unicode): Name of the component to remove.\n        RETURNS (tuple): A `(name, component)` tuple of the removed component.\n\n        DOCS: https://spacy.io/api/language#remove_pipe\n        \"\"\"\n        if name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\n        removed = self.pipeline.pop(self.pipe_names.index(name))\n        if ENABLE_PIPELINE_ANALYSIS:\n            analyze_all_pipes(self.pipeline)\n        return removed",
        "begin_line": 400,
        "end_line": 413,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.__call__#415",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.__call__(self, text, disable=[], component_cfg=None)",
        "snippet": "    def __call__(self, text, disable=[], component_cfg=None):\n        \"\"\"Apply the pipeline to some text. The text can span multiple sentences,\n        and can contain arbtrary whitespace. Alignment into the original string\n        is preserved.\n\n        text (unicode): The text to be processed.\n        disable (list): Names of the pipeline components to disable.\n        component_cfg (dict): An optional dictionary with extra keyword arguments\n            for specific components.\n        RETURNS (Doc): A container for accessing the annotations.\n\n        DOCS: https://spacy.io/api/language#call\n        \"\"\"\n        if len(text) > self.max_length:\n            raise ValueError(\n                Errors.E088.format(length=len(text), max_length=self.max_length)\n            )\n        doc = self.make_doc(text)\n        if component_cfg is None:\n            component_cfg = {}\n        for name, proc in self.pipeline:\n            if name in disable:\n                continue\n            if not hasattr(proc, \"__call__\"):\n                raise ValueError(Errors.E003.format(component=type(proc), name=name))\n            doc = proc(doc, **component_cfg.get(name, {}))\n            if doc is None:\n                raise ValueError(Errors.E005.format(name=name))\n        return doc",
        "begin_line": 415,
        "end_line": 443,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4565265831810684e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.disable_pipes#445",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.disable_pipes(self, *names)",
        "snippet": "    def disable_pipes(self, *names):\n        \"\"\"Disable one or more pipeline components. If used as a context\n        manager, the pipeline will be restored to the initial state at the end\n        of the block. Otherwise, a DisabledPipes object is returned, that has\n        a `.restore()` method you can use to undo your changes.\n\n        DOCS: https://spacy.io/api/language#disable_pipes\n        \"\"\"\n        if len(names) == 1 and isinstance(names[0], (list, tuple)):\n            names = names[0]  # support list of names instead of spread\n        return DisabledPipes(self, *names)",
        "begin_line": 445,
        "end_line": 455,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.468874290566207e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.make_doc#457",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.make_doc(self, text)",
        "snippet": "    def make_doc(self, text):\n        return self.tokenizer(text)",
        "begin_line": 457,
        "end_line": 458,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.416570974295557e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language._format_docs_and_golds#460",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language._format_docs_and_golds(self, docs, golds)",
        "snippet": "    def _format_docs_and_golds(self, docs, golds):\n        \"\"\"Format golds and docs before update models.\"\"\"\n        expected_keys = (\"words\", \"tags\", \"heads\", \"deps\", \"entities\", \"cats\", \"links\")\n        gold_objs = []\n        doc_objs = []\n        for doc, gold in zip(docs, golds):\n            if isinstance(doc, basestring_):\n                doc = self.make_doc(doc)\n            if not isinstance(gold, GoldParse):\n                unexpected = [k for k in gold if k not in expected_keys]\n                if unexpected:\n                    err = Errors.E151.format(unexp=unexpected, exp=expected_keys)\n                    raise ValueError(err)\n                gold = GoldParse(doc, **gold)\n            doc_objs.append(doc)\n            gold_objs.append(gold)\n\n        return doc_objs, gold_objs",
        "begin_line": 460,
        "end_line": 477,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.update#479",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg=None)",
        "snippet": "    def update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg=None):\n        \"\"\"Update the models in the pipeline.\n\n        docs (iterable): A batch of `Doc` objects.\n        golds (iterable): A batch of `GoldParse` objects.\n        drop (float): The dropout rate.\n        sgd (callable): An optimizer.\n        losses (dict): Dictionary to update with the loss, keyed by component.\n        component_cfg (dict): Config parameters for specific pipeline\n            components, keyed by component name.\n\n        DOCS: https://spacy.io/api/language#update\n        \"\"\"\n        if len(docs) != len(golds):\n            raise IndexError(Errors.E009.format(n_docs=len(docs), n_golds=len(golds)))\n        if len(docs) == 0:\n            return\n        if sgd is None:\n            if self._optimizer is None:\n                self._optimizer = create_default_optimizer(Model.ops)\n            sgd = self._optimizer\n        # Allow dict of args to GoldParse, instead of GoldParse objects.\n        docs, golds = self._format_docs_and_golds(docs, golds)\n        grads = {}\n\n        def get_grads(W, dW, key=None):\n            grads[key] = (W, dW)\n\n        get_grads.alpha = sgd.alpha\n        get_grads.b1 = sgd.b1\n        get_grads.b2 = sgd.b2\n        pipes = list(self.pipeline)\n        random.shuffle(pipes)\n        if component_cfg is None:\n            component_cfg = {}\n        for name, proc in pipes:\n            if not hasattr(proc, \"update\"):\n                continue\n            grads = {}\n            kwargs = component_cfg.get(name, {})\n            kwargs.setdefault(\"drop\", drop)\n            proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n            for key, (W, dW) in grads.items():\n                sgd(W, dW, key=key)",
        "begin_line": 479,
        "end_line": 522,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.get_grads#504",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.get_grads(W, dW, key=None)",
        "snippet": "        def get_grads(W, dW, key=None):\n            grads[key] = (W, dW)",
        "begin_line": 504,
        "end_line": 505,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.begin_training#589",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.begin_training(self, get_gold_tuples=None, sgd=None, component_cfg=None, **cfg)",
        "snippet": "    def begin_training(self, get_gold_tuples=None, sgd=None, component_cfg=None, **cfg):\n        \"\"\"Allocate models, pre-process training data and acquire a trainer and\n        optimizer. Used as a contextmanager.\n\n        get_gold_tuples (function): Function returning gold data\n        component_cfg (dict): Config parameters for specific components.\n        **cfg: Config parameters.\n        RETURNS: An optimizer.\n\n        DOCS: https://spacy.io/api/language#begin_training\n        \"\"\"\n        if get_gold_tuples is None:\n            get_gold_tuples = lambda: []\n        # Populate vocab\n        else:\n            for _, annots_brackets in get_gold_tuples():\n                _ = annots_brackets.pop()\n                for annots, _ in annots_brackets:\n                    for word in annots[1]:\n                        _ = self.vocab[word]  # noqa: F841\n        if cfg.get(\"device\", -1) >= 0:\n            util.use_gpu(cfg[\"device\"])\n            if self.vocab.vectors.data.shape[1] >= 1:\n                self.vocab.vectors.data = Model.ops.asarray(self.vocab.vectors.data)\n        link_vectors_to_models(self.vocab)\n        if self.vocab.vectors.data.shape[1]:\n            cfg[\"pretrained_vectors\"] = self.vocab.vectors.name\n            cfg[\"pretrained_dims\"] = self.vocab.vectors.data.shape[1]\n        if sgd is None:\n            sgd = create_default_optimizer(Model.ops)\n        self._optimizer = sgd\n        if component_cfg is None:\n            component_cfg = {}\n        for name, proc in self.pipeline:\n            if hasattr(proc, \"begin_training\"):\n                kwargs = component_cfg.get(name, {})\n                kwargs.update(cfg)\n                proc.begin_training(\n                    get_gold_tuples,\n                    pipeline=self.pipeline,\n                    sgd=self._optimizer,\n                    **kwargs\n                )\n        return self._optimizer",
        "begin_line": 589,
        "end_line": 632,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.evaluate#658",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.evaluate(self, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None)",
        "snippet": "    def evaluate(\n        self, docs_golds, verbose=False, batch_size=256, scorer=None, component_cfg=None\n    ):\n        \"\"\"Evaluate a model's pipeline components.\n\n        docs_golds (iterable): Tuples of `Doc` and `GoldParse` objects.\n        verbose (bool): Print debugging information.\n        batch_size (int): Batch size to use.\n        scorer (Scorer): Optional `Scorer` to use. If not passed in, a new one\n            will be created.\n        component_cfg (dict): An optional dictionary with extra keyword\n            arguments for specific components.\n        RETURNS (Scorer): The scorer containing the evaluation results.\n\n        DOCS: https://spacy.io/api/language#evaluate\n        \"\"\"\n        if scorer is None:\n            scorer = Scorer(pipeline=self.pipeline)\n        if component_cfg is None:\n            component_cfg = {}\n        docs, golds = zip(*docs_golds)\n        docs = [\n            self.make_doc(doc) if isinstance(doc, basestring_) else doc for doc in docs\n        ]\n        golds = list(golds)\n        for name, pipe in self.pipeline:\n            kwargs = component_cfg.get(name, {})\n            kwargs.setdefault(\"batch_size\", batch_size)\n            if not hasattr(pipe, \"pipe\"):\n                docs = _pipe(docs, pipe, kwargs)\n            else:\n                docs = pipe.pipe(docs, **kwargs)\n        for doc, gold in zip(docs, golds):\n            if not isinstance(gold, GoldParse):\n                gold = GoldParse(doc, **gold)\n            if verbose:\n                print(doc)\n            kwargs = component_cfg.get(\"scorer\", {})\n            kwargs.setdefault(\"verbose\", verbose)\n            scorer.score(doc, gold, **kwargs)\n        return scorer",
        "begin_line": 658,
        "end_line": 698,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.pipe#732",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.pipe(self, texts, as_tuples=False, n_threads=-1, batch_size=1000, disable=[], cleanup=False, component_cfg=None, n_process=1)",
        "snippet": "    def pipe(\n        self,\n        texts,\n        as_tuples=False,\n        n_threads=-1,\n        batch_size=1000,\n        disable=[],\n        cleanup=False,\n        component_cfg=None,\n        n_process=1,\n    ):\n        \"\"\"Process texts as a stream, and yield `Doc` objects in order.\n\n        texts (iterator): A sequence of texts to process.\n        as_tuples (bool): If set to True, inputs should be a sequence of\n            (text, context) tuples. Output will then be a sequence of\n            (doc, context) tuples. Defaults to False.\n        batch_size (int): The number of texts to buffer.\n        disable (list): Names of the pipeline components to disable.\n        cleanup (bool): If True, unneeded strings are freed to control memory\n            use. Experimental.\n        component_cfg (dict): An optional dictionary with extra keyword\n            arguments for specific components.\n        n_process (int): Number of processors to process texts, only supported\n            in Python3. If -1, set `multiprocessing.cpu_count()`.\n        YIELDS (Doc): Documents in the order of the original text.\n\n        DOCS: https://spacy.io/api/language#pipe\n        \"\"\"\n        if is_python2 and n_process != 1:\n            warnings.warn(Warnings.W023)\n            n_process = 1\n        if n_threads != -1:\n            warnings.warn(Warnings.W016, DeprecationWarning)\n        if n_process == -1:\n            n_process = mp.cpu_count()\n        if as_tuples:\n            text_context1, text_context2 = itertools.tee(texts)\n            texts = (tc[0] for tc in text_context1)\n            contexts = (tc[1] for tc in text_context2)\n            docs = self.pipe(\n                texts,\n                batch_size=batch_size,\n                disable=disable,\n                n_process=n_process,\n                component_cfg=component_cfg,\n            )\n            for doc, context in izip(docs, contexts):\n                yield (doc, context)\n            return\n        if component_cfg is None:\n            component_cfg = {}\n\n        pipes = (\n            []\n        )  # contains functools.partial objects to easily create multiprocess worker.\n        for name, proc in self.pipeline:\n            if name in disable:\n                continue\n            kwargs = component_cfg.get(name, {})\n            # Allow component_cfg to overwrite the top-level kwargs.\n            kwargs.setdefault(\"batch_size\", batch_size)\n            if hasattr(proc, \"pipe\"):\n                f = functools.partial(proc.pipe, **kwargs)\n            else:\n                # Apply the function, but yield the doc\n                f = functools.partial(_pipe, proc=proc, kwargs=kwargs)\n            pipes.append(f)\n\n        if n_process != 1:\n            docs = self._multiprocessing_pipe(texts, pipes, n_process, batch_size)\n        else:\n            # if n_process == 1, no processes are forked.\n            docs = (self.make_doc(text) for text in texts)\n            for pipe in pipes:\n                docs = pipe(docs)\n\n        # Track weakrefs of \"recent\" documents, so that we can see when they\n        # expire from memory. When they do, we know we don't need old strings.\n        # This way, we avoid maintaining an unbounded growth in string entries\n        # in the string store.\n        recent_refs = weakref.WeakSet()\n        old_refs = weakref.WeakSet()\n        # Keep track of the original string data, so that if we flush old strings,\n        # we can recover the original ones. However, we only want to do this if we're\n        # really adding strings, to save up-front costs.\n        original_strings_data = None\n        nr_seen = 0\n        for doc in docs:\n            yield doc\n            if cleanup:\n                recent_refs.add(doc)\n                if nr_seen < 10000:\n                    old_refs.add(doc)\n                    nr_seen += 1\n                elif len(old_refs) == 0:\n                    old_refs, recent_refs = recent_refs, old_refs\n                    if original_strings_data is None:\n                        original_strings_data = list(self.vocab.strings)\n                    else:\n                        keys, strings = self.vocab.strings._cleanup_stale_strings(\n                            original_strings_data\n                        )\n                        self.vocab._reset_cache(keys, strings)\n                        self.tokenizer._reset_cache(keys)\n                    nr_seen = 0",
        "begin_line": 732,
        "end_line": 837,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language._multiprocessing_pipe#839",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language._multiprocessing_pipe(self, texts, pipes, n_process, batch_size)",
        "snippet": "    def _multiprocessing_pipe(self, texts, pipes, n_process, batch_size):\n        # raw_texts is used later to stop iteration.\n        texts, raw_texts = itertools.tee(texts)\n        # for sending texts to worker\n        texts_q = [mp.Queue() for _ in range(n_process)]\n        # for receiving byte-encoded docs from worker\n        bytedocs_recv_ch, bytedocs_send_ch = zip(\n            *[mp.Pipe(False) for _ in range(n_process)]\n        )\n\n        batch_texts = minibatch(texts, batch_size)\n        # Sender sends texts to the workers.\n        # This is necessary to properly handle infinite length of texts.\n        # (In this case, all data cannot be sent to the workers at once)\n        sender = _Sender(batch_texts, texts_q, chunk_size=n_process)\n        # send twice to make process busy\n        sender.send()\n        sender.send()\n\n        procs = [\n            mp.Process(\n                target=_apply_pipes,\n                args=(\n                    self.make_doc,\n                    pipes,\n                    rch,\n                    sch,\n                    Underscore.get_state(),\n                    load_nlp.VECTORS,\n                ),\n            )\n            for rch, sch in zip(texts_q, bytedocs_send_ch)\n        ]\n        for proc in procs:\n            proc.start()\n\n        # Cycle channels not to break the order of docs.\n        # The received object is a batch of byte-encoded docs, so flatten them with chain.from_iterable.\n        byte_docs = chain.from_iterable(recv.recv() for recv in cycle(bytedocs_recv_ch))\n        docs = (Doc(self.vocab).from_bytes(byte_doc) for byte_doc in byte_docs)\n        try:\n            for i, (_, doc) in enumerate(zip(raw_texts, docs), 1):\n                yield doc\n                if i % batch_size == 0:\n                    # tell `sender` that one batch was consumed.\n                    sender.step()\n        finally:\n            for proc in procs:\n                proc.terminate()",
        "begin_line": 839,
        "end_line": 887,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.to_disk#889",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.to_disk(self, path, exclude=tuple(), disable=None)",
        "snippet": "    def to_disk(self, path, exclude=tuple(), disable=None):\n        \"\"\"Save the current state to a directory.  If a model is loaded, this\n        will include the model.\n\n        path (unicode or Path): Path to a directory, which will be created if\n            it doesn't exist.\n        exclude (list): Names of components or serialization fields to exclude.\n\n        DOCS: https://spacy.io/api/language#to_disk\n        \"\"\"\n        if disable is not None:\n            warnings.warn(Warnings.W014, DeprecationWarning)\n            exclude = disable\n        path = util.ensure_path(path)\n        serializers = OrderedDict()\n        serializers[\"tokenizer\"] = lambda p: self.tokenizer.to_disk(\n            p, exclude=[\"vocab\"]\n        )\n        serializers[\"meta.json\"] = lambda p: p.open(\"w\").write(\n            srsly.json_dumps(self.meta)\n        )\n        for name, proc in self.pipeline:\n            if not hasattr(proc, \"name\"):\n                continue\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"to_disk\"):\n                continue\n            serializers[name] = lambda p, proc=proc: proc.to_disk(p, exclude=[\"vocab\"])\n        serializers[\"vocab\"] = lambda p: self.vocab.to_disk(p)\n        util.to_disk(path, serializers, exclude)",
        "begin_line": 889,
        "end_line": 919,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.1111111111111111,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.1111111111111111
        }
    },
    {
        "name": "spacy.language.Language.from_disk#921",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.from_disk(self, path, exclude=tuple(), disable=None)",
        "snippet": "    def from_disk(self, path, exclude=tuple(), disable=None):\n        \"\"\"Loads state from a directory. Modifies the object in place and\n        returns it. If the saved `Language` object contains a model, the\n        model will be loaded.\n\n        path (unicode or Path): A path to a directory.\n        exclude (list): Names of components or serialization fields to exclude.\n        RETURNS (Language): The modified `Language` object.\n\n        DOCS: https://spacy.io/api/language#from_disk\n        \"\"\"\n        if disable is not None:\n            warnings.warn(Warnings.W014, DeprecationWarning)\n            exclude = disable\n        path = util.ensure_path(path)\n        deserializers = OrderedDict()\n        deserializers[\"meta.json\"] = lambda p: self.meta.update(srsly.read_json(p))\n        deserializers[\"vocab\"] = lambda p: self.vocab.from_disk(\n            p\n        ) and _fix_pretrained_vectors_name(self)\n        deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(\n            p, exclude=[\"vocab\"]\n        )\n        for name, proc in self.pipeline:\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"from_disk\"):\n                continue\n            deserializers[name] = lambda p, proc=proc: proc.from_disk(\n                p, exclude=[\"vocab\"]\n            )\n        if not (path / \"vocab\").exists() and \"vocab\" not in exclude:\n            # Convert to list here in case exclude is (default) tuple\n            exclude = list(exclude) + [\"vocab\"]\n        util.from_disk(path, deserializers, exclude)\n        self._path = path\n        return self",
        "begin_line": 921,
        "end_line": 957,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.1111111111111111,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.1111111111111111
        }
    },
    {
        "name": "spacy.language.Language.to_bytes#959",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.to_bytes(self, exclude=tuple(), disable=None, **kwargs)",
        "snippet": "    def to_bytes(self, exclude=tuple(), disable=None, **kwargs):\n        \"\"\"Serialize the current state to a binary string.\n\n        exclude (list): Names of components or serialization fields to exclude.\n        RETURNS (bytes): The serialized form of the `Language` object.\n\n        DOCS: https://spacy.io/api/language#to_bytes\n        \"\"\"\n        if disable is not None:\n            warnings.warn(Warnings.W014, DeprecationWarning)\n            exclude = disable\n        serializers = OrderedDict()\n        serializers[\"vocab\"] = lambda: self.vocab.to_bytes()\n        serializers[\"tokenizer\"] = lambda: self.tokenizer.to_bytes(exclude=[\"vocab\"])\n        serializers[\"meta.json\"] = lambda: srsly.json_dumps(OrderedDict(sorted(self.meta.items())))\n        for name, proc in self.pipeline:\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"to_bytes\"):\n                continue\n            serializers[name] = lambda proc=proc: proc.to_bytes(exclude=[\"vocab\"])\n        exclude = util.get_serialization_exclude(serializers, exclude, kwargs)\n        return util.to_bytes(serializers, exclude)",
        "begin_line": 959,
        "end_line": 981,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.Language.from_bytes#983",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.Language",
        "signature": "spacy.language.Language.from_bytes(self, bytes_data, exclude=tuple(), disable=None, **kwargs)",
        "snippet": "    def from_bytes(self, bytes_data, exclude=tuple(), disable=None, **kwargs):\n        \"\"\"Load state from a binary string.\n\n        bytes_data (bytes): The data to load from.\n        exclude (list): Names of components or serialization fields to exclude.\n        RETURNS (Language): The `Language` object.\n\n        DOCS: https://spacy.io/api/language#from_bytes\n        \"\"\"\n        if disable is not None:\n            warnings.warn(Warnings.W014, DeprecationWarning)\n            exclude = disable\n        deserializers = OrderedDict()\n        deserializers[\"meta.json\"] = lambda b: self.meta.update(srsly.json_loads(b))\n        deserializers[\"vocab\"] = lambda b: self.vocab.from_bytes(\n            b\n        ) and _fix_pretrained_vectors_name(self)\n        deserializers[\"tokenizer\"] = lambda b: self.tokenizer.from_bytes(\n            b, exclude=[\"vocab\"]\n        )\n        for name, proc in self.pipeline:\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"from_bytes\"):\n                continue\n            deserializers[name] = lambda b, proc=proc: proc.from_bytes(\n                b, exclude=[\"vocab\"]\n            )\n        exclude = util.get_serialization_exclude(deserializers, exclude, kwargs)\n        util.from_bytes(bytes_data, deserializers, exclude)\n        return self",
        "begin_line": 983,
        "end_line": 1013,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.component.__init__#1028",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.component",
        "signature": "spacy.language.component.__init__(self, name=None, assigns=tuple(), requires=tuple(), retokenizes=False)",
        "snippet": "    def __init__(self, name=None, assigns=tuple(), requires=tuple(), retokenizes=False):\n        \"\"\"Decorate a pipeline component.\n\n        name (unicode): Default component and factory name.\n        assigns (list): Attributes assigned by component, e.g. `[\"token.pos\"]`.\n        requires (list): Attributes required by component, e.g. `[\"token.dep\"]`.\n        retokenizes (bool): Whether the component changes the tokenization.\n        \"\"\"\n        self.name = name\n        self.assigns = validate_attrs(assigns)\n        self.requires = validate_attrs(requires)\n        self.retokenizes = retokenizes",
        "begin_line": 1028,
        "end_line": 1039,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.component.__call__#1041",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.component",
        "signature": "spacy.language.component.__call__(self, *args, **kwargs)",
        "snippet": "    def __call__(self, *args, **kwargs):\n        obj = args[0]\n        args = args[1:]\n        factory_name = self.name or util.get_component_name(obj)\n        obj.name = factory_name\n        obj.factory = factory_name\n        obj.assigns = self.assigns\n        obj.requires = self.requires\n        obj.retokenizes = self.retokenizes\n\n        def factory(nlp, **cfg):\n            if hasattr(obj, \"from_nlp\"):\n                return obj.from_nlp(nlp, **cfg)\n            elif isinstance(obj, class_types):\n                return obj()\n            return obj\n\n        Language.factories[obj.factory] = factory\n        return obj",
        "begin_line": 1041,
        "end_line": 1059,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.component.factory#1051",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.component",
        "signature": "spacy.language.component.factory(nlp, **cfg)",
        "snippet": "        def factory(nlp, **cfg):\n            if hasattr(obj, \"from_nlp\"):\n                return obj.from_nlp(nlp, **cfg)\n            elif isinstance(obj, class_types):\n                return obj()\n            return obj",
        "begin_line": 1051,
        "end_line": 1056,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language._fix_pretrained_vectors_name#1062",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language",
        "signature": "spacy.language._fix_pretrained_vectors_name(nlp)",
        "snippet": "def _fix_pretrained_vectors_name(nlp):\n    # TODO: Replace this once we handle vectors consistently as static\n    # data\n    if \"vectors\" in nlp.meta and nlp.meta[\"vectors\"].get(\"name\"):\n        nlp.vocab.vectors.name = nlp.meta[\"vectors\"][\"name\"]\n    elif not nlp.vocab.vectors.size:\n        nlp.vocab.vectors.name = None\n    elif \"name\" in nlp.meta and \"lang\" in nlp.meta:\n        vectors_name = \"%s_%s.vectors\" % (nlp.meta[\"lang\"], nlp.meta[\"name\"])\n        nlp.vocab.vectors.name = vectors_name\n    else:\n        raise ValueError(Errors.E092)\n    if nlp.vocab.vectors.size != 0:\n        link_vectors_to_models(nlp.vocab)\n    for name, proc in nlp.pipeline:\n        if not hasattr(proc, \"cfg\"):\n            continue\n        proc.cfg.setdefault(\"deprecation_fixes\", {})\n        proc.cfg[\"deprecation_fixes\"][\"vectors_name\"] = nlp.vocab.vectors.name",
        "begin_line": 1062,
        "end_line": 1080,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.06666666666666667,
            "pseudo_dstar_susp": 0.043478260869565216,
            "pseudo_tarantula_susp": 0.1111111111111111,
            "pseudo_op2_susp": 0.043478260869565216,
            "pseudo_barinel_susp": 0.1111111111111111
        }
    },
    {
        "name": "spacy.language.DisabledPipes.__init__#1086",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.DisabledPipes",
        "signature": "spacy.language.DisabledPipes.__init__(self, nlp, *names)",
        "snippet": "    def __init__(self, nlp, *names):\n        self.nlp = nlp\n        self.names = names\n        # Important! Not deep copy -- we just want the container (but we also\n        # want to support people providing arbitrarily typed nlp.pipeline\n        # objects.)\n        self.original_pipeline = copy(nlp.pipeline)\n        list.__init__(self)\n        self.extend(nlp.remove_pipe(name) for name in names)",
        "begin_line": 1086,
        "end_line": 1094,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.465282429113641e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.DisabledPipes.__enter__#1096",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.DisabledPipes",
        "signature": "spacy.language.DisabledPipes.__enter__(self)",
        "snippet": "    def __enter__(self):\n        return self",
        "begin_line": 1096,
        "end_line": 1097,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4676763615243714e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.DisabledPipes.__exit__#1099",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.DisabledPipes",
        "signature": "spacy.language.DisabledPipes.__exit__(self, *args)",
        "snippet": "    def __exit__(self, *args):\n        self.restore()",
        "begin_line": 1099,
        "end_line": 1100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4676763615243714e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language.DisabledPipes.restore#1102",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language.DisabledPipes",
        "signature": "spacy.language.DisabledPipes.restore(self)",
        "snippet": "    def restore(self):\n        \"\"\"Restore the pipeline to its state when DisabledPipes was created.\"\"\"\n        current, self.nlp.pipeline = self.nlp.pipeline, self.original_pipeline\n        unexpected = [name for name, pipe in current if not self.nlp.has_pipe(name)]\n        if unexpected:\n            # Don't change the pipeline if we're raising an error.\n            self.nlp.pipeline = current\n            raise ValueError(Errors.E008.format(names=unexpected))\n        self[:] = []",
        "begin_line": 1102,
        "end_line": 1110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.465282429113641e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language._pipe#1113",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language",
        "signature": "spacy.language._pipe(docs, proc, kwargs)",
        "snippet": "def _pipe(docs, proc, kwargs):\n    # We added some args for pipe that __call__ doesn't expect.\n    kwargs = dict(kwargs)\n    for arg in [\"n_threads\", \"batch_size\"]:\n        if arg in kwargs:\n            kwargs.pop(arg)\n    for doc in docs:\n        doc = proc(doc, **kwargs)\n        yield doc",
        "begin_line": 1113,
        "end_line": 1121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.616592031762153e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language._Sender.__init__#1148",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language._Sender",
        "signature": "spacy.language._Sender.__init__(self, data, queues, chunk_size)",
        "snippet": "    def __init__(self, data, queues, chunk_size):\n        self.data = iter(data)\n        self.queues = iter(cycle(queues))\n        self.chunk_size = chunk_size\n        self.count = 0",
        "begin_line": 1148,
        "end_line": 1152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language._Sender.send#1154",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language._Sender",
        "signature": "spacy.language._Sender.send(self)",
        "snippet": "    def send(self):\n        \"\"\"Send chunk_size items from self.data to channels.\"\"\"\n        for item, q in itertools.islice(\n            zip(self.data, cycle(self.queues)), self.chunk_size\n        ):\n            # cycle channels so that distribute the texts evenly\n            q.put(item)",
        "begin_line": 1154,
        "end_line": 1160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.language._Sender.step#1162",
        "src_path": "spacy/language.py",
        "class_name": "spacy.language._Sender",
        "signature": "spacy.language._Sender.step(self)",
        "snippet": "    def step(self):\n        \"\"\"Tell sender that comsumed one item.\n\n        Data is sent to the workers after every chunk_size calls.\"\"\"\n        self.count += 1\n        if self.count >= self.chunk_size:\n            self.count = 0\n            self.send()",
        "begin_line": 1162,
        "end_line": 1169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.el.__init__.GreekDefaults.create_lemmatizer#37",
        "src_path": "spacy/lang/el/__init__.py",
        "class_name": "spacy.lang.el.__init__.GreekDefaults",
        "signature": "spacy.lang.el.__init__.GreekDefaults.create_lemmatizer(cls, nlp=None, lookups=None)",
        "snippet": "    def create_lemmatizer(cls, nlp=None, lookups=None):\n        if lookups is None:\n            lookups = Lookups()\n        return GreekLemmatizer(lookups)",
        "begin_line": 37,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.sr.lex_attrs.like_num#54",
        "src_path": "spacy/lang/sr/lex_attrs.py",
        "class_name": "spacy.lang.sr.lex_attrs",
        "signature": "spacy.lang.sr.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 54,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.zh.lex_attrs.like_num#77",
        "src_path": "spacy/lang/zh/lex_attrs.py",
        "class_name": "spacy.lang.zh.lex_attrs",
        "signature": "spacy.lang.zh.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"\uff0c\", \"\").replace(\"\u3002\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _single_num_words:\n        return True\n    # fmt: off\n    if re.match('^((' + '|'.join(_count_num_words) + '){1}'\n                + '(' + '|'.join(_base_num_words) + '){1})+'\n                + '(' + '|'.join(_count_num_words) + ')?$', text):\n        return True\n    # fmt: on\n    return False",
        "begin_line": 77,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.da.lex_attrs.like_num#37",
        "src_path": "spacy/lang/da/lex_attrs.py",
        "class_name": "spacy.lang.da.lex_attrs",
        "signature": "spacy.lang.da.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    if text.lower() in _ordinal_words:\n        return True\n    return False",
        "begin_line": 37,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.sk.lex_attrs.like_num#47",
        "src_path": "spacy/lang/sk/lex_attrs.py",
        "class_name": "spacy.lang.sk.lex_attrs",
        "signature": "spacy.lang.sk.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.pretrain.make_docs#275",
        "src_path": "spacy/cli/pretrain.py",
        "class_name": "spacy.cli.pretrain",
        "signature": "spacy.cli.pretrain.make_docs(nlp, batch, min_length, max_length)",
        "snippet": "def make_docs(nlp, batch, min_length, max_length):\n    docs = []\n    skip_count = 0\n    for record in batch:\n        if not isinstance(record, dict):\n            raise TypeError(Errors.E137.format(type=type(record), line=record))\n        if \"tokens\" in record:\n            words = record[\"tokens\"]\n            if not words:\n                skip_count += 1\n                continue\n            doc = Doc(nlp.vocab, words=words)\n        elif \"text\" in record:\n            text = record[\"text\"]\n            if not text:\n                skip_count += 1\n                continue\n            doc = nlp.make_doc(text)\n        else:\n            raise ValueError(Errors.E138.format(text=record))\n        if \"heads\" in record:\n            heads = record[\"heads\"]\n            heads = numpy.asarray(heads, dtype=\"uint64\")\n            heads = heads.reshape((len(doc), 1))\n            doc = doc.from_array([HEAD], heads)\n        if len(doc) >= min_length and len(doc) < max_length:\n            docs.append(doc)\n    return docs, skip_count",
        "begin_line": 275,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.errors.ErrorsWithCodes.__getattribute__#8",
        "src_path": "spacy/errors.py",
        "class_name": "spacy.errors.ErrorsWithCodes",
        "signature": "spacy.errors.ErrorsWithCodes.__getattribute__(self, code)",
        "snippet": "        def __getattribute__(self, code):\n            msg = getattr(err_cls, code)\n            return \"[{code}] {msg}\".format(code=code, msg=msg)",
        "begin_line": 8,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4152059693584704e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.errors.MatchPatternError.__init__#576",
        "src_path": "spacy/errors.py",
        "class_name": "spacy.errors.MatchPatternError",
        "signature": "spacy.errors.MatchPatternError.__init__(self, key, errors)",
        "snippet": "    def __init__(self, key, errors):\n        \"\"\"Custom error for validating match patterns.\n\n        key (unicode): The name of the matcher rule.\n        errors (dict): Validation errors (sequence of strings) mapped to pattern\n            ID, i.e. the index of the added pattern.\n        \"\"\"\n        msg = \"Invalid token patterns for matcher rule '{}'\\n\".format(key)\n        for pattern_idx, error_msgs in errors.items():\n            pattern_errors = \"\\n\".join([\"- {}\".format(e) for e in error_msgs])\n            msg += \"\\nPattern {}:\\n{}\\n\".format(pattern_idx, pattern_errors)\n        ValueError.__init__(self, msg)",
        "begin_line": 576,
        "end_line": 587,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conll_ner2json.conll_ner2json#12",
        "src_path": "spacy/cli/converters/conll_ner2json.py",
        "class_name": "spacy.cli.converters.conll_ner2json",
        "signature": "spacy.cli.converters.conll_ner2json.conll_ner2json(input_data, n_sents=10, seg_sents=False, model=None, no_print=False, **kwargs)",
        "snippet": "def conll_ner2json(\n    input_data, n_sents=10, seg_sents=False, model=None, no_print=False, **kwargs\n):\n    \"\"\"\n    Convert files in the CoNLL-2003 NER format and similar\n    whitespace-separated columns into JSON format for use with train cli.\n\n    The first column is the tokens, the final column is the IOB tags. If an\n    additional second column is present, the second column is the tags.\n\n    Sentences are separated with whitespace and documents can be separated\n    using the line \"-DOCSTART- -X- O O\".\n\n    Sample format:\n\n    -DOCSTART- -X- O O\n\n    I O\n    like O\n    London B-GPE\n    and O\n    New B-GPE\n    York I-GPE\n    City I-GPE\n    . O\n\n    \"\"\"\n    msg = Printer(no_print=no_print)\n    doc_delimiter = \"-DOCSTART- -X- O O\"\n    # check for existing delimiters, which should be preserved\n    if \"\\n\\n\" in input_data and seg_sents:\n        msg.warn(\n            \"Sentence boundaries found, automatic sentence segmentation with \"\n            \"`-s` disabled.\"\n        )\n        seg_sents = False\n    if doc_delimiter in input_data and n_sents:\n        msg.warn(\n            \"Document delimiters found, automatic document segmentation with \"\n            \"`-n` disabled.\"\n        )\n        n_sents = 0\n    # do document segmentation with existing sentences\n    if \"\\n\\n\" in input_data and doc_delimiter not in input_data and n_sents:\n        n_sents_info(msg, n_sents)\n        input_data = segment_docs(input_data, n_sents, doc_delimiter)\n    # do sentence segmentation with existing documents\n    if \"\\n\\n\" not in input_data and doc_delimiter in input_data and seg_sents:\n        input_data = segment_sents_and_docs(input_data, 0, \"\", model=model, msg=msg)\n    # do both sentence segmentation and document segmentation according\n    # to options\n    if \"\\n\\n\" not in input_data and doc_delimiter not in input_data:\n        # sentence segmentation required for document segmentation\n        if n_sents > 0 and not seg_sents:\n            msg.warn(\n                \"No sentence boundaries found to use with option `-n {}`. \"\n                \"Use `-s` to automatically segment sentences or `-n 0` \"\n                \"to disable.\".format(n_sents)\n            )\n        else:\n            n_sents_info(msg, n_sents)\n            input_data = segment_sents_and_docs(\n                input_data, n_sents, doc_delimiter, model=model, msg=msg\n            )\n    # provide warnings for problematic data\n    if \"\\n\\n\" not in input_data:\n        msg.warn(\n            \"No sentence boundaries found. Use `-s` to automatically segment \"\n            \"sentences.\"\n        )\n    if doc_delimiter not in input_data:\n        msg.warn(\n            \"No document delimiters found. Use `-n` to automatically group \"\n            \"sentences into documents.\"\n        )\n    output_docs = []\n    for doc in input_data.strip().split(doc_delimiter):\n        doc = doc.strip()\n        if not doc:\n            continue\n        output_doc = []\n        for sent in doc.split(\"\\n\\n\"):\n            sent = sent.strip()\n            if not sent:\n                continue\n            lines = [line.strip() for line in sent.split(\"\\n\") if line.strip()]\n            cols = list(zip(*[line.split() for line in lines]))\n            if len(cols) < 2:\n                raise ValueError(\n                    \"The token-per-line NER file is not formatted correctly. \"\n                    \"Try checking whitespace and delimiters. See \"\n                    \"https://spacy.io/api/cli#convert\"\n                )\n            words = cols[0]\n            iob_ents = cols[-1]\n            if len(cols) > 2:\n                tags = cols[1]\n            else:\n                tags = [\"-\"] * len(words)\n            biluo_ents = iob_to_biluo(iob_ents)\n            output_doc.append(\n                {\n                    \"tokens\": [\n                        {\"orth\": w, \"tag\": tag, \"ner\": ent}\n                        for (w, tag, ent) in zip(words, tags, biluo_ents)\n                    ]\n                }\n            )\n        output_docs.append(\n            {\"id\": len(output_docs), \"paragraphs\": [{\"sentences\": output_doc}]}\n        )\n        output_doc = []\n    return output_docs",
        "begin_line": 12,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.conll_ner2json.n_sents_info#168",
        "src_path": "spacy/cli/converters/conll_ner2json.py",
        "class_name": "spacy.cli.converters.conll_ner2json",
        "signature": "spacy.cli.converters.conll_ner2json.n_sents_info(msg, n_sents)",
        "snippet": "def n_sents_info(msg, n_sents):\n    msg.info(\"Grouping every {} sentences into a document.\".format(n_sents))\n    if n_sents == 1:\n        msg.warn(\n            \"To generate better training data, you may want to group \"\n            \"sentences into documents with `-n 10`.\"\n        )",
        "begin_line": 168,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.eu.lex_attrs.like_num#62",
        "src_path": "spacy/lang/eu/lex_attrs.py",
        "class_name": "spacy.lang.eu.lex_attrs",
        "signature": "spacy.lang.eu.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    if text in _ordinal_words:\n        return True\n    return False",
        "begin_line": 62,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.id.lex_attrs.like_num#41",
        "src_path": "spacy/lang/id/lex_attrs.py",
        "class_name": "spacy.lang.id.lex_attrs",
        "signature": "spacy.lang.id.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    if text.count(\"-\") == 1:\n        _, num = text.split(\"-\")\n        if num.isdigit() or num in _num_words:\n            return True\n    return False",
        "begin_line": 41,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.id.lex_attrs.is_currency#60",
        "src_path": "spacy/lang/id/lex_attrs.py",
        "class_name": "spacy.lang.id.lex_attrs",
        "signature": "spacy.lang.id.lex_attrs.is_currency(text)",
        "snippet": "def is_currency(text):\n    if text in LIST_CURRENCY:\n        return True\n\n    for char in text:\n        if unicodedata.category(char) != \"Sc\":\n            return False\n    return True",
        "begin_line": 60,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4972117287281886e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.sv.syntax_iterators.noun_chunks#7",
        "src_path": "spacy/lang/sv/syntax_iterators.py",
        "class_name": "spacy.lang.sv.syntax_iterators",
        "signature": "spacy.lang.sv.syntax_iterators.noun_chunks(obj)",
        "snippet": "def noun_chunks(obj):\n    \"\"\"\n    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n    \"\"\"\n    labels = [\n        \"nsubj\",\n        \"nsubj:pass\",\n        \"dobj\",\n        \"obj\",\n        \"iobj\",\n        \"ROOT\",\n        \"appos\",\n        \"nmod\",\n        \"nmod:poss\",\n    ]\n    doc = obj.doc  # Ensure works on both Doc and Span.\n    np_deps = [doc.vocab.strings[label] for label in labels]\n    conj = doc.vocab.strings.add(\"conj\")\n    np_label = doc.vocab.strings.add(\"NP\")\n    seen = set()\n    for i, word in enumerate(obj):\n        if word.pos not in (NOUN, PROPN, PRON):\n            continue\n        # Prevent nested chunks from being produced\n        if word.i in seen:\n            continue\n        if word.dep in np_deps:\n            if any(w.i in seen for w in word.subtree):\n                continue\n            seen.update(j for j in range(word.left_edge.i, word.right_edge.i + 1))\n            yield word.left_edge.i, word.right_edge.i + 1, np_label\n        elif word.dep == conj:\n            head = word.head\n            while head.dep == conj and head.head.i < head.i:\n                head = head.head\n            # If the head is an NP, and we're coordinated to it, we're an NP\n            if head.dep in np_deps:\n                if any(w.i in seen for w in word.subtree):\n                    continue\n                seen.update(j for j in range(word.left_edge.i, word.right_edge.i + 1))\n                yield word.left_edge.i, word.right_edge.i + 1, np_label",
        "begin_line": 7,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.en.syntax_iterators.noun_chunks#7",
        "src_path": "spacy/lang/en/syntax_iterators.py",
        "class_name": "spacy.lang.en.syntax_iterators",
        "signature": "spacy.lang.en.syntax_iterators.noun_chunks(obj)",
        "snippet": "def noun_chunks(obj):\n    \"\"\"\n    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n    \"\"\"\n    labels = [\n        \"nsubj\",\n        \"dobj\",\n        \"nsubjpass\",\n        \"pcomp\",\n        \"pobj\",\n        \"dative\",\n        \"appos\",\n        \"attr\",\n        \"ROOT\",\n    ]\n    doc = obj.doc  # Ensure works on both Doc and Span.\n    np_deps = [doc.vocab.strings.add(label) for label in labels]\n    conj = doc.vocab.strings.add(\"conj\")\n    np_label = doc.vocab.strings.add(\"NP\")\n    seen = set()\n    for i, word in enumerate(obj):\n        if word.pos not in (NOUN, PROPN, PRON):\n            continue\n        # Prevent nested chunks from being produced\n        if word.i in seen:\n            continue\n        if word.dep in np_deps:\n            if any(w.i in seen for w in word.subtree):\n                continue\n            seen.update(j for j in range(word.left_edge.i, word.i + 1))\n            yield word.left_edge.i, word.i + 1, np_label\n        elif word.dep == conj:\n            head = word.head\n            while head.dep == conj and head.head.i < head.i:\n                head = head.head\n            # If the head is an NP, and we're coordinated to it, we're an NP\n            if head.dep in np_deps:\n                if any(w.i in seen for w in word.subtree):\n                    continue\n                seen.update(j for j in range(word.left_edge.i, word.i + 1))\n                yield word.left_edge.i, word.i + 1, np_label",
        "begin_line": 7,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.fa.lex_attrs.like_num#84",
        "src_path": "spacy/lang/fa/lex_attrs.py",
        "class_name": "spacy.lang.fa.lex_attrs",
        "signature": "spacy.lang.fa.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    \"\"\"\n    check if text resembles a number\n    \"\"\"\n    text = (\n        text.replace(\",\", \"\")\n        .replace(\".\", \"\")\n        .replace(\"\u060c\", \"\")\n        .replace(\"\u066b\", \"\")\n        .replace(\"/\", \"\")\n    )\n    if text.isdigit():\n        return True\n    if text in _num_words:\n        return True\n    if text in _ordinal_words:\n        return True\n    return False",
        "begin_line": 84,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.de.syntax_iterators.noun_chunks#7",
        "src_path": "spacy/lang/de/syntax_iterators.py",
        "class_name": "spacy.lang.de.syntax_iterators",
        "signature": "spacy.lang.de.syntax_iterators.noun_chunks(obj)",
        "snippet": "def noun_chunks(obj):\n    \"\"\"\n    Detect base noun phrases from a dependency parse. Works on both Doc and Span.\n    \"\"\"\n    # this iterator extracts spans headed by NOUNs starting from the left-most\n    # syntactic dependent until the NOUN itself for close apposition and\n    # measurement construction, the span is sometimes extended to the right of\n    # the NOUN. Example: \"eine Tasse Tee\" (a cup (of) tea) returns \"eine Tasse Tee\"\n    # and not just \"eine Tasse\", same for \"das Thema Familie\".\n    labels = [\n        \"sb\",\n        \"oa\",\n        \"da\",\n        \"nk\",\n        \"mo\",\n        \"ag\",\n        \"ROOT\",\n        \"root\",\n        \"cj\",\n        \"pd\",\n        \"og\",\n        \"app\",\n    ]\n    doc = obj.doc  # Ensure works on both Doc and Span.\n    np_label = doc.vocab.strings.add(\"NP\")\n    np_deps = set(doc.vocab.strings.add(label) for label in labels)\n    close_app = doc.vocab.strings.add(\"nk\")\n\n    rbracket = 0\n    for i, word in enumerate(obj):\n        if i < rbracket:\n            continue\n        if word.pos in (NOUN, PROPN, PRON) and word.dep in np_deps:\n            rbracket = word.i + 1\n            # try to extend the span to the right\n            # to capture close apposition/measurement constructions\n            for rdep in doc[word.i].rights:\n                if rdep.pos in (NOUN, PROPN) and rdep.dep == close_app:\n                    rbracket = rdep.i + 1\n            yield word.left_edge.i, rbracket, np_label",
        "begin_line": 7,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.lang_class_is_loaded#55",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.lang_class_is_loaded(lang)",
        "snippet": "def lang_class_is_loaded(lang):\n    \"\"\"Check whether a Language class is already loaded. Language classes are\n    loaded lazily, to avoid expensive setup code associated with the language\n    data.\n\n    lang (unicode): Two-letter language code, e.g. 'en'.\n    RETURNS (bool): Whether a Language class has been loaded.\n    \"\"\"\n    return lang in registry.languages",
        "begin_line": 55,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5442152140325365e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_lang_class#66",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_lang_class(lang)",
        "snippet": "def get_lang_class(lang):\n    \"\"\"Import and load a Language class.\n\n    lang (unicode): Two-letter language code, e.g. 'en'.\n    RETURNS (Language): Language class.\n    \"\"\"\n    # Check if language is registered / entry point is available\n    if lang in registry.languages:\n        return registry.languages.get(lang)\n    else:\n        try:\n            module = importlib.import_module(\".lang.%s\" % lang, \"spacy\")\n        except ImportError as err:\n            raise ImportError(Errors.E048.format(lang=lang, err=err))\n        set_lang_class(lang, getattr(module, module.__all__[0]))\n    return registry.languages.get(lang)",
        "begin_line": 66,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0025252525252525255,
            "pseudo_dstar_susp": 0.0025252525252525255,
            "pseudo_tarantula_susp": 0.0025252525252525255,
            "pseudo_op2_susp": 0.0025252525252525255,
            "pseudo_barinel_susp": 0.0025252525252525255
        }
    },
    {
        "name": "spacy.util.set_lang_class#84",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.set_lang_class(name, cls)",
        "snippet": "def set_lang_class(name, cls):\n    \"\"\"Set a custom Language class name that can be loaded via get_lang_class.\n\n    name (unicode): Name of Language class.\n    cls (Language): Language class.\n    \"\"\"\n    registry.languages.register(name, func=cls)",
        "begin_line": 84,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.439117503440316e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_data_path#93",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_data_path(require_exists=True)",
        "snippet": "def get_data_path(require_exists=True):\n    \"\"\"Get path to spaCy data directory.\n\n    require_exists (bool): Only return path if it exists, otherwise None.\n    RETURNS (Path or None): Data path or None.\n    \"\"\"\n    if not require_exists:\n        return _data_path\n    else:\n        return _data_path if _data_path.exists() else None",
        "begin_line": 93,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.034482758620689655,
            "pseudo_dstar_susp": 0.027777777777777776,
            "pseudo_tarantula_susp": 0.043478260869565216,
            "pseudo_op2_susp": 0.027777777777777776,
            "pseudo_barinel_susp": 0.043478260869565216
        }
    },
    {
        "name": "spacy.util.ensure_path#119",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.ensure_path(path)",
        "snippet": "def ensure_path(path):\n    \"\"\"Ensure string is converted to a Path.\n\n    path: Anything. If string, it's converted to Path.\n    RETURNS: Path or original argument.\n    \"\"\"\n    if isinstance(path, basestring_):\n        return Path(path)\n    else:\n        return path",
        "begin_line": 119,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006944444444444444,
            "pseudo_dstar_susp": 0.006666666666666667,
            "pseudo_tarantula_susp": 0.006993006993006993,
            "pseudo_op2_susp": 0.006666666666666667,
            "pseudo_barinel_susp": 0.006993006993006993
        }
    },
    {
        "name": "spacy.util.get_module_path#147",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_module_path(module)",
        "snippet": "def get_module_path(module):\n    if not hasattr(module, \"__module__\"):\n        raise ValueError(Errors.E169.format(module=repr(module)))\n    return Path(sys.modules[module.__module__].__file__).parent",
        "begin_line": 147,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003861003861003861,
            "pseudo_dstar_susp": 0.003861003861003861,
            "pseudo_tarantula_susp": 0.00392156862745098,
            "pseudo_op2_susp": 0.003861003861003861,
            "pseudo_barinel_susp": 0.00392156862745098
        }
    },
    {
        "name": "spacy.util.load_model#153",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.load_model(name, **overrides)",
        "snippet": "def load_model(name, **overrides):\n    \"\"\"Load a model from a shortcut link, package or data path.\n\n    name (unicode): Package name, shortcut link or model path.\n    **overrides: Specific overrides, like pipeline components to disable.\n    RETURNS (Language): `Language` class with the loaded model.\n    \"\"\"\n    data_path = get_data_path()\n    if not data_path or not data_path.exists():\n        raise IOError(Errors.E049.format(path=path2str(data_path)))\n    if isinstance(name, basestring_):  # in data dir / shortcut\n        if name in set([d.name for d in data_path.iterdir()]):\n            return load_model_from_link(name, **overrides)\n        if is_package(name):  # installed as package\n            return load_model_from_package(name, **overrides)\n        if Path(name).exists():  # path to model data directory\n            return load_model_from_path(Path(name), **overrides)\n    elif hasattr(name, \"exists\"):  # Path or Path-like to model data\n        return load_model_from_path(name, **overrides)\n    raise IOError(Errors.E050.format(name=name))",
        "begin_line": 153,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.034482758620689655,
            "pseudo_dstar_susp": 0.027777777777777776,
            "pseudo_tarantula_susp": 0.043478260869565216,
            "pseudo_op2_susp": 0.027777777777777776,
            "pseudo_barinel_susp": 0.043478260869565216
        }
    },
    {
        "name": "spacy.util.load_model_from_path#191",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.load_model_from_path(model_path, meta=False, **overrides)",
        "snippet": "def load_model_from_path(model_path, meta=False, **overrides):\n    \"\"\"Load a model from a data directory path. Creates Language class with\n    pipeline from meta.json and then calls from_disk() with path.\"\"\"\n    if not meta:\n        meta = get_model_meta(model_path)\n    # Support language factories registered via entry points (e.g. custom\n    # language subclass) while keeping top-level language identifier \"lang\"\n    lang = meta.get(\"lang_factory\", meta[\"lang\"])\n    cls = get_lang_class(lang)\n    nlp = cls(meta=meta, **overrides)\n    pipeline = meta.get(\"pipeline\", [])\n    factories = meta.get(\"factories\", {})\n    disable = overrides.get(\"disable\", [])\n    if pipeline is True:\n        pipeline = nlp.Defaults.pipe_names\n    elif pipeline in (False, None):\n        pipeline = []\n    for name in pipeline:\n        if name not in disable:\n            config = meta.get(\"pipeline_args\", {}).get(name, {})\n            factory = factories.get(name, name)\n            component = nlp.create_pipe(factory, config=config)\n            nlp.add_pipe(component, name=name)\n    return nlp.from_disk(model_path, exclude=disable)",
        "begin_line": 191,
        "end_line": 214,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.06666666666666667,
            "pseudo_dstar_susp": 0.043478260869565216,
            "pseudo_tarantula_susp": 0.1111111111111111,
            "pseudo_op2_susp": 0.043478260869565216,
            "pseudo_barinel_susp": 0.1111111111111111
        }
    },
    {
        "name": "spacy.util.get_model_meta#234",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_model_meta(path)",
        "snippet": "def get_model_meta(path):\n    \"\"\"Get model meta.json from a directory path and validate its contents.\n\n    path (unicode or Path): Path to model directory.\n    RETURNS (dict): The model's meta data.\n    \"\"\"\n    model_path = ensure_path(path)\n    if not model_path.exists():\n        raise IOError(Errors.E052.format(path=path2str(model_path)))\n    meta_path = model_path / \"meta.json\"\n    if not meta_path.is_file():\n        raise IOError(Errors.E053.format(path=meta_path))\n    meta = srsly.read_json(meta_path)\n    for setting in [\"lang\", \"name\", \"version\"]:\n        if setting not in meta or not meta[setting]:\n            raise ValueError(Errors.E054.format(setting=setting))\n    return meta",
        "begin_line": 234,
        "end_line": 250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.022222222222222223,
            "pseudo_dstar_susp": 0.019230769230769232,
            "pseudo_tarantula_susp": 0.02564102564102564,
            "pseudo_op2_susp": 0.019230769230769232,
            "pseudo_barinel_susp": 0.02564102564102564
        }
    },
    {
        "name": "spacy.util.is_package#253",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.is_package(name)",
        "snippet": "def is_package(name):\n    \"\"\"Check if string maps to a package installed via pip.\n\n    name (unicode): Name of package.\n    RETURNS (bool): True if installed package, False if not.\n    \"\"\"\n    import pkg_resources\n\n    name = name.lower()  # compare package name against lowercase name\n    packages = pkg_resources.working_set.by_key.keys()\n    for package in packages:\n        if package.lower().replace(\"-\", \"_\") == name:\n            return True\n    return False",
        "begin_line": 253,
        "end_line": 266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_package_path#269",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_package_path(name)",
        "snippet": "def get_package_path(name):\n    \"\"\"Get the path to an installed package.\n\n    name (unicode): Package name.\n    RETURNS (Path): Path to installed package.\n    \"\"\"\n    name = name.lower()  # use lowercase version to be safe\n    # Here we're importing the module just to find it. This is worryingly\n    # indirect, but it's otherwise very difficult to find the package.\n    pkg = importlib.import_module(name)\n    return Path(pkg.__file__).parent",
        "begin_line": 269,
        "end_line": 279,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.is_in_jupyter#282",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.is_in_jupyter()",
        "snippet": "def is_in_jupyter():\n    \"\"\"Check if user is running spaCy from a Jupyter notebook by detecting the\n    IPython kernel. Mainly used for the displaCy visualizer.\n    RETURNS (bool): True if in Jupyter, False if not.\n    \"\"\"\n    # https://stackoverflow.com/a/39662359/6400719\n    try:\n        shell = get_ipython().__class__.__name__\n        if shell == \"ZMQInteractiveShell\":\n            return True  # Jupyter notebook or qtconsole\n    except NameError:\n        return False  # Probably standard Python interpreter\n    return False",
        "begin_line": 282,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_component_name#297",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_component_name(component)",
        "snippet": "def get_component_name(component):\n    if hasattr(component, \"name\"):\n        return component.name\n    if hasattr(component, \"__name__\"):\n        return component.__name__\n    if hasattr(component, \"__class__\") and hasattr(component.__class__, \"__name__\"):\n        return component.__class__.__name__\n    return repr(component)",
        "begin_line": 297,
        "end_line": 304,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005917159763313609,
            "pseudo_dstar_susp": 0.005780346820809248,
            "pseudo_tarantula_susp": 0.005988023952095809,
            "pseudo_op2_susp": 0.005780346820809248,
            "pseudo_barinel_susp": 0.005988023952095809
        }
    },
    {
        "name": "spacy.util.get_cuda_stream#307",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_cuda_stream(require=False, non_blocking=True)",
        "snippet": "def get_cuda_stream(require=False, non_blocking=True):\n    if CudaStream is None:\n        return None\n    elif isinstance(Model.ops, NumpyOps):\n        return None\n    else:\n        return CudaStream(non_blocking=non_blocking)",
        "begin_line": 307,
        "end_line": 313,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.481491440351349e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.env_opt#325",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.env_opt(name, default=None)",
        "snippet": "def env_opt(name, default=None):\n    if type(default) is float:\n        type_convert = float\n    else:\n        type_convert = int\n    if \"SPACY_\" + name.upper() in os.environ:\n        value = type_convert(os.environ[\"SPACY_\" + name.upper()])\n        if _PRINT_ENV:\n            print(name, \"=\", repr(value), \"via\", \"$SPACY_\" + name.upper())\n        return value\n    elif name in os.environ:\n        value = type_convert(os.environ[name])\n        if _PRINT_ENV:\n            print(name, \"=\", repr(value), \"via\", \"$\" + name)\n        return value\n    else:\n        if _PRINT_ENV:\n            print(name, \"=\", repr(default), \"by default\")\n        return default",
        "begin_line": 325,
        "end_line": 343,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.430071324148319e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.compile_prefix_regex#356",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.compile_prefix_regex(entries)",
        "snippet": "def compile_prefix_regex(entries):\n    \"\"\"Compile a sequence of prefix rules into a regex object.\n\n    entries (tuple): The prefix rules, e.g. spacy.lang.punctuation.TOKENIZER_PREFIXES.\n    RETURNS (regex object): The regex object. to be used for Tokenizer.prefix_search.\n    \"\"\"\n    if \"(\" in entries:\n        # Handle deprecated data\n        expression = \"|\".join(\n            [\"^\" + re.escape(piece) for piece in entries if piece.strip()]\n        )\n        return re.compile(expression)\n    else:\n        expression = \"|\".join([\"^\" + piece for piece in entries if piece.strip()])\n        return re.compile(expression)",
        "begin_line": 356,
        "end_line": 370,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0048543689320388345,
            "pseudo_dstar_susp": 0.06666666666666667,
            "pseudo_tarantula_susp": 0.0034129692832764505,
            "pseudo_op2_susp": 0.06666666666666667,
            "pseudo_barinel_susp": 0.0034129692832764505
        }
    },
    {
        "name": "spacy.util.compile_suffix_regex#373",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.compile_suffix_regex(entries)",
        "snippet": "def compile_suffix_regex(entries):\n    \"\"\"Compile a sequence of suffix rules into a regex object.\n\n    entries (tuple): The suffix rules, e.g. spacy.lang.punctuation.TOKENIZER_SUFFIXES.\n    RETURNS (regex object): The regex object. to be used for Tokenizer.suffix_search.\n    \"\"\"\n    expression = \"|\".join([piece + \"$\" for piece in entries if piece.strip()])\n    return re.compile(expression)",
        "begin_line": 373,
        "end_line": 380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004901960784313725,
            "pseudo_dstar_susp": 0.07692307692307693,
            "pseudo_tarantula_susp": 0.003472222222222222,
            "pseudo_op2_susp": 0.07692307692307693,
            "pseudo_barinel_susp": 0.003472222222222222
        }
    },
    {
        "name": "spacy.util.compile_infix_regex#383",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.compile_infix_regex(entries)",
        "snippet": "def compile_infix_regex(entries):\n    \"\"\"Compile a sequence of infix rules into a regex object.\n\n    entries (tuple): The infix rules, e.g. spacy.lang.punctuation.TOKENIZER_INFIXES.\n    RETURNS (regex object): The regex object. to be used for Tokenizer.infix_finditer.\n    \"\"\"\n    expression = \"|\".join([piece for piece in entries if piece.strip()])\n    return re.compile(expression)",
        "begin_line": 383,
        "end_line": 390,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004901960784313725,
            "pseudo_dstar_susp": 0.07692307692307693,
            "pseudo_tarantula_susp": 0.003472222222222222,
            "pseudo_op2_susp": 0.07692307692307693,
            "pseudo_barinel_susp": 0.003472222222222222
        }
    },
    {
        "name": "spacy.util.add_lookups#393",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.add_lookups(default_func, *lookups)",
        "snippet": "def add_lookups(default_func, *lookups):\n    \"\"\"Extend an attribute function with special cases. If a word is in the\n    lookups, the value is returned. Otherwise the previous function is used.\n\n    default_func (callable): The default function to execute.\n    *lookups (dict): Lookup dictionary mapping string to attribute value.\n    RETURNS (callable): Lexical attribute getter.\n    \"\"\"\n    # This is implemented as functools.partial instead of a closure, to allow\n    # pickle to work.\n    return functools.partial(_get_attr_unless_lookup, default_func, lookups)",
        "begin_line": 393,
        "end_line": 403,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.523454109558059e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util._get_attr_unless_lookup#406",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util._get_attr_unless_lookup(default_func, lookups, string)",
        "snippet": "def _get_attr_unless_lookup(default_func, lookups, string):\n    for lookup in lookups:\n        if string in lookup:\n            return lookup[string]\n    return default_func(string)",
        "begin_line": 406,
        "end_line": 410,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004424778761061947,
            "pseudo_dstar_susp": 0.004424778761061947,
            "pseudo_tarantula_susp": 0.0045045045045045045,
            "pseudo_op2_susp": 0.004424778761061947,
            "pseudo_barinel_susp": 0.0045045045045045045
        }
    },
    {
        "name": "spacy.util.update_exc#413",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.update_exc(base_exceptions, *addition_dicts)",
        "snippet": "def update_exc(base_exceptions, *addition_dicts):\n    \"\"\"Update and validate tokenizer exceptions. Will overwrite exceptions.\n\n    base_exceptions (dict): Base exceptions.\n    *addition_dicts (dict): Exceptions to add to the base dict, in order.\n    RETURNS (dict): Combined tokenizer exceptions.\n    \"\"\"\n    exc = dict(base_exceptions)\n    for additions in addition_dicts:\n        for orth, token_attrs in additions.items():\n            if not all(isinstance(attr[ORTH], unicode_) for attr in token_attrs):\n                raise ValueError(Errors.E055.format(key=orth, orths=token_attrs))\n            described_orth = \"\".join(attr[ORTH] for attr in token_attrs)\n            if orth != described_orth:\n                raise ValueError(Errors.E056.format(key=orth, orths=described_orth))\n        exc.update(additions)\n    exc = expand_exc(exc, \"'\", \"\u2019\")\n    return exc",
        "begin_line": 413,
        "end_line": 430,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.513857542655954e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.expand_exc#433",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.expand_exc(excs, search, replace)",
        "snippet": "def expand_exc(excs, search, replace):\n    \"\"\"Find string in tokenizer exceptions, duplicate entry and replace string.\n    For example, to add additional versions with typographic apostrophes.\n\n    excs (dict): Tokenizer exceptions.\n    search (unicode): String to find and replace.\n    replace (unicode): Replacement.\n    RETURNS (dict): Combined tokenizer exceptions.\n    \"\"\"\n\n    def _fix_token(token, search, replace):\n        fixed = dict(token)\n        fixed[ORTH] = fixed[ORTH].replace(search, replace)\n        return fixed\n\n    new_excs = dict(excs)\n    for token_string, tokens in excs.items():\n        if search in token_string:\n            new_key = token_string.replace(search, replace)\n            new_value = [_fix_token(t, search, replace) for t in tokens]\n            new_excs[new_key] = new_value\n    return new_excs",
        "begin_line": 433,
        "end_line": 454,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.504301608035674e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util._fix_token#443",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util._fix_token(token, search, replace)",
        "snippet": "    def _fix_token(token, search, replace):\n        fixed = dict(token)\n        fixed[ORTH] = fixed[ORTH].replace(search, replace)\n        return fixed",
        "begin_line": 443,
        "end_line": 446,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.504301608035674e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.normalize_slice#457",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.normalize_slice(length, start, stop, step=None)",
        "snippet": "def normalize_slice(length, start, stop, step=None):\n    if not (step is None or step == 1):\n        raise ValueError(Errors.E057)\n    if start is None:\n        start = 0\n    elif start < 0:\n        start += length\n    start = min(length, max(0, start))\n    if stop is None:\n        stop = length\n    elif stop < 0:\n        stop += length\n    stop = min(length, max(start, stop))\n    return start, stop",
        "begin_line": 457,
        "end_line": 470,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.minibatch#473",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.minibatch(items, size=8)",
        "snippet": "def minibatch(items, size=8):\n    \"\"\"Iterate over batches of items. `size` may be an iterator,\n    so that batch-size can vary on each step.\n    \"\"\"\n    if isinstance(size, int):\n        size_ = itertools.repeat(size)\n    else:\n        size_ = size\n    items = iter(items)\n    while True:\n        batch_size = next(size_)\n        batch = list(itertools.islice(items, int(batch_size)))\n        if len(batch) == 0:\n            break\n        yield list(batch)",
        "begin_line": 473,
        "end_line": 487,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.compounding#490",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.compounding(start, stop, compound)",
        "snippet": "def compounding(start, stop, compound):\n    \"\"\"Yield an infinite series of compounding values. Each time the\n    generator is called, a value is produced by multiplying the previous\n    value by the compound rate.\n\n    EXAMPLE:\n      >>> sizes = compounding(1., 10., 1.5)\n      >>> assert next(sizes) == 1.\n      >>> assert next(sizes) == 1 * 1.5\n      >>> assert next(sizes) == 1.5 * 1.5\n    \"\"\"\n\n    def clip(value):\n        return max(value, stop) if (start > stop) else min(value, stop)\n\n    curr = float(start)\n    while True:\n        yield clip(curr)\n        curr *= compound",
        "begin_line": 490,
        "end_line": 508,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.clip#502",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.clip(value)",
        "snippet": "    def clip(value):\n        return max(value, stop) if (start > stop) else min(value, stop)",
        "begin_line": 502,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.decaying#534",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.decaying(start, stop, decay)",
        "snippet": "def decaying(start, stop, decay):\n    \"\"\"Yield an infinite series of linearly decaying values.\"\"\"\n\n    curr = float(start)\n    while True:\n        yield max(curr, stop)\n        curr -= decay",
        "begin_line": 534,
        "end_line": 540,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.filter_spans#601",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.filter_spans(spans)",
        "snippet": "def filter_spans(spans):\n    \"\"\"Filter a sequence of spans and remove duplicates or overlaps. Useful for\n    creating named entities (where one token can only be part of one entity) or\n    when merging spans with `Retokenizer.merge`. When spans overlap, the (first)\n    longest span is preferred over shorter spans.\n\n    spans (iterable): The spans to filter.\n    RETURNS (list): The filtered spans.\n    \"\"\"\n    get_sort_key = lambda span: (span.end - span.start, -span.start)\n    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n    result = []\n    seen_tokens = set()\n    for span in sorted_spans:\n        # Check for end - 1 here because boundaries are inclusive\n        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n            result.append(span)\n        seen_tokens.update(range(span.start, span.end))\n    result = sorted(result, key=lambda span: span.start)\n    return result",
        "begin_line": 601,
        "end_line": 620,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.to_bytes#623",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.to_bytes(getters, exclude)",
        "snippet": "def to_bytes(getters, exclude):\n    serialized = OrderedDict()\n    for key, getter in getters.items():\n        # Split to support file names like meta.json\n        if key.split(\".\")[0] not in exclude:\n            serialized[key] = getter()\n    return srsly.msgpack_dumps(serialized)",
        "begin_line": 623,
        "end_line": 629,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006172839506172839,
            "pseudo_dstar_susp": 0.006024096385542169,
            "pseudo_tarantula_susp": 0.00625,
            "pseudo_op2_susp": 0.006024096385542169,
            "pseudo_barinel_susp": 0.00625
        }
    },
    {
        "name": "spacy.util.from_bytes#632",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.from_bytes(bytes_data, setters, exclude)",
        "snippet": "def from_bytes(bytes_data, setters, exclude):\n    msg = srsly.msgpack_loads(bytes_data)\n    for key, setter in setters.items():\n        # Split to support file names like meta.json\n        if key.split(\".\")[0] not in exclude and key in msg:\n            setter(msg[key])\n    return msg",
        "begin_line": 632,
        "end_line": 638,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005917159763313609,
            "pseudo_dstar_susp": 0.005780346820809248,
            "pseudo_tarantula_susp": 0.005988023952095809,
            "pseudo_op2_susp": 0.005780346820809248,
            "pseudo_barinel_susp": 0.005988023952095809
        }
    },
    {
        "name": "spacy.util.to_disk#641",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.to_disk(path, writers, exclude)",
        "snippet": "def to_disk(path, writers, exclude):\n    path = ensure_path(path)\n    if not path.exists():\n        path.mkdir()\n    for key, writer in writers.items():\n        # Split to support file names like meta.json\n        if key.split(\".\")[0] not in exclude:\n            writer(path / key)\n    return path",
        "begin_line": 641,
        "end_line": 649,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007142857142857143,
            "pseudo_dstar_susp": 0.00684931506849315,
            "pseudo_tarantula_susp": 0.007194244604316547,
            "pseudo_op2_susp": 0.00684931506849315,
            "pseudo_barinel_susp": 0.007194244604316547
        }
    },
    {
        "name": "spacy.util.from_disk#652",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.from_disk(path, readers, exclude)",
        "snippet": "def from_disk(path, readers, exclude):\n    path = ensure_path(path)\n    for key, reader in readers.items():\n        # Split to support file names like meta.json\n        if key.split(\".\")[0] not in exclude:\n            reader(path / key)\n    return path",
        "begin_line": 652,
        "end_line": 658,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007518796992481203,
            "pseudo_dstar_susp": 0.007194244604316547,
            "pseudo_tarantula_susp": 0.007575757575757576,
            "pseudo_op2_susp": 0.007194244604316547,
            "pseudo_barinel_susp": 0.007575757575757576
        }
    },
    {
        "name": "spacy.util.escape_html#672",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.escape_html(text)",
        "snippet": "def escape_html(text):\n    \"\"\"Replace <, >, &, \" with their HTML encoded representation. Intended to\n    prevent HTML errors in rendered displaCy markup.\n\n    text (unicode): The original text.\n    RETURNS (unicode): Equivalent text to be safely used within HTML.\n    \"\"\"\n    text = text.replace(\"&\", \"&amp;\")\n    text = text.replace(\"<\", \"&lt;\")\n    text = text.replace(\">\", \"&gt;\")\n    text = text.replace('\"', \"&quot;\")\n    return text",
        "begin_line": 672,
        "end_line": 683,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.575402635431918e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.fix_random_seed#700",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.fix_random_seed(seed=0)",
        "snippet": "def fix_random_seed(seed=0):\n    random.seed(seed)\n    numpy.random.seed(seed)\n    if cupy is not None:\n        cupy.random.seed(seed)",
        "begin_line": 700,
        "end_line": 704,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_json_validator#707",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_json_validator(schema)",
        "snippet": "def get_json_validator(schema):\n    # We're using a helper function here to make it easier to change the\n    # validator that's used (e.g. different draft implementation), without\n    # having to change it all across the codebase.\n    # TODO: replace with (stable) Draft6Validator, if available\n    if jsonschema is None:\n        raise ValueError(Errors.E136)\n    return jsonschema.Draft4Validator(schema)",
        "begin_line": 707,
        "end_line": 714,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.4632894443204644e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.validate_schema#717",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.validate_schema(schema)",
        "snippet": "def validate_schema(schema):\n    \"\"\"Validate a given schema. This just checks if the schema itself is valid.\"\"\"\n    validator = get_json_validator(schema)\n    validator.check_schema(schema)",
        "begin_line": 717,
        "end_line": 720,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.validate_json#723",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.validate_json(data, validator)",
        "snippet": "def validate_json(data, validator):\n    \"\"\"Validate data against a given JSON schema (see https://json-schema.org).\n\n    data: JSON-serializable data to validate.\n    validator (jsonschema.DraftXValidator): The validator.\n    RETURNS (list): A list of error messages, if available.\n    \"\"\"\n    errors = []\n    for err in sorted(validator.iter_errors(data), key=lambda e: e.path):\n        if err.path:\n            err_path = \"[{}]\".format(\" -> \".join([str(p) for p in err.path]))\n        else:\n            err_path = \"\"\n        msg = err.message + \" \" + err_path\n        if err.context:  # Error has suberrors, e.g. if schema uses anyOf\n            suberrs = [\"  - {}\".format(suberr.message) for suberr in err.context]\n            msg += \":\\n{}\".format(\"\".join(suberrs))\n        errors.append(msg)\n    return errors",
        "begin_line": 723,
        "end_line": 741,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.util.get_serialization_exclude#744",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_serialization_exclude(serializers, exclude, kwargs)",
        "snippet": "def get_serialization_exclude(serializers, exclude, kwargs):\n    \"\"\"Helper function to validate serialization args and manage transition from\n    keyword arguments (pre v2.1) to exclude argument.\n    \"\"\"\n    exclude = list(exclude)\n    # Split to support file names like meta.json\n    options = [name.split(\".\")[0] for name in serializers]\n    for key, value in kwargs.items():\n        if key in (\"vocab\",) and value is False:\n            warnings.warn(Warnings.W015.format(arg=key), DeprecationWarning)\n            exclude.append(key)\n        elif key.split(\".\")[0] in options:\n            raise ValueError(Errors.E128.format(arg=key))\n        # TODO: user warning?\n    return exclude",
        "begin_line": 744,
        "end_line": 758,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007692307692307693,
            "pseudo_dstar_susp": 0.1111111111111111,
            "pseudo_tarantula_susp": 0.006622516556291391,
            "pseudo_op2_susp": 0.1111111111111111,
            "pseudo_barinel_susp": 0.006622516556291391
        }
    },
    {
        "name": "spacy.util.get_words_and_spaces#761",
        "src_path": "spacy/util.py",
        "class_name": "spacy.util",
        "signature": "spacy.util.get_words_and_spaces(words, text)",
        "snippet": "def get_words_and_spaces(words, text):\n    if \"\".join(\"\".join(words).split()) != \"\".join(text.split()):\n        raise ValueError(Errors.E194.format(text=text, words=words))\n    text_words = []\n    text_spaces = []\n    text_pos = 0\n    # normalize words to remove all whitespace tokens\n    norm_words = [word for word in words if not word.isspace()]\n    # align words with text\n    for word in norm_words:\n        try:\n            word_start = text[text_pos:].index(word)\n        except ValueError:\n            raise ValueError(Errors.E194.format(text=text, words=words))\n        if word_start > 0:\n            text_words.append(text[text_pos:text_pos+word_start])\n            text_spaces.append(False)\n            text_pos += word_start\n        text_words.append(word)\n        text_spaces.append(False)\n        text_pos += len(word)\n        if text_pos < len(text) and text[text_pos] == \" \":\n            text_spaces[-1] = True\n            text_pos += 1\n    if text_pos < len(text):\n        text_words.append(text[text_pos:])\n        text_spaces.append(False)\n    return (text_words, text_spaces)",
        "begin_line": 761,
        "end_line": 788,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.PRFScore.__init__#15",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.PRFScore",
        "signature": "spacy.scorer.PRFScore.__init__(self)",
        "snippet": "    def __init__(self):\n        self.tp = 0\n        self.fp = 0\n        self.fn = 0",
        "begin_line": 15,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.PRFScore.score_set#20",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.PRFScore",
        "signature": "spacy.scorer.PRFScore.score_set(self, cand, gold)",
        "snippet": "    def score_set(self, cand, gold):\n        self.tp += len(cand.intersection(gold))\n        self.fp += len(cand - gold)\n        self.fn += len(gold - cand)",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.PRFScore.precision#26",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.PRFScore",
        "signature": "spacy.scorer.PRFScore.precision(self)",
        "snippet": "    def precision(self):\n        return self.tp / (self.tp + self.fp + 1e-100)",
        "begin_line": 26,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.PRFScore.recall#30",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.PRFScore",
        "signature": "spacy.scorer.PRFScore.recall(self)",
        "snippet": "    def recall(self):\n        return self.tp / (self.tp + self.fn + 1e-100)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.PRFScore.fscore#34",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.PRFScore",
        "signature": "spacy.scorer.PRFScore.fscore(self)",
        "snippet": "    def fscore(self):\n        p = self.precision\n        r = self.recall\n        return 2 * ((p * r) / (p + r + 1e-100))",
        "begin_line": 34,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.ROCAUCScore.__init__#45",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.ROCAUCScore",
        "signature": "spacy.scorer.ROCAUCScore.__init__(self)",
        "snippet": "    def __init__(self):\n        self.golds = []\n        self.cands = []\n        self.saved_score = 0.0\n        self.saved_score_at_len = 0",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.ROCAUCScore.score_set#51",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.ROCAUCScore",
        "signature": "spacy.scorer.ROCAUCScore.score_set(self, cand, gold)",
        "snippet": "    def score_set(self, cand, gold):\n        self.cands.append(cand)\n        self.golds.append(gold)",
        "begin_line": 51,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.ROCAUCScore.score#56",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.ROCAUCScore",
        "signature": "spacy.scorer.ROCAUCScore.score(self)",
        "snippet": "    def score(self):\n        if len(self.golds) == self.saved_score_at_len:\n            return self.saved_score\n        try:\n            self.saved_score = _roc_auc_score(self.golds, self.cands)\n        # catch ValueError: Only one class present in y_true.\n        # ROC AUC score is not defined in that case.\n        except ValueError:\n            self.saved_score = -float(\"inf\")\n        self.saved_score_at_len = len(self.golds)\n        return self.saved_score",
        "begin_line": 56,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.__init__#72",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.__init__(self, eval_punct=False, pipeline=None)",
        "snippet": "    def __init__(self, eval_punct=False, pipeline=None):\n        \"\"\"Initialize the Scorer.\n\n        eval_punct (bool): Evaluate the dependency attachments to and from\n            punctuation.\n        RETURNS (Scorer): The newly created object.\n\n        DOCS: https://spacy.io/api/scorer#init\n        \"\"\"\n        self.tokens = PRFScore()\n        self.sbd = PRFScore()\n        self.unlabelled = PRFScore()\n        self.labelled = PRFScore()\n        self.labelled_per_dep = dict()\n        self.tags = PRFScore()\n        self.ner = PRFScore()\n        self.ner_per_ents = dict()\n        self.eval_punct = eval_punct\n        self.textcat = None\n        self.textcat_per_cat = dict()\n        self.textcat_positive_label = None\n        self.textcat_multilabel = False\n\n        if pipeline:\n            for name, model in pipeline:\n                if name == \"textcat\":\n                    self.textcat_positive_label = model.cfg.get(\"positive_label\", None)\n                    if self.textcat_positive_label:\n                        self.textcat = PRFScore()\n                    if not model.cfg.get(\"exclusive_classes\", False):\n                        self.textcat_multilabel = True\n                        for label in model.cfg.get(\"labels\", []):\n                            self.textcat_per_cat[label] = ROCAUCScore()\n                    else:\n                        for label in model.cfg.get(\"labels\", []):\n                            self.textcat_per_cat[label] = PRFScore()",
        "begin_line": 72,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.tags_acc#110",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.tags_acc(self)",
        "snippet": "    def tags_acc(self):\n        \"\"\"RETURNS (float): Part-of-speech tag accuracy (fine grained tags,\n            i.e. `Token.tag`).\n        \"\"\"\n        return self.tags.fscore * 100",
        "begin_line": 110,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.token_acc#117",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.token_acc(self)",
        "snippet": "    def token_acc(self):\n        \"\"\"RETURNS (float): Tokenization accuracy.\"\"\"\n        return self.tokens.precision * 100",
        "begin_line": 117,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.uas#122",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.uas(self)",
        "snippet": "    def uas(self):\n        \"\"\"RETURNS (float): Unlabelled dependency score.\"\"\"\n        return self.unlabelled.fscore * 100",
        "begin_line": 122,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.las#127",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.las(self)",
        "snippet": "    def las(self):\n        \"\"\"RETURNS (float): Labelled dependency score.\"\"\"\n        return self.labelled.fscore * 100",
        "begin_line": 127,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.las_per_type#132",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.las_per_type(self)",
        "snippet": "    def las_per_type(self):\n        \"\"\"RETURNS (dict): Scores per dependency label.\n        \"\"\"\n        return {\n            k: {\"p\": v.precision * 100, \"r\": v.recall * 100, \"f\": v.fscore * 100}\n            for k, v in self.labelled_per_dep.items()\n        }",
        "begin_line": 132,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.ents_p#141",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.ents_p(self)",
        "snippet": "    def ents_p(self):\n        \"\"\"RETURNS (float): Named entity accuracy (precision).\"\"\"\n        return self.ner.precision * 100",
        "begin_line": 141,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.ents_r#146",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.ents_r(self)",
        "snippet": "    def ents_r(self):\n        \"\"\"RETURNS (float): Named entity accuracy (recall).\"\"\"\n        return self.ner.recall * 100",
        "begin_line": 146,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.ents_f#151",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.ents_f(self)",
        "snippet": "    def ents_f(self):\n        \"\"\"RETURNS (float): Named entity accuracy (F-score).\"\"\"\n        return self.ner.fscore * 100",
        "begin_line": 151,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.ents_per_type#156",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.ents_per_type(self)",
        "snippet": "    def ents_per_type(self):\n        \"\"\"RETURNS (dict): Scores per entity label.\n        \"\"\"\n        return {\n            k: {\"p\": v.precision * 100, \"r\": v.recall * 100, \"f\": v.fscore * 100}\n            for k, v in self.ner_per_ents.items()\n        }",
        "begin_line": 156,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.textcat_score#165",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.textcat_score(self)",
        "snippet": "    def textcat_score(self):\n        \"\"\"RETURNS (float): f-score on positive label for binary exclusive,\n        macro-averaged f-score for 3+ exclusive,\n        macro-averaged AUC ROC score for multilabel (-1 if undefined)\n        \"\"\"\n        if not self.textcat_multilabel:\n            # binary multiclass\n            if self.textcat_positive_label:\n                return self.textcat.fscore * 100\n            # other multiclass\n            return (\n                sum([score.fscore for label, score in self.textcat_per_cat.items()])\n                / (len(self.textcat_per_cat) + 1e-100)\n                * 100\n            )\n        # multilabel\n        return max(\n            sum([score.score for label, score in self.textcat_per_cat.items()])\n            / (len(self.textcat_per_cat) + 1e-100),\n            -1,\n        )",
        "begin_line": 165,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.textcats_per_cat#188",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.textcats_per_cat(self)",
        "snippet": "    def textcats_per_cat(self):\n        \"\"\"RETURNS (dict): Scores per textcat label.\n        \"\"\"\n        if not self.textcat_multilabel:\n            return {\n                k: {\"p\": v.precision * 100, \"r\": v.recall * 100, \"f\": v.fscore * 100}\n                for k, v in self.textcat_per_cat.items()\n            }\n        return {\n            k: {\"roc_auc_score\": max(v.score, -1)}\n            for k, v in self.textcat_per_cat.items()\n        }",
        "begin_line": 188,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.scores#202",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.scores(self)",
        "snippet": "    def scores(self):\n        \"\"\"RETURNS (dict): All scores with keys `uas`, `las`, `ents_p`,\n            `ents_r`, `ents_f`, `tags_acc`, `token_acc`, and `textcat_score`.\n        \"\"\"\n        return {\n            \"uas\": self.uas,\n            \"las\": self.las,\n            \"las_per_type\": self.las_per_type,\n            \"ents_p\": self.ents_p,\n            \"ents_r\": self.ents_r,\n            \"ents_f\": self.ents_f,\n            \"ents_per_type\": self.ents_per_type,\n            \"tags_acc\": self.tags_acc,\n            \"token_acc\": self.token_acc,\n            \"textcat_score\": self.textcat_score,\n            \"textcats_per_cat\": self.textcats_per_cat,\n        }",
        "begin_line": 202,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer.Scorer.score#220",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer.Scorer",
        "signature": "spacy.scorer.Scorer.score(self, doc, gold, verbose=False, punct_labels=('p', 'punct'))",
        "snippet": "    def score(self, doc, gold, verbose=False, punct_labels=(\"p\", \"punct\")):\n        \"\"\"Update the evaluation scores from a single Doc / GoldParse pair.\n\n        doc (Doc): The predicted annotations.\n        gold (GoldParse): The correct annotations.\n        verbose (bool): Print debugging information.\n        punct_labels (tuple): Dependency labels for punctuation. Used to\n            evaluate dependency attachments to punctuation if `eval_punct` is\n            `True`.\n\n        DOCS: https://spacy.io/api/scorer#score\n        \"\"\"\n        if len(doc) != len(gold):\n            gold = GoldParse.from_annot_tuples(\n                doc, zip(*gold.orig_annot), cats=gold.cats,\n            )\n        gold_deps = set()\n        gold_deps_per_dep = {}\n        gold_tags = set()\n        gold_ents = set(tags_to_entities([annot[-1] for annot in gold.orig_annot]))\n        for id_, word, tag, head, dep, ner in gold.orig_annot:\n            gold_tags.add((id_, tag))\n            if dep not in (None, \"\") and dep.lower() not in punct_labels:\n                gold_deps.add((id_, head, dep.lower()))\n                if dep.lower() not in self.labelled_per_dep:\n                    self.labelled_per_dep[dep.lower()] = PRFScore()\n                if dep.lower() not in gold_deps_per_dep:\n                    gold_deps_per_dep[dep.lower()] = set()\n                gold_deps_per_dep[dep.lower()].add((id_, head, dep.lower()))\n        cand_deps = set()\n        cand_deps_per_dep = {}\n        cand_tags = set()\n        for token in doc:\n            if token.orth_.isspace():\n                continue\n            gold_i = gold.cand_to_gold[token.i]\n            if gold_i is None:\n                self.tokens.fp += 1\n            else:\n                self.tokens.tp += 1\n                cand_tags.add((gold_i, token.tag_))\n            if token.dep_.lower() not in punct_labels and token.orth_.strip():\n                gold_head = gold.cand_to_gold[token.head.i]\n                # None is indistinct, so we can't just add it to the set\n                # Multiple (None, None) deps are possible\n                if gold_i is None or gold_head is None:\n                    self.unlabelled.fp += 1\n                    self.labelled.fp += 1\n                else:\n                    cand_deps.add((gold_i, gold_head, token.dep_.lower()))\n                    if token.dep_.lower() not in self.labelled_per_dep:\n                        self.labelled_per_dep[token.dep_.lower()] = PRFScore()\n                    if token.dep_.lower() not in cand_deps_per_dep:\n                        cand_deps_per_dep[token.dep_.lower()] = set()\n                    cand_deps_per_dep[token.dep_.lower()].add(\n                        (gold_i, gold_head, token.dep_.lower())\n                    )\n        if \"-\" not in [token[-1] for token in gold.orig_annot]:\n            # Find all NER labels in gold and doc\n            ent_labels = set([x[0] for x in gold_ents] + [k.label_ for k in doc.ents])\n            # Set up all labels for per type scoring and prepare gold per type\n            gold_per_ents = {ent_label: set() for ent_label in ent_labels}\n            for ent_label in ent_labels:\n                if ent_label not in self.ner_per_ents:\n                    self.ner_per_ents[ent_label] = PRFScore()\n                gold_per_ents[ent_label].update(\n                    [x for x in gold_ents if x[0] == ent_label]\n                )\n            # Find all candidate labels, for all and per type\n            cand_ents = set()\n            cand_per_ents = {ent_label: set() for ent_label in ent_labels}\n            for ent in doc.ents:\n                first = gold.cand_to_gold[ent.start]\n                last = gold.cand_to_gold[ent.end - 1]\n                if first is None or last is None:\n                    self.ner.fp += 1\n                    self.ner_per_ents[ent.label_].fp += 1\n                else:\n                    cand_ents.add((ent.label_, first, last))\n                    cand_per_ents[ent.label_].add((ent.label_, first, last))\n            # Scores per ent\n            for k, v in self.ner_per_ents.items():\n                if k in cand_per_ents:\n                    v.score_set(cand_per_ents[k], gold_per_ents[k])\n            # Score for all ents\n            self.ner.score_set(cand_ents, gold_ents)\n        self.tags.score_set(cand_tags, gold_tags)\n        self.labelled.score_set(cand_deps, gold_deps)\n        for dep in self.labelled_per_dep:\n            self.labelled_per_dep[dep].score_set(\n                cand_deps_per_dep.get(dep, set()), gold_deps_per_dep.get(dep, set())\n            )\n        self.unlabelled.score_set(\n            set(item[:2] for item in cand_deps), set(item[:2] for item in gold_deps)\n        )\n        if (\n            len(gold.cats) > 0\n            and set(self.textcat_per_cat) == set(gold.cats)\n            and set(gold.cats) == set(doc.cats)\n        ):\n            goldcat = max(gold.cats, key=gold.cats.get)\n            candcat = max(doc.cats, key=doc.cats.get)\n            if self.textcat_positive_label:\n                self.textcat.score_set(\n                    set([self.textcat_positive_label]) & set([candcat]),\n                    set([self.textcat_positive_label]) & set([goldcat]),\n                )\n            for label in self.textcat_per_cat:\n                if self.textcat_multilabel:\n                    self.textcat_per_cat[label].score_set(\n                        doc.cats[label], gold.cats[label]\n                    )\n                else:\n                    self.textcat_per_cat[label].score_set(\n                        set([label]) & set([candcat]), set([label]) & set([goldcat])\n                    )\n        elif len(self.textcat_per_cat) > 0:\n            model_labels = set(self.textcat_per_cat)\n            eval_labels = set(gold.cats)\n            raise ValueError(\n                Errors.E162.format(model_labels=model_labels, eval_labels=eval_labels)\n            )\n        if verbose:\n            gold_words = [item[1] for item in gold.orig_annot]\n            for w_id, h_id, dep in cand_deps - gold_deps:\n                print(\"F\", gold_words[w_id], dep, gold_words[h_id])\n            for w_id, h_id, dep in gold_deps - cand_deps:\n                print(\"M\", gold_words[w_id], dep, gold_words[h_id])",
        "begin_line": 220,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer._roc_auc_score#388",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer",
        "signature": "spacy.scorer._roc_auc_score(y_true, y_score)",
        "snippet": "def _roc_auc_score(y_true, y_score):\n    \"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n    from prediction scores.\n\n    Note: this implementation is restricted to the binary classification task\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples] or [n_samples, n_classes]\n        True binary labels or binary label indicators.\n        The multiclass case expects shape = [n_samples] and labels\n        with values in ``range(n_classes)``.\n\n    y_score : array, shape = [n_samples] or [n_samples, n_classes]\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers). For binary\n        y_true, y_score is supposed to be the score of the class with greater\n        label. The multiclass case expects shape = [n_samples, n_classes]\n        where the scores correspond to probability estimates.\n\n    Returns\n    -------\n    auc : float\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Receiver operating characteristic\n            <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n    .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n           Letters, 2006, 27(8):861-874.\n\n    .. [3] `Analyzing a portion of the ROC curve. McClish, 1989\n            <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n    \"\"\"\n    if len(np.unique(y_true)) != 2:\n        raise ValueError(Errors.E165)\n    fpr, tpr, _ = _roc_curve(y_true, y_score)\n    return _auc(fpr, tpr)",
        "begin_line": 388,
        "end_line": 427,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer._roc_curve#430",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer",
        "signature": "spacy.scorer._roc_curve(y_true, y_score)",
        "snippet": "def _roc_curve(y_true, y_score):\n    \"\"\"Compute Receiver operating characteristic (ROC)\n\n    Note: this implementation is restricted to the binary classification task.\n\n    Parameters\n    ----------\n\n    y_true : array, shape = [n_samples]\n        True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n        pos_label should be explicitly given.\n\n    y_score : array, shape = [n_samples]\n        Target scores, can either be probability estimates of the positive\n        class, confidence values, or non-thresholded measure of decisions\n        (as returned by \"decision_function\" on some classifiers).\n\n    Returns\n    -------\n    fpr : array, shape = [>2]\n        Increasing false positive rates such that element i is the false\n        positive rate of predictions with score >= thresholds[i].\n\n    tpr : array, shape = [>2]\n        Increasing true positive rates such that element i is the true\n        positive rate of predictions with score >= thresholds[i].\n\n    thresholds : array, shape = [n_thresholds]\n        Decreasing thresholds on the decision function used to compute\n        fpr and tpr. `thresholds[0]` represents no instances being predicted\n        and is arbitrarily set to `max(y_score) + 1`.\n\n    Notes\n    -----\n    Since the thresholds are sorted from low to high values, they\n    are reversed upon returning them to ensure they correspond to both ``fpr``\n    and ``tpr``, which are sorted in reversed order during their calculation.\n\n    References\n    ----------\n    .. [1] `Wikipedia entry for the Receiver operating characteristic\n            <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n\n    .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n           Letters, 2006, 27(8):861-874.\n    \"\"\"\n    fps, tps, thresholds = _binary_clf_curve(y_true, y_score)\n\n    # Add an extra threshold position\n    # to make sure that the curve starts at (0, 0)\n    tps = np.r_[0, tps]\n    fps = np.r_[0, fps]\n    thresholds = np.r_[thresholds[0] + 1, thresholds]\n\n    if fps[-1] <= 0:\n        fpr = np.repeat(np.nan, fps.shape)\n    else:\n        fpr = fps / fps[-1]\n\n    if tps[-1] <= 0:\n        tpr = np.repeat(np.nan, tps.shape)\n    else:\n        tpr = tps / tps[-1]\n\n    return fpr, tpr, thresholds",
        "begin_line": 430,
        "end_line": 494,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer._binary_clf_curve#497",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer",
        "signature": "spacy.scorer._binary_clf_curve(y_true, y_score)",
        "snippet": "def _binary_clf_curve(y_true, y_score):\n    \"\"\"Calculate true and false positives per binary classification threshold.\n\n    Parameters\n    ----------\n    y_true : array, shape = [n_samples]\n        True targets of binary classification\n\n    y_score : array, shape = [n_samples]\n        Estimated probabilities or decision function\n\n    Returns\n    -------\n    fps : array, shape = [n_thresholds]\n        A count of false positives, at index i being the number of negative\n        samples assigned a score >= thresholds[i]. The total number of\n        negative samples is equal to fps[-1] (thus true negatives are given by\n        fps[-1] - fps).\n\n    tps : array, shape = [n_thresholds <= len(np.unique(y_score))]\n        An increasing count of true positives, at index i being the number\n        of positive samples assigned a score >= thresholds[i]. The total\n        number of positive samples is equal to tps[-1] (thus false negatives\n        are given by tps[-1] - tps).\n\n    thresholds : array, shape = [n_thresholds]\n        Decreasing score values.\n    \"\"\"\n    pos_label = 1.0\n\n    y_true = np.ravel(y_true)\n    y_score = np.ravel(y_score)\n\n    # make y_true a boolean vector\n    y_true = y_true == pos_label\n\n    # sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n    weight = 1.0\n\n    # y_score typically has many tied values. Here we extract\n    # the indices associated with the distinct values. We also\n    # concatenate a value for the end of the curve.\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    # accumulate the true positives with decreasing threshold\n    tps = _stable_cumsum(y_true * weight)[threshold_idxs]\n    fps = 1 + threshold_idxs - tps\n    return fps, tps, y_score[threshold_idxs]",
        "begin_line": 497,
        "end_line": 548,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer._stable_cumsum#551",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer",
        "signature": "spacy.scorer._stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08)",
        "snippet": "def _stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):\n    \"\"\"Use high precision for cumsum and check that final value matches sum\n\n    Parameters\n    ----------\n    arr : array-like\n        To be cumulatively summed as flat\n    axis : int, optional\n        Axis along which the cumulative sum is computed.\n        The default (None) is to compute the cumsum over the flattened array.\n    rtol : float\n        Relative tolerance, see ``np.allclose``\n    atol : float\n        Absolute tolerance, see ``np.allclose``\n    \"\"\"\n    out = np.cumsum(arr, axis=axis, dtype=np.float64)\n    expected = np.sum(arr, axis=axis, dtype=np.float64)\n    if not np.all(\n        np.isclose(\n            out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=True\n        )\n    ):\n        raise ValueError(Errors.E163)\n    return out",
        "begin_line": 551,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.scorer._auc#577",
        "src_path": "spacy/scorer.py",
        "class_name": "spacy.scorer",
        "signature": "spacy.scorer._auc(x, y)",
        "snippet": "def _auc(x, y):\n    \"\"\"Compute Area Under the Curve (AUC) using the trapezoidal rule\n\n    This is a general function, given points on a curve.  For computing the\n    area under the ROC-curve, see :func:`roc_auc_score`.\n\n    Parameters\n    ----------\n    x : array, shape = [n]\n        x coordinates. These must be either monotonic increasing or monotonic\n        decreasing.\n    y : array, shape = [n]\n        y coordinates.\n\n    Returns\n    -------\n    auc : float\n    \"\"\"\n    x = np.ravel(x)\n    y = np.ravel(y)\n\n    direction = 1\n    dx = np.diff(x)\n    if np.any(dx < 0):\n        if np.all(dx <= 0):\n            direction = -1\n        else:\n            raise ValueError(Errors.E164.format(x))\n\n    area = direction * np.trapz(y, x)\n    if isinstance(area, np.memmap):\n        # Reductions such as .sum used internally in np.trapz do not return a\n        # scalar by default for numpy.memmap instances contrary to\n        # regular numpy.ndarray instances.\n        area = area.dtype.type(area)\n    return area",
        "begin_line": 577,
        "end_line": 612,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.iob2json.iob2json#11",
        "src_path": "spacy/cli/converters/iob2json.py",
        "class_name": "spacy.cli.converters.iob2json",
        "signature": "spacy.cli.converters.iob2json.iob2json(input_data, n_sents=10, no_print=False, *args, **kwargs)",
        "snippet": "def iob2json(input_data, n_sents=10, no_print=False, *args, **kwargs):\n    \"\"\"\n    Convert IOB files with one sentence per line and tags separated with '|'\n    into JSON format for use with train cli. IOB and IOB2 are accepted.\n\n    Sample formats:\n\n    I|O like|O London|I-GPE and|O New|B-GPE York|I-GPE City|I-GPE .|O\n    I|O like|O London|B-GPE and|O New|B-GPE York|I-GPE City|I-GPE .|O\n    I|PRP|O like|VBP|O London|NNP|I-GPE and|CC|O New|NNP|B-GPE York|NNP|I-GPE City|NNP|I-GPE .|.|O\n    I|PRP|O like|VBP|O London|NNP|B-GPE and|CC|O New|NNP|B-GPE York|NNP|I-GPE City|NNP|I-GPE .|.|O\n    \"\"\"\n    msg = Printer(no_print=no_print)\n    docs = read_iob(input_data.split(\"\\n\"))\n    if n_sents > 0:\n        n_sents_info(msg, n_sents)\n        docs = merge_sentences(docs, n_sents)\n    return docs",
        "begin_line": 11,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.iob2json.read_iob#31",
        "src_path": "spacy/cli/converters/iob2json.py",
        "class_name": "spacy.cli.converters.iob2json",
        "signature": "spacy.cli.converters.iob2json.read_iob(raw_sents)",
        "snippet": "def read_iob(raw_sents):\n    sentences = []\n    for line in raw_sents:\n        if not line.strip():\n            continue\n        tokens = [t.split(\"|\") for t in line.split()]\n        if len(tokens[0]) == 3:\n            words, pos, iob = zip(*tokens)\n        elif len(tokens[0]) == 2:\n            words, iob = zip(*tokens)\n            pos = [\"-\"] * len(words)\n        else:\n            raise ValueError(\n                \"The sentence-per-line IOB/IOB2 file is not formatted correctly. Try checking whitespace and delimiters. See https://spacy.io/api/cli#convert\"\n            )\n        biluo = iob_to_biluo(iob)\n        sentences.append(\n            [\n                {\"orth\": w, \"tag\": p, \"ner\": ent}\n                for (w, p, ent) in zip(words, pos, biluo)\n            ]\n        )\n    sentences = [{\"tokens\": sent} for sent in sentences]\n    paragraphs = [{\"sentences\": [sent]} for sent in sentences]\n    docs = [{\"id\": i, \"paragraphs\": [para]} for i, para in enumerate(paragraphs)]\n    return docs",
        "begin_line": 31,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.cli.converters.iob2json.merge_sentences#59",
        "src_path": "spacy/cli/converters/iob2json.py",
        "class_name": "spacy.cli.converters.iob2json",
        "signature": "spacy.cli.converters.iob2json.merge_sentences(docs, n_sents)",
        "snippet": "def merge_sentences(docs, n_sents):\n    merged = []\n    for group in minibatch(docs, size=n_sents):\n        group = list(group)\n        first = group.pop(0)\n        to_extend = first[\"paragraphs\"][0][\"sentences\"]\n        for sent in group:\n            to_extend.extend(sent[\"paragraphs\"][0][\"sentences\"])\n        merged.append(first)\n    return merged",
        "begin_line": 59,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.hi.lex_attrs.norm#59",
        "src_path": "spacy/lang/hi/lex_attrs.py",
        "class_name": "spacy.lang.hi.lex_attrs",
        "signature": "spacy.lang.hi.lex_attrs.norm(string)",
        "snippet": "def norm(string):\n    # normalise base exceptions,  e.g. punctuation or currency symbols\n    if string in BASE_NORMS:\n        return BASE_NORMS[string]\n    # set stem word as norm,  if available,  adapted from:\n    # http://computing.open.ac.uk/Sites/EACLSouthAsia/Papers/p6-Ramanathan.pdf\n    # http://research.variancia.com/hindi_stemmer/\n    # https://github.com/taranjeet/hindi-tokenizer/blob/master/HindiTokenizer.py#L142\n    for suffix_group in reversed(_stem_suffixes):\n        length = len(suffix_group[0])\n        if len(string) <= length:\n            break\n        for suffix in suffix_group:\n            if string.endswith(suffix):\n                return string[:-length]\n    return string",
        "begin_line": 59,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.hi.lex_attrs.like_num#77",
        "src_path": "spacy/lang/hi/lex_attrs.py",
        "class_name": "spacy.lang.hi.lex_attrs",
        "signature": "spacy.lang.hi.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\", \", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 77,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.nl.__init__.DutchDefaults.create_lemmatizer#34",
        "src_path": "spacy/lang/nl/__init__.py",
        "class_name": "spacy.lang.nl.__init__.DutchDefaults",
        "signature": "spacy.lang.nl.__init__.DutchDefaults.create_lemmatizer(cls, nlp=None, lookups=None)",
        "snippet": "    def create_lemmatizer(cls, nlp=None, lookups=None):\n        if lookups is None:\n            lookups = Lookups()\n        return DutchLemmatizer(lookups)",
        "begin_line": 34,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.te.lex_attrs.like_num#44",
        "src_path": "spacy/lang/te/lex_attrs.py",
        "class_name": "spacy.lang.te.lex_attrs",
        "signature": "spacy.lang.te.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 44,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.__init__#16",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.__init__(self, extensions, obj, start=None, end=None)",
        "snippet": "    def __init__(self, extensions, obj, start=None, end=None):\n        object.__setattr__(self, \"_extensions\", extensions)\n        object.__setattr__(self, \"_obj\", obj)\n        # Assumption is that for doc values, _start and _end will both be None\n        # Span will set non-None values for _start and _end\n        # Token will have _start be non-None, _end be None\n        # This lets us key everything into the doc.user_data dictionary,\n        # (see _get_key), and lets us use a single Underscore class.\n        object.__setattr__(self, \"_doc\", obj.doc)\n        object.__setattr__(self, \"_start\", start)\n        object.__setattr__(self, \"_end\", end)",
        "begin_line": 16,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.499842505512307e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.__dir__#28",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.__dir__(self)",
        "snippet": "    def __dir__(self):\n        # Hack to enable autocomplete on custom extensions\n        extensions = list(self._extensions.keys())\n        return [\"set\", \"get\", \"has\"] + extensions",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.__getattr__#33",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name not in self._extensions:\n            raise AttributeError(Errors.E046.format(name=name))\n        default, method, getter, setter = self._extensions[name]\n        if getter is not None:\n            return getter(self._obj)\n        elif method is not None:\n            method_partial = functools.partial(method, self._obj)\n            # Hack to port over docstrings of the original function\n            # See https://stackoverflow.com/q/27362727/6400719\n            method_docstring = method.__doc__ or \"\"\n            method_docstring_prefix = (\n                \"This method is a partial function and its first argument \"\n                \"(the object it's called on) will be filled automatically. \"\n            )\n            method_partial.__doc__ = method_docstring_prefix + method_docstring\n            return method_partial\n        else:\n            key = self._get_key(name)\n            if key in self._doc.user_data:\n                return self._doc.user_data[key]\n            elif isinstance(default, self.mutable_types):\n                # Handle mutable default arguments (see #2581)\n                new_default = copy.copy(default)\n                self.__setattr__(name, new_default)\n                return new_default\n            return default",
        "begin_line": 33,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.__setattr__#61",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if name not in self._extensions:\n            raise AttributeError(Errors.E047.format(name=name))\n        default, method, getter, setter = self._extensions[name]\n        if setter is not None:\n            return setter(self._obj, value)\n        else:\n            self._doc.user_data[self._get_key(name)] = value",
        "begin_line": 61,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.set#70",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.set(self, name, value)",
        "snippet": "    def set(self, name, value):\n        return self.__setattr__(name, value)",
        "begin_line": 70,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.get#73",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.get(self, name)",
        "snippet": "    def get(self, name):\n        return self.__getattr__(name)",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.5985468591924955e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.has#76",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.has(self, name)",
        "snippet": "    def has(self, name):\n        return name in self._extensions",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore._get_key#79",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore._get_key(self, name)",
        "snippet": "    def _get_key(self, name):\n        return (\"._.\", name, self._start, self._end)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.534941725998821e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.Underscore.get_state#83",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore.Underscore",
        "signature": "spacy.tokens.underscore.Underscore.get_state(cls)",
        "snippet": "    def get_state(cls):\n        return cls.token_extensions, cls.span_extensions, cls.doc_extensions",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.get_ext_args#91",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore",
        "signature": "spacy.tokens.underscore.get_ext_args(**kwargs)",
        "snippet": "def get_ext_args(**kwargs):\n    \"\"\"Validate and convert arguments. Reused in Doc, Token and Span.\"\"\"\n    default = kwargs.get(\"default\")\n    getter = kwargs.get(\"getter\")\n    setter = kwargs.get(\"setter\")\n    method = kwargs.get(\"method\")\n    if getter is None and setter is not None:\n        raise ValueError(Errors.E089)\n    valid_opts = (\"default\" in kwargs, method is not None, getter is not None)\n    nr_defined = sum(t is True for t in valid_opts)\n    if nr_defined != 1:\n        raise ValueError(Errors.E083.format(nr_defined=nr_defined))\n    if setter is not None and not hasattr(setter, \"__call__\"):\n        raise ValueError(Errors.E091.format(name=\"setter\", value=repr(setter)))\n    if getter is not None and not hasattr(getter, \"__call__\"):\n        raise ValueError(Errors.E091.format(name=\"getter\", value=repr(getter)))\n    if method is not None and not hasattr(method, \"__call__\"):\n        raise ValueError(Errors.E091.format(name=\"method\", value=repr(method)))\n    return (default, method, getter, setter)",
        "begin_line": 91,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.tokens.underscore.is_writable_attr#112",
        "src_path": "spacy/tokens/underscore.py",
        "class_name": "spacy.tokens.underscore",
        "signature": "spacy.tokens.underscore.is_writable_attr(ext)",
        "snippet": "def is_writable_attr(ext):\n    \"\"\"Check if an extension attribute is writable.\n    ext (tuple): The (default, getter, setter, method) tuple available  via\n        {Doc,Span,Token}.get_extension.\n    RETURNS (bool): Whether the attribute is writable.\n    \"\"\"\n    default, method, getter, setter = ext\n    # Extension is writable if it has a setter (getter + setter), if it has a\n    # default value (or, if its default value is none, none of the other values\n    # should be set).\n    if setter is not None or default is not None or all(e is None for e in ext):\n        return True\n    return False",
        "begin_line": 112,
        "end_line": 124,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.__init__#27",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.__init__(self, options={})",
        "snippet": "    def __init__(self, options={}):\n        \"\"\"Initialise dependency renderer.\n\n        options (dict): Visualiser-specific options (compact, word_spacing,\n            arrow_spacing, arrow_width, arrow_stroke, distance, offset_x,\n            color, bg, font)\n        \"\"\"\n        self.compact = options.get(\"compact\", False)\n        self.word_spacing = options.get(\"word_spacing\", 45)\n        self.arrow_spacing = options.get(\"arrow_spacing\", 12 if self.compact else 20)\n        self.arrow_width = options.get(\"arrow_width\", 6 if self.compact else 10)\n        self.arrow_stroke = options.get(\"arrow_stroke\", 2)\n        self.distance = options.get(\"distance\", 150 if self.compact else 175)\n        self.offset_x = options.get(\"offset_x\", 50)\n        self.color = options.get(\"color\", \"#000000\")\n        self.bg = options.get(\"bg\", \"#ffffff\")\n        self.font = options.get(\"font\", \"Arial\")\n        self.direction = DEFAULT_DIR\n        self.lang = DEFAULT_LANG",
        "begin_line": 27,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.render#47",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.render(self, parsed, page=False, minify=False)",
        "snippet": "    def render(self, parsed, page=False, minify=False):\n        \"\"\"Render complete markup.\n\n        parsed (list): Dependency parses to render.\n        page (bool): Render parses wrapped as full HTML page.\n        minify (bool): Minify HTML markup.\n        RETURNS (unicode): Rendered SVG or HTML markup.\n        \"\"\"\n        # Create a random ID prefix to make sure parses don't receive the\n        # same ID, even if they're identical\n        id_prefix = uuid.uuid4().hex\n        rendered = []\n        for i, p in enumerate(parsed):\n            if i == 0:\n                settings = p.get(\"settings\", {})\n                self.direction = settings.get(\"direction\", DEFAULT_DIR)\n                self.lang = settings.get(\"lang\", DEFAULT_LANG)\n            render_id = \"{}-{}\".format(id_prefix, i)\n            svg = self.render_svg(render_id, p[\"words\"], p[\"arcs\"])\n            rendered.append(svg)\n        if page:\n            content = \"\".join([TPL_FIGURE.format(content=svg) for svg in rendered])\n            markup = TPL_PAGE.format(\n                content=content, lang=self.lang, dir=self.direction\n            )\n        else:\n            markup = \"\".join(rendered)\n        if minify:\n            return minify_html(markup)\n        return markup",
        "begin_line": 47,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.render_svg#78",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.render_svg(self, render_id, words, arcs)",
        "snippet": "    def render_svg(self, render_id, words, arcs):\n        \"\"\"Render SVG.\n\n        render_id (int): Unique ID, typically index of document.\n        words (list): Individual words and their tags.\n        arcs (list): Individual arcs and their start, end, direction and label.\n        RETURNS (unicode): Rendered SVG markup.\n        \"\"\"\n        self.levels = self.get_levels(arcs)\n        self.highest_level = len(self.levels)\n        self.offset_y = self.distance / 2 * self.highest_level + self.arrow_stroke\n        self.width = self.offset_x + len(words) * self.distance\n        self.height = self.offset_y + 3 * self.word_spacing\n        self.id = render_id\n        words = [\n            self.render_word(w[\"text\"], w[\"tag\"], w.get(\"lemma\", None), i)\n            for i, w in enumerate(words)\n        ]\n        arcs = [\n            self.render_arrow(a[\"label\"], a[\"start\"], a[\"end\"], a[\"dir\"], i)\n            for i, a in enumerate(arcs)\n        ]\n        content = \"\".join(words) + \"\".join(arcs)\n        return TPL_DEP_SVG.format(\n            id=self.id,\n            width=self.width,\n            height=self.height,\n            color=self.color,\n            bg=self.bg,\n            font=self.font,\n            content=content,\n            dir=self.direction,\n            lang=self.lang,\n        )",
        "begin_line": 78,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.render_word#113",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.render_word(self, text, tag, lemma, i)",
        "snippet": "    def render_word(\n        self, text, tag, lemma, i,\n    ):\n        \"\"\"Render individual word.\n\n        text (unicode): Word text.\n        tag (unicode): Part-of-speech tag.\n        i (int): Unique ID, typically word index.\n        RETURNS (unicode): Rendered SVG markup.\n        \"\"\"\n        y = self.offset_y + self.word_spacing\n        x = self.offset_x + i * self.distance\n        if self.direction == \"rtl\":\n            x = self.width - x\n        html_text = escape_html(text)\n        if lemma is not None:\n            return TPL_DEP_WORDS_LEMMA.format(\n                text=html_text, tag=tag, lemma=lemma, x=x, y=y\n            )\n        return TPL_DEP_WORDS.format(text=html_text, tag=tag, x=x, y=y)",
        "begin_line": 113,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.render_arrow#134",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.render_arrow(self, label, start, end, direction, i)",
        "snippet": "    def render_arrow(self, label, start, end, direction, i):\n        \"\"\"Render individual arrow.\n\n        label (unicode): Dependency label.\n        start (int): Index of start word.\n        end (int): Index of end word.\n        direction (unicode): Arrow direction, 'left' or 'right'.\n        i (int): Unique ID, typically arrow index.\n        RETURNS (unicode): Rendered SVG markup.\n        \"\"\"\n        if start < 0 or end < 0:\n            error_args = dict(start=start, end=end, label=label, dir=direction)\n            raise ValueError(Errors.E157.format(**error_args))\n        level = self.levels.index(end - start) + 1\n        x_start = self.offset_x + start * self.distance + self.arrow_spacing\n        if self.direction == \"rtl\":\n            x_start = self.width - x_start\n        y = self.offset_y\n        x_end = (\n            self.offset_x\n            + (end - start) * self.distance\n            + start * self.distance\n            - self.arrow_spacing * (self.highest_level - level) / 4\n        )\n        if self.direction == \"rtl\":\n            x_end = self.width - x_end\n        y_curve = self.offset_y - level * self.distance / 2\n        if self.compact:\n            y_curve = self.offset_y - level * self.distance / 6\n        if y_curve == 0 and len(self.levels) > 5:\n            y_curve = -self.distance\n        arrowhead = self.get_arrowhead(direction, x_start, y, x_end)\n        arc = self.get_arc(x_start, y, y_curve, x_end)\n        label_side = \"right\" if self.direction == \"rtl\" else \"left\"\n        return TPL_DEP_ARCS.format(\n            id=self.id,\n            i=i,\n            stroke=self.arrow_stroke,\n            head=arrowhead,\n            label=label,\n            label_side=label_side,\n            arc=arc,\n        )",
        "begin_line": 134,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.get_arc#178",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.get_arc(self, x_start, y, y_curve, x_end)",
        "snippet": "    def get_arc(self, x_start, y, y_curve, x_end):\n        \"\"\"Render individual arc.\n\n        x_start (int): X-coordinate of arrow start point.\n        y (int): Y-coordinate of arrow start and end point.\n        y_curve (int): Y-corrdinate of Cubic B\u00e9zier y_curve point.\n        x_end (int): X-coordinate of arrow end point.\n        RETURNS (unicode): Definition of the arc path ('d' attribute).\n        \"\"\"\n        template = \"M{x},{y} C{x},{c} {e},{c} {e},{y}\"\n        if self.compact:\n            template = \"M{x},{y} {x},{c} {e},{c} {e},{y}\"\n        return template.format(x=x_start, y=y, c=y_curve, e=x_end)",
        "begin_line": 178,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.get_arrowhead#192",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.get_arrowhead(self, direction, x, y, end)",
        "snippet": "    def get_arrowhead(self, direction, x, y, end):\n        \"\"\"Render individual arrow head.\n\n        direction (unicode): Arrow direction, 'left' or 'right'.\n        x (int): X-coordinate of arrow start point.\n        y (int): Y-coordinate of arrow start and end point.\n        end (int): X-coordinate of arrow end point.\n        RETURNS (unicode): Definition of the arrow head path ('d' attribute).\n        \"\"\"\n        if direction == \"left\":\n            pos1, pos2, pos3 = (x, x - self.arrow_width + 2, x + self.arrow_width - 2)\n        else:\n            pos1, pos2, pos3 = (\n                end,\n                end + self.arrow_width - 2,\n                end - self.arrow_width + 2,\n            )\n        arrowhead = (\n            pos1,\n            y + 2,\n            pos2,\n            y - self.arrow_width,\n            pos3,\n            y - self.arrow_width,\n        )\n        return \"M{},{} L{},{} {},{}\".format(*arrowhead)",
        "begin_line": 192,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.DependencyRenderer.get_levels#219",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.DependencyRenderer",
        "signature": "spacy.displacy.render.DependencyRenderer.get_levels(self, arcs)",
        "snippet": "    def get_levels(self, arcs):\n        \"\"\"Calculate available arc height \"levels\".\n        Used to calculate arrow heights dynamically and without wasting space.\n\n        args (list): Individual arcs and their start, end, direction and label.\n        RETURNS (list): Arc levels sorted from lowest to highest.\n        \"\"\"\n        levels = set(map(lambda arc: arc[\"end\"] - arc[\"start\"], arcs))\n        return sorted(list(levels))",
        "begin_line": 219,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.EntityRenderer.__init__#235",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.EntityRenderer",
        "signature": "spacy.displacy.render.EntityRenderer.__init__(self, options={})",
        "snippet": "    def __init__(self, options={}):\n        \"\"\"Initialise dependency renderer.\n\n        options (dict): Visualiser-specific options (colors, ents)\n        \"\"\"\n        colors = {\n            \"ORG\": \"#7aecec\",\n            \"PRODUCT\": \"#bfeeb7\",\n            \"GPE\": \"#feca74\",\n            \"LOC\": \"#ff9561\",\n            \"PERSON\": \"#aa9cfc\",\n            \"NORP\": \"#c887fb\",\n            \"FACILITY\": \"#9cc9cc\",\n            \"EVENT\": \"#ffeb80\",\n            \"LAW\": \"#ff8197\",\n            \"LANGUAGE\": \"#ff8197\",\n            \"WORK_OF_ART\": \"#f0d0ff\",\n            \"DATE\": \"#bfe1d9\",\n            \"TIME\": \"#bfe1d9\",\n            \"MONEY\": \"#e4e7d2\",\n            \"QUANTITY\": \"#e4e7d2\",\n            \"ORDINAL\": \"#e4e7d2\",\n            \"CARDINAL\": \"#e4e7d2\",\n            \"PERCENT\": \"#e4e7d2\",\n        }\n        user_colors = registry.displacy_colors.get_all()\n        for user_color in user_colors.values():\n            colors.update(user_color)\n        colors.update(options.get(\"colors\", {}))\n        self.default_color = \"#ddd\"\n        self.colors = colors\n        self.ents = options.get(\"ents\", None)\n        self.direction = DEFAULT_DIR\n        self.lang = DEFAULT_LANG\n\n        template = options.get(\"template\")\n        if template:\n            self.ent_template = template\n        else:\n            if self.direction == \"rtl\":\n                self.ent_template = TPL_ENT_RTL\n            else:\n                self.ent_template = TPL_ENT",
        "begin_line": 235,
        "end_line": 277,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.EntityRenderer.render#279",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.EntityRenderer",
        "signature": "spacy.displacy.render.EntityRenderer.render(self, parsed, page=False, minify=False)",
        "snippet": "    def render(self, parsed, page=False, minify=False):\n        \"\"\"Render complete markup.\n\n        parsed (list): Dependency parses to render.\n        page (bool): Render parses wrapped as full HTML page.\n        minify (bool): Minify HTML markup.\n        RETURNS (unicode): Rendered HTML markup.\n        \"\"\"\n        rendered = []\n        for i, p in enumerate(parsed):\n            if i == 0:\n                settings = p.get(\"settings\", {})\n                self.direction = settings.get(\"direction\", DEFAULT_DIR)\n                self.lang = settings.get(\"lang\", DEFAULT_LANG)\n            rendered.append(self.render_ents(p[\"text\"], p[\"ents\"], p.get(\"title\")))\n        if page:\n            docs = \"\".join([TPL_FIGURE.format(content=doc) for doc in rendered])\n            markup = TPL_PAGE.format(content=docs, lang=self.lang, dir=self.direction)\n        else:\n            markup = \"\".join(rendered)\n        if minify:\n            return minify_html(markup)\n        return markup",
        "begin_line": 279,
        "end_line": 301,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.render.EntityRenderer.render_ents#303",
        "src_path": "spacy/displacy/render.py",
        "class_name": "spacy.displacy.render.EntityRenderer",
        "signature": "spacy.displacy.render.EntityRenderer.render_ents(self, text, spans, title)",
        "snippet": "    def render_ents(self, text, spans, title):\n        \"\"\"Render entities in text.\n\n        text (unicode): Original text.\n        spans (list): Individual entity spans and their start, end and label.\n        title (unicode or None): Document title set in Doc.user_data['title'].\n        \"\"\"\n        markup = \"\"\n        offset = 0\n        for span in spans:\n            label = span[\"label\"]\n            start = span[\"start\"]\n            end = span[\"end\"]\n            additional_params = span.get(\"params\", {})\n            entity = escape_html(text[start:end])\n            fragments = text[offset:start].split(\"\\n\")\n            for i, fragment in enumerate(fragments):\n                markup += escape_html(fragment)\n                if len(fragments) > 1 and i != len(fragments) - 1:\n                    markup += \"</br>\"\n            if self.ents is None or label.upper() in self.ents:\n                color = self.colors.get(label.upper(), self.default_color)\n                ent_settings = {\"label\": label, \"text\": entity, \"bg\": color}\n                ent_settings.update(additional_params)\n                markup += self.ent_template.format(**ent_settings)\n            else:\n                markup += entity\n            offset = end\n        markup += escape_html(text[offset:])\n        markup = TPL_ENTS.format(content=markup, dir=self.direction)\n        if title:\n            markup = TPL_TITLE.format(title=title) + markup\n        return markup",
        "begin_line": 303,
        "end_line": 335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.fi.lex_attrs.like_num#44",
        "src_path": "spacy/lang/fi/lex_attrs.py",
        "class_name": "spacy.lang.fi.lex_attrs",
        "signature": "spacy.lang.fi.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\".\", \"\").replace(\",\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    return False",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.pl.lex_attrs.like_num#56",
        "src_path": "spacy/lang/pl/lex_attrs.py",
        "class_name": "spacy.lang.pl.lex_attrs",
        "signature": "spacy.lang.pl.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 56,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.lt.__init__._return_lt#18",
        "src_path": "spacy/lang/lt/__init__.py",
        "class_name": "spacy.lang.lt.__init__",
        "signature": "spacy.lang.lt.__init__._return_lt(_)",
        "snippet": "def _return_lt(_):\n    return \"lt\"",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.501260352898812e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.nl.lemmatizer.DutchLemmatizer.__call__#40",
        "src_path": "spacy/lang/nl/lemmatizer.py",
        "class_name": "spacy.lang.nl.lemmatizer.DutchLemmatizer",
        "signature": "spacy.lang.nl.lemmatizer.DutchLemmatizer.__call__(self, string, univ_pos, morphology=None)",
        "snippet": "    def __call__(self, string, univ_pos, morphology=None):\n        # Difference 1: self.rules is assumed to be non-None, so no\n        # 'is None' check required.\n        # String lowercased from the get-go. All lemmatization results in\n        # lowercased strings. For most applications, this shouldn't pose\n        # any problems, and it keeps the exceptions indexes small. If this\n        # creates problems for proper nouns, we can introduce a check for\n        # univ_pos == \"PROPN\".\n        string = string.lower()\n        try:\n            univ_pos = self.univ_pos_name_variants[univ_pos]\n        except KeyError:\n            # Because PROPN not in self.univ_pos_name_variants, proper names\n            # are not lemmatized. They are lowercased, however.\n            return [string]\n            # if string in self.lemma_index.get(univ_pos)\n        index_table = self.lookups.get_table(\"lemma_index\", {})\n        lemma_index = index_table.get(univ_pos, {})\n        # string is already lemma\n        if string in lemma_index:\n            return [string]\n        exc_table = self.lookups.get_table(\"lemma_exc\", {})\n        exceptions = exc_table.get(univ_pos, {})\n        # string is irregular token contained in exceptions index.\n        try:\n            lemma = exceptions[string]\n            return [lemma[0]]\n        except KeyError:\n            pass\n        # string corresponds to key in lookup table\n        lookup_table = self.lookups.get_table(\"lemma_lookup\", {})\n        looked_up_lemma = lookup_table.get(string)\n        if looked_up_lemma and looked_up_lemma in lemma_index:\n            return [looked_up_lemma]\n        rules_table = self.lookups.get_table(\"lemma_rules\", {})\n        forms, is_known = self.lemmatize(\n            string, lemma_index, exceptions, rules_table.get(univ_pos, [])\n        )\n        # Back-off through remaining return value candidates.\n        if forms:\n            if is_known:\n                return forms\n            else:\n                for form in forms:\n                    if form in exceptions:\n                        return [form]\n            if looked_up_lemma:\n                return [looked_up_lemma]\n            else:\n                return forms\n        elif looked_up_lemma:\n            return [looked_up_lemma]\n        else:\n            return [string]",
        "begin_line": 40,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ta.lex_attrs.suffix_filter#58",
        "src_path": "spacy/lang/ta/lex_attrs.py",
        "class_name": "spacy.lang.ta.lex_attrs",
        "signature": "spacy.lang.ta.lex_attrs.suffix_filter(text)",
        "snippet": "def suffix_filter(text):\n    # text without numeral suffixes\n    for num_suffix in _numeral_suffixes.keys():\n        length = len(num_suffix)\n        if len(text) < length:\n            break\n        elif text.endswith(num_suffix):\n            return text[:-length] + _numeral_suffixes[num_suffix]\n    return text",
        "begin_line": 58,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ta.lex_attrs.like_num#69",
        "src_path": "spacy/lang/ta/lex_attrs.py",
        "class_name": "spacy.lang.ta.lex_attrs",
        "signature": "spacy.lang.ta.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    elif suffix_filter(text) in _num_words:\n        return True\n    return False",
        "begin_line": 69,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.lb.lex_attrs.like_num#26",
        "src_path": "spacy/lang/lb/lex_attrs.py",
        "class_name": "spacy.lang.lb.lex_attrs",
        "signature": "spacy.lang.lb.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    \"\"\"\n    check if text resembles a number\n    \"\"\"\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    if text in _ordinal_words:\n        return True\n    return False",
        "begin_line": 26,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ar.lex_attrs.like_num#79",
        "src_path": "spacy/lang/ar/lex_attrs.py",
        "class_name": "spacy.lang.ar.lex_attrs",
        "signature": "spacy.lang.ar.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    \"\"\"\n    Check if text resembles a number\n    \"\"\"\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    if text in _ordinal_words:\n        return True\n    return False",
        "begin_line": 79,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.analyze_pipes#13",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.analyze_pipes(pipeline, name, pipe, index, warn=True)",
        "snippet": "def analyze_pipes(pipeline, name, pipe, index, warn=True):\n    \"\"\"Analyze a pipeline component with respect to its position in the current\n    pipeline and the other components. Will check whether requirements are\n    fulfilled (e.g. if previous components assign the attributes).\n\n    pipeline (list): A list of (name, pipe) tuples e.g. nlp.pipeline.\n    name (unicode): The name of the pipeline component to analyze.\n    pipe (callable): The pipeline component function to analyze.\n    index (int): The index of the component in the pipeline.\n    warn (bool): Show user warning if problem is found.\n    RETURNS (list): The problems found for the given pipeline component.\n    \"\"\"\n    assert pipeline[index][0] == name\n    prev_pipes = pipeline[:index]\n    pipe_requires = getattr(pipe, \"requires\", [])\n    requires = OrderedDict([(annot, False) for annot in pipe_requires])\n    if requires:\n        for prev_name, prev_pipe in prev_pipes:\n            prev_assigns = getattr(prev_pipe, \"assigns\", [])\n            for annot in prev_assigns:\n                requires[annot] = True\n    problems = []\n    for annot, fulfilled in requires.items():\n        if not fulfilled:\n            problems.append(annot)\n            if warn:\n                warnings.warn(Warnings.W025.format(name=name, attr=annot))\n    return problems",
        "begin_line": 13,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006756756756756757,
            "pseudo_dstar_susp": 0.1,
            "pseudo_tarantula_susp": 0.005714285714285714,
            "pseudo_op2_susp": 0.1,
            "pseudo_barinel_susp": 0.005714285714285714
        }
    },
    {
        "name": "spacy.analysis.analyze_all_pipes#43",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.analyze_all_pipes(pipeline, warn=True)",
        "snippet": "def analyze_all_pipes(pipeline, warn=True):\n    \"\"\"Analyze all pipes in the pipeline in order.\n\n    pipeline (list): A list of (name, pipe) tuples e.g. nlp.pipeline.\n    warn (bool): Show user warning if problem is found.\n    RETURNS (dict): The problems found, keyed by component name.\n    \"\"\"\n    problems = {}\n    for i, (name, pipe) in enumerate(pipeline):\n        problems[name] = analyze_pipes(pipeline, name, pipe, i, warn=warn)\n    return problems",
        "begin_line": 43,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.711647191858274e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.dot_to_dict#56",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.dot_to_dict(values)",
        "snippet": "def dot_to_dict(values):\n    \"\"\"Convert dot notation to a dict. For example: [\"token.pos\", \"token._.xyz\"]\n    become {\"token\": {\"pos\": True, \"_\": {\"xyz\": True }}}.\n\n    values (iterable): The values to convert.\n    RETURNS (dict): The converted values.\n    \"\"\"\n    result = {}\n    for value in values:\n        path = result\n        parts = value.lower().split(\".\")\n        for i, item in enumerate(parts):\n            is_last = i == len(parts) - 1\n            path = path.setdefault(item, True if is_last else {})\n    return result",
        "begin_line": 56,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.534941725998821e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.validate_attrs#73",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.validate_attrs(values)",
        "snippet": "def validate_attrs(values):\n    \"\"\"Validate component attributes provided to \"assigns\", \"requires\" etc.\n    Raises error for invalid attributes and formatting. Doesn't check if\n    custom extension attributes are registered, since this is something the\n    user might want to do themselves later in the component.\n\n    values (iterable): The string attributes to check, e.g. `[\"token.pos\"]`.\n    RETURNS (iterable): The checked attributes.\n    \"\"\"\n    data = dot_to_dict(values)\n    objs = {\"doc\": Doc, \"token\": Token, \"span\": Span}\n    for obj_key, attrs in data.items():\n        if obj_key == \"span\":\n            # Support Span only for custom extension attributes\n            span_attrs = [attr for attr in values if attr.startswith(\"span.\")]\n            span_attrs = [attr for attr in span_attrs if not attr.startswith(\"span._.\")]\n            if span_attrs:\n                raise ValueError(Errors.E180.format(attrs=\", \".join(span_attrs)))\n        if obj_key not in objs:  # first element is not doc/token/span\n            invalid_attrs = \", \".join(a for a in values if a.startswith(obj_key))\n            raise ValueError(Errors.E181.format(obj=obj_key, attrs=invalid_attrs))\n        if not isinstance(attrs, dict):  # attr is something like \"doc\"\n            raise ValueError(Errors.E182.format(attr=obj_key))\n        for attr, value in attrs.items():\n            if attr == \"_\":\n                if value is True:  # attr is something like \"doc._\"\n                    raise ValueError(Errors.E182.format(attr=\"{}._\".format(obj_key)))\n                for ext_attr, ext_value in value.items():\n                    # We don't check whether the attribute actually exists\n                    if ext_value is not True:  # attr is something like doc._.x.y\n                        good = \"{}._.{}\".format(obj_key, ext_attr)\n                        bad = \"{}.{}\".format(good, \".\".join(ext_value))\n                        raise ValueError(Errors.E183.format(attr=bad, solution=good))\n                continue  # we can't validate those further\n            if attr.endswith(\"_\"):  # attr is something like \"token.pos_\"\n                raise ValueError(Errors.E184.format(attr=attr, solution=attr[:-1]))\n            if value is not True:  # attr is something like doc.x.y\n                good = \"{}.{}\".format(obj_key, attr)\n                bad = \"{}.{}\".format(good, \".\".join(value))\n                raise ValueError(Errors.E183.format(attr=bad, solution=good))\n            obj = objs[obj_key]\n            if not hasattr(obj, attr):\n                raise ValueError(Errors.E185.format(obj=obj_key, attr=attr))\n    return values",
        "begin_line": 73,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis._get_feature_for_attr#119",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis._get_feature_for_attr(pipeline, attr, feature)",
        "snippet": "def _get_feature_for_attr(pipeline, attr, feature):\n    assert feature in [\"assigns\", \"requires\"]\n    result = []\n    for pipe_name, pipe in pipeline:\n        pipe_assigns = getattr(pipe, feature, [])\n        if attr in pipe_assigns:\n            result.append((pipe_name, pipe))\n    return result",
        "begin_line": 119,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.get_assigns_for_attr#129",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.get_assigns_for_attr(pipeline, attr)",
        "snippet": "def get_assigns_for_attr(pipeline, attr):\n    \"\"\"Get all pipeline components that assign an attr, e.g. \"doc.tensor\".\n\n    pipeline (list): A list of (name, pipe) tuples e.g. nlp.pipeline.\n    attr (unicode): The attribute to check.\n    RETURNS (list): (name, pipeline) tuples of components that assign the attr.\n    \"\"\"\n    return _get_feature_for_attr(pipeline, attr, \"assigns\")",
        "begin_line": 129,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.get_requires_for_attr#139",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.get_requires_for_attr(pipeline, attr)",
        "snippet": "def get_requires_for_attr(pipeline, attr):\n    \"\"\"Get all pipeline components that require an attr, e.g. \"doc.tensor\".\n\n    pipeline (list): A list of (name, pipe) tuples e.g. nlp.pipeline.\n    attr (unicode): The attribute to check.\n    RETURNS (list): (name, pipeline) tuples of components that require the attr.\n    \"\"\"\n    return _get_feature_for_attr(pipeline, attr, \"requires\")",
        "begin_line": 139,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.analysis.print_summary#149",
        "src_path": "spacy/analysis.py",
        "class_name": "spacy.analysis",
        "signature": "spacy.analysis.print_summary(nlp, pretty=True, no_print=False)",
        "snippet": "def print_summary(nlp, pretty=True, no_print=False):\n    \"\"\"Print a formatted summary for the current nlp object's pipeline. Shows\n    a table with the pipeline components and why they assign and require, as\n    well as any problems if available.\n\n    nlp (Language): The nlp object.\n    pretty (bool): Pretty-print the results (color etc).\n    no_print (bool): Don't print anything, just return the data.\n    RETURNS (dict): A dict with \"overview\" and \"problems\".\n    \"\"\"\n    msg = Printer(pretty=pretty, no_print=no_print)\n    overview = []\n    problems = {}\n    for i, (name, pipe) in enumerate(nlp.pipeline):\n        requires = getattr(pipe, \"requires\", [])\n        assigns = getattr(pipe, \"assigns\", [])\n        retok = getattr(pipe, \"retokenizes\", False)\n        overview.append((i, name, requires, assigns, retok))\n        problems[name] = analyze_pipes(nlp.pipeline, name, pipe, i, warn=False)\n    msg.divider(\"Pipeline Overview\")\n    header = (\"#\", \"Component\", \"Requires\", \"Assigns\", \"Retokenizes\")\n    msg.table(overview, header=header, divider=True, multiline=True)\n    n_problems = sum(len(p) for p in problems.values())\n    if any(p for p in problems.values()):\n        msg.divider(\"Problems ({})\".format(n_problems))\n        for name, problem in problems.items():\n            if problem:\n                problem = \", \".join(problem)\n                msg.warn(\"'{}' requirements not met: {}\".format(name, problem))\n    else:\n        msg.good(\"No problems found.\")\n    if no_print:\n        return {\"overview\": overview, \"problems\": problems}",
        "begin_line": 149,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.tl.lex_attrs.like_num#47",
        "src_path": "spacy/lang/tl/lex_attrs.py",
        "class_name": "spacy.lang.tl.lex_attrs",
        "signature": "spacy.lang.tl.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    return False",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.pt.lex_attrs.like_num#105",
        "src_path": "spacy/lang/pt/lex_attrs.py",
        "class_name": "spacy.lang.pt.lex_attrs",
        "signature": "spacy.lang.pt.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\"\u00ba\", \"\").replace(\"\u00aa\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    if text.lower() in _ordinal_words:\n        return True\n    return False",
        "begin_line": 105,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.fr.__init__.FrenchDefaults.create_lemmatizer#38",
        "src_path": "spacy/lang/fr/__init__.py",
        "class_name": "spacy.lang.fr.__init__.FrenchDefaults",
        "signature": "spacy.lang.fr.__init__.FrenchDefaults.create_lemmatizer(cls, nlp=None, lookups=None)",
        "snippet": "    def create_lemmatizer(cls, nlp=None, lookups=None):\n        if lookups is None:\n            lookups = Lookups()\n        return FrenchLemmatizer(lookups)",
        "begin_line": 38,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.si.lex_attrs.like_num#51",
        "src_path": "spacy/lang/si/lex_attrs.py",
        "class_name": "spacy.lang.si.lex_attrs",
        "signature": "spacy.lang.si.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 51,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ur.lex_attrs.like_num#31",
        "src_path": "spacy/lang/ur/lex_attrs.py",
        "class_name": "spacy.lang.ur.lex_attrs",
        "signature": "spacy.lang.ur.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    if text in _ordinal_words:\n        return True\n    return False",
        "begin_line": 31,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.es.lex_attrs.like_num#44",
        "src_path": "spacy/lang/es/lex_attrs.py",
        "class_name": "spacy.lang.es.lex_attrs",
        "signature": "spacy.lang.es.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.__init__.render#23",
        "src_path": "spacy/displacy/__init__.py",
        "class_name": "spacy.displacy.__init__",
        "signature": "spacy.displacy.__init__.render(docs, style='dep', page=False, minify=False, jupyter=None, options={}, manual=False)",
        "snippet": "def render(\n    docs, style=\"dep\", page=False, minify=False, jupyter=None, options={}, manual=False\n):\n    \"\"\"Render displaCy visualisation.\n\n    docs (list or Doc): Document(s) to visualise.\n    style (unicode): Visualisation style, 'dep' or 'ent'.\n    page (bool): Render markup as full HTML page.\n    minify (bool): Minify HTML markup.\n    jupyter (bool): Override Jupyter auto-detection.\n    options (dict): Visualiser-specific options, e.g. colors.\n    manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.\n    RETURNS (unicode): Rendered HTML markup.\n\n    DOCS: https://spacy.io/api/top-level#displacy.render\n    USAGE: https://spacy.io/usage/visualizers\n    \"\"\"\n    factories = {\n        \"dep\": (DependencyRenderer, parse_deps),\n        \"ent\": (EntityRenderer, parse_ents),\n    }\n    if style not in factories:\n        raise ValueError(Errors.E087.format(style=style))\n    if isinstance(docs, (Doc, Span, dict)):\n        docs = [docs]\n    docs = [obj if not isinstance(obj, Span) else obj.as_doc() for obj in docs]\n    if not all(isinstance(obj, (Doc, Span, dict)) for obj in docs):\n        raise ValueError(Errors.E096)\n    renderer, converter = factories[style]\n    renderer = renderer(options=options)\n    parsed = [converter(doc, options) for doc in docs] if not manual else docs\n    _html[\"parsed\"] = renderer.render(parsed, page=page, minify=minify).strip()\n    html = _html[\"parsed\"]\n    if RENDER_WRAPPER is not None:\n        html = RENDER_WRAPPER(html)\n    if jupyter or (jupyter is None and is_in_jupyter()):\n        # return HTML rendered by IPython display()\n        # See #4840 for details on span wrapper to disable mathjax\n        from IPython.core.display import display, HTML\n\n        return display(HTML('<span class=\"tex2jax_ignore\">{}</span>'.format(html)))\n    return html",
        "begin_line": 23,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.__init__.parse_deps#116",
        "src_path": "spacy/displacy/__init__.py",
        "class_name": "spacy.displacy.__init__",
        "signature": "spacy.displacy.__init__.parse_deps(orig_doc, options={})",
        "snippet": "def parse_deps(orig_doc, options={}):\n    \"\"\"Generate dependency parse in {'words': [], 'arcs': []} format.\n\n    doc (Doc): Document do parse.\n    RETURNS (dict): Generated dependency parse keyed by words and arcs.\n    \"\"\"\n    doc = Doc(orig_doc.vocab).from_bytes(orig_doc.to_bytes(exclude=[\"user_data\"]))\n    if not doc.is_parsed:\n        warnings.warn(Warnings.W005)\n    if options.get(\"collapse_phrases\", False):\n        with doc.retokenize() as retokenizer:\n            for np in list(doc.noun_chunks):\n                attrs = {\n                    \"tag\": np.root.tag_,\n                    \"lemma\": np.root.lemma_,\n                    \"ent_type\": np.root.ent_type_,\n                }\n                retokenizer.merge(np, attrs=attrs)\n    if options.get(\"collapse_punct\", True):\n        spans = []\n        for word in doc[:-1]:\n            if word.is_punct or not word.nbor(1).is_punct:\n                continue\n            start = word.i\n            end = word.i + 1\n            while end < len(doc) and doc[end].is_punct:\n                end += 1\n            span = doc[start:end]\n            spans.append((span, word.tag_, word.lemma_, word.ent_type_))\n        with doc.retokenize() as retokenizer:\n            for span, tag, lemma, ent_type in spans:\n                attrs = {\"tag\": tag, \"lemma\": lemma, \"ent_type\": ent_type}\n                retokenizer.merge(span, attrs=attrs)\n    fine_grained = options.get(\"fine_grained\")\n    add_lemma = options.get(\"add_lemma\")\n    words = [\n        {\n            \"text\": w.text,\n            \"tag\": w.tag_ if fine_grained else w.pos_,\n            \"lemma\": w.lemma_ if add_lemma else None,\n        }\n        for w in doc\n    ]\n\n    arcs = []\n    for word in doc:\n        if word.i < word.head.i:\n            arcs.append(\n                {\"start\": word.i, \"end\": word.head.i, \"label\": word.dep_, \"dir\": \"left\"}\n            )\n        elif word.i > word.head.i:\n            arcs.append(\n                {\n                    \"start\": word.head.i,\n                    \"end\": word.i,\n                    \"label\": word.dep_,\n                    \"dir\": \"right\",\n                }\n            )\n    return {\"words\": words, \"arcs\": arcs, \"settings\": get_doc_settings(orig_doc)}",
        "begin_line": 116,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.849190185239065e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.__init__.parse_ents#178",
        "src_path": "spacy/displacy/__init__.py",
        "class_name": "spacy.displacy.__init__",
        "signature": "spacy.displacy.__init__.parse_ents(doc, options={})",
        "snippet": "def parse_ents(doc, options={}):\n    \"\"\"Generate named entities in [{start: i, end: i, label: 'label'}] format.\n\n    doc (Doc): Document do parse.\n    RETURNS (dict): Generated entities keyed by text (original text) and ents.\n    \"\"\"\n    ents = [\n        {\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n        for ent in doc.ents\n    ]\n    if not ents:\n        warnings.warn(Warnings.W006)\n    title = doc.user_data.get(\"title\", None) if hasattr(doc, \"user_data\") else None\n    settings = get_doc_settings(doc)\n    return {\"text\": doc.text, \"ents\": ents, \"title\": title, \"settings\": settings}",
        "begin_line": 178,
        "end_line": 192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.6576618537494175e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.__init__.set_render_wrapper#195",
        "src_path": "spacy/displacy/__init__.py",
        "class_name": "spacy.displacy.__init__",
        "signature": "spacy.displacy.__init__.set_render_wrapper(func)",
        "snippet": "def set_render_wrapper(func):\n    \"\"\"Set an optional wrapper function that is called around the generated\n    HTML markup on displacy.render. This can be used to allow integration into\n    other platforms, similar to Jupyter Notebooks that require functions to be\n    called around the HTML. It can also be used to implement custom callbacks\n    on render, or to embed the visualization in a custom page.\n\n    func (callable): Function to call around markup before rendering it. Needs\n        to take one argument, the HTML markup, and should return the desired\n        output of displacy.render.\n    \"\"\"\n    global RENDER_WRAPPER\n    if not hasattr(func, \"__call__\"):\n        raise ValueError(Errors.E110.format(obj=type(func)))\n    RENDER_WRAPPER = func",
        "begin_line": 195,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.displacy.__init__.get_doc_settings#212",
        "src_path": "spacy/displacy/__init__.py",
        "class_name": "spacy.displacy.__init__",
        "signature": "spacy.displacy.__init__.get_doc_settings(doc)",
        "snippet": "def get_doc_settings(doc):\n    return {\n        \"lang\": doc.lang_,\n        \"direction\": doc.vocab.writing_system.get(\"direction\", \"ltr\"),\n    }",
        "begin_line": 212,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.556016219417741e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.ru.lex_attrs.like_num#54",
        "src_path": "spacy/lang/ru/lex_attrs.py",
        "class_name": "spacy.lang.ru.lex_attrs",
        "signature": "spacy.lang.ru.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 54,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.lt.lex_attrs.like_num#1140",
        "src_path": "spacy/lang/lt/lex_attrs.py",
        "class_name": "spacy.lang.lt.lex_attrs",
        "signature": "spacy.lang.lt.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text.lower() in _num_words:\n        return True\n    return False",
        "begin_line": 1140,
        "end_line": 1150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 9.665571235260003e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    },
    {
        "name": "spacy.lang.tt.lex_attrs.like_num#46",
        "src_path": "spacy/lang/tt/lex_attrs.py",
        "class_name": "spacy.lang.tt.lex_attrs",
        "signature": "spacy.lang.tt.lex_attrs.like_num(text)",
        "snippet": "def like_num(text):\n    if text.startswith((\"+\", \"-\", \"\u00b1\", \"~\")):\n        text = text[1:]\n    text = text.replace(\",\", \"\").replace(\".\", \"\")\n    if text.isdigit():\n        return True\n    if text.count(\"/\") == 1:\n        num, denom = text.split(\"/\")\n        if num.isdigit() and denom.isdigit():\n            return True\n    if text in _num_words:\n        return True\n    return False",
        "begin_line": 46,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 8.673779165582444e-05,
            "pseudo_dstar_susp": 8.673779165582444e-05,
            "pseudo_tarantula_susp": 8.673779165582444e-05,
            "pseudo_op2_susp": 4.7521741196597446e-05,
            "pseudo_barinel_susp": 8.673779165582444e-05
        }
    }
]