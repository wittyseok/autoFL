[
    {
        "name": "scrapy.spiderloader.SpiderLoader.__init__#20",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.spider_modules = settings.getlist('SPIDER_MODULES')\n        self.warn_only = settings.getbool('SPIDER_LOADER_WARN_ONLY')\n        self._spiders = {}\n        self._found = defaultdict(list)\n        self._load_all_spiders()",
        "begin_line": 20,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016603021749958492,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._check_name_duplicates#27",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._check_name_duplicates(self)",
        "snippet": "    def _check_name_duplicates(self):\n        dupes = [\"\\n\".join(\"  {cls} named {name!r} (in {module})\".format(\n                                module=mod, cls=cls, name=name)\n                           for (mod, cls) in locations)\n                 for name, locations in self._found.items()\n                 if len(locations)>1]\n        if dupes:\n            msg = (\"There are several spiders with the same name:\\n\\n\"\n                   \"{}\\n\\n  This can cause unexpected behavior.\".format(\n                        \"\\n\\n\".join(dupes)))\n            warnings.warn(msg, UserWarning)",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_spiders#39",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_spiders(self, module)",
        "snippet": "    def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._found[spcls.name].append((module.__name__, spcls.__name__))\n            self._spiders[spcls.name] = spcls",
        "begin_line": 39,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_all_spiders#44",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_all_spiders(self)",
        "snippet": "    def _load_all_spiders(self):\n        for name in self.spider_modules:\n            try:\n                for module in walk_modules(name):\n                    self._load_spiders(module)\n            except ImportError as e:\n                if self.warn_only:\n                    msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n                           \"See above traceback for details.\".format(\n                                modname=name, tb=traceback.format_exc()))\n                    warnings.warn(msg, RuntimeWarning)\n                else:\n                    raise\n        self._check_name_duplicates()",
        "begin_line": 44,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.from_settings#60",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings)",
        "begin_line": 60,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016603021749958492,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.load#63",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.load(self, spider_name)",
        "snippet": "    def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(\"Spider not found: {}\".format(spider_name))",
        "begin_line": 63,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.find_by_request#73",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.find_by_request(self, request)",
        "snippet": "    def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [name for name, cls in self._spiders.items()\n                if cls.handles_request(request)]",
        "begin_line": 73,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.list#80",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.list(self)",
        "snippet": "    def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())",
        "begin_line": 80,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.default.UrlContract.adjust_request_args#18",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.UrlContract",
        "signature": "scrapy.contracts.default.UrlContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args",
        "begin_line": 18,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.default.CallbackKeywordArgumentsContract.adjust_request_args#32",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.CallbackKeywordArgumentsContract",
        "signature": "scrapy.contracts.default.CallbackKeywordArgumentsContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['cb_kwargs'] = json.loads(' '.join(self.args))\n        return args",
        "begin_line": 32,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.__init__#58",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ReturnsContract, self).__init__(*args, **kwargs)\n\n        assert len(self.args) in [1, 2, 3]\n        self.obj_name = self.args[0] or None\n        self.obj_type = self.objects[self.obj_name]\n\n        try:\n            self.min_bound = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound = int(self.args[2])\n        except IndexError:\n            self.max_bound = float('inf')",
        "begin_line": 58,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.post_process#75",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if isinstance(x, self.obj_type):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = '%s..%s' % (self.min_bound, self.max_bound)\n\n            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n                (occurrences, self.obj_name, expected))",
        "begin_line": 75,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.default.ScrapesContract.post_process#100",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ScrapesContract",
        "signature": "scrapy.contracts.default.ScrapesContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        for x in output:\n            if isinstance(x, (BaseItem, dict)):\n                missing = [arg for arg in self.args if arg not in x]\n                if missing:\n                    raise ContractFail(\n                        \"Missing fields: %s\" % \", \".join(missing))",
        "begin_line": 100,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._path_safe#13",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues",
        "signature": "scrapy.pqueues._path_safe(text)",
        "snippet": "def _path_safe(text):\n    \"\"\"\n    Return a filesystem-safe version of a string ``text``\n\n    >>> _path_safe('simple.org').startswith('simple.org')\n    True\n    >>> _path_safe('dash-underscore_.org').startswith('dash-underscore_.org')\n    True\n    >>> _path_safe('some@symbol?').startswith('some_symbol_')\n    True\n    \"\"\"\n    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n                             for c in text])\n    # as we replace some letters we can get collision for different slots\n    # add we add unique part\n    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n    return '-'.join([pathable_slot, unique_slot])",
        "begin_line": 13,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._Priority.__str__#46",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._Priority",
        "signature": "scrapy.pqueues._Priority.__str__(self)",
        "snippet": "    def __str__(self):\n        return '%s_%s' % (self.priority, _path_safe(str(self.slot)))",
        "begin_line": 46,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.__init__#52",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.__init__(self, pqfactory, slot_startprios=None)",
        "snippet": "    def __init__(self, pqfactory, slot_startprios=None):\n        \"\"\"\n        ``pqfactory`` is a factory for creating new PriorityQueues.\n        It must be a function which accepts a single optional ``startprios``\n        argument, with a list of priorities to create queues for.\n\n        ``slot_startprios`` is a ``{slot: startprios}`` dict.\n        \"\"\"\n        self.pqfactory = pqfactory\n        self.pqueues = {}  # slot -> priority queue\n        for slot, startprios in (slot_startprios or {}).items():\n            self.pqueues[slot] = self.pqfactory(startprios)",
        "begin_line": 52,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.pop_slot#65",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.pop_slot(self, slot)",
        "snippet": "    def pop_slot(self, slot):\n        \"\"\" Pop an object from a priority queue for this slot \"\"\"\n        queue = self.pqueues[slot]\n        request = queue.pop()\n        if len(queue) == 0:\n            del self.pqueues[slot]\n        return request",
        "begin_line": 65,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.push_slot#73",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.push_slot(self, slot, obj, priority)",
        "snippet": "    def push_slot(self, slot, obj, priority):\n        \"\"\" Push an object to a priority queue for this slot \"\"\"\n        if slot not in self.pqueues:\n            self.pqueues[slot] = self.pqfactory()\n        queue = self.pqueues[slot]\n        queue.push(obj, priority)",
        "begin_line": 73,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.close#80",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.close(self)",
        "snippet": "    def close(self):\n        active = {slot: queue.close()\n                  for slot, queue in self.pqueues.items()}\n        self.pqueues.clear()\n        return active",
        "begin_line": 80,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.__len__#86",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.__len__(self)",
        "snippet": "    def __len__(self):\n        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues._SlotPriorityQueues.__contains__#89",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues._SlotPriorityQueues",
        "signature": "scrapy.pqueues._SlotPriorityQueues.__contains__(self, slot)",
        "snippet": "    def __contains__(self, slot):\n        return slot in self.pqueues",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.ScrapyPriorityQueue.__init__#98",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.ScrapyPriorityQueue",
        "signature": "scrapy.pqueues.ScrapyPriorityQueue.__init__(self, crawler, qfactory, startprios=(), serialize=False)",
        "snippet": "    def __init__(self, crawler, qfactory, startprios=(), serialize=False):\n        super(ScrapyPriorityQueue, self).__init__(qfactory, startprios)\n        self.serialize = serialize\n        self.spider = crawler.spider",
        "begin_line": 98,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001812250815512867,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.ScrapyPriorityQueue.from_crawler#104",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.ScrapyPriorityQueue",
        "signature": "scrapy.pqueues.ScrapyPriorityQueue.from_crawler(cls, crawler, qfactory, startprios=(), serialize=False)",
        "snippet": "    def from_crawler(cls, crawler, qfactory, startprios=(), serialize=False):\n        return cls(crawler, qfactory, startprios, serialize)",
        "begin_line": 104,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.ScrapyPriorityQueue.push#107",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.ScrapyPriorityQueue",
        "signature": "scrapy.pqueues.ScrapyPriorityQueue.push(self, request, priority=0)",
        "snippet": "    def push(self, request, priority=0):\n        if self.serialize:\n            request = request_to_dict(request, self.spider)\n        super(ScrapyPriorityQueue, self).push(request, priority)",
        "begin_line": 107,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.ScrapyPriorityQueue.pop#112",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.ScrapyPriorityQueue",
        "signature": "scrapy.pqueues.ScrapyPriorityQueue.pop(self)",
        "snippet": "    def pop(self):\n        request = super(ScrapyPriorityQueue, self).pop()\n        if request and self.serialize:\n            request = request_from_dict(request, self.spider)\n        return request",
        "begin_line": 112,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderInterface.__init__#121",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderInterface",
        "signature": "scrapy.pqueues.DownloaderInterface.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.downloader = crawler.engine.downloader",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderInterface.stats#124",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderInterface",
        "signature": "scrapy.pqueues.DownloaderInterface.stats(self, possible_slots)",
        "snippet": "    def stats(self, possible_slots):\n        return [(self._active_downloads(slot), slot)\n                for slot in possible_slots]",
        "begin_line": 124,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderInterface.get_slot_key#128",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderInterface",
        "signature": "scrapy.pqueues.DownloaderInterface.get_slot_key(self, request)",
        "snippet": "    def get_slot_key(self, request):\n        return self.downloader._get_slot_key(request, None)",
        "begin_line": 128,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderInterface._active_downloads#131",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderInterface",
        "signature": "scrapy.pqueues.DownloaderInterface._active_downloads(self, slot)",
        "snippet": "    def _active_downloads(self, slot):\n        \"\"\" Return a number of requests in a Downloader for a given slot \"\"\"\n        if slot not in self.downloader.slots:\n            return 0\n        return len(self.downloader.slots[slot].active)",
        "begin_line": 131,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.from_crawler#145",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.from_crawler(cls, crawler, qfactory, slot_startprios=None, serialize=False)",
        "snippet": "    def from_crawler(cls, crawler, qfactory, slot_startprios=None, serialize=False):\n        return cls(crawler, qfactory, slot_startprios, serialize)",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.__init__#148",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.__init__(self, crawler, qfactory, slot_startprios=None, serialize=False)",
        "snippet": "    def __init__(self, crawler, qfactory, slot_startprios=None, serialize=False):\n        if crawler.settings.getint('CONCURRENT_REQUESTS_PER_IP') != 0:\n            raise ValueError('\"%s\" does not support CONCURRENT_REQUESTS_PER_IP'\n                             % (self.__class__,))\n\n        if slot_startprios and not isinstance(slot_startprios, dict):\n            raise ValueError(\"DownloaderAwarePriorityQueue accepts \"\n                             \"``slot_startprios`` as a dict; %r instance \"\n                             \"is passed. Most likely, it means the state is\"\n                             \"created by an incompatible priority queue. \"\n                             \"Only a crawl started with the same priority \"\n                             \"queue class can be resumed.\" %\n                             slot_startprios.__class__)\n\n        slot_startprios = {\n            slot: [_Priority(p, slot) for p in startprios]\n            for slot, startprios in (slot_startprios or {}).items()}\n\n        def pqfactory(startprios=()):\n            return ScrapyPriorityQueue(crawler, qfactory, startprios, serialize)\n        self._slot_pqueues = _SlotPriorityQueues(pqfactory, slot_startprios)\n        self.serialize = serialize\n        self._downloader_interface = DownloaderInterface(crawler)",
        "begin_line": 148,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.pqfactory#166",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.pqfactory(startprios=())",
        "snippet": "        def pqfactory(startprios=()):\n            return ScrapyPriorityQueue(crawler, qfactory, startprios, serialize)",
        "begin_line": 166,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.pop#172",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.pop(self)",
        "snippet": "    def pop(self):\n        stats = self._downloader_interface.stats(self._slot_pqueues.pqueues)\n\n        if not stats:\n            return\n\n        slot = min(stats)[1]\n        request = self._slot_pqueues.pop_slot(slot)\n        return request",
        "begin_line": 172,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.push#182",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.push(self, request, priority)",
        "snippet": "    def push(self, request, priority):\n        slot = self._downloader_interface.get_slot_key(request)\n        priority_slot = _Priority(priority=priority, slot=slot)\n        self._slot_pqueues.push_slot(slot, request, priority_slot)",
        "begin_line": 182,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.close#187",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.close(self)",
        "snippet": "    def close(self):\n        active = self._slot_pqueues.close()\n        return {slot: [p.priority for p in startprios]\n                for slot, startprios in active.items()}",
        "begin_line": 187,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pqueues.DownloaderAwarePriorityQueue.__len__#192",
        "src_path": "scrapy/pqueues.py",
        "class_name": "scrapy.pqueues.DownloaderAwarePriorityQueue",
        "signature": "scrapy.pqueues.DownloaderAwarePriorityQueue.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._slot_pqueues)",
        "begin_line": 192,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.curl.CurlParser.error#12",
        "src_path": "scrapy/utils/curl.py",
        "class_name": "scrapy.utils.curl.CurlParser",
        "signature": "scrapy.utils.curl.CurlParser.error(self, message)",
        "snippet": "    def error(self, message):\n        error_msg = \\\n            'There was an error parsing the curl command: {}'.format(message)\n        raise ValueError(error_msg)",
        "begin_line": 12,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.curl.curl_to_request_kwargs#39",
        "src_path": "scrapy/utils/curl.py",
        "class_name": "scrapy.utils.curl",
        "signature": "scrapy.utils.curl.curl_to_request_kwargs(curl_command, ignore_unknown_options=True)",
        "snippet": "def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n    \"\"\"Convert a cURL command syntax to Request kwargs.\n\n    :param str curl_command: string containing the curl command\n    :param bool ignore_unknown_options: If true, only a warning is emitted when\n    cURL options are unknown. Otherwise raises an error. (default: True)\n    :return: dictionary of Request kwargs\n    \"\"\"\n\n    curl_args = split(curl_command)\n\n    if curl_args[0] != 'curl':\n        raise ValueError('A curl command must start with \"curl\"')\n\n    parsed_args, argv = curl_parser.parse_known_args(curl_args[1:])\n\n    if argv:\n        msg = 'Unrecognized options: {}'.format(', '.join(argv))\n        if ignore_unknown_options:\n            warnings.warn(msg)\n        else:\n            raise ValueError(msg)\n\n    url = parsed_args.url\n\n    # curl automatically prepends 'http' if the scheme is missing, but Request\n    # needs the scheme to work\n    parsed_url = urlparse(url)\n    if not parsed_url.scheme:\n        url = 'http://' + url\n\n    result = {'method': parsed_args.method.upper(), 'url': url}\n\n    headers = []\n    cookies = {}\n    for header in parsed_args.headers or ():\n        name, val = header.split(':', 1)\n        name = name.strip()\n        val = val.strip()\n        if name.title() == 'Cookie':\n            for name, morsel in iteritems(SimpleCookie(val)):\n                cookies[name] = morsel.value\n        else:\n            headers.append((name, val))\n\n    if parsed_args.auth:\n        user, password = parsed_args.auth.split(':', 1)\n        headers.append(('Authorization', basic_auth_header(user, password)))\n\n    if headers:\n        result['headers'] = headers\n    if cookies:\n        result['cookies'] = cookies\n    if parsed_args.data:\n        result['body'] = parsed_args.data\n\n    return result",
        "begin_line": 39,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators.xmliter#18",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.xmliter(obj, nodename)",
        "snippet": "def xmliter(obj, nodename):\n    \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n       given the name of the node to iterate. Useful for parsing XML feeds.\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n    \"\"\"\n    nodename_patt = re.escape(nodename)\n\n    HEADER_START_RE = re.compile(r'^(.*?)<\\s*%s(?:\\s|>)' % nodename_patt, re.S)\n    HEADER_END_RE = re.compile(r'<\\s*/%s\\s*>' % nodename_patt, re.S)\n    text = _body_or_str(obj)\n\n    header_start = re.search(HEADER_START_RE, text)\n    header_start = header_start.group(1).strip() if header_start else ''\n    header_end = re_rsearch(HEADER_END_RE, text)\n    header_end = text[header_end[1]:].strip() if header_end else ''\n\n    r = re.compile(r'<%(np)s[\\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)\n    for match in r.finditer(text):\n        nodetext = header_start + match.group() + header_end\n        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]",
        "begin_line": 18,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators.xmliter_lxml#44",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.xmliter_lxml(obj, nodename, namespace=None, prefix='x')",
        "snippet": "def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):\n    from lxml import etree\n    reader = _StreamReader(obj)\n    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename\n    iterable = etree.iterparse(reader, tag=tag, encoding=reader.encoding)\n    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)\n    for _, node in iterable:\n        nodetext = etree.tostring(node, encoding='unicode')\n        node.clear()\n        xs = Selector(text=nodetext, type='xml')\n        if namespace:\n            xs.register_namespace(prefix, namespace)\n        yield xs.xpath(selxpath)[0]",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader.__init__#61",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader.__init__(self, obj)",
        "snippet": "    def __init__(self, obj):\n        self._ptr = 0\n        if isinstance(obj, Response):\n            self._text, self.encoding = obj.body, obj.encoding\n        else:\n            self._text, self.encoding = obj, 'utf-8'\n        self._is_unicode = isinstance(self._text, six.text_type)",
        "begin_line": 61,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader.read#69",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader.read(self, n=65535)",
        "snippet": "    def read(self, n=65535):\n        self.read = self._read_unicode if self._is_unicode else self._read_string\n        return self.read(n).lstrip()",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader._read_string#73",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader._read_string(self, n=65535)",
        "snippet": "    def _read_string(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e]",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators._StreamReader._read_unicode#78",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators._StreamReader",
        "signature": "scrapy.utils.iterators._StreamReader._read_unicode(self, n=65535)",
        "snippet": "    def _read_unicode(self, n=65535):\n        s, e = self._ptr, self._ptr + n\n        self._ptr = e\n        return self._text[s:e].encode('utf-8')",
        "begin_line": 78,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators.csviter#84",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None)",
        "snippet": "def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n    \"\"\" Returns an iterator of dictionaries from the given csv object\n\n    obj can be:\n    - a Response object\n    - a unicode string\n    - a string encoded as utf-8\n\n    delimiter is the character used to separate fields on the given obj.\n\n    headers is an iterable that when provided offers the keys\n    for the returned dictionaries, if not the first row is used.\n\n    quotechar is the character used to enclosure fields on the given obj.\n    \"\"\"\n\n    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'\n\n    def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]\n\n    # Python 3 csv reader input object needs to return strings\n    if six.PY3:\n        lines = StringIO(_body_or_str(obj, unicode=True))\n    else:\n        lines = BytesIO(_body_or_str(obj, unicode=False))\n\n    kwargs = {}\n    if delimiter: kwargs[\"delimiter\"] = delimiter\n    if quotechar: kwargs[\"quotechar\"] = quotechar\n    csv_r = csv.reader(lines, **kwargs)\n\n    if not headers:\n        try:\n            row = next(csv_r)\n        except StopIteration:\n            return\n        headers = row_to_unicode(row)\n\n    for row in csv_r:\n        row = row_to_unicode(row)\n        if len(row) != len(headers):\n            logger.warning(\"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n                           \"should be: %(csvheader)d)\",\n                           {'csvlnum': csv_r.line_num, 'csvrow': len(row),\n                            'csvheader': len(headers)})\n            continue\n        else:\n            yield dict(zip(headers, row))",
        "begin_line": 84,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators.row_to_unicode#102",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators.row_to_unicode(row_)",
        "snippet": "    def row_to_unicode(row_):\n        return [to_unicode(field, encoding) for field in row_]",
        "begin_line": 102,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.iterators._body_or_str#135",
        "src_path": "scrapy/utils/iterators.py",
        "class_name": "scrapy.utils.iterators",
        "signature": "scrapy.utils.iterators._body_or_str(obj, unicode=True)",
        "snippet": "def _body_or_str(obj, unicode=True):\n    expected_types = (Response, six.text_type, six.binary_type)\n    assert isinstance(obj, expected_types), \\\n        \"obj must be %s, not %s\" % (\n            \" or \".join(t.__name__ for t in expected_types),\n            type(obj).__name__)\n    if isinstance(obj, Response):\n        if not unicode:\n            return obj.body\n        elif isinstance(obj, TextResponse):\n            return obj.text\n        else:\n            return obj.body.decode('utf-8')\n    elif isinstance(obj, six.text_type):\n        return obj if unicode else obj.encode('utf-8')\n    else:\n        return obj.decode('utf-8') if unicode else obj",
        "begin_line": 135,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.ItemMeta.__new__#52",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.ItemMeta",
        "signature": "scrapy.item.ItemMeta.__new__(mcs, class_name, bases, attrs)",
        "snippet": "    def __new__(mcs, class_name, bases, attrs):\n        classcell = attrs.pop('__classcell__', None)\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        if classcell is not None:\n            new_attrs['__classcell__'] = classcell\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)",
        "begin_line": 52,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__new__#77",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        if issubclass(cls, DictItem) and not issubclass(cls, Item):\n            warn('scrapy.item.DictItem is deprecated, please use '\n                 'scrapy.item.Item instead',\n                 ScrapyDeprecationWarning, stacklevel=2)\n        return super(DictItem, cls).__new__(cls, *args, **kwargs)",
        "begin_line": 77,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__init__#84",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._values = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in six.iteritems(dict(*args, **kwargs)):\n                self[k] = v",
        "begin_line": 84,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017677214071062401,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__getitem__#90",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._values[key]",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001756543123133673,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__setitem__#93",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(\"%s does not support field: %s\" %\n                (self.__class__.__name__, key))",
        "begin_line": 93,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__getattr__#103",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(\"Use item[%r] to get field value\" % name)\n        raise AttributeError(name)",
        "begin_line": 103,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__setattr__#108",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                (name, value))\n        super(DictItem, self).__setattr__(name, value)",
        "begin_line": 108,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__len__#114",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._values)",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__iter__#117",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self._values)",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.keys#122",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.keys(self)",
        "snippet": "    def keys(self):\n        return self._values.keys()",
        "begin_line": 122,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022972662531587412,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.__repr__#125",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return pformat(dict(self))",
        "begin_line": 125,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.copy#128",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.copy(self)",
        "snippet": "    def copy(self):\n        return self.__class__(self)",
        "begin_line": 128,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.item.DictItem.deepcopy#131",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.deepcopy(self)",
        "snippet": "    def deepcopy(self):\n        \"\"\"Return a `deep copy`_ of this item.\n\n        .. _deep copy: https://docs.python.org/library/copy.html#copy.deepcopy\n        \"\"\"\n        return deepcopy(self)",
        "begin_line": 131,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.__init__#9",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.__init__(self, user_agent='Scrapy')",
        "snippet": "    def __init__(self, user_agent='Scrapy'):\n        self.user_agent = user_agent",
        "begin_line": 9,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001816860465116279,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.from_crawler#13",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings['USER_AGENT'])\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 13,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001816860465116279,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.spider_opened#18",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.user_agent = getattr(spider, 'user_agent', self.user_agent)",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001921598770176787,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.process_request#21",
        "src_path": "scrapy/downloadermiddlewares/useragent.py",
        "class_name": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware",
        "signature": "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if self.user_agent:\n            request.headers.setdefault(b'User-Agent', self.user_agent)",
        "begin_line": 21,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001921598770176787,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager._get_mwlist_from_settings#15",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))",
        "begin_line": 15,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager._add_middleware#18",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager._add_middleware(self, pipe)",
        "snippet": "    def _add_middleware(self, pipe):\n        super(ItemPipelineManager, self)._add_middleware(pipe)\n        if hasattr(pipe, 'process_item'):\n            self.methods['process_item'].append(pipe.process_item)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.__init__.ItemPipelineManager.process_item#23",
        "src_path": "scrapy/pipelines/__init__.py",
        "class_name": "scrapy.pipelines.__init__.ItemPipelineManager",
        "signature": "scrapy.pipelines.__init__.ItemPipelineManager.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        return self._process_chain('process_item', item, spider)",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002632964718272775,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.SpiderInfo.__init__#24",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.SpiderInfo",
        "signature": "scrapy.pipelines.media.SpiderInfo.__init__(self, spider)",
        "snippet": "        def __init__(self, spider):\n            self.spider = spider\n            self.downloading = set()\n            self.downloaded = {}\n            self.waiting = defaultdict(list)",
        "begin_line": 24,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024242424242424242,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.__init__#30",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.__init__(self, download_func=None, settings=None)",
        "snippet": "    def __init__(self, download_func=None, settings=None):\n        self.download_func = download_func\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=\"MediaPipeline\",\n                                    settings=settings)\n        self.allow_redirects = settings.getbool(\n            resolve('MEDIA_ALLOW_REDIRECTS'), False\n        )\n        self._handle_statuses(self.allow_redirects)",
        "begin_line": 30,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020275750202757503,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._handle_statuses#43",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._handle_statuses(self, allow_redirects)",
        "snippet": "    def _handle_statuses(self, allow_redirects):\n        self.handle_httpstatus_list = None\n        if allow_redirects:\n            self.handle_httpstatus_list = SequenceExclude(range(300, 400))",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._key_for_pipe#48",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._key_for_pipe(self, key, base_class_name=None, settings=None)",
        "snippet": "    def _key_for_pipe(self, key, base_class_name=None,\n                      settings=None):\n        \"\"\"\n        >>> MediaPipeline()._key_for_pipe(\"IMAGES\")\n        'IMAGES'\n        >>> class MyPipe(MediaPipeline):\n        ...     pass\n        >>> MyPipe()._key_for_pipe(\"IMAGES\", base_class_name=\"MediaPipeline\")\n        'MYPIPE_IMAGES'\n        \"\"\"\n        class_name = self.__class__.__name__\n        formatted_key = \"{}_{}\".format(class_name.upper(), key)\n        if class_name == base_class_name or not base_class_name \\\n            or (settings and not settings.get(formatted_key)):\n            return key\n        return formatted_key",
        "begin_line": 48,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.from_crawler#66",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        try:\n            pipe = cls.from_settings(crawler.settings)\n        except AttributeError:\n            pipe = cls()\n        pipe.crawler = crawler\n        return pipe",
        "begin_line": 66,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.open_spider#74",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024242424242424242,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.process_item#77",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=1)\n        return dfd.addCallback(self.item_completed, item, info)",
        "begin_line": 77,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._process_request#84",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._process_request(self, request, info)",
        "snippet": "    def _process_request(self, request, info):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info)\n        dfd.addCallback(self._check_media_to_download, request, info)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)  # it must return wad at last",
        "begin_line": 84,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._modify_media_request#113",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._modify_media_request(self, request)",
        "snippet": "    def _modify_media_request(self, request):\n        if self.handle_httpstatus_list:\n            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n        else:\n            request.meta['handle_httpstatus_all'] = True",
        "begin_line": 113,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._check_media_to_download#119",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._check_media_to_download(self, result, request, info)",
        "snippet": "    def _check_media_to_download(self, result, request, info):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            self._modify_media_request(request)\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd",
        "begin_line": 119,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters#136",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters(self, result, fp, info)",
        "snippet": "    def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n\n            # This code fixes a memory leak by avoiding to keep references to\n            # the Request and Response objects on the Media Pipeline cache.\n            #\n            # Twisted inline callbacks pass return values using the function\n            # twisted.internet.defer.returnValue, which encapsulates the return\n            # value inside a _DefGen_Return base exception.\n            #\n            # What happens when the media_downloaded callback raises another\n            # exception, for example a FileException('download-error') when\n            # the Response status code is not 200 OK, is that it stores the\n            # _DefGen_Return exception on the FileException context.\n            #\n            # To avoid keeping references to the Response and therefore Request\n            # objects on the Media Pipeline cache, we should wipe the context of\n            # the exception encapsulated by the Twisted Failure when its a\n            # _DefGen_Return instance.\n            #\n            # This problem does not occur in Python 2.7 since we don't have\n            # Exception Chaining (https://www.python.org/dev/peps/pep-3134/).\n            context = getattr(result.value, '__context__', None)\n            if isinstance(context, _DefGen_Return):\n                setattr(result.value, '__context__', None)\n\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)",
        "begin_line": 136,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_to_download#172",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        \"\"\"Check request before starting download\"\"\"\n        pass",
        "begin_line": 172,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.get_media_requests#176",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass",
        "begin_line": 176,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_downloaded#180",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        \"\"\"Handler for success downloads\"\"\"\n        return response",
        "begin_line": 180,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_failed#184",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_failed(self, failure, request, info)",
        "snippet": "    def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure",
        "begin_line": 184,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.item_completed#188",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item",
        "begin_line": 188,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__init__#20",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None, flags=None, cb_kwargs=None)",
        "snippet": "    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None, flags=None, cb_kwargs=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        if callback is not None and not callable(callback):\n            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)\n        if errback is not None and not callable(errback):\n            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None\n        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 20,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02631578947368421,
            "pseudo_dstar_susp": 0.05555555555555555,
            "pseudo_tarantula_susp": 0.022222222222222223,
            "pseudo_op2_susp": 0.05555555555555555,
            "pseudo_barinel_susp": 0.022222222222222223
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.cb_kwargs#48",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.cb_kwargs(self)",
        "snippet": "    def cb_kwargs(self):\n        if self._cb_kwargs is None:\n            self._cb_kwargs = {}\n        return self._cb_kwargs",
        "begin_line": 48,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001738828029907842,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.meta#54",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.meta(self)",
        "snippet": "    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta",
        "begin_line": 54,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015837820715869496,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_url#59",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001549426712116517,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_url#62",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        s = safe_url_string(url, self.encoding)\n        self._url = escape_ajax(s)\n\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)",
        "begin_line": 62,
        "end_line": 70,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 1.0,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 1.0
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_body#74",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015800284405119292,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_body#77",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)",
        "begin_line": 77,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.045454545454545456,
            "pseudo_dstar_susp": 0.5,
            "pseudo_tarantula_susp": 0.034482758620689655,
            "pseudo_op2_susp": 0.5,
            "pseudo_barinel_susp": 0.034482758620689655
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.encoding#86",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._encoding",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02631578947368421,
            "pseudo_dstar_susp": 0.05555555555555555,
            "pseudo_tarantula_susp": 0.022222222222222223,
            "pseudo_op2_susp": 0.05555555555555555,
            "pseudo_barinel_susp": 0.022222222222222223
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__str__#89",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017803097739006588,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.copy#94",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()",
        "begin_line": 94,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002535496957403651,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.replace#98",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', 'flags',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback', 'cb_kwargs']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 98,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.from_curl#109",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs)",
        "snippet": "    def from_curl(cls, curl_command, ignore_unknown_options=True, **kwargs):\n        \"\"\"Create a Request object from a string containing a `cURL\n        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n        URL, the headers, the cookies and the body. It accepts the same\n        arguments as the :class:`Request` class, taking preference and\n        overriding the values of the same arguments contained in the cURL\n        command.\n\n        Unrecognized options are ignored by default. To raise an error when\n        finding unknown options call this method by passing\n        ``ignore_unknown_options=False``.\n\n        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                     :ref:`downloader middlewares <topics-downloader-middleware>`\n                     and\n                     :ref:`spider middlewares <topics-spider-middleware>`\n                     enabled, such as\n                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n                     or\n                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n                     may modify the :class:`~scrapy.http.Request` object.\n\n       \"\"\"\n        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n        request_kwargs.update(kwargs)\n        return cls(**request_kwargs)",
        "begin_line": 109,
        "end_line": 137,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.rpc.XmlRpcRequest.__init__#18",
        "src_path": "scrapy/http/request/rpc.py",
        "class_name": "scrapy.http.request.rpc.XmlRpcRequest",
        "signature": "scrapy.http.request.rpc.XmlRpcRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        encoding = kwargs.get('encoding', None)\n        if 'body' not in kwargs and 'params' in kwargs:\n            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n            kwargs['body'] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault('method', 'POST')\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault('dont_filter', True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs['encoding'] = encoding\n\n        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n        self.headers.setdefault('Content-Type', 'text/xml')",
        "begin_line": 18,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.2,
            "pseudo_dstar_susp": 0.016129032258064516,
            "pseudo_tarantula_susp": 0.2,
            "pseudo_op2_susp": 0.016129032258064516,
            "pseudo_barinel_susp": 0.2
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__#25",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n            raise NotConfigured\n        self._default_useragent = crawler.settings.get('USER_AGENT', 'Scrapy')\n        self._robotstxt_useragent = crawler.settings.get('ROBOTSTXT_USER_AGENT', None)\n        self.crawler = crawler\n        self._parsers = {}\n        self._parserimpl = load_object(crawler.settings.get('ROBOTSTXT_PARSER'))\n\n        # check if parser dependencies are met, this should throw an error otherwise.\n        self._parserimpl.from_crawler(self.crawler, b'')",
        "begin_line": 25,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.from_crawler#38",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request#41",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        d = maybeDeferred(self.robot_parser, request, spider)\n        d.addCallback(self.process_request_2, request, spider)\n        return d",
        "begin_line": 41,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request_2#48",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request_2(self, rp, request, spider)",
        "snippet": "    def process_request_2(self, rp, request, spider):\n        if rp is None:\n            return\n\n        useragent = self._robotstxt_useragent\n        if not useragent:\n            useragent = request.headers.get(b'User-Agent', self._default_useragent)\n        if not rp.allowed(request.url, useragent):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            self.crawler.stats.inc_value('robotstxt/forbidden')\n            raise IgnoreRequest(\"Forbidden by robots.txt\")",
        "begin_line": 48,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser#61",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser(self, request, spider)",
        "snippet": "    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n\n        if netloc not in self._parsers:\n            self._parsers[netloc] = Deferred()\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots, netloc, spider)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n            dfd.addErrback(self._robots_error, netloc)\n            self.crawler.stats.inc_value('robotstxt/request_count')\n\n        if isinstance(self._parsers[netloc], Deferred):\n            d = Deferred()\n\n            def cb(result):\n                d.callback(result)\n                return result\n            self._parsers[netloc].addCallback(cb)\n            return d\n        else:\n            return self._parsers[netloc]",
        "begin_line": 61,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.cb#82",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.cb(result)",
        "snippet": "            def cb(result):\n                d.callback(result)\n                return result",
        "begin_line": 82,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._logerror#90",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._logerror(self, failure, request, spider)",
        "snippet": "    def _logerror(self, failure, request, spider):\n        if failure.type is not IgnoreRequest:\n            logger.error(\"Error downloading %(request)s: %(f_exception)s\",\n                         {'request': request, 'f_exception': failure.value},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 90,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots#98",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots(self, response, netloc, spider)",
        "snippet": "    def _parse_robots(self, response, netloc, spider):\n        self.crawler.stats.inc_value('robotstxt/response_count')\n        self.crawler.stats.inc_value('robotstxt/response_status_count/{}'.format(response.status))\n        rp = self._parserimpl.from_crawler(self.crawler, response.body)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = rp\n        rp_dfd.callback(rp)",
        "begin_line": 98,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._robots_error#106",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._robots_error(self, failure, netloc)",
        "snippet": "    def _robots_error(self, failure, netloc):\n        if failure.type is not IgnoreRequest:\n            key = 'robotstxt/exception_count/{}'.format(failure.type)\n            self.crawler.stats.inc_value(key)\n        rp_dfd = self._parsers[netloc]\n        self._parsers[netloc] = None\n        rp_dfd.callback(None)",
        "begin_line": 106,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.spider.iterate_spider_output#12",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iterate_spider_output(result)",
        "snippet": "def iterate_spider_output(result):\n    return arg_to_iter(result)",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00021843599825251202,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.spider.iter_spider_classes#16",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iter_spider_classes(module)",
        "snippet": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (ie. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in six.itervalues(vars(module)):\n        if inspect.isclass(obj) and \\\n           issubclass(obj, Spider) and \\\n           obj.__module__ == module.__name__ and \\\n           getattr(obj, 'name', None):\n            yield obj",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.process_results#28",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.process_results(self, response, results)",
        "snippet": "    def process_results(self, response, results):\n        \"\"\"This overridable method is called for each result (item or request)\n        returned by the spider, and it's intended to perform any last time\n        processing required before returning the results to the framework core,\n        for example setting the item GUIDs. It receives a list of results and\n        the response which originated that results. It must return a list of\n        results (Items or Requests).\n        \"\"\"\n        return results",
        "begin_line": 28,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.adapt_response#38",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.adapt_response(self, response)",
        "snippet": "    def adapt_response(self, response):\n        \"\"\"You can override this function in order to make any changes you want\n        to into the feed before parsing it. This function must return a\n        response.\n        \"\"\"\n        return response",
        "begin_line": 38,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.parse_nodes#51",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.parse_nodes(self, response, nodes)",
        "snippet": "    def parse_nodes(self, response, nodes):\n        \"\"\"This method is called for the nodes matching the provided tag name\n        (itertag). Receives the response and an Selector for each node.\n        Overriding this method is mandatory. Otherwise, you spider won't work.\n        This method must return either a BaseItem, a Request, or a list\n        containing any of them.\n        \"\"\"\n\n        for selector in nodes:\n            ret = iterate_spider_output(self.parse_node(response, selector))\n            for result_item in self.process_results(response, ret):\n                yield result_item",
        "begin_line": 51,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider.parse#64",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider.parse(self, response)",
        "snippet": "    def parse(self, response):\n        if not hasattr(self, 'parse_node'):\n            raise NotConfigured('You must define parse_node method in order to scrape this XML feed')\n\n        response = self.adapt_response(response)\n        if self.iterator == 'iternodes':\n            nodes = self._iternodes(response)\n        elif self.iterator == 'xml':\n            selector = Selector(response, type='xml')\n            self._register_namespaces(selector)\n            nodes = selector.xpath('//%s' % self.itertag)\n        elif self.iterator == 'html':\n            selector = Selector(response, type='html')\n            self._register_namespaces(selector)\n            nodes = selector.xpath('//%s' % self.itertag)\n        else:\n            raise NotSupported('Unsupported node iterator')\n\n        return self.parse_nodes(response, nodes)",
        "begin_line": 64,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider._iternodes#84",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider._iternodes(self, response)",
        "snippet": "    def _iternodes(self, response):\n        for node in xmliter(response, self.itertag):\n            self._register_namespaces(node)\n            yield node",
        "begin_line": 84,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.feed.XMLFeedSpider._register_namespaces#89",
        "src_path": "scrapy/spiders/feed.py",
        "class_name": "scrapy.spiders.feed.XMLFeedSpider",
        "signature": "scrapy.spiders.feed.XMLFeedSpider._register_namespaces(self, selector)",
        "snippet": "    def _register_namespaces(self, selector):\n        for (prefix, uri) in self.namespaces:\n            selector.register_namespace(prefix, uri)",
        "begin_line": 89,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.stripped_referrer#39",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.stripped_referrer(self, url)",
        "snippet": "    def stripped_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.strip_url(url)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.origin_referrer#43",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.origin_referrer(self, url)",
        "snippet": "    def origin_referrer(self, url):\n        if urlparse(url).scheme not in self.NOREFERRER_SCHEMES:\n            return self.origin(url)",
        "begin_line": 43,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.strip_url#47",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.strip_url(self, url, origin_only=False)",
        "snippet": "    def strip_url(self, url, origin_only=False):\n        \"\"\"\n        https://www.w3.org/TR/referrer-policy/#strip-url\n\n        If url is null, return no referrer.\n        If url's scheme is a local scheme, then return no referrer.\n        Set url's username to the empty string.\n        Set url's password to null.\n        Set url's fragment to null.\n        If the origin-only flag is true, then:\n            Set url's path to null.\n            Set url's query to null.\n        Return url.\n        \"\"\"\n        if not url:\n            return None\n        return strip_url(url,\n                         strip_credentials=True,\n                         strip_fragment=True,\n                         strip_default_port=True,\n                         origin_only=origin_only)",
        "begin_line": 47,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023730422401518748,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.origin#69",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.origin(self, url)",
        "snippet": "    def origin(self, url):\n        \"\"\"Return serialized origin (scheme, host, path) for a request or response URL.\"\"\"\n        return self.strip_url(url, origin_only=True)",
        "begin_line": 69,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.potentially_trustworthy#73",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.potentially_trustworthy(self, url)",
        "snippet": "    def potentially_trustworthy(self, url):\n        # Note: this does not follow https://w3c.github.io/webappsec-secure-contexts/#is-url-trustworthy\n        parsed_url = urlparse(url)\n        if parsed_url.scheme in ('data',):\n            return False\n        return self.tls_protected(url)",
        "begin_line": 73,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.ReferrerPolicy.tls_protected#80",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.ReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.ReferrerPolicy.tls_protected(self, url)",
        "snippet": "    def tls_protected(self, url):\n        return urlparse(url).scheme in ('https', 'ftps')",
        "begin_line": 80,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00025839793281653745,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.NoReferrerPolicy.referrer#94",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.NoReferrerPolicy",
        "signature": "scrapy.spidermiddlewares.referer.NoReferrerPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        return None",
        "begin_line": 94,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy.referrer#114",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy",
        "signature": "scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        if not self.tls_protected(response_url) or self.tls_protected(request_url):\n            return self.stripped_referrer(response_url)",
        "begin_line": 114,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.SameOriginPolicy.referrer#131",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.SameOriginPolicy",
        "signature": "scrapy.spidermiddlewares.referer.SameOriginPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        if self.origin(response_url) == self.origin(request_url):\n            return self.stripped_referrer(response_url)",
        "begin_line": 131,
        "end_line": 133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.OriginPolicy.referrer#147",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.OriginPolicy",
        "signature": "scrapy.spidermiddlewares.referer.OriginPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        return self.origin_referrer(response_url)",
        "begin_line": 147,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.StrictOriginPolicy.referrer#166",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.StrictOriginPolicy",
        "signature": "scrapy.spidermiddlewares.referer.StrictOriginPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        if ((self.tls_protected(response_url) and\n             self.potentially_trustworthy(request_url))\n            or not self.tls_protected(response_url)):\n            return self.origin_referrer(response_url)",
        "begin_line": 166,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy.referrer#186",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy",
        "signature": "scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        else:\n            return origin",
        "begin_line": 186,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy.referrer#213",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy",
        "signature": "scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        origin = self.origin(response_url)\n        if origin == self.origin(request_url):\n            return self.stripped_referrer(response_url)\n        elif ((self.tls_protected(response_url) and\n               self.potentially_trustworthy(request_url))\n              or not self.tls_protected(response_url)):\n            return self.origin_referrer(response_url)",
        "begin_line": 213,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.UnsafeUrlPolicy.referrer#238",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.UnsafeUrlPolicy",
        "signature": "scrapy.spidermiddlewares.referer.UnsafeUrlPolicy.referrer(self, response_url, request_url)",
        "snippet": "    def referrer(self, response_url, request_url):\n        return self.stripped_referrer(response_url)",
        "begin_line": 238,
        "end_line": 239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer._load_policy_class#268",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer",
        "signature": "scrapy.spidermiddlewares.referer._load_policy_class(policy, warning_only=False)",
        "snippet": "def _load_policy_class(policy, warning_only=False):\n    \"\"\"\n    Expect a string for the path to the policy class,\n    otherwise try to interpret the string as a standard value\n    from https://www.w3.org/TR/referrer-policy/#referrer-policies\n    \"\"\"\n    try:\n        return load_object(policy)\n    except ValueError:\n        try:\n            return _policy_classes[policy.lower()]\n        except KeyError:\n            msg = \"Could not load referrer policy %r\" % policy\n            if not warning_only:\n                raise RuntimeError(msg)\n            else:\n                warnings.warn(msg, RuntimeWarning)\n                return None",
        "begin_line": 268,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.__init__#290",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.__init__(self, settings=None)",
        "snippet": "    def __init__(self, settings=None):\n        self.default_policy = DefaultReferrerPolicy\n        if settings is not None:\n            self.default_policy = _load_policy_class(\n                settings.get('REFERRER_POLICY'))",
        "begin_line": 290,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017583963425356076,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.from_crawler#297",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('REFERER_ENABLED'):\n            raise NotConfigured\n        mw = cls(crawler.settings)\n\n        # Note: this hook is a bit of a hack to intercept redirections\n        crawler.signals.connect(mw.request_scheduled, signal=signals.request_scheduled)\n\n        return mw",
        "begin_line": 297,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.policy#307",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.policy(self, resp_or_url, request)",
        "snippet": "    def policy(self, resp_or_url, request):\n        \"\"\"\n        Determine Referrer-Policy to use from a parent Response (or URL),\n        and a Request to be sent.\n\n        - if a valid policy is set in Request meta, it is used.\n        - if the policy is set in meta but is wrong (e.g. a typo error),\n          the policy from settings is used\n        - if the policy is not set in Request meta,\n          but there is a Referrer-policy header in the parent response,\n          it is used if valid\n        - otherwise, the policy from settings is used.\n        \"\"\"\n        policy_name = request.meta.get('referrer_policy')\n        if policy_name is None:\n            if isinstance(resp_or_url, Response):\n                policy_header = resp_or_url.headers.get('Referrer-Policy')\n                if policy_header is not None:\n                    policy_name = to_native_str(policy_header.decode('latin1'))\n        if policy_name is None:\n            return self.default_policy()\n\n        cls = _load_policy_class(policy_name, warning_only=True)\n        return cls() if cls else self.default_policy()",
        "begin_line": 307,
        "end_line": 330,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.process_spider_output#332",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r\n        return (_set_referer(r) for r in result or ())",
        "begin_line": 332,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018060321473722233,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware._set_referer#333",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware._set_referer(r)",
        "snippet": "        def _set_referer(r):\n            if isinstance(r, Request):\n                referrer = self.policy(response, r).referrer(response.url, r.url)\n                if referrer is not None:\n                    r.headers.setdefault('Referer', referrer)\n            return r",
        "begin_line": 333,
        "end_line": 338,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000235626767200754,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.referer.RefererMiddleware.request_scheduled#341",
        "src_path": "scrapy/spidermiddlewares/referer.py",
        "class_name": "scrapy.spidermiddlewares.referer.RefererMiddleware",
        "signature": "scrapy.spidermiddlewares.referer.RefererMiddleware.request_scheduled(self, request, spider)",
        "snippet": "    def request_scheduled(self, request, spider):\n        # check redirected request to patch \"Referer\" header if necessary\n        redirected_urls = request.meta.get('redirect_urls', [])\n        if redirected_urls:\n            request_referrer = request.headers.get('Referer')\n            # we don't patch the referrer value if there is none\n            if request_referrer is not None:\n                # the request's referrer header value acts as a surrogate\n                # for the parent response URL\n                #\n                # Note: if the 3xx response contained a Referrer-Policy header,\n                #       the information is not available using this hook\n                parent_url = safe_url_string(request_referrer)\n                policy_referrer = self.policy(parent_url, request).referrer(\n                    parent_url, request.url)\n                if policy_referrer != request_referrer:\n                    if policy_referrer is None:\n                        request.headers.pop('Referer')\n                    else:\n                        request.headers['Referer'] = policy_referrer",
        "begin_line": 341,
        "end_line": 360,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest.execute#14",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest.execute(self, args, check_code=True, settings=None)",
        "snippet": "    def execute(self, args, check_code=True, settings=None):\n        env = os.environ.copy()\n        if settings is not None:\n            env['SCRAPY_SETTINGS_MODULE'] = settings\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest._process_finished#24",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest._process_finished(self, pp, cmd, check_code)",
        "snippet": "    def _process_finished(self, pp, cmd, check_code):\n        if pp.exitcode and check_code:\n            msg = \"process %s exit with code %d\" % (cmd, pp.exitcode)\n            msg += \"\\n>>> stdout <<<\\n%s\" % pp.out\n            msg += \"\\n\"\n            msg += \"\\n>>> stderr <<<\\n%s\" % pp.err\n            raise RuntimeError(msg)\n        return pp.exitcode, pp.out, pp.err",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.__init__#36",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.__init__(self)",
        "snippet": "    def __init__(self):\n        self.deferred = defer.Deferred()\n        self.out = b''\n        self.err = b''\n        self.exitcode = None",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.outReceived#42",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.outReceived(self, data)",
        "snippet": "    def outReceived(self, data):\n        self.out += data",
        "begin_line": 42,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000249500998003992,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.errReceived#45",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.errReceived(self, data)",
        "snippet": "    def errReceived(self, data):\n        self.err += data",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.processEnded#48",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.processEnded(self, status)",
        "snippet": "    def processEnded(self, status):\n        self.exitcode = status.value.exitCode\n        self.deferred.callback(self)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.ssl.ffi_buf_to_string#14",
        "src_path": "scrapy/utils/ssl.py",
        "class_name": "scrapy.utils.ssl",
        "signature": "scrapy.utils.ssl.ffi_buf_to_string(buf)",
        "snippet": "def ffi_buf_to_string(buf):\n    return to_native_str(pyOpenSSLutil.ffi.string(buf))",
        "begin_line": 14,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.ssl.x509name_to_string#18",
        "src_path": "scrapy/utils/ssl.py",
        "class_name": "scrapy.utils.ssl",
        "signature": "scrapy.utils.ssl.x509name_to_string(x509name)",
        "snippet": "def x509name_to_string(x509name):\n    # from OpenSSL.crypto.X509Name.__repr__\n    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n\n    return ffi_buf_to_string(result_buffer)",
        "begin_line": 18,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.ssl.get_temp_key_info#26",
        "src_path": "scrapy/utils/ssl.py",
        "class_name": "scrapy.utils.ssl",
        "signature": "scrapy.utils.ssl.get_temp_key_info(ssl_object)",
        "snippet": "def get_temp_key_info(ssl_object):\n    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n        return None\n\n    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n    if not pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p):\n        return None\n    temp_key = temp_key_p[0]\n    if temp_key == pyOpenSSLutil.ffi.NULL:\n        return None\n    temp_key = pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n    key_info = []\n    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n        key_info.append('RSA')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n        key_info.append('DH')\n    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n        key_info.append('ECDH')\n        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n        ec_key = pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n        if cname == pyOpenSSLutil.ffi.NULL:\n            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n        key_info.append(ffi_buf_to_string(cname))\n    else:\n        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n    key_info.append('%s bits' % pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key))\n    return ', '.join(key_info)",
        "begin_line": 26,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.ssl.get_openssl_version#59",
        "src_path": "scrapy/utils/ssl.py",
        "class_name": "scrapy.utils.ssl",
        "signature": "scrapy.utils.ssl.get_openssl_version()",
        "snippet": "def get_openssl_version():\n    system_openssl = OpenSSL.SSL.SSLeay_version(\n        OpenSSL.SSL.SSLEAY_VERSION\n    ).decode('ascii', errors='replace')\n    return '{} ({})'.format(OpenSSL.version.__version__, system_openssl)",
        "begin_line": 59,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpError.__init__#16",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpError",
        "signature": "scrapy.spidermiddlewares.httperror.HttpError.__init__(self, response, *args, **kwargs)",
        "snippet": "    def __init__(self, response, *args, **kwargs):\n        self.response = response\n        super(HttpError, self).__init__(*args, **kwargs)",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.from_crawler#24",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 24,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.__init__#27",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')\n        self.handle_httpstatus_list = settings.getlist('HTTPERROR_ALLOWED_CODES')",
        "begin_line": 27,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001816860465116279,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_input#31",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_input(self, response, spider)",
        "snippet": "    def process_spider_input(self, response, spider):\n        if 200 <= response.status < 300:  # common case\n            return\n        meta = response.meta\n        if 'handle_httpstatus_all' in meta:\n            return\n        if 'handle_httpstatus_list' in meta:\n            allowed_statuses = meta['handle_httpstatus_list']\n        elif self.handle_httpstatus_all:\n            return\n        else:\n            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)\n        if response.status in allowed_statuses:\n            return\n        raise HttpError(response, 'Ignoring non-200 response')",
        "begin_line": 31,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_exception#47",
        "src_path": "scrapy/spidermiddlewares/httperror.py",
        "class_name": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware",
        "signature": "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware.process_spider_exception(self, response, exception, spider)",
        "snippet": "    def process_spider_exception(self, response, exception, spider):\n        if isinstance(exception, HttpError):\n            spider.crawler.stats.inc_value('httperror/response_ignored_count')\n            spider.crawler.stats.inc_value(\n                'httperror/response_ignored_status_count/%s' % response.status\n            )\n            logger.info(\n                \"Ignoring response %(response)r: HTTP status code is not handled or not allowed\",\n                {'response': response}, extra={'spider': spider},\n            )\n            return []",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.common.wrap_loader_context#6",
        "src_path": "scrapy/loader/common.py",
        "class_name": "scrapy.loader.common",
        "signature": "scrapy.loader.common.wrap_loader_context(function, context)",
        "snippet": "def wrap_loader_context(function, context):\n    \"\"\"Wrap functions that receive loader_context to contain the context\n    \"pre-loaded\" and expose a interface that receives only one argument\n    \"\"\"\n    if 'loader_context' in get_func_args(function):\n        return partial(function, loader_context=context)\n    else:\n        return function",
        "begin_line": 6,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__(self, debug=False)",
        "snippet": "    def __init__(self, debug=False):\n        self.jars = defaultdict(CookieJar)\n        self.debug = debug",
        "begin_line": 17,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler#22",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COOKIES_ENABLED'):\n            raise NotConfigured\n        return cls(crawler.settings.getbool('COOKIES_DEBUG'))",
        "begin_line": 22,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request#27",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        for cookie in cookies:\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)",
        "begin_line": 27,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response#42",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response",
        "begin_line": 42,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie#54",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie(self, request, spider)",
        "snippet": "    def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in request.headers.getlist('Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Sending cookies to: {}\\n{}\".format(request, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 54,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie#63",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie(self, response, spider)",
        "snippet": "    def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in response.headers.getlist('Set-Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 63,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie#72",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie(self, cookie)",
        "snippet": "    def _format_cookie(self, cookie):\n        # build cookie string\n        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n\n        if cookie.get('path', None):\n            cookie_str += '; Path=%s' % cookie['path']\n        if cookie.get('domain', None):\n            cookie_str += '; Domain=%s' % cookie['domain']\n\n        return cookie_str",
        "begin_line": 72,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies#83",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies(self, jar, request)",
        "snippet": "    def _get_request_cookies(self, jar, request):\n        if isinstance(request.cookies, dict):\n            cookie_list = [{'name': k, 'value': v} for k, v in \\\n                    six.iteritems(request.cookies)]\n        else:\n            cookie_list = request.cookies\n\n        cookies = [self._format_cookie(x) for x in cookie_list]\n        headers = {'Set-Cookie': cookies}\n        response = Response(request.url, headers=headers)\n\n        return jar.make_cookies(response, request)",
        "begin_line": 83,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.__init__#26",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._encoding = kwargs.pop('encoding', None)\n        self._cached_benc = None\n        self._cached_ubody = None\n        self._cached_selector = None\n        super(TextResponse, self).__init__(*args, **kwargs)",
        "begin_line": 26,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000157035175879397,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_url#33",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, six.text_type):\n            if six.PY2 and self.encoding is None:\n                raise TypeError(\"Cannot convert unicode url - %s \"\n                                \"has no encoding\" % type(self).__name__)\n            self._url = to_native_str(url, self.encoding)\n        else:\n            super(TextResponse, self)._set_url(url)",
        "begin_line": 33,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_body#42",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, six.text_type):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                    type(self).__name__)\n            self._body = body.encode(self._encoding)\n        else:\n            super(TextResponse, self)._set_body(body)",
        "begin_line": 42,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.replace#52",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)",
        "begin_line": 52,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.encoding#57",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()",
        "begin_line": 57,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015661707126076742,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._declared_encoding#60",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._declared_encoding(self)",
        "snippet": "    def _declared_encoding(self):\n        return self._encoding or self._headers_encoding() \\\n            or self._body_declared_encoding()",
        "begin_line": 60,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001584786053882726,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.body_as_unicode#64",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.body_as_unicode(self)",
        "snippet": "    def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        return self.text",
        "begin_line": 64,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.text#69",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.text(self)",
        "snippet": "    def text(self):\n        \"\"\" Body as unicode \"\"\"\n        # access self.encoding before _cached_ubody to make sure\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = 'charset=%s' % benc\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody",
        "begin_line": 69,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017292062943109114,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.urljoin#79",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023518344308560678,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._headers_encoding#85",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._headers_encoding(self)",
        "snippet": "    def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_native_str(content_type))",
        "begin_line": 85,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015822784810126583,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_inferred_encoding#89",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_inferred_encoding(self)",
        "snippet": "    def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                    auto_detect_fun=self._auto_detect_fun,\n                    default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc",
        "begin_line": 89,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015875535799333228,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._auto_detect_fun#99",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._auto_detect_fun(self, text)",
        "snippet": "    def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)",
        "begin_line": 99,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_declared_encoding#108",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_declared_encoding(self)",
        "snippet": "    def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001584786053882726,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.selector#112",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.selector(self)",
        "snippet": "    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector",
        "begin_line": 112,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001779042874933286,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.xpath#118",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.xpath(self, query, **kwargs)",
        "snippet": "    def xpath(self, query, **kwargs):\n        return self.selector.xpath(query, **kwargs)",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002550369803621525,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.css#121",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.selector.css(query)",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002632964718272775,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.follow#124",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.follow(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding=None, priority=0, dont_filter=False, errback=None, cb_kwargs=None)",
        "snippet": "    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding=None, priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be not only an absolute URL, but also\n        \n        * a relative URL;\n        * a scrapy.link.Link object (e.g. a link extractor result);\n        * an attribute Selector (not SelectorList) - e.g.\n          ``response.css('a::attr(href)')[0]`` or\n          ``response.xpath('//img/@src')[0]``.\n        * a Selector for ``<a>`` or ``<link>`` element, e.g.\n          ``response.css('a.my_link')[0]``.\n          \n        See :ref:`response-follow-example` for usage examples.\n        \"\"\"\n        if isinstance(url, parsel.Selector):\n            url = _url_from_selector(url)\n        elif isinstance(url, parsel.SelectorList):\n            raise ValueError(\"SelectorList is not supported\")\n        encoding = self.encoding if encoding is None else encoding\n        return super(TextResponse, self).follow(url, callback,\n            method=method,\n            headers=headers,\n            body=body,\n            cookies=cookies,\n            meta=meta,\n            encoding=encoding,\n            priority=priority,\n            dont_filter=dont_filter,\n            errback=errback,\n            cb_kwargs=cb_kwargs,\n        )",
        "begin_line": 124,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.text._url_from_selector#162",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text",
        "signature": "scrapy.http.response.text._url_from_selector(sel)",
        "snippet": "def _url_from_selector(sel):\n    # type: (parsel.Selector) -> str\n    if isinstance(sel.root, six.string_types):\n        # e.g. ::attr(href) result\n        return strip_html5_whitespace(sel.root)\n    if not hasattr(sel.root, 'tag'):\n        raise ValueError(\"Unsupported selector: %s\" % sel)\n    if sel.root.tag not in ('a', 'link'):\n        raise ValueError(\"Only <a> and <link> elements are supported; got <%s>\" %\n                         sel.root.tag)\n    href = sel.root.get('href')\n    if href is None:\n        raise ValueError(\"<%s> element has no href attribute: %s\" %\n                         (sel.root.tag, sel))\n    return strip_html5_whitespace(href)",
        "begin_line": 162,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.serialize.ScrapyJSONEncoder.default#16",
        "src_path": "scrapy/utils/serialize.py",
        "class_name": "scrapy.utils.serialize.ScrapyJSONEncoder",
        "signature": "scrapy.utils.serialize.ScrapyJSONEncoder.default(self, o)",
        "snippet": "    def default(self, o):\n        if isinstance(o, set):\n            return list(o)\n        elif isinstance(o, datetime.datetime):\n            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif isinstance(o, BaseItem):\n            return dict(o)\n        elif isinstance(o, Request):\n            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n        elif isinstance(o, Response):\n            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n        else:\n            return super(ScrapyJSONEncoder, self).default(o)",
        "begin_line": 16,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.from_crawler#17",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls()\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.spider_opened#22",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        usr = getattr(spider, 'http_user', '')\n        pwd = getattr(spider, 'http_pass', '')\n        if usr or pwd:\n            self.auth = basic_auth_header(usr, pwd)",
        "begin_line": 22,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.process_request#28",
        "src_path": "scrapy/downloadermiddlewares/httpauth.py",
        "class_name": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        auth = getattr(self, 'auth', None)\n        if auth and b'Authorization' not in request.headers:\n            request.headers[b'Authorization'] = auth",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider.__init__#21",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider.__init__(self, *a, **kw)",
        "snippet": "    def __init__(self, *a, **kw):\n        super(SitemapSpider, self).__init__(*a, **kw)\n        self._cbs = []\n        for r, c in self.sitemap_rules:\n            if isinstance(c, six.string_types):\n                c = getattr(self, c)\n            self._cbs.append((regex(r), c))\n        self._follow = [regex(x) for x in self.sitemap_follow]",
        "begin_line": 21,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider.start_requests#30",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        for url in self.sitemap_urls:\n            yield Request(url, self._parse_sitemap)",
        "begin_line": 30,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider.sitemap_filter#34",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider.sitemap_filter(self, entries)",
        "snippet": "    def sitemap_filter(self, entries):\n        \"\"\"This method can be used to filter sitemap entries by their\n        attributes, for example, you can filter locs with lastmod greater\n        than a given date (see docs).\n        \"\"\"\n        for entry in entries:\n            yield entry",
        "begin_line": 34,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider._parse_sitemap#42",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider._parse_sitemap(self, response)",
        "snippet": "    def _parse_sitemap(self, response):\n        if response.url.endswith('/robots.txt'):\n            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                yield Request(url, callback=self._parse_sitemap)\n        else:\n            body = self._get_sitemap_body(response)\n            if body is None:\n                logger.warning(\"Ignoring invalid sitemap: %(response)s\",\n                               {'response': response}, extra={'spider': self})\n                return\n\n            s = Sitemap(body)\n            it = self.sitemap_filter(s)\n\n            if s.type == 'sitemapindex':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    if any(x.search(loc) for x in self._follow):\n                        yield Request(loc, callback=self._parse_sitemap)\n            elif s.type == 'urlset':\n                for loc in iterloc(it, self.sitemap_alternate_links):\n                    for r, c in self._cbs:\n                        if r.search(loc):\n                            yield Request(loc, callback=c)\n                            break",
        "begin_line": 42,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.SitemapSpider._get_sitemap_body#67",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap.SitemapSpider",
        "signature": "scrapy.spiders.sitemap.SitemapSpider._get_sitemap_body(self, response)",
        "snippet": "    def _get_sitemap_body(self, response):\n        \"\"\"Return the sitemap body contained in the given response,\n        or None if the response is not a sitemap.\n        \"\"\"\n        if isinstance(response, XmlResponse):\n            return response.body\n        elif gzip_magic_number(response):\n            return gunzip(response.body)\n        # actual gzipped sitemap files are decompressed above ;\n        # if we are here (response body is not gzipped)\n        # and have a response for .xml.gz,\n        # it usually means that it was already gunzipped\n        # by HttpCompression middleware,\n        # the HTTP response being sent with \"Content-Encoding: gzip\"\n        # without actually being a .xml.gz file in the first place,\n        # merely XML gzip-compressed on the fly,\n        # in other word, here, we have plain XML\n        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n            return response.body",
        "begin_line": 67,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.regex#88",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap",
        "signature": "scrapy.spiders.sitemap.regex(x)",
        "snippet": "def regex(x):\n    if isinstance(x, six.string_types):\n        return re.compile(x)\n    return x",
        "begin_line": 88,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.sitemap.iterloc#94",
        "src_path": "scrapy/spiders/sitemap.py",
        "class_name": "scrapy.spiders.sitemap",
        "signature": "scrapy.spiders.sitemap.iterloc(it, alt=False)",
        "snippet": "def iterloc(it, alt=False):\n    for d in it:\n        yield d['loc']\n\n        # Also consider alternate URLs (xhtml:link rel=\"alternate\")\n        if alt and 'alternate' in d:\n            for l in d['alternate']:\n                yield l",
        "begin_line": 94,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.__init__#22",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool('AJAXCRAWL_ENABLED'):\n            raise NotConfigured\n\n        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n        # middleware parses first 4k. 4k turns out to be insufficient\n        # for this middleware, and parsing 100k could be slow.\n        # We use something in between (32K) by default.\n        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)",
        "begin_line": 22,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.from_crawler#33",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001816860465116279,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.process_response#36",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n\n        if not isinstance(response, HtmlResponse) or response.status != 200:\n            return response\n\n        if request.method != 'GET':\n            # other HTTP methods are either not safe or don't have a body\n            return response\n\n        if 'ajax_crawlable' in request.meta:  # prevent loops\n            return response\n\n        if not self._has_ajax_crawlable_variant(response):\n            return response\n\n        # scrapy already handles #! links properly\n        ajax_crawl_request = request.replace(url=request.url+'#!')\n        logger.debug(\"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n                     {'ajax_crawl_request': ajax_crawl_request, 'request': request},\n                     extra={'spider': spider})\n\n        ajax_crawl_request.meta['ajax_crawlable'] = True\n        return ajax_crawl_request",
        "begin_line": 36,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware._has_ajax_crawlable_variant#60",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware._has_ajax_crawlable_variant(self, response)",
        "snippet": "    def _has_ajax_crawlable_variant(self, response):\n        \"\"\"\n        Return True if a page without hash fragment could be \"AJAX crawlable\"\n        according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n        \"\"\"\n        body = response.text[:self.lookup_bytes]\n        return _has_ajaxcrawlable_meta(body)",
        "begin_line": 60,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.ajaxcrawl._has_ajaxcrawlable_meta#71",
        "src_path": "scrapy/downloadermiddlewares/ajaxcrawl.py",
        "class_name": "scrapy.downloadermiddlewares.ajaxcrawl",
        "signature": "scrapy.downloadermiddlewares.ajaxcrawl._has_ajaxcrawlable_meta(text)",
        "snippet": "def _has_ajaxcrawlable_meta(text):\n    \"\"\"\n    >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n    True\n    >>> _has_ajaxcrawlable_meta(\"<html><head><meta name='fragment' content='!'></head></html>\")\n    True\n    >>> _has_ajaxcrawlable_meta('<html><head><!--<meta name=\"fragment\"  content=\"!\"/>--></head><body></body></html>')\n    False\n    >>> _has_ajaxcrawlable_meta('<html></html>')\n    False\n    \"\"\"\n\n    # Stripping scripts and comments is slow (about 20x slower than\n    # just checking if a string is in text); this is a quick fail-fast\n    # path that should work for most pages.\n    if 'fragment' not in text:\n        return False\n    if 'content' not in text:\n        return False\n\n    text = html.remove_tags_with_content(text, ('script', 'noscript'))\n    text = html.replace_entities(text)\n    text = html.remove_comments(text)\n    return _ajax_crawlable_re.search(text) is not None",
        "begin_line": 71,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.__init__#8",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.__init__(self, sender=dispatcher.Anonymous)",
        "snippet": "    def __init__(self, sender=dispatcher.Anonymous):\n        self.sender = sender",
        "begin_line": 8,
        "end_line": 9,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.connect#11",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.connect(self, receiver, signal, **kwargs)",
        "snippet": "    def connect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.connect(receiver, signal, **kwargs)",
        "begin_line": 11,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.send_catch_log#37",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.send_catch_log(self, signal, **kwargs)",
        "snippet": "    def send_catch_log(self, signal, **kwargs):\n        \"\"\"\n        Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log(signal, **kwargs)",
        "begin_line": 37,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001921598770176787,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.send_catch_log_deferred#47",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.send_catch_log_deferred(self, signal, **kwargs)",
        "snippet": "    def send_catch_log_deferred(self, signal, **kwargs):\n        \"\"\"\n        Like :meth:`send_catch_log` but supports returning `deferreds`_ from\n        signal handlers.\n\n        Returns a Deferred that gets fired once all signal handlers\n        deferreds were fired. Send a signal, catch exceptions and log them.\n\n        The keyword arguments are passed to the signal handlers (connected\n        through the :meth:`connect` method).\n\n        .. _deferreds: https://twistedmatrix.com/documents/current/core/howto/defer.html\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return _signal.send_catch_log_deferred(signal, **kwargs)",
        "begin_line": 47,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.__init__#17",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n\n        self.close_on = {\n            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),\n            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),\n            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),\n            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n            }\n\n        if not any(self.close_on.values()):\n            raise NotConfigured\n\n        self.counter = defaultdict(int)\n\n        if self.close_on.get('errorcount'):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get('pagecount'):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get('timeout'):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get('itemcount'):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)",
        "begin_line": 17,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.from_crawler#43",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.error_count#46",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.error_count(self, failure, response, spider)",
        "snippet": "    def error_count(self, failure, response, spider):\n        self.counter['errorcount'] += 1\n        if self.counter['errorcount'] == self.close_on['errorcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_errorcount')",
        "begin_line": 46,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.page_count#51",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.page_count(self, response, request, spider)",
        "snippet": "    def page_count(self, response, request, spider):\n        self.counter['pagecount'] += 1\n        if self.counter['pagecount'] == self.close_on['pagecount']:\n            self.crawler.engine.close_spider(spider, 'closespider_pagecount')",
        "begin_line": 51,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_opened#56",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.task = reactor.callLater(self.close_on['timeout'], \\\n            self.crawler.engine.close_spider, spider, \\\n            reason='closespider_timeout')",
        "begin_line": 56,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.item_scraped#61",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        self.counter['itemcount'] += 1\n        if self.counter['itemcount'] == self.close_on['itemcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_itemcount')",
        "begin_line": 61,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_closed#66",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        task = getattr(self, 'task', False)\n        if task and task.active():\n            task.cancel()",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.mail._to_bytes_or_none#33",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail",
        "signature": "scrapy.mail._to_bytes_or_none(text)",
        "snippet": "def _to_bytes_or_none(text):\n    if text is None:\n        return None\n    return to_bytes(text)",
        "begin_line": 33,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016035920461834508,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.mail.MailSender.__init__#41",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.__init__(self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False)",
        "snippet": "    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n        self.smtphost = smtphost\n        self.smtpport = smtpport\n        self.smtpuser = _to_bytes_or_none(smtpuser)\n        self.smtppass = _to_bytes_or_none(smtppass)\n        self.smtptls = smtptls\n        self.smtpssl = smtpssl\n        self.mailfrom = mailfrom\n        self.debug = debug",
        "begin_line": 41,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016035920461834508,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.mail.MailSender.from_settings#53",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings['MAIL_HOST'], settings['MAIL_FROM'], settings['MAIL_USER'],\n            settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n            settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))",
        "begin_line": 53,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.mail.MailSender.send#58",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None)",
        "snippet": "    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split('/', 1))\n\n        to = list(arg_to_iter(to))\n        cc = list(arg_to_iter(cc))\n\n        msg['From'] = self.mailfrom\n        msg['To'] = COMMASPACE.join(to)\n        msg['Date'] = formatdate(localtime=True)\n        msg['Subject'] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg['Cc'] = COMMASPACE.join(cc)\n\n        if charset:\n            msg.set_charset(charset)\n\n        if attachs:\n            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split('/'))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' \\\n                    % attach_name)\n                msg.attach(part)\n        else:\n            msg.set_payload(body)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                          'mailattachs': len(attachs)})\n            return\n\n        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n            callbackArgs=[to, cc, subject, len(attachs)],\n            errbackArgs=[to, cc, subject, len(attachs)])\n        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n        return dfd",
        "begin_line": 58,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log#20",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop('dont_log', _IgnoredException)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender,\n                *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses",
        "begin_line": 20,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log_deferred#47",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d",
        "begin_line": 47,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.signal.logerror#52",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.logerror(failure, recv)",
        "snippet": "    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.signal.disconnect_all#74",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.disconnect_all(signal=Any, sender=Any)",
        "snippet": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)",
        "begin_line": 74,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.trackref.object_ref.__new__#30",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref.object_ref",
        "signature": "scrapy.utils.trackref.object_ref.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj",
        "begin_line": 30,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015873015873015872,
            "pseudo_dstar_susp": 0.024390243902439025,
            "pseudo_tarantula_susp": 0.014285714285714285,
            "pseudo_op2_susp": 0.024390243902439025,
            "pseudo_barinel_susp": 0.014285714285714285
        }
    },
    {
        "name": "scrapy.utils.trackref.format_live_refs#36",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.format_live_refs(ignore=NoneType)",
        "snippet": "def format_live_refs(ignore=NoneType):\n    \"\"\"Return a tabular representation of tracked objects\"\"\"\n    s = \"Live References\\n\\n\"\n    now = time()\n    for cls, wdict in sorted(six.iteritems(live_refs),\n                             key=lambda x: x[0].__name__):\n        if not wdict:\n            continue\n        if issubclass(cls, ignore):\n            continue\n        oldest = min(six.itervalues(wdict))\n        s += \"%-30s %6d   oldest: %ds ago\\n\" % (\n            cls.__name__, len(wdict), now - oldest\n        )\n    return s",
        "begin_line": 36,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.trackref.print_live_refs#53",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.print_live_refs(*a, **kw)",
        "snippet": "def print_live_refs(*a, **kw):\n    \"\"\"Print tracked objects\"\"\"\n    print(format_live_refs(*a, **kw))",
        "begin_line": 53,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.trackref.get_oldest#58",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.get_oldest(class_name)",
        "snippet": "def get_oldest(class_name):\n    \"\"\"Get the oldest object for a specific class name\"\"\"\n    for cls, wdict in six.iteritems(live_refs):\n        if cls.__name__ == class_name:\n            if not wdict:\n                break\n            return min(six.iteritems(wdict), key=itemgetter(1))[0]",
        "begin_line": 58,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.trackref.iter_all#67",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref",
        "signature": "scrapy.utils.trackref.iter_all(class_name)",
        "snippet": "def iter_all(class_name):\n    \"\"\"Iterate over all objects of the same class by its class name\"\"\"\n    for cls, wdict in six.iteritems(live_refs):\n        if cls.__name__ == class_name:\n            return six.iterkeys(wdict)",
        "begin_line": 67,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.from_crawler#22",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n            raise NotConfigured\n        return cls()",
        "begin_line": 22,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_request#27",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        request.headers.setdefault('Accept-Encoding',\n                                   b\",\".join(ACCEPTED_ENCODINGS))",
        "begin_line": 27,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_response#31",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n\n        if request.method == 'HEAD':\n            return response\n        if isinstance(response, Response):\n            content_encoding = response.headers.getlist('Content-Encoding')\n            if content_encoding:\n                encoding = content_encoding.pop()\n                decoded_body = self._decode(response.body, encoding.lower())\n                respcls = responsetypes.from_args(headers=response.headers, \\\n                    url=response.url, body=decoded_body)\n                kwargs = dict(cls=respcls, body=decoded_body)\n                if issubclass(respcls, TextResponse):\n                    # force recalculating the encoding until we make sure the\n                    # responsetypes guessing is reliable\n                    kwargs['encoding'] = None\n                response = response.replace(**kwargs)\n                if not content_encoding:\n                    del response.headers['Content-Encoding']\n\n        return response",
        "begin_line": 31,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware._decode#53",
        "src_path": "scrapy/downloadermiddlewares/httpcompression.py",
        "class_name": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware._decode(self, body, encoding)",
        "snippet": "    def _decode(self, body, encoding):\n        if encoding == b'gzip' or encoding == b'x-gzip':\n            body = gunzip(body)\n\n        if encoding == b'deflate':\n            try:\n                body = zlib.decompress(body)\n            except zlib.error:\n                # ugly hack to work with raw deflate content that may\n                # be sent by microsoft servers. For more information, see:\n                # http://carsten.codimi.de/gzip.yaws/\n                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\n                # http://www.gzip.org/zlib/zlib_faq.html#faq38\n                body = zlib.decompress(body, -15)\n        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:\n            body = brotli.decompress(body)\n        return body",
        "begin_line": 53,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.decorators.deco#14",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.deco(func)",
        "snippet": "    def deco(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            message = \"Call to deprecated function %s.\" % func.__name__\n            if use_instead:\n                message += \" Use %s instead.\" % use_instead\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapped",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.decorators.wrapped#16",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.wrapped(*args, **kwargs)",
        "snippet": "        def wrapped(*args, **kwargs):\n            message = \"Call to deprecated function %s.\" % func.__name__\n            if use_instead:\n                message += \" Use %s instead.\" % use_instead\n            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)",
        "begin_line": 16,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.decorators.defers#30",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.defers(func)",
        "snippet": "def defers(func):\n    \"\"\"Decorator to make sure a function always returns a deferred\"\"\"\n    @wraps(func)\n    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)\n    return wrapped",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.decorators.wrapped#33",
        "src_path": "scrapy/utils/decorators.py",
        "class_name": "scrapy.utils.decorators",
        "signature": "scrapy.utils.decorators.wrapped(*a, **kw)",
        "snippet": "    def wrapped(*a, **kw):\n        return defer.maybeDeferred(func, *a, **kw)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.__init__#50",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.__init__(self, store_uri, download_func=None, settings=None)",
        "snippet": "    def __init__(self, store_uri, download_func=None, settings=None):\n        super(ImagesPipeline, self).__init__(store_uri, settings=settings,\n                                             download_func=download_func)\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=\"ImagesPipeline\",\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n        )\n\n        if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n            self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n        if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n            self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n\n        self.images_urls_field = settings.get(\n            resolve('IMAGES_URLS_FIELD'),\n            self.IMAGES_URLS_FIELD\n        )\n        self.images_result_field = settings.get(\n            resolve('IMAGES_RESULT_FIELD'),\n            self.IMAGES_RESULT_FIELD\n        )\n        self.min_width = settings.getint(\n            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH\n        )\n        self.min_height = settings.getint(\n            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT\n        )\n        self.thumbs = settings.get(\n            resolve('IMAGES_THUMBS'), self.THUMBS\n        )",
        "begin_line": 50,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.from_settings#88",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n\n        store_uri = settings['IMAGES_STORE']\n        return cls(store_uri, settings=settings)",
        "begin_line": 88,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.file_downloaded#105",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.file_downloaded(self, response, request, info)",
        "snippet": "    def file_downloaded(self, response, request, info):\n        return self.image_downloaded(response, request, info)",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.image_downloaded#108",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.image_downloaded(self, response, request, info)",
        "snippet": "    def image_downloaded(self, response, request, info):\n        checksum = None\n        for path, image, buf in self.get_images(response, request, info):\n            if checksum is None:\n                buf.seek(0)\n                checksum = md5sum(buf)\n            width, height = image.size\n            self.store.persist_file(\n                path, buf, info,\n                meta={'width': width, 'height': height},\n                headers={'Content-Type': 'image/jpeg'})\n        return checksum",
        "begin_line": 108,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.get_images#121",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.get_images(self, response, request, info)",
        "snippet": "    def get_images(self, response, request, info):\n        path = self.file_path(request, response=response, info=info)\n        orig_image = Image.open(BytesIO(response.body))\n\n        width, height = orig_image.size\n        if width < self.min_width or height < self.min_height:\n            raise ImageException(\"Image too small (%dx%d < %dx%d)\" %\n                                 (width, height, self.min_width, self.min_height))\n\n        image, buf = self.convert_image(orig_image)\n        yield path, image, buf\n\n        for thumb_id, size in six.iteritems(self.thumbs):\n            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)\n            thumb_image, thumb_buf = self.convert_image(image, size)\n            yield thumb_path, thumb_image, thumb_buf",
        "begin_line": 121,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.convert_image#138",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.convert_image(self, image, size=None)",
        "snippet": "    def convert_image(self, image, size=None):\n        if image.format == 'PNG' and image.mode == 'RGBA':\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode == 'P':\n            image = image.convert(\"RGBA\")\n            background = Image.new('RGBA', image.size, (255, 255, 255))\n            background.paste(image, image)\n            image = background.convert('RGB')\n        elif image.mode != 'RGB':\n            image = image.convert('RGB')\n\n        if size:\n            image = image.copy()\n            image.thumbnail(size, Image.ANTIALIAS)\n\n        buf = BytesIO()\n        image.save(buf, 'JPEG')\n        return image, buf",
        "begin_line": 138,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.get_media_requests#159",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.images_urls_field, [])]",
        "begin_line": 159,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.item_completed#162",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.images_result_field in item.fields:\n            item[self.images_result_field] = [x for ok, x in results if ok]\n        return item",
        "begin_line": 162,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.file_path#167",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.file_path(self, request, response=None, info=None)",
        "snippet": "    def file_path(self, request, response=None, info=None):\n        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return 'full/%s.jpg' % (image_guid)",
        "begin_line": 167,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.images.ImagesPipeline.thumb_path#171",
        "src_path": "scrapy/pipelines/images.py",
        "class_name": "scrapy.pipelines.images.ImagesPipeline",
        "signature": "scrapy.pipelines.images.ImagesPipeline.thumb_path(self, request, thumb_id, response=None, info=None)",
        "snippet": "    def thumb_path(self, request, thumb_id, response=None, info=None):\n        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)",
        "begin_line": 171,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.get_settings_priority#26",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.get_settings_priority(priority)",
        "snippet": "def get_settings_priority(priority):\n    \"\"\"\n    Small helper function that looks up a given string priority in the\n    :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its\n    numerical value, or directly returns a given numerical priority.\n    \"\"\"\n    if isinstance(priority, six.string_types):\n        return SETTINGS_PRIORITIES[priority]\n    else:\n        return priority",
        "begin_line": 26,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015971889474524837,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__init__#46",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__init__(self, value, priority)",
        "snippet": "    def __init__(self, value, priority):\n        self.value = value\n        if isinstance(self.value, BaseSettings):\n            self.priority = max(self.value.maxpriority(), priority)\n        else:\n            self.priority = priority",
        "begin_line": 46,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.set#53",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.set(self, value, priority)",
        "snippet": "    def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            if isinstance(self.value, BaseSettings):\n                value = BaseSettings(value, priority=priority)\n            self.value = value\n            self.priority = priority",
        "begin_line": 53,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002575328354365182,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__str__#61",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<SettingsAttribute value={self.value!r} \" \\\n               \"priority={self.priority}>\".format(self=self)",
        "begin_line": 61,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__init__#90",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        self.frozen = False\n        self.attributes = {}\n        self.update(values, priority)",
        "begin_line": 90,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015513496742165683,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__getitem__#95",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        if opt_name not in self:\n            return None\n        return self.attributes[opt_name].value",
        "begin_line": 95,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015931177314003505,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__contains__#100",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__contains__(self, name)",
        "snippet": "    def __contains__(self, name):\n        return name in self.attributes",
        "begin_line": 100,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015537600994406463,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.get#103",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.get(self, name, default=None)",
        "snippet": "    def get(self, name, default=None):\n        \"\"\"\n        Get a setting value without affecting its original type.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return self[name] if self[name] is not None else default",
        "begin_line": 103,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015649452269170578,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getbool#115",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getbool(self, name, default=False)",
        "snippet": "    def getbool(self, name, default=False):\n        \"\"\"\n        Get a setting value as a boolean.\n\n        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,\n        while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``.\n\n        For example, settings populated through environment variables set to\n        ``'0'`` will return ``False`` when using this method.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        got = self.get(name, default)\n        try:\n            return bool(int(got))\n        except ValueError:\n            if got in (\"True\", \"true\"):\n                return True\n            if got in (\"False\", \"false\"):\n                return False\n            raise ValueError(\"Supported values for boolean settings \"\n                             \"are 0/1, True/False, '0'/'1', \"\n                             \"'True'/'False' and 'true'/'false'\")",
        "begin_line": 115,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getint#143",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getint(self, name, default=0)",
        "snippet": "    def getint(self, name, default=0):\n        \"\"\"\n        Get a setting value as an int.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return int(self.get(name, default))",
        "begin_line": 143,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015792798483891344,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getfloat#155",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getfloat(self, name, default=0.0)",
        "snippet": "    def getfloat(self, name, default=0.0):\n        \"\"\"\n        Get a setting value as a float.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        return float(self.get(name, default))",
        "begin_line": 155,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016160310277957336,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getlist#167",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getlist(self, name, default=None)",
        "snippet": "    def getlist(self, name, default=None):\n        \"\"\"\n        Get a setting value as a list. If the setting original type is a list, a\n        copy of it will be returned. If it's a string it will be split by \",\".\n\n        For example, settings populated through environment variables set to\n        ``'one,two'`` will return a list ['one', 'two'] when using this method.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default or [])\n        if isinstance(value, six.string_types):\n            value = value.split(',')\n        return list(value)",
        "begin_line": 167,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getdict#186",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getdict(self, name, default=None)",
        "snippet": "    def getdict(self, name, default=None):\n        \"\"\"\n        Get a setting value as a dictionary. If the setting original type is a\n        dictionary, a copy of it will be returned. If it is a string it will be\n        evaluated as a JSON dictionary. In the case that it is a\n        :class:`~scrapy.settings.BaseSettings` instance itself, it will be\n        converted to a dictionary, containing all its current settings values\n        as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`,\n        and losing all information about priority and mutability.\n\n        :param name: the setting name\n        :type name: string\n\n        :param default: the value to return if no setting is found\n        :type default: any\n        \"\"\"\n        value = self.get(name, default or {})\n        if isinstance(value, six.string_types):\n            value = json.loads(value)\n        return dict(value)",
        "begin_line": 186,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getwithbase#207",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getwithbase(self, name)",
        "snippet": "    def getwithbase(self, name):\n        \"\"\"Get a composition of a dictionary-like setting and its `_BASE`\n        counterpart.\n\n        :param name: name of the dictionary-like setting\n        :type name: string\n        \"\"\"\n        compbs = BaseSettings()\n        compbs.update(self[name + '_BASE'])\n        compbs.update(self[name])\n        return compbs",
        "begin_line": 207,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016160310277957336,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.getpriority#219",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.getpriority(self, name)",
        "snippet": "    def getpriority(self, name):\n        \"\"\"\n        Return the current numerical priority value of a setting, or ``None`` if\n        the given ``name`` does not exist.\n\n        :param name: the setting name\n        :type name: string\n        \"\"\"\n        if name not in self:\n            return None\n        return self.attributes[name].priority",
        "begin_line": 219,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016105653084232566,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.maxpriority#231",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.maxpriority(self)",
        "snippet": "    def maxpriority(self):\n        \"\"\"\n        Return the numerical value of the highest priority present throughout\n        all settings, or the numerical value for ``default`` from\n        :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings\n        stored.\n        \"\"\"\n        if len(self) > 0:\n            return max(self.getpriority(name) for name in self)\n        else:\n            return get_settings_priority('default')",
        "begin_line": 231,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__setitem__#243",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__setitem__(self, name, value)",
        "snippet": "    def __setitem__(self, name, value):\n        self.set(name, value)",
        "begin_line": 243,
        "end_line": 244,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.set#246",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.set(self, name, value, priority='project')",
        "snippet": "    def set(self, name, value, priority='project'):\n        \"\"\"\n        Store a key/value attribute with a given priority.\n\n        Settings should be populated *before* configuring the Crawler object\n        (through the :meth:`~scrapy.crawler.Crawler.configure` method),\n        otherwise they won't have any effect.\n\n        :param name: the setting name\n        :type name: string\n\n        :param value: the value to associate with the setting\n        :type value: any\n\n        :param priority: the priority of the setting. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if name not in self:\n            if isinstance(value, SettingsAttribute):\n                self.attributes[name] = value\n            else:\n                self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)",
        "begin_line": 246,
        "end_line": 272,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.setdict#274",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.setdict(self, values, priority='project')",
        "snippet": "    def setdict(self, values, priority='project'):\n        self.update(values, priority)",
        "begin_line": 274,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015997440409534473,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.setmodule#277",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.setmodule(self, module, priority='project')",
        "snippet": "    def setmodule(self, module, priority='project'):\n        \"\"\"\n        Store settings from a module with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every globally declared\n        uppercase variable of ``module`` with the provided ``priority``.\n\n        :param module: the module or the path of the module\n        :type module: module object or string\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(module, six.string_types):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)",
        "begin_line": 277,
        "end_line": 297,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.update#299",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.update(self, values, priority='project')",
        "snippet": "    def update(self, values, priority='project'):\n        \"\"\"\n        Store key/value pairs with a given priority.\n\n        This is a helper function that calls\n        :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values``\n        with the provided ``priority``.\n\n        If ``values`` is a string, it is assumed to be JSON-encoded and parsed\n        into a dict with ``json.loads()`` first. If it is a\n        :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities\n        will be used and the ``priority`` parameter ignored. This allows\n        inserting/updating settings with different priorities with a single\n        command.\n\n        :param values: the settings names and values\n        :type values: dict or string or :class:`~scrapy.settings.BaseSettings`\n\n        :param priority: the priority of the settings. Should be a key of\n            :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer\n        :type priority: string or int\n        \"\"\"\n        self._assert_mutability()\n        if isinstance(values, six.string_types):\n            values = json.loads(values)\n        if values is not None:\n            if isinstance(values, BaseSettings):\n                for name, value in six.iteritems(values):\n                    self.set(name, value, values.getpriority(name))\n            else:\n                for name, value in six.iteritems(values):\n                    self.set(name, value, priority)",
        "begin_line": 299,
        "end_line": 330,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.delete#332",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.delete(self, name, priority='project')",
        "snippet": "    def delete(self, name, priority='project'):\n        self._assert_mutability()\n        priority = get_settings_priority(priority)\n        if priority >= self.getpriority(name):\n            del self.attributes[name]",
        "begin_line": 332,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__delitem__#338",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__delitem__(self, name)",
        "snippet": "    def __delitem__(self, name):\n        self._assert_mutability()\n        del self.attributes[name]",
        "begin_line": 338,
        "end_line": 340,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings._assert_mutability#342",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings._assert_mutability(self)",
        "snippet": "    def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")",
        "begin_line": 342,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.copy#346",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"\n        Make a deep copy of current settings.\n\n        This method returns a new instance of the :class:`Settings` class,\n        populated with the same values and their priorities.\n\n        Modifications to the new object won't be reflected on the original\n        settings.\n        \"\"\"\n        return copy.deepcopy(self)",
        "begin_line": 346,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015997440409534473,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.freeze#358",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.freeze(self)",
        "snippet": "    def freeze(self):\n        \"\"\"\n        Disable further changes to the current settings.\n\n        After calling this method, the present state of the settings will become\n        immutable. Trying to change values through the :meth:`~set` method and\n        its variants won't be possible and will be alerted.\n        \"\"\"\n        self.frozen = True",
        "begin_line": 358,
        "end_line": 366,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016005121638924455,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.frozencopy#368",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.frozencopy(self)",
        "snippet": "    def frozencopy(self):\n        \"\"\"\n        Return an immutable copy of the current settings.\n\n        Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`.\n        \"\"\"\n        copy = self.copy()\n        copy.freeze()\n        return copy",
        "begin_line": 368,
        "end_line": 376,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016652789342214822,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__iter__#378",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self.attributes)",
        "begin_line": 378,
        "end_line": 379,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015595757953836556,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.__len__#381",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self.attributes)",
        "begin_line": 381,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020828993959591752,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings._to_dict#384",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings._to_dict(self)",
        "snippet": "    def _to_dict(self):\n        return {k: (v._to_dict() if isinstance(v, BaseSettings) else v)\n                for k, v in six.iteritems(self)}",
        "begin_line": 384,
        "end_line": 386,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.BaseSettings.copy_to_dict#388",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.BaseSettings",
        "signature": "scrapy.settings.__init__.BaseSettings.copy_to_dict(self)",
        "snippet": "    def copy_to_dict(self):\n        \"\"\"\n        Make a copy of current settings and convert to a dict.\n\n        This method returns a new dict populated with the same values\n        and their priorities as the current settings.\n\n        Modifications to the returned dict won't be reflected on the original\n        settings.\n\n        This method can be useful for example for printing settings\n        in Scrapy shell.\n        \"\"\"\n        settings = self.copy()\n        return settings._to_dict()",
        "begin_line": 388,
        "end_line": 402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__init__#446",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        # Do not pass kwarg values here. We don't want to promote user-defined\n        # dicts, and we want to update, not replace, default dicts with the\n        # values given by the user\n        super(Settings, self).__init__()\n        self.setmodule(default_settings, 'default')\n        # Promote default dictionaries to BaseSettings instances for per-key\n        # priorities\n        for name, val in six.iteritems(self):\n            if isinstance(val, dict):\n                self.set(name, BaseSettings(val, 'default'), 'default')\n        self.update(values, priority)",
        "begin_line": 446,
        "end_line": 457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015625,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.iter_default_settings#460",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.iter_default_settings()",
        "snippet": "def iter_default_settings():\n    \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n    for name in dir(default_settings):\n        if name.isupper():\n            yield name, getattr(default_settings, name)",
        "begin_line": 460,
        "end_line": 464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.settings.__init__.overridden_settings#467",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__",
        "signature": "scrapy.settings.__init__.overridden_settings(settings)",
        "snippet": "def overridden_settings(settings):\n    \"\"\"Return a dict of the settings that have been overridden\"\"\"\n    for name, defvalue in iter_default_settings():\n        value = settings[name]\n        if not isinstance(defvalue, dict) and value != defvalue:\n            yield name, value",
        "begin_line": 467,
        "end_line": 472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.gz.read1#26",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.read1(gzf, size=-1)",
        "snippet": "    def read1(gzf, size=-1):\n        return gzf.read1(size)",
        "begin_line": 26,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.gz.gunzip#30",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gunzip(data)",
        "snippet": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output_list = []\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = read1(f, 8196)\n            output_list.append(chunk)\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output_list is empty and f.extrabuf\n            # contains the whole page content\n            if output_list or getattr(f, 'extrabuf', None):\n                try:\n                    output_list.append(f.extrabuf[-f.extrasize:])\n                finally:\n                    break\n            else:\n                raise\n    return b''.join(output_list)",
        "begin_line": 30,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.gz.is_gzipped#60",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.is_gzipped(response)",
        "snippet": "def is_gzipped(response):\n    \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n    ctype = response.headers.get('Content-Type', b'')\n    cenc = response.headers.get('Content-Encoding', b'').lower()\n    return (_is_gzipped(ctype) or\n            (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))",
        "begin_line": 60,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.gz.gzip_magic_number#68",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gzip_magic_number(response)",
        "snippet": "def gzip_magic_number(response):\n    return response.body[:3] == b'\\x1f\\x8b\\x08'",
        "begin_line": 68,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.__init__#17",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.__init__(self, *middlewares)",
        "snippet": "    def __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self.methods = defaultdict(deque)\n        for mw in middlewares:\n            self._add_middleware(mw)",
        "begin_line": 17,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016079755587715066,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_settings#28",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_settings(cls, settings, crawler=None)",
        "snippet": "    def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        enabled = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                mw = create_instance(mwcls, settings, crawler)\n                middlewares.append(mw)\n                enabled.append(clspath)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': pprint.pformat(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)",
        "begin_line": 28,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016160310277957336,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_crawler#52",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings, crawler)",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._add_middleware#55",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].appendleft(mw.close_spider)",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._process_parallel#61",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._process_parallel(self, methodname, obj, *args)",
        "snippet": "    def _process_parallel(self, methodname, obj, *args):\n        return process_parallel(self.methods[methodname], obj, *args)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._process_chain#64",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._process_chain(self, methodname, obj, *args)",
        "snippet": "    def _process_chain(self, methodname, obj, *args):\n        return process_chain(self.methods[methodname], obj, *args)",
        "begin_line": 64,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.open_spider#71",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        return self._process_parallel('open_spider', spider)",
        "begin_line": 71,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.close_spider#74",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        return self._process_parallel('close_spider', spider)",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.ossignal.install_shutdown_handlers#16",
        "src_path": "scrapy/utils/ossignal.py",
        "class_name": "scrapy.utils.ossignal",
        "signature": "scrapy.utils.ossignal.install_shutdown_handlers(function, override_sigint=True)",
        "snippet": "def install_shutdown_handlers(function, override_sigint=True):\n    \"\"\"Install the given function as a signal handler for all common shutdown\n    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n    SIGINT handler won't be install if there is already a handler in place\n    (e.g.  Pdb)\n    \"\"\"\n    reactor._handleSignals()\n    signal.signal(signal.SIGTERM, function)\n    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or \\\n            override_sigint:\n        signal.signal(signal.SIGINT, function)\n    # Catch Ctrl-Break in windows\n    if hasattr(signal, 'SIGBREAK'):\n        signal.signal(signal.SIGBREAK, function)",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__init__#16",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__init__(self, xmltext)",
        "snippet": "    def __init__(self, xmltext):\n        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__iter__#22",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__iter__(self)",
        "snippet": "    def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d",
        "begin_line": 22,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.sitemap.sitemap_urls_from_robots#39",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap",
        "signature": "scrapy.utils.sitemap.sitemap_urls_from_robots(robots_text, base_url=None)",
        "snippet": "def sitemap_urls_from_robots(robots_text, base_url=None):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().lower().startswith('sitemap:'):\n            url = line.split(':', 1)[1].strip()\n            yield urljoin(base_url, url)",
        "begin_line": 39,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient._parsed_url_args#14",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient",
        "signature": "scrapy.core.downloader.webclient._parsed_url_args(parsed)",
        "snippet": "def _parsed_url_args(parsed):\n    # Assume parsed is urlparse-d from Request.url,\n    # which was passed via safe_url_string and is ascii-only.\n    b = lambda s: to_bytes(s, encoding='ascii')\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    path = b(path)\n    host = b(parsed.hostname)\n    port = parsed.port\n    scheme = b(parsed.scheme)\n    netloc = b(parsed.netloc)\n    if port is None:\n        port = 443 if scheme == b'https' else 80\n    return scheme, netloc, host, port, path",
        "begin_line": 14,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient._parse#29",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient",
        "signature": "scrapy.core.downloader.webclient._parse(url)",
        "snippet": "def _parse(url):\n    \"\"\" Return tuple of (scheme, netloc, host, port, path),\n    all in bytes except for port which is int.\n    Assume url is from Request.url, which was passed via safe_url_string\n    and is ascii-only.\n    \"\"\"\n    url = url.strip()\n    parsed = urlparse(url)\n    return _parsed_url_args(parsed)",
        "begin_line": 29,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionMade#44",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionMade(self)",
        "snippet": "    def connectionMade(self):\n        self.headers = Headers() # bucket for response headers\n\n        # Method command\n        self.sendCommand(self.factory.method, self.factory.path)\n        # Headers\n        for key, values in self.factory.headers.items():\n            for value in values:\n                self.sendHeader(key, value)\n        self.endHeaders()\n        # Body\n        if self.factory.body is not None:\n            self.transport.write(self.factory.body)",
        "begin_line": 44,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.lineReceived#58",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.lineReceived(self, line)",
        "snippet": "    def lineReceived(self, line):\n        return HTTPClient.lineReceived(self, line.rstrip())",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleHeader#61",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleHeader(self, key, value)",
        "snippet": "    def handleHeader(self, key, value):\n        self.headers.appendlist(key, value)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleStatus#64",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleStatus(self, version, status, message)",
        "snippet": "    def handleStatus(self, version, status, message):\n        self.factory.gotStatus(version, status, message)",
        "begin_line": 64,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleEndHeaders#67",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleEndHeaders(self)",
        "snippet": "    def handleEndHeaders(self):\n        self.factory.gotHeaders(self.headers)",
        "begin_line": 67,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionLost#70",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.connectionLost(self, reason)",
        "snippet": "    def connectionLost(self, reason):\n        self._connection_lost_reason = reason\n        HTTPClient.connectionLost(self, reason)\n        self.factory.noPage(reason)",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023917723032767282,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleResponse#75",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.handleResponse(self, response)",
        "snippet": "    def handleResponse(self, response):\n        if self.factory.method.upper() == b'HEAD':\n            self.factory.page(b'')\n        elif self.length is not None and self.length > 0:\n            self.factory.noPage(self._connection_lost_reason)\n        else:\n            self.factory.page(response)\n        self.transport.loseConnection()",
        "begin_line": 75,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.timeout#84",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPPageGetter.timeout(self)",
        "snippet": "    def timeout(self):\n        self.transport.loseConnection()\n\n        # transport cleanup needed for HTTPS connections\n        if self.factory.url.startswith(b'https'):\n            self.transport.stopProducing()\n\n        self.factory.noPage(\\\n                defer.TimeoutError(\"Getting %s took longer than %s seconds.\" % \\\n                (self.factory.url, self.factory.timeout)))",
        "begin_line": 84,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.__init__#108",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.__init__(self, request, timeout=180)",
        "snippet": "    def __init__(self, request, timeout=180):\n        self._url = urldefrag(request.url)[0]\n        # converting to bytes to comply to Twisted interface\n        self.url = to_bytes(self._url, encoding='ascii')\n        self.method = to_bytes(request.method, encoding='ascii')\n        self.body = request.body or None\n        self.headers = Headers(request.headers)\n        self.response_headers = None\n        self.timeout = request.meta.get('download_timeout') or timeout\n        self.start_time = time()\n        self.deferred = defer.Deferred().addCallback(self._build_response, request)\n\n        # Fixes Twisted 11.1.0+ support as HTTPClientFactory is expected\n        # to have _disconnectedDeferred. See Twisted r32329.\n        # As Scrapy implements it's own logic to handle redirects is not\n        # needed to add the callback _waitForDisconnect.\n        # Specifically this avoids the AttributeError exception when\n        # clientConnectionFailed method is called.\n        self._disconnectedDeferred = defer.Deferred()\n\n        self._set_connection_attributes(request)\n\n        # set Host header based on url\n        self.headers.setdefault('Host', self.netloc)\n\n        # set Content-Length based len of body\n        if self.body is not None:\n            self.headers['Content-Length'] = len(self.body)\n            # just in case a broken http/1.1 decides to keep connection alive\n            self.headers.setdefault(\"Connection\", \"close\")\n        # Content-Length must be specified in POST method even with no body\n        elif self.method == b'POST':\n            self.headers['Content-Length'] = 0",
        "begin_line": 108,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._build_response#142",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._build_response(self, body, request)",
        "snippet": "    def _build_response(self, body, request):\n        request.meta['download_latency'] = self.headers_time-self.start_time\n        status = int(self.status)\n        headers = Headers(self.response_headers)\n        respcls = responsetypes.from_args(headers=headers, url=self._url)\n        return respcls(url=self._url, status=status, headers=headers, body=body)",
        "begin_line": 142,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002516356316054353,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._set_connection_attributes#149",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory._set_connection_attributes(self, request)",
        "snippet": "    def _set_connection_attributes(self, request):\n        parsed = urlparse_cached(request)\n        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n        proxy = request.meta.get('proxy')\n        if proxy:\n            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n            self.path = self.url",
        "begin_line": 149,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.gotHeaders#157",
        "src_path": "scrapy/core/downloader/webclient.py",
        "class_name": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory",
        "signature": "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory.gotHeaders(self, headers)",
        "snippet": "    def gotHeaders(self, headers):\n        self.headers_time = time()\n        self.response_headers = headers",
        "begin_line": 157,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__init__#17",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.default_loader_context = default_loader_context",
        "begin_line": 17,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__call__#21",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        values = arg_to_iter(value)\n        if loader_context:\n            context = ChainMap(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            next_values = []\n            for v in values:\n                try:\n                    next_values += arg_to_iter(func(v))\n                except Exception as e:\n                    raise ValueError(\"Error in MapCompose with \"\n                                     \"%s value=%r error='%s: %s'\" %\n                                     (str(func), value, type(e).__name__,\n                                      str(e)))\n            values = next_values\n        return values",
        "begin_line": 21,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__init__#44",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.stop_on_none = default_loader_context.get('stop_on_none', True)\n        self.default_loader_context = default_loader_context",
        "begin_line": 44,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__call__#49",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        if loader_context:\n            context = ChainMap(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            if value is None and self.stop_on_none:\n                break\n            try:\n                value = func(value)\n            except Exception as e:\n                raise ValueError(\"Error in Compose with \"\n                                 \"%s value=%r error='%s: %s'\" %\n                                 (str(func), value, type(e).__name__, str(e)))\n        return value",
        "begin_line": 49,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.TakeFirst.__call__#69",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.TakeFirst",
        "signature": "scrapy.loader.processors.TakeFirst.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        for value in values:\n            if value is not None and value != '':\n                return value",
        "begin_line": 69,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.Identity.__call__#77",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Identity",
        "signature": "scrapy.loader.processors.Identity.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return values",
        "begin_line": 77,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023299161230195712,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__init__#88",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__init__(self, json_path)",
        "snippet": "    def __init__(self, json_path):\n        self.json_path = json_path\n        import jmespath\n        self.compiled_path = jmespath.compile(self.json_path)",
        "begin_line": 88,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__call__#93",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__call__(self, value)",
        "snippet": "    def __call__(self, value):\n        \"\"\"Query value for the jmespath query and return answer\n        :param value: a data structure (dict, list) to extract from\n        :return: Element extracted according to jmespath query\n        \"\"\"\n        return self.compiled_path.search(value)",
        "begin_line": 93,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__init__#103",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__init__(self, separator=u' ')",
        "snippet": "    def __init__(self, separator=u' '):\n        self.separator = separator",
        "begin_line": 103,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__call__#106",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return self.separator.join(values)",
        "begin_line": 106,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.__init__#11",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats\n        self.start_time = None",
        "begin_line": 11,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.from_crawler#16",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o",
        "begin_line": 16,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_opened#25",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.start_time = datetime.utcnow()\n        self.stats.set_value('start_time', self.start_time, spider=spider)",
        "begin_line": 25,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_closed#29",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        finish_time = datetime.utcnow()\n        elapsed_time = finish_time - self.start_time\n        elapsed_time_seconds = elapsed_time.total_seconds()\n        self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n        self.stats.set_value('finish_time', finish_time, spider=spider)\n        self.stats.set_value('finish_reason', reason, spider=spider)",
        "begin_line": 29,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.item_scraped#37",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        self.stats.inc_value('item_scraped_count', spider=spider)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00026014568158168577,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.response_received#40",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.response_received(self, spider)",
        "snippet": "    def response_received(self, spider):\n        self.stats.inc_value('response_received_count', spider=spider)",
        "begin_line": 40,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022114108801415304,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.item_dropped#43",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.item_dropped(self, item, spider, exception)",
        "snippet": "    def item_dropped(self, item, spider, exception):\n        reason = exception.__class__.__name__\n        self.stats.inc_value('item_dropped_count', spider=spider)\n        self.stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)",
        "begin_line": 43,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.__init__#15",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.__init__(self, contracts)",
        "snippet": "    def __init__(self, contracts):\n        for contract in contracts:\n            self.contracts[contract.name] = contract",
        "begin_line": 15,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.tested_methods_from_spidercls#19",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.tested_methods_from_spidercls(self, spidercls)",
        "snippet": "    def tested_methods_from_spidercls(self, spidercls):\n        methods = []\n        for key, value in getmembers(spidercls):\n            if (callable(value) and value.__doc__ and\n                    re.search(r'^\\s*@', value.__doc__, re.MULTILINE)):\n                methods.append(key)\n\n        return methods",
        "begin_line": 19,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.extract_contracts#28",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.extract_contracts(self, method)",
        "snippet": "    def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts",
        "begin_line": 28,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_spider#41",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_spider(self, spider, results)",
        "snippet": "    def from_spider(self, spider, results):\n        requests = []\n        for method in self.tested_methods_from_spidercls(type(spider)):\n            bound_method = spider.__getattribute__(method)\n            try:\n                requests.append(self.from_method(bound_method, results))\n            except Exception:\n                case = _create_testcase(bound_method, 'contract')\n                results.addError(case, sys.exc_info())\n\n        return requests",
        "begin_line": 41,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_method#53",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_method(self, method, results)",
        "snippet": "    def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            request_cls = Request\n            for contract in contracts:\n                if contract.request_cls is not None:\n                    request_cls = contract.request_cls\n\n            # calculate request args\n            args, kwargs = get_spec(request_cls.__init__)\n\n            # Don't filter requests to allow\n            # testing different callbacks on the same URL.\n            kwargs['dont_filter'] = True\n            kwargs['callback'] = method\n\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            args.remove('self')\n\n            # check if all positional arguments are defined in kwargs\n            if set(args).issubset(set(kwargs)):\n                request = request_cls(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request",
        "begin_line": 53,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager._clean_req#87",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager._clean_req(self, request, method, results)",
        "snippet": "    def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper",
        "begin_line": 87,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.cb_wrapper#93",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.cb_wrapper(response, **cb_kwargs)",
        "snippet": "        def cb_wrapper(response, **cb_kwargs):\n            try:\n                output = cb(response, **cb_kwargs)\n                output = list(iterate_spider_output(output))\n            except Exception:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())",
        "begin_line": 93,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.eb_wrapper#101",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.eb_wrapper(failure)",
        "snippet": "        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.type, failure.value, failure.getTracebackObject()\n            results.addError(case, exc_info)",
        "begin_line": 101,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.__init__#114",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.__init__(self, method, *args)",
        "snippet": "    def __init__(self, method, *args):\n        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n        self.args = args",
        "begin_line": 114,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_pre_hook#119",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_pre_hook(self, request, results)",
        "snippet": "    def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response, **cb_kwargs)))\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 119,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_post_hook#142",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_post_hook(self, request, results)",
        "snippet": "    def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 142,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.wrapper#147",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.wrapper(response, **cb_kwargs)",
        "snippet": "            def wrapper(response, **cb_kwargs):\n                output = list(iterate_spider_output(cb(response, **cb_kwargs)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output",
        "begin_line": 147,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.adjust_request_args#166",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        return args",
        "begin_line": 166,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__._create_testcase#170",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__",
        "signature": "scrapy.contracts.__init__._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 170,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase._create_testcase#170",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 170,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase.__str__#174",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase.__str__(_self)",
        "snippet": "        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)",
        "begin_line": 174,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.__init__#33",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self._pool = HTTPConnectionPool(reactor, persistent=True)\n        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n        self._pool._factory.noisy = False\n\n        self._sslMethod = openssl_methods[settings.get('DOWNLOADER_CLIENT_TLS_METHOD')]\n        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n        # try method-aware context factory\n        try:\n            self._contextFactory = create_instance(\n                self._contextFactoryClass,\n                settings=settings,\n                crawler=None,\n                method=self._sslMethod,\n            )\n        except TypeError:\n            # use context factory defaults\n            self._contextFactory = create_instance(\n                self._contextFactoryClass,\n                settings=settings,\n                crawler=None,\n            )\n            msg = \"\"\"\n '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n e.g. OpenSSL.SSL.SSLv23_METHOD) and/or `tls_verbose_logging` argument and/or `tls_ciphers` argument.\\\n Please upgrade your context factory class to handle them or ignore them.\"\"\" % (\n                settings['DOWNLOADER_CLIENTCONTEXTFACTORY'],)\n            warnings.warn(msg)\n        self._default_maxsize = settings.getint('DOWNLOAD_MAXSIZE')\n        self._default_warnsize = settings.getint('DOWNLOAD_WARNSIZE')\n        self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\n        self._disconnect_timeout = 1",
        "begin_line": 33,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001684068710003368,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.download_request#66",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        agent = ScrapyAgent(\n            contextFactory=self._contextFactory,\n            pool=self._pool,\n            maxsize=getattr(spider, 'download_maxsize', self._default_maxsize),\n            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),\n            fail_on_dataloss=self._fail_on_dataloss,\n        )\n        return agent.download_request(request)",
        "begin_line": 66,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016963528413910093,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.close#77",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.close(self)",
        "snippet": "    def close(self):\n        d = self._pool.closeCachedConnections()\n        # closeCachedConnections will hang on network or server issues, so\n        # we'll manually timeout the deferred.\n        #\n        # Twisted issue addressing this problem can be found here:\n        # https://twistedmatrix.com/trac/ticket/7738.\n        #\n        # closeCachedConnections doesn't handle external errbacks, so we'll\n        # issue a callback after `_disconnect_timeout` seconds.\n        delayed_call = reactor.callLater(self._disconnect_timeout, d.callback, [])\n\n        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result\n\n        d.addBoth(cancel_delayed_call)\n        return d",
        "begin_line": 77,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016877637130801687,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.cancel_delayed_call#89",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http11.HTTP11DownloadHandler.cancel_delayed_call(result)",
        "snippet": "        def cancel_delayed_call(result):\n            if delayed_call.active():\n                delayed_call.cancel()\n            return result",
        "begin_line": 89,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016877637130801687,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.__init__#113",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.__init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None)",
        "snippet": "    def __init__(self, reactor, host, port, proxyConf, contextFactory, timeout=30, bindAddress=None):\n        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n        super(TunnelingTCP4ClientEndpoint, self).__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n        self._tunnelReadyDeferred = defer.Deferred()\n        self._tunneledHost = host\n        self._tunneledPort = port\n        self._contextFactory = contextFactory\n        self._connectBuffer = bytearray()",
        "begin_line": 113,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.requestTunnel#122",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.requestTunnel(self, protocol)",
        "snippet": "    def requestTunnel(self, protocol):\n        \"\"\"Asks the proxy to open a tunnel.\"\"\"\n        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort, self._proxyAuthHeader)\n        protocol.transport.write(tunnelReq)\n        self._protocolDataReceived = protocol.dataReceived\n        protocol.dataReceived = self.processProxyResponse\n        self._protocol = protocol\n        return protocol",
        "begin_line": 122,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.processProxyResponse#131",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.processProxyResponse(self, rcvd_bytes)",
        "snippet": "    def processProxyResponse(self, rcvd_bytes):\n        \"\"\"Processes the response from the proxy. If the tunnel is successfully\n        created, notifies the client that we are ready to send requests. If not\n        raises a TunnelError.\n        \"\"\"\n        self._connectBuffer += rcvd_bytes\n        # make sure that enough (all) bytes are consumed\n        # and that we've got all HTTP headers (ending with a blank line)\n        # from the proxy so that we don't send those bytes to the TLS layer\n        #\n        # see https://github.com/scrapy/scrapy/issues/2491\n        if b'\\r\\n\\r\\n' not in self._connectBuffer:\n            return\n        self._protocol.dataReceived = self._protocolDataReceived\n        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)\n        if respm and int(respm.group('status')) == 200:\n            # set proper Server Name Indication extension\n            sslOptions = self._contextFactory.creatorForNetloc(self._tunneledHost, self._tunneledPort)\n            self._protocol.transport.startTLS(sslOptions, self._protocolFactory)\n            self._tunnelReadyDeferred.callback(self._protocol)\n        else:\n            if respm:\n                extra = {'status': int(respm.group('status')),\n                         'reason': respm.group('reason').strip()}\n            else:\n                extra = rcvd_bytes[:32]\n            self._tunnelReadyDeferred.errback(\n                TunnelError('Could not open CONNECT tunnel with proxy %s:%s [%r]' % (\n                    self._host, self._port, extra)))",
        "begin_line": 131,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.connect#165",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingTCP4ClientEndpoint.connect(self, protocolFactory)",
        "snippet": "    def connect(self, protocolFactory):\n        self._protocolFactory = protocolFactory\n        connectDeferred = super(TunnelingTCP4ClientEndpoint, self).connect(protocolFactory)\n        connectDeferred.addCallback(self.requestTunnel)\n        connectDeferred.addErrback(self.connectFailed)\n        return self._tunnelReadyDeferred",
        "begin_line": 165,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.tunnel_request_data#173",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11",
        "signature": "scrapy.core.downloader.handlers.http11.tunnel_request_data(host, port, proxy_auth_header=None)",
        "snippet": "def tunnel_request_data(host, port, proxy_auth_header=None):\n    r\"\"\"\n    Return binary content of a CONNECT request.\n\n    >>> from scrapy.utils.python import to_native_str as s\n    >>> s(tunnel_request_data(\"example.com\", 8080))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n    'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\nProxy-Authorization: 123\\r\\n\\r\\n'\n    >>> s(tunnel_request_data(b\"example.com\", \"8090\"))\n    'CONNECT example.com:8090 HTTP/1.1\\r\\nHost: example.com:8090\\r\\n\\r\\n'\n    \"\"\"\n    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))\n    tunnel_req = b'CONNECT ' + host_value + b' HTTP/1.1\\r\\n'\n    tunnel_req += b'Host: ' + host_value + b'\\r\\n'\n    if proxy_auth_header:\n        tunnel_req += b'Proxy-Authorization: ' + proxy_auth_header + b'\\r\\n'\n    tunnel_req += b'\\r\\n'\n    return tunnel_req",
        "begin_line": 173,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent.__init__#202",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent.__init__(self, reactor, proxyConf, contextFactory=None, connectTimeout=None, bindAddress=None, pool=None)",
        "snippet": "    def __init__(self, reactor, proxyConf, contextFactory=None,\n                 connectTimeout=None, bindAddress=None, pool=None):\n        super(TunnelingAgent, self).__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n        self._proxyConf = proxyConf\n        self._contextFactory = contextFactory",
        "begin_line": 202,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent._getEndpoint#208",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent._getEndpoint(self, uri)",
        "snippet": "    def _getEndpoint(self, uri):\n        return TunnelingTCP4ClientEndpoint(\n            reactor=self._reactor,\n            host=uri.host,\n            port=uri.port,\n            proxyConf=self._proxyConf,\n            contextFactory=self._contextFactory,\n            timeout=self._endpointFactory._connectTimeout,\n            bindAddress=self._endpointFactory._bindAddress,\n        )",
        "begin_line": 208,
        "end_line": 217,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.TunnelingAgent._requestWithEndpoint#219",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.TunnelingAgent",
        "signature": "scrapy.core.downloader.handlers.http11.TunnelingAgent._requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath)",
        "snippet": "    def _requestWithEndpoint(self, key, endpoint, method, parsedURI, headers, bodyProducer, requestPath):\n        # proxy host and port are required for HTTP pool `key`\n        # otherwise, same remote host connection request could reuse\n        # a cached tunneled connection to a different proxy\n        key = key + self._proxyConf\n        return super(TunnelingAgent, self)._requestWithEndpoint(\n            key=key,\n            endpoint=endpoint,\n            method=method,\n            parsedURI=parsedURI,\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=requestPath,\n        )",
        "begin_line": 219,
        "end_line": 232,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent.__init__#237",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent.__init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None)",
        "snippet": "    def __init__(self, reactor, proxyURI, connectTimeout=None, bindAddress=None, pool=None):\n        super(ScrapyProxyAgent, self).__init__(\n            reactor=reactor,\n            connectTimeout=connectTimeout,\n            bindAddress=bindAddress,\n            pool=pool,\n        )\n        self._proxyURI = URI.fromBytes(proxyURI)",
        "begin_line": 237,
        "end_line": 244,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent.request#246",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyProxyAgent.request(self, method, uri, headers=None, bodyProducer=None)",
        "snippet": "    def request(self, method, uri, headers=None, bodyProducer=None):\n        \"\"\"\n        Issue a new request via the configured proxy.\n        \"\"\"\n        # Cache *all* connections under the same key, since we are only\n        # connecting to a single destination, the proxy:\n        return self._requestWithEndpoint(\n            key=(\"http-proxy\", self._proxyURI.host, self._proxyURI.port),\n            endpoint=self._getEndpoint(self._proxyURI),\n            method=method,\n            parsedURI=URI.fromBytes(uri),\n            headers=headers,\n            bodyProducer=bodyProducer,\n            requestPath=uri,\n        )",
        "begin_line": 246,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent.__init__#269",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent.__init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None, maxsize=0, warnsize=0, fail_on_dataloss=True)",
        "snippet": "    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,\n                 maxsize=0, warnsize=0, fail_on_dataloss=True):\n        self._contextFactory = contextFactory\n        self._connectTimeout = connectTimeout\n        self._bindAddress = bindAddress\n        self._pool = pool\n        self._maxsize = maxsize\n        self._warnsize = warnsize\n        self._fail_on_dataloss = fail_on_dataloss\n        self._txresponse = None",
        "begin_line": 269,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016963528413910093,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._get_agent#280",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._get_agent(self, request, timeout)",
        "snippet": "    def _get_agent(self, request, timeout):\n        bindaddress = request.meta.get('bindaddress') or self._bindAddress\n        proxy = request.meta.get('proxy')\n        if proxy:\n            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n            scheme = _parse(request.url)[0]\n            proxyHost = to_unicode(proxyHost)\n            omitConnectTunnel = b'noconnect' in proxyParams\n            if scheme == b'https' and not omitConnectTunnel:\n                proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                proxyConf = (proxyHost, proxyPort, proxyAuth)\n                return self._TunnelingAgent(\n                    reactor=reactor,\n                    proxyConf=proxyConf,\n                    contextFactory=self._contextFactory,\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n            else:\n                return self._ProxyAgent(\n                    reactor=reactor,\n                    proxyURI=to_bytes(proxy, encoding='ascii'),\n                    connectTimeout=timeout,\n                    bindAddress=bindaddress,\n                    pool=self._pool,\n                )\n\n        return self._Agent(\n            reactor=reactor,\n            contextFactory=self._contextFactory,\n            connectTimeout=timeout,\n            bindAddress=bindaddress,\n            pool=self._pool,\n        )",
        "begin_line": 280,
        "end_line": 314,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent.download_request#316",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent.download_request(self, request)",
        "snippet": "    def download_request(self, request):\n        timeout = request.meta.get('download_timeout') or self._connectTimeout\n        agent = self._get_agent(request, timeout)\n\n        # request details\n        url = urldefrag(request.url)[0]\n        method = to_bytes(request.method)\n        headers = TxHeaders(request.headers)\n        if isinstance(agent, self._TunnelingAgent):\n            headers.removeHeader(b'Proxy-Authorization')\n        if request.body:\n            bodyproducer = _RequestBodyProducer(request.body)\n        elif method == b'POST':\n            # Setting Content-Length: 0 even for POST requests is not a\n            # MUST per HTTP RFCs, but it's common behavior, and some\n            # servers require this, otherwise returning HTTP 411 Length required\n            #\n            # RFC 7230#section-3.3.2:\n            # \"a Content-Length header field is normally sent in a POST\n            # request even when the value is 0 (indicating an empty payload body).\"\n            #\n            # Twisted < 17 will not add \"Content-Length: 0\" by itself;\n            # Twisted >= 17 fixes this;\n            # Using a producer with an empty-string sends `0` as Content-Length\n            # for all versions of Twisted.\n            bodyproducer = _RequestBodyProducer(b'')\n        else:\n            bodyproducer = None\n        start_time = time()\n        d = agent.request(method, to_bytes(url, encoding='ascii'), headers, bodyproducer)\n        # set download latency\n        d.addCallback(self._cb_latency, request, start_time)\n        # response body is ready to be consumed\n        d.addCallback(self._cb_bodyready, request)\n        d.addCallback(self._cb_bodydone, request, url)\n        # check download timeout\n        self._timeout_cl = reactor.callLater(timeout, d.cancel)\n        d.addBoth(self._cb_timeout, request, url, timeout)\n        return d",
        "begin_line": 316,
        "end_line": 354,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_timeout#356",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_timeout(self, result, request, url, timeout)",
        "snippet": "    def _cb_timeout(self, result, request, url, timeout):\n        if self._timeout_cl.active():\n            self._timeout_cl.cancel()\n            return result\n        # needed for HTTPS requests, otherwise _ResponseReader doesn't\n        # receive connectionLost()\n        if self._txresponse:\n            self._txresponse._transport.stopProducing()\n\n        raise TimeoutError(\"Getting %s took longer than %s seconds.\" % (url, timeout))",
        "begin_line": 356,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_latency#367",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_latency(self, result, request, start_time)",
        "snippet": "    def _cb_latency(self, result, request, start_time):\n        request.meta['download_latency'] = time() - start_time\n        return result",
        "begin_line": 367,
        "end_line": 369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017067759003242875,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodyready#371",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodyready(self, txresponse, request)",
        "snippet": "    def _cb_bodyready(self, txresponse, request):\n        # deliverBody hangs for responses without body\n        if txresponse.length == 0:\n            return txresponse, b'', None\n\n        maxsize = request.meta.get('download_maxsize', self._maxsize)\n        warnsize = request.meta.get('download_warnsize', self._warnsize)\n        expected_size = txresponse.length if txresponse.length != UNKNOWN_LENGTH else -1\n        fail_on_dataloss = request.meta.get('download_fail_on_dataloss', self._fail_on_dataloss)\n\n        if maxsize and expected_size > maxsize:\n            error_msg = (\"Cancelling download of %(url)s: expected response \"\n                         \"size (%(size)s) larger than download max size (%(maxsize)s).\")\n            error_args = {'url': request.url, 'size': expected_size, 'maxsize': maxsize}\n\n            logger.error(error_msg, error_args)\n            txresponse._transport._producer.loseConnection()\n            raise defer.CancelledError(error_msg % error_args)\n\n        if warnsize and expected_size > warnsize:\n            logger.warning(\"Expected response size (%(size)s) larger than \"\n                           \"download warn size (%(warnsize)s) in request %(request)s.\",\n                           {'size': expected_size, 'warnsize': warnsize, 'request': request})\n\n        def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()\n\n        d = defer.Deferred(_cancel)\n        txresponse.deliverBody(\n            _ResponseReader(d, txresponse, request, maxsize, warnsize, fail_on_dataloss)\n        )\n\n        # save response for timeouts\n        self._txresponse = txresponse\n\n        return d",
        "begin_line": 371,
        "end_line": 407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cancel#395",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cancel(_)",
        "snippet": "        def _cancel(_):\n            # Abort connection immediately.\n            txresponse._transport._producer.abortConnection()",
        "begin_line": 395,
        "end_line": 397,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodydone#409",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11.ScrapyAgent",
        "signature": "scrapy.core.downloader.handlers.http11.ScrapyAgent._cb_bodydone(self, result, request, url)",
        "snippet": "    def _cb_bodydone(self, result, request, url):\n        txresponse, body, flags = result\n        status = int(txresponse.code)\n        headers = Headers(txresponse.headers.getAllRawHeaders())\n        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)",
        "begin_line": 409,
        "end_line": 414,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017190991920233798,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.__init__#420",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.__init__(self, body)",
        "snippet": "    def __init__(self, body):\n        self.body = body\n        self.length = len(body)",
        "begin_line": 420,
        "end_line": 422,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002632964718272775,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.startProducing#424",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.startProducing(self, consumer)",
        "snippet": "    def startProducing(self, consumer):\n        consumer.write(self.body)\n        return defer.succeed(None)",
        "begin_line": 424,
        "end_line": 426,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002632964718272775,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.pauseProducing#428",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._RequestBodyProducer",
        "signature": "scrapy.core.downloader.handlers.http11._RequestBodyProducer.pauseProducing(self)",
        "snippet": "    def pauseProducing(self):\n        pass",
        "begin_line": 428,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.__init__#437",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.__init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss)",
        "snippet": "    def __init__(self, finished, txresponse, request, maxsize, warnsize, fail_on_dataloss):\n        self._finished = finished\n        self._txresponse = txresponse\n        self._request = request\n        self._bodybuf = BytesIO()\n        self._maxsize = maxsize\n        self._warnsize = warnsize\n        self._fail_on_dataloss = fail_on_dataloss\n        self._fail_on_dataloss_warned = False\n        self._reached_warnsize = False\n        self._bytes_received = 0",
        "begin_line": 437,
        "end_line": 447,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017238407171177384,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.dataReceived#449",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.dataReceived(self, bodyBytes)",
        "snippet": "    def dataReceived(self, bodyBytes):\n        # This maybe called several times after cancel was called with buffered data.\n        if self._finished.called:\n            return\n\n        self._bodybuf.write(bodyBytes)\n        self._bytes_received += len(bodyBytes)\n\n        if self._maxsize and self._bytes_received > self._maxsize:\n            logger.error(\"Received (%(bytes)s) bytes larger than download \"\n                         \"max size (%(maxsize)s) in request %(request)s.\",\n                         {'bytes': self._bytes_received,\n                          'maxsize': self._maxsize,\n                          'request': self._request})\n            # Clear buffer earlier to avoid keeping data in memory for a long time.\n            self._bodybuf.truncate(0)\n            self._finished.cancel()\n\n        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:\n            self._reached_warnsize = True\n            logger.warning(\"Received more bytes than download \"\n                           \"warn size (%(warnsize)s) in request %(request)s.\",\n                           {'warnsize': self._warnsize,\n                            'request': self._request})",
        "begin_line": 449,
        "end_line": 472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http11._ResponseReader.connectionLost#474",
        "src_path": "scrapy/core/downloader/handlers/http11.py",
        "class_name": "scrapy.core.downloader.handlers.http11._ResponseReader",
        "signature": "scrapy.core.downloader.handlers.http11._ResponseReader.connectionLost(self, reason)",
        "snippet": "    def connectionLost(self, reason):\n        if self._finished.called:\n            return\n\n        body = self._bodybuf.getvalue()\n        if reason.check(ResponseDone):\n            self._finished.callback((self._txresponse, body, None))\n            return\n\n        if reason.check(PotentialDataLoss):\n            self._finished.callback((self._txresponse, body, ['partial']))\n            return\n\n        if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n            if not self._fail_on_dataloss:\n                self._finished.callback((self._txresponse, body, ['dataloss']))\n                return\n\n            elif not self._fail_on_dataloss_warned:\n                logger.warning(\"Got data loss in %s. If you want to process broken \"\n                               \"responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False\"\n                               \" -- This message won't be shown in further requests\",\n                               self._txresponse.request.absoluteURI.decode())\n                self._fail_on_dataloss_warned = True\n\n        self._finished.errback(reason)",
        "begin_line": 474,
        "end_line": 499,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.console._embed_standard_shell#43",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console._embed_standard_shell(namespace={}, banner='')",
        "snippet": "def _embed_standard_shell(namespace={}, banner=''):\n    \"\"\"Start a standard python shell\"\"\"\n    import code\n    try: # readline module is only available on unix systems\n        import readline\n    except ImportError:\n        pass\n    else:\n        import rlcompleter\n        readline.parse_and_bind(\"tab:complete\")\n    @wraps(_embed_standard_shell)\n    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)\n    return wrapper",
        "begin_line": 43,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.console.wrapper#54",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console.wrapper(namespace=namespace, banner='')",
        "snippet": "    def wrapper(namespace=namespace, banner=''):\n        code.interact(banner=banner, local=namespace)",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.console.get_shell_embed_func#65",
        "src_path": "scrapy/utils/console.py",
        "class_name": "scrapy.utils.console",
        "signature": "scrapy.utils.console.get_shell_embed_func(shells=None, known_shells=None)",
        "snippet": "def get_shell_embed_func(shells=None, known_shells=None):\n    \"\"\"Return the first acceptable shell-embed function\n    from a given list of shell names.\n    \"\"\"\n    if shells is None: # list, preference order of shells\n        shells = DEFAULT_PYTHON_SHELLS.keys()\n    if known_shells is None: # available embeddable shells\n        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n    for shell in shells:\n        if shell in known_shells:\n            try:\n                # function test: run all setup code (imports),\n                # but dont fall into the shell\n                return known_shells[shell]()\n            except ImportError:\n                continue",
        "begin_line": 65,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.from_settings#11",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls()",
        "begin_line": 11,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.request_seen#14",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.request_seen(self, request)",
        "snippet": "    def request_seen(self, request):\n        return False",
        "begin_line": 14,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.open#17",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.open(self)",
        "snippet": "    def open(self):  # can return deferred\n        pass",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017988846914912754,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.close#20",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.close(self, reason)",
        "snippet": "    def close(self, reason):  # can return a deferred\n        pass",
        "begin_line": 20,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.__init__#30",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.__init__(self, path=None, debug=False)",
        "snippet": "    def __init__(self, path=None, debug=False):\n        self.file = None\n        self.fingerprints = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.file.seek(0)\n            self.fingerprints.update(x.rstrip() for x in self.file)",
        "begin_line": 30,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.from_settings#42",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        debug = settings.getbool('DUPEFILTER_DEBUG')\n        return cls(job_dir(settings), debug)",
        "begin_line": 42,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_seen#46",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_seen(self, request)",
        "snippet": "    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)",
        "begin_line": 46,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint#54",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint(self, request)",
        "snippet": "    def request_fingerprint(self, request):\n        return request_fingerprint(request)",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002535496957403651,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.close#57",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.file:\n            self.file.close()",
        "begin_line": 57,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.log#61",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.log(self, request, spider)",
        "snippet": "    def log(self, request, spider):\n        if self.debug:\n            msg = \"Filtered duplicate request: %(request)s (referer: %(referer)s)\"\n            args = {'request': request, 'referer': referer_str(request) }\n            self.logger.debug(msg, args, extra={'spider': spider})\n        elif self.logdupes:\n            msg = (\"Filtered duplicate request: %(request)s\"\n                   \" - no more duplicates will be shown\"\n                   \" (see DUPEFILTER_DEBUG to show all duplicates)\")\n            self.logger.debug(msg, {'request': request}, extra={'spider': spider})\n            self.logdupes = False\n\n        spider.crawler.stats.inc_value('dupefilter/filtered', spider=spider)",
        "begin_line": 61,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.engine.get_engine_status#6",
        "src_path": "scrapy/utils/engine.py",
        "class_name": "scrapy.utils.engine",
        "signature": "scrapy.utils.engine.get_engine_status(engine)",
        "snippet": "def get_engine_status(engine):\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"engine.has_capacity()\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle(engine.spider)\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]\n        except Exception as e:\n            checks += [(test, \"%s (exception)\" % type(e).__name__)]\n\n    return checks",
        "begin_line": 6,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__#17",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__(self, maxlength)",
        "snippet": "    def __init__(self, maxlength):\n        self.maxlength = maxlength",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.from_settings#21",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        maxlength = settings.getint('URLLENGTH_LIMIT')\n        if not maxlength:\n            raise NotConfigured\n        return cls(maxlength)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output#27",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022686025408348456,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter#28",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.template.render_templatefile#8",
        "src_path": "scrapy/utils/template.py",
        "class_name": "scrapy.utils.template",
        "signature": "scrapy.utils.template.render_templatefile(path, **kwargs)",
        "snippet": "def render_templatefile(path, **kwargs):\n    with open(path, 'rb') as fp:\n        raw = fp.read().decode('utf8')\n\n    content = string.Template(raw).substitute(**kwargs)\n\n    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n    with open(render_path, 'wb') as fp:\n        fp.write(content.encode('utf8'))\n    if path.endswith('.tmpl'):\n        os.remove(path)",
        "begin_line": 8,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__init__#10",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__init__(self, seq=None, encoding='utf-8')",
        "snippet": "    def __init__(self, seq=None, encoding='utf-8'):\n        self.encoding = encoding\n        super(Headers, self).__init__(seq)",
        "begin_line": 10,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017543859649122806,
            "pseudo_dstar_susp": 0.02857142857142857,
            "pseudo_tarantula_susp": 0.015625,
            "pseudo_op2_susp": 0.02857142857142857,
            "pseudo_barinel_susp": 0.015625
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normkey#14",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.012345679012345678,
            "pseudo_dstar_susp": 0.017857142857142856,
            "pseudo_tarantula_susp": 0.012658227848101266,
            "pseudo_op2_susp": 0.017857142857142856,
            "pseudo_barinel_susp": 0.012658227848101266
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normvalue#18",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (six.text_type, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]",
        "begin_line": 18,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014084507042253521,
            "pseudo_dstar_susp": 0.022727272727272728,
            "pseudo_tarantula_susp": 0.0136986301369863,
            "pseudo_op2_susp": 0.022727272727272728,
            "pseudo_barinel_susp": 0.0136986301369863
        }
    },
    {
        "name": "scrapy.http.headers.Headers._tobytes#29",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers._tobytes(self, x)",
        "snippet": "    def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, six.text_type):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return six.text_type(x).encode(self.encoding)\n        else:\n            raise TypeError('Unsupported value type: {}'.format(type(x)))",
        "begin_line": 29,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013888888888888888,
            "pseudo_dstar_susp": 0.02040816326530612,
            "pseudo_tarantula_susp": 0.01639344262295082,
            "pseudo_op2_susp": 0.02040816326530612,
            "pseudo_barinel_susp": 0.01639344262295082
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__getitem__#39",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        try:\n            return super(Headers, self).__getitem__(key)[-1]\n        except IndexError:\n            return None",
        "begin_line": 39,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.get#45",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        try:\n            return super(Headers, self).get(key, def_val)[-1]\n        except IndexError:\n            return None",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015976993129892953,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.getlist#51",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.getlist(self, key, def_val=None)",
        "snippet": "    def getlist(self, key, def_val=None):\n        try:\n            return super(Headers, self).__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []",
        "begin_line": 51,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlist#59",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlist(self, key, list_)",
        "snippet": "    def setlist(self, key, list_):\n        self[key] = list_",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlistdefault#62",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlistdefault(self, key, default_list=())",
        "snippet": "    def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.appendlist#65",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.appendlist(self, key, value)",
        "snippet": "    def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst",
        "begin_line": 65,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023299161230195712,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.items#70",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.items(self)",
        "snippet": "    def items(self):\n        return list(self.iteritems())",
        "begin_line": 70,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015910898965791568,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.iteritems#73",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.iteritems(self)",
        "snippet": "    def iteritems(self):\n        return ((k, self.getlist(k)) for k in self.keys())",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015910898965791568,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.values#76",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.values(self)",
        "snippet": "    def values(self):\n        return [self[k] for k in self.keys()]",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_string#79",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_string(self)",
        "snippet": "    def to_string(self):\n        return headers_dict_to_raw(self)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_unicode_dict#82",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_unicode_dict(self)",
        "snippet": "    def to_unicode_dict(self):\n        \"\"\" Return headers as a CaselessDict with unicode keys\n        and unicode values. Multiple values are joined with ','.\n        \"\"\"\n        return CaselessDict(\n            (to_unicode(key, encoding=self.encoding),\n             to_unicode(b','.join(value), encoding=self.encoding))\n            for key, value in self.items())",
        "begin_line": 82,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__copy__#91",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 91,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.__init__#12",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.__init__(self, headers)",
        "snippet": "    def __init__(self, headers):\n        self._headers = headers",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018291567587342235,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.from_crawler#16",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])\n        return cls(headers.items())",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018291567587342235,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.process_request#20",
        "src_path": "scrapy/downloadermiddlewares/defaultheaders.py",
        "class_name": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware",
        "signature": "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        for k, v in self._headers:\n            request.headers.setdefault(k, v)",
        "begin_line": 20,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf.build_component_list#17",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.build_component_list(compdict, custom=None, convert=update_classpath)",
        "snippet": "def build_component_list(compdict, custom=None, convert=update_classpath):\n    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))\n\n    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in six.iteritems(compdict):\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError('Some paths in {!r} convert to the same '\n                                     'object, please update your settings'\n                                     ''.format(list(compdict.keys())))\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in six.iteritems(compdict)}\n\n    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in six.iteritems(compdict):\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                 'a real number or None instead'.format(value, name))\n\n    # BEGIN Backward compatibility for old (base, custom) call signature\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    if custom is not None:\n        compdict.update(custom)\n    # END Backward compatibility\n\n    _validate_values(compdict)\n    compdict = without_none_values(_map_keys(compdict))\n    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]",
        "begin_line": 17,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf._check_components#20",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._check_components(complist)",
        "snippet": "    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf._map_keys#25",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._map_keys(compdict)",
        "snippet": "    def _map_keys(compdict):\n        if isinstance(compdict, BaseSettings):\n            compbs = BaseSettings()\n            for k, v in six.iteritems(compdict):\n                prio = compdict.getpriority(k)\n                if compbs.getpriority(convert(k)) == prio:\n                    raise ValueError('Some paths in {!r} convert to the same '\n                                     'object, please update your settings'\n                                     ''.format(list(compdict.keys())))\n                else:\n                    compbs.set(convert(k), v, priority=prio)\n            return compbs\n        else:\n            _check_components(compdict)\n            return {convert(k): v for k, v in six.iteritems(compdict)}",
        "begin_line": 25,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf._validate_values#41",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._validate_values(compdict)",
        "snippet": "    def _validate_values(compdict):\n        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n        for name, value in six.iteritems(compdict):\n            if value is not None and not isinstance(value, numbers.Real):\n                raise ValueError('Invalid value {} for component {}, please provide ' \\\n                                 'a real number or None instead'.format(value, name))",
        "begin_line": 41,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf.arglist_to_dict#62",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.arglist_to_dict(arglist)",
        "snippet": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)",
        "begin_line": 62,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf.closest_scrapy_cfg#69",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.closest_scrapy_cfg(path='.', prevpath=None)",
        "snippet": "def closest_scrapy_cfg(path='.', prevpath=None):\n    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n    directory and its parents\n    \"\"\"\n    if path == prevpath:\n        return ''\n    path = os.path.abspath(path)\n    cfgfile = os.path.join(path, 'scrapy.cfg')\n    if os.path.exists(cfgfile):\n        return cfgfile\n    return closest_scrapy_cfg(os.path.dirname(path), path)",
        "begin_line": 69,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf.get_config#97",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.get_config(use_closest=True)",
        "snippet": "def get_config(use_closest=True):\n    \"\"\"Get Scrapy config file as a ConfigParser\"\"\"\n    sources = get_sources(use_closest)\n    cfg = ConfigParser()\n    cfg.read(sources)\n    return cfg",
        "begin_line": 97,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.conf.get_sources#105",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.get_sources(use_closest=True)",
        "snippet": "def get_sources(use_closest=True):\n    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\n        os.path.expanduser('~/.config')\n    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n               xdg_config_home + '/scrapy.cfg',\n               os.path.expanduser('~/.scrapy.cfg')]\n    if use_closest:\n        sources.append(closest_scrapy_cfg())\n    return sources",
        "begin_line": 105,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.__init__#11",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n        self._settings = settings",
        "begin_line": 11,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00026014568158168577,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.download_request#16",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        \"\"\"Return a deferred for the HTTP download\"\"\"\n        factory = self.HTTPClientFactory(request)\n        self._connect(factory)\n        return factory.deferred",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00026014568158168577,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler._connect#22",
        "src_path": "scrapy/core/downloader/handlers/http10.py",
        "class_name": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.http10.HTTP10DownloadHandler._connect(self, factory)",
        "snippet": "    def _connect(self, factory):\n        host, port = to_unicode(factory.host), factory.port\n        if factory.scheme == b'https':\n            client_context_factory = create_instance(self.ClientContextFactory, settings=self._settings, crawler=None)\n            return reactor.connectSSL(host, port, factory, client_context_factory)\n        else:\n            return reactor.connectTCP(host, port, factory)",
        "begin_line": 22,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.__init__#42",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.__init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None, pqclass=None, crawler=None)",
        "snippet": "    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,\n                 logunser=False, stats=None, pqclass=None, crawler=None):\n        self.df = dupefilter\n        self.dqdir = self._dqdir(jobdir)\n        self.pqclass = pqclass\n        self.dqclass = dqclass\n        self.mqclass = mqclass\n        self.logunser = logunser\n        self.stats = stats\n        self.crawler = crawler",
        "begin_line": 42,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.from_crawler#54",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n        dupefilter = create_instance(dupefilter_cls, settings, crawler)\n        pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n        if pqclass is PriorityQueue:\n            warnings.warn(\"SCHEDULER_PRIORITY_QUEUE='queuelib.PriorityQueue'\"\n                          \" is no longer supported because of API changes; \"\n                          \"please use 'scrapy.pqueues.ScrapyPriorityQueue'\",\n                          ScrapyDeprecationWarning)\n            from scrapy.pqueues import ScrapyPriorityQueue\n            pqclass = ScrapyPriorityQueue\n\n        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n        mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS',\n                                    settings.getbool('SCHEDULER_DEBUG'))\n        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                   stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n                   mqclass=mqclass, crawler=crawler)",
        "begin_line": 54,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.has_pending_requests#75",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.has_pending_requests(self)",
        "snippet": "    def has_pending_requests(self):\n        return len(self) > 0",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.open#78",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.open(self, spider)",
        "snippet": "    def open(self, spider):\n        self.spider = spider\n        self.mqs = self._mq()\n        self.dqs = self._dq() if self.dqdir else None\n        return self.df.open()",
        "begin_line": 78,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001812250815512867,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.close#84",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.dqs:\n            state = self.dqs.close()\n            self._write_dqs_state(self.dqdir, state)\n        return self.df.close(reason)",
        "begin_line": 84,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.enqueue_request#90",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.enqueue_request(self, request)",
        "snippet": "    def enqueue_request(self, request):\n        if not request.dont_filter and self.df.request_seen(request):\n            self.df.log(request, self.spider)\n            return False\n        dqok = self._dqpush(request)\n        if dqok:\n            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n        else:\n            self._mqpush(request)\n            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n        return True",
        "begin_line": 90,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.next_request#103",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.next_request(self)",
        "snippet": "    def next_request(self):\n        request = self.mqs.pop()\n        if request:\n            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n        else:\n            request = self._dqpop()\n            if request:\n                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n        if request:\n            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n        return request",
        "begin_line": 103,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler.__len__#115",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)",
        "begin_line": 115,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqpush#118",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqpush(self, request)",
        "snippet": "    def _dqpush(self, request):\n        if self.dqs is None:\n            return\n        try:\n            self.dqs.push(request, -request.priority)\n        except ValueError as e:  # non serializable request\n            if self.logunser:\n                msg = (\"Unable to serialize request: %(request)s - reason:\"\n                       \" %(reason)s - no more unserializable requests will be\"\n                       \" logged (stats being collected)\")\n                logger.warning(msg, {'request': request, 'reason': e},\n                               exc_info=True, extra={'spider': self.spider})\n                self.logunser = False\n            self.stats.inc_value('scheduler/unserializable',\n                                 spider=self.spider)\n            return\n        else:\n            return True",
        "begin_line": 118,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._mqpush#137",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._mqpush(self, request)",
        "snippet": "    def _mqpush(self, request):\n        self.mqs.push(request, -request.priority)",
        "begin_line": 137,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqpop#140",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqpop(self)",
        "snippet": "    def _dqpop(self):\n        if self.dqs:\n            return self.dqs.pop()",
        "begin_line": 140,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._newmq#144",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._newmq(self, priority)",
        "snippet": "    def _newmq(self, priority):\n        \"\"\" Factory for creating memory queues. \"\"\"\n        return self.mqclass()",
        "begin_line": 144,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._newdq#148",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._newdq(self, priority)",
        "snippet": "    def _newdq(self, priority):\n        \"\"\" Factory for creating disk queues. \"\"\"\n        path = join(self.dqdir, 'p%s' % (priority, ))\n        return self.dqclass(path)",
        "begin_line": 148,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._mq#153",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._mq(self)",
        "snippet": "    def _mq(self):\n        \"\"\" Create a new priority queue instance, with in-memory storage \"\"\"\n        return create_instance(self.pqclass, None, self.crawler, self._newmq,\n                               serialize=False)",
        "begin_line": 153,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001808972503617945,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dq#158",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dq(self)",
        "snippet": "    def _dq(self):\n        \"\"\" Create a new priority queue instance, with disk storage \"\"\"\n        state = self._read_dqs_state(self.dqdir)\n        q = create_instance(self.pqclass,\n                            None,\n                            self.crawler,\n                            self._newdq,\n                            state,\n                            serialize=True)\n        if q:\n            logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                        {'queuesize': len(q)}, extra={'spider': self.spider})\n        return q",
        "begin_line": 158,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._dqdir#172",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._dqdir(self, jobdir)",
        "snippet": "    def _dqdir(self, jobdir):\n        \"\"\" Return a folder name to keep disk queue state at \"\"\"\n        if jobdir:\n            dqdir = join(jobdir, 'requests.queue')\n            if not exists(dqdir):\n                os.makedirs(dqdir)\n            return dqdir",
        "begin_line": 172,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._read_dqs_state#180",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._read_dqs_state(self, dqdir)",
        "snippet": "    def _read_dqs_state(self, dqdir):\n        path = join(dqdir, 'active.json')\n        if not exists(path):\n            return ()\n        with open(path) as f:\n            return json.load(f)",
        "begin_line": 180,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scheduler.Scheduler._write_dqs_state#187",
        "src_path": "scrapy/core/scheduler.py",
        "class_name": "scrapy.core.scheduler.Scheduler",
        "signature": "scrapy.core.scheduler.Scheduler._write_dqs_state(self, dqdir, state)",
        "snippet": "    def _write_dqs_state(self, dqdir, state):\n        with open(join(dqdir, 'active.json'), 'w') as f:\n            json.dump(state, f)",
        "begin_line": 187,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.file.FileDownloadHandler.__init__#9",
        "src_path": "scrapy/core/downloader/handlers/file.py",
        "class_name": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.file.FileDownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        pass",
        "begin_line": 9,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018635855385762206,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.file.FileDownloadHandler.download_request#13",
        "src_path": "scrapy/core/downloader/handlers/file.py",
        "class_name": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.file.FileDownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        filepath = file_uri_to_path(request.url)\n        with open(filepath, 'rb') as fo:\n            body = fo.read()\n        respcls = responsetypes.from_args(filename=filepath, body=body)\n        return respcls(url=request.url, body=body)",
        "begin_line": 13,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__init__#197",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__init__(self, seq=None)",
        "snippet": "    def __init__(self, seq=None):\n        super(CaselessDict, self).__init__()\n        if seq:\n            self.update(seq)",
        "begin_line": 197,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.016666666666666666,
            "pseudo_dstar_susp": 0.02631578947368421,
            "pseudo_tarantula_susp": 0.014925373134328358,
            "pseudo_op2_susp": 0.02631578947368421,
            "pseudo_barinel_susp": 0.014925373134328358
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__getitem__#202",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))",
        "begin_line": 202,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001585791309863622,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__setitem__#205",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))",
        "begin_line": 205,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001762114537444934,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__delitem__#208",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))",
        "begin_line": 208,
        "end_line": 209,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__contains__#211",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))",
        "begin_line": 211,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001594896331738437,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__copy__#215",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 215,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normkey#219",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()",
        "begin_line": 219,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000273224043715847,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normvalue#223",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value",
        "begin_line": 223,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000273224043715847,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.get#227",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 227,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015738117721120554,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.setdefault#230",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.setdefault(self, key, def_val=None)",
        "snippet": "    def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 230,
        "end_line": 231,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.018518518518518517,
            "pseudo_dstar_susp": 0.022222222222222223,
            "pseudo_tarantula_susp": 0.037037037037037035,
            "pseudo_op2_susp": 0.02127659574468085,
            "pseudo_barinel_susp": 0.037037037037037035
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.update#233",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.update(self, seq)",
        "snippet": "    def update(self, seq):\n        seq = seq.items() if isinstance(seq, Mapping) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super(CaselessDict, self).update(iseq)",
        "begin_line": 233,
        "end_line": 236,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015830299192654741,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.fromkeys#239",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.fromkeys(cls, keys, value=None)",
        "snippet": "    def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)",
        "begin_line": 239,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.pop#242",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)",
        "begin_line": 242,
        "end_line": 243,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017777777777777779,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.SequenceExclude.__init__#326",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.SequenceExclude",
        "signature": "scrapy.utils.datatypes.SequenceExclude.__init__(self, seq)",
        "snippet": "    def __init__(self, seq):\n        self.seq = seq",
        "begin_line": 326,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.datatypes.SequenceExclude.__contains__#329",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.SequenceExclude",
        "signature": "scrapy.utils.datatypes.SequenceExclude.__contains__(self, item)",
        "snippet": "    def __contains__(self, item):\n        return item not in self.seq",
        "begin_line": 329,
        "end_line": 330,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.decode_robotstxt#10",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt",
        "signature": "scrapy.robotstxt.decode_robotstxt(robotstxt_body, spider, to_native_str_type=False)",
        "snippet": "def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n    try:\n        if to_native_str_type:\n            robotstxt_body = to_native_str(robotstxt_body)\n        else:\n            robotstxt_body = robotstxt_body.decode('utf-8')\n    except UnicodeDecodeError:\n        # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n        # Switch to 'allow all' state.\n        logger.warning(\"Failure while parsing robots.txt. \"\n                       \"File either contains garbage or is in an encoding other than UTF-8, treating it as an empty file.\",\n                       exc_info=sys.exc_info(),\n                       extra={'spider': spider})\n        robotstxt_body = ''\n    return robotstxt_body",
        "begin_line": 10,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.RobotParser.from_crawler#29",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.RobotParser",
        "signature": "scrapy.robotstxt.RobotParser.from_crawler(cls, crawler, robotstxt_body)",
        "snippet": "    def from_crawler(cls, crawler, robotstxt_body):\n        \"\"\"Parse the content of a robots.txt_ file as bytes. This must be a class method.\n        It must return a new instance of the parser backend.\n\n        :param crawler: crawler which made the request\n        :type crawler: :class:`~scrapy.crawler.Crawler` instance\n\n        :param robotstxt_body: content of a robots.txt_ file.\n        :type robotstxt_body: bytes\n        \"\"\"\n        pass",
        "begin_line": 29,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.RobotParser.allowed#42",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.RobotParser",
        "signature": "scrapy.robotstxt.RobotParser.allowed(self, url, user_agent)",
        "snippet": "    def allowed(self, url, user_agent):\n        \"\"\"Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``.\n\n        :param url: Absolute URL\n        :type url: string\n\n        :param user_agent: User agent\n        :type user_agent: string\n        \"\"\"\n        pass",
        "begin_line": 42,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.PythonRobotParser.__init__#55",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.PythonRobotParser",
        "signature": "scrapy.robotstxt.PythonRobotParser.__init__(self, robotstxt_body, spider)",
        "snippet": "    def __init__(self, robotstxt_body, spider):\n        from six.moves.urllib_robotparser import RobotFileParser\n        self.spider = spider\n        robotstxt_body = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n        self.rp = RobotFileParser()\n        self.rp.parse(robotstxt_body.splitlines())",
        "begin_line": 55,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.PythonRobotParser.from_crawler#63",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.PythonRobotParser",
        "signature": "scrapy.robotstxt.PythonRobotParser.from_crawler(cls, crawler, robotstxt_body)",
        "snippet": "    def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o",
        "begin_line": 63,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.PythonRobotParser.allowed#68",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.PythonRobotParser",
        "signature": "scrapy.robotstxt.PythonRobotParser.allowed(self, url, user_agent)",
        "snippet": "    def allowed(self, url, user_agent):\n        user_agent = to_native_str(user_agent)\n        url = to_native_str(url)\n        return self.rp.can_fetch(user_agent, url)",
        "begin_line": 68,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ReppyRobotParser.__init__#75",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ReppyRobotParser",
        "signature": "scrapy.robotstxt.ReppyRobotParser.__init__(self, robotstxt_body, spider)",
        "snippet": "    def __init__(self, robotstxt_body, spider):\n        from reppy.robots import Robots\n        self.spider = spider\n        self.rp = Robots.parse('', robotstxt_body)",
        "begin_line": 75,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ReppyRobotParser.from_crawler#81",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ReppyRobotParser",
        "signature": "scrapy.robotstxt.ReppyRobotParser.from_crawler(cls, crawler, robotstxt_body)",
        "snippet": "    def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o",
        "begin_line": 81,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ReppyRobotParser.allowed#86",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ReppyRobotParser",
        "signature": "scrapy.robotstxt.ReppyRobotParser.allowed(self, url, user_agent)",
        "snippet": "    def allowed(self, url, user_agent):\n        return self.rp.allowed(url, user_agent)",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.RerpRobotParser.__init__#91",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.RerpRobotParser",
        "signature": "scrapy.robotstxt.RerpRobotParser.__init__(self, robotstxt_body, spider)",
        "snippet": "    def __init__(self, robotstxt_body, spider):\n        from robotexclusionrulesparser import RobotExclusionRulesParser\n        self.spider = spider\n        self.rp = RobotExclusionRulesParser()\n        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n        self.rp.parse(robotstxt_body)",
        "begin_line": 91,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.RerpRobotParser.from_crawler#99",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.RerpRobotParser",
        "signature": "scrapy.robotstxt.RerpRobotParser.from_crawler(cls, crawler, robotstxt_body)",
        "snippet": "    def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o",
        "begin_line": 99,
        "end_line": 102,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.RerpRobotParser.allowed#104",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.RerpRobotParser",
        "signature": "scrapy.robotstxt.RerpRobotParser.allowed(self, url, user_agent)",
        "snippet": "    def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.is_allowed(user_agent, url)",
        "begin_line": 104,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ProtegoRobotParser.__init__#111",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ProtegoRobotParser",
        "signature": "scrapy.robotstxt.ProtegoRobotParser.__init__(self, robotstxt_body, spider)",
        "snippet": "    def __init__(self, robotstxt_body, spider):\n        from protego import Protego\n        self.spider = spider\n        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n        self.rp = Protego.parse(robotstxt_body)",
        "begin_line": 111,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ProtegoRobotParser.from_crawler#118",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ProtegoRobotParser",
        "signature": "scrapy.robotstxt.ProtegoRobotParser.from_crawler(cls, crawler, robotstxt_body)",
        "snippet": "    def from_crawler(cls, crawler, robotstxt_body):\n        spider = None if not crawler else crawler.spider\n        o = cls(robotstxt_body, spider)\n        return o",
        "begin_line": 118,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.robotstxt.ProtegoRobotParser.allowed#123",
        "src_path": "scrapy/robotstxt.py",
        "class_name": "scrapy.robotstxt.ProtegoRobotParser",
        "signature": "scrapy.robotstxt.ProtegoRobotParser.allowed(self, url, user_agent)",
        "snippet": "    def allowed(self, url, user_agent):\n        user_agent = to_unicode(user_agent)\n        url = to_unicode(url)\n        return self.rp.can_fetch(url, user_agent)",
        "begin_line": 123,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__#28",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__(self)",
        "snippet": "    def __init__(self):\n        self._formats = {\n            'tar': self._is_tar,\n            'zip': self._is_zip,\n            'gz': self._is_gzip,\n            'bz2': self._is_bzip2\n        }",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar#36",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar(self, response)",
        "snippet": "    def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip#47",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip(self, response)",
        "snippet": "    def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip#59",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip(self, response)",
        "snippet": "    def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 59,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2#69",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2(self, response)",
        "snippet": "    def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 69,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response#78",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in six.iteritems(self._formats):\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response",
        "begin_line": 78,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.__init__#24",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.should_cache_request#28",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.should_cache_request(self, request)",
        "snippet": "    def should_cache_request(self, request):\n        return urlparse_cached(request).scheme not in self.ignore_schemes",
        "begin_line": 28,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.should_cache_response#31",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.should_cache_response(self, response, request)",
        "snippet": "    def should_cache_response(self, response, request):\n        return response.status not in self.ignore_http_codes",
        "begin_line": 31,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_fresh#34",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_fresh(self, cachedresponse, request)",
        "snippet": "    def is_cached_response_fresh(self, cachedresponse, request):\n        return True",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_valid#37",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DummyPolicy",
        "signature": "scrapy.extensions.httpcache.DummyPolicy.is_cached_response_valid(self, cachedresponse, response, request)",
        "snippet": "    def is_cached_response_valid(self, cachedresponse, response, request):\n        return True",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.__init__#45",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n        self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n        self.ignore_response_cache_controls = [to_bytes(cc) for cc in\n            settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')]\n        self._cc_parsed = WeakKeyDictionary()",
        "begin_line": 45,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002557544757033248,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._parse_cachecontrol#52",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._parse_cachecontrol(self, r)",
        "snippet": "    def _parse_cachecontrol(self, r):\n        if r not in self._cc_parsed:\n            cch = r.headers.get(b'Cache-Control', b'')\n            parsed = parse_cachecontrol(cch)\n            if isinstance(r, Response):\n                for key in self.ignore_response_cache_controls:\n                    parsed.pop(key, None)\n            self._cc_parsed[r] = parsed\n        return self._cc_parsed[r]",
        "begin_line": 52,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_request#62",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_request(self, request)",
        "snippet": "    def should_cache_request(self, request):\n        if urlparse_cached(request).scheme in self.ignore_schemes:\n            return False\n        cc = self._parse_cachecontrol(request)\n        # obey user-agent directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Any other is eligible for caching\n        return True",
        "begin_line": 62,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_response#72",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.should_cache_response(self, response, request)",
        "snippet": "    def should_cache_response(self, response, request):\n        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n        # Status code 206 is not included because cache can not deal with partial contents\n        cc = self._parse_cachecontrol(response)\n        # obey directive \"Cache-Control: no-store\"\n        if b'no-store' in cc:\n            return False\n        # Never cache 304 (Not Modified) responses\n        elif response.status == 304:\n            return False\n        # Cache unconditionally if configured to do so\n        elif self.always_store:\n            return True\n        # Any hint on response expiration is good\n        elif b'max-age' in cc or b'Expires' in response.headers:\n            return True\n        # Firefox fallbacks this statuses to one year expiration if none is set\n        elif response.status in (300, 301, 308):\n            return True\n        # Other statuses without expiration requires at least one validator\n        elif response.status in (200, 203, 401):\n            return b'Last-Modified' in response.headers or b'ETag' in response.headers\n        # Any other is probably not eligible for caching\n        # Makes no sense to cache responses that does not contain expiration\n        # info and can not be revalidated\n        else:\n            return False",
        "begin_line": 72,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_fresh#101",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_fresh(self, cachedresponse, request)",
        "snippet": "    def is_cached_response_fresh(self, cachedresponse, request):\n        cc = self._parse_cachecontrol(cachedresponse)\n        ccreq = self._parse_cachecontrol(request)\n        if b'no-cache' in cc or b'no-cache' in ccreq:\n            return False\n\n        now = time()\n        freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n        currentage = self._compute_current_age(cachedresponse, request, now)\n\n        reqmaxage = self._get_max_age(ccreq)\n        if reqmaxage is not None:\n            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n\n        if currentage < freshnesslifetime:\n            return True\n\n        if b'max-stale' in ccreq and b'must-revalidate' not in cc:\n            # From RFC2616: \"Indicates that the client is willing to\n            # accept a response that has exceeded its expiration time.\n            # If max-stale is assigned a value, then the client is\n            # willing to accept a response that has exceeded its\n            # expiration time by no more than the specified number of\n            # seconds. If no value is assigned to max-stale, then the\n            # client is willing to accept a stale response of any age.\"\n            staleage = ccreq[b'max-stale']\n            if staleage is None:\n                return True\n\n            try:\n                if currentage < freshnesslifetime + max(0, int(staleage)):\n                    return True\n            except ValueError:\n                pass\n\n        # Cached response is stale, try to set validators if any\n        self._set_conditional_validators(request, cachedresponse)\n        return False",
        "begin_line": 101,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_valid#140",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy.is_cached_response_valid(self, cachedresponse, response, request)",
        "snippet": "    def is_cached_response_valid(self, cachedresponse, response, request):\n        # Use the cached response if the new response is a server error,\n        # as long as the old response didn't specify must-revalidate.\n        if response.status >= 500:\n            cc = self._parse_cachecontrol(cachedresponse)\n            if b'must-revalidate' not in cc:\n                return True\n\n        # Use the cached response if the server says it hasn't changed.\n        return response.status == 304",
        "begin_line": 140,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._set_conditional_validators#151",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._set_conditional_validators(self, request, cachedresponse)",
        "snippet": "    def _set_conditional_validators(self, request, cachedresponse):\n        if b'Last-Modified' in cachedresponse.headers:\n            request.headers[b'If-Modified-Since'] = cachedresponse.headers[b'Last-Modified']\n\n        if b'ETag' in cachedresponse.headers:\n            request.headers[b'If-None-Match'] = cachedresponse.headers[b'ETag']",
        "begin_line": 151,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._get_max_age#158",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._get_max_age(self, cc)",
        "snippet": "    def _get_max_age(self, cc):\n        try:\n            return max(0, int(cc[b'max-age']))\n        except (KeyError, ValueError):\n            return None",
        "begin_line": 158,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._compute_freshness_lifetime#164",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._compute_freshness_lifetime(self, response, request, now)",
        "snippet": "    def _compute_freshness_lifetime(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n        cc = self._parse_cachecontrol(response)\n        maxage = self._get_max_age(cc)\n        if maxage is not None:\n            return maxage\n\n        # Parse date header or synthesize it if none exists\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n\n        # Try HTTP/1.0 Expires header\n        if b'Expires' in response.headers:\n            expires = rfc1123_to_epoch(response.headers[b'Expires'])\n            # When parsing Expires header fails RFC 2616 section 14.21 says we\n            # should treat this as an expiration time in the past.\n            return max(0, expires - date) if expires else 0\n\n        # Fallback to heuristic using last-modified header\n        # This is not in RFC but on Firefox caching implementation\n        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))\n        if lastmodified and lastmodified <= date:\n            return (date - lastmodified) / 10\n\n        # This request can be cached indefinitely\n        if response.status in (300, 301, 308):\n            return self.MAXAGE\n\n        # Insufficient information to compute fresshness lifetime\n        return 0",
        "begin_line": 164,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.RFC2616Policy._compute_current_age#195",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.RFC2616Policy",
        "signature": "scrapy.extensions.httpcache.RFC2616Policy._compute_current_age(self, response, request, now)",
        "snippet": "    def _compute_current_age(self, response, request, now):\n        # Reference nsHttpResponseHead::ComputeCurrentAge\n        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n        currentage = 0\n        # If Date header is not set we assume it is a fast connection, and\n        # clock is in sync with the server\n        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now\n        if now > date:\n            currentage = now - date\n\n        if b'Age' in response.headers:\n            try:\n                age = int(response.headers[b'Age'])\n                currentage = max(currentage, age)\n            except ValueError:\n                pass\n\n        return currentage",
        "begin_line": 195,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.__init__#217",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n        self.db = None",
        "begin_line": 217,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00025839793281653745,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.open_spider#223",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n        self.db = self.dbmodule.open(dbpath, 'c')\n\n        logger.debug(\"Using DBM cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})",
        "begin_line": 223,
        "end_line": 227,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00025839793281653745,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.close_spider#229",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        self.db.close()",
        "begin_line": 229,
        "end_line": 230,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00025839793281653745,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.retrieve_response#232",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 232,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage.store_response#244",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)\n        self.db['%s_time' % key] = str(time())",
        "begin_line": 244,
        "end_line": 253,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage._read_data#255",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage._read_data(self, spider, request)",
        "snippet": "    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        db = self.db\n        tkey = '%s_time' % key\n        if tkey not in db:\n            return  # not found\n\n        ts = db[tkey]\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        return pickle.loads(db['%s_data' % key])",
        "begin_line": 255,
        "end_line": 266,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.DbmCacheStorage._request_key#268",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.DbmCacheStorage",
        "signature": "scrapy.extensions.httpcache.DbmCacheStorage._request_key(self, request)",
        "snippet": "    def _request_key(self, request):\n        return request_fingerprint(request)",
        "begin_line": 268,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00026014568158168577,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.__init__#274",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n        self._open = gzip.open if self.use_gzip else open",
        "begin_line": 274,
        "end_line": 278,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.open_spider#280",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        logger.debug(\"Using filesystem cache storage in %(cachedir)s\" % {'cachedir': self.cachedir},\n                     extra={'spider': spider})",
        "begin_line": 280,
        "end_line": 282,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.close_spider#284",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        pass",
        "begin_line": 284,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.retrieve_response#287",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n        metadata = self._read_meta(spider, request)\n        if metadata is None:\n            return  # not cached\n        rpath = self._get_request_path(spider, request)\n        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n            body = f.read()\n        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n            rawheaders = f.read()\n        url = metadata.get('response_url')\n        status = metadata['status']\n        headers = Headers(headers_raw_to_dict(rawheaders))\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 287,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage.store_response#304",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        \"\"\"Store the given response in the cache.\"\"\"\n        rpath = self._get_request_path(spider, request)\n        if not os.path.exists(rpath):\n            os.makedirs(rpath)\n        metadata = {\n            'url': request.url,\n            'method': request.method,\n            'status': response.status,\n            'response_url': response.url,\n            'timestamp': time(),\n        }\n        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n            f.write(to_bytes(repr(metadata)))\n        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n            pickle.dump(metadata, f, protocol=2)\n        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(response.headers))\n        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n            f.write(response.body)\n        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n            f.write(headers_dict_to_raw(request.headers))\n        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n            f.write(request.body)",
        "begin_line": 304,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage._get_request_path#329",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage._get_request_path(self, spider, request)",
        "snippet": "    def _get_request_path(self, spider, request):\n        key = request_fingerprint(request)\n        return os.path.join(self.cachedir, spider.name, key[0:2], key)",
        "begin_line": 329,
        "end_line": 331,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.FilesystemCacheStorage._read_meta#333",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.FilesystemCacheStorage",
        "signature": "scrapy.extensions.httpcache.FilesystemCacheStorage._read_meta(self, spider, request)",
        "snippet": "    def _read_meta(self, spider, request):\n        rpath = self._get_request_path(spider, request)\n        metapath = os.path.join(rpath, 'pickled_meta')\n        if not os.path.exists(metapath):\n            return  # not found\n        mtime = os.stat(metapath).st_mtime\n        if 0 < self.expiration_secs < time() - mtime:\n            return  # expired\n        with self._open(metapath, 'rb') as f:\n            return pickle.load(f)",
        "begin_line": 333,
        "end_line": 342,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.__init__#347",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        import leveldb\n        self._leveldb = leveldb\n        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n        self.db = None",
        "begin_line": 347,
        "end_line": 352,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.open_spider#354",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        dbpath = os.path.join(self.cachedir, '%s.leveldb' % spider.name)\n        self.db = self._leveldb.LevelDB(dbpath)\n\n        logger.debug(\"Using LevelDB cache storage in %(cachepath)s\" % {'cachepath': dbpath}, extra={'spider': spider})",
        "begin_line": 354,
        "end_line": 358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.close_spider#360",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        # Do compactation each time to save space and also recreate files to\n        # avoid them being removed in storages with timestamp-based autoremoval.\n        self.db.CompactRange()\n        del self.db\n        garbage_collect()",
        "begin_line": 360,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.retrieve_response#367",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.retrieve_response(self, spider, request)",
        "snippet": "    def retrieve_response(self, spider, request):\n        data = self._read_data(spider, request)\n        if data is None:\n            return  # not cached\n        url = data['url']\n        status = data['status']\n        headers = Headers(data['headers'])\n        body = data['body']\n        respcls = responsetypes.from_args(headers=headers, url=url)\n        response = respcls(url=url, headers=headers, status=status, body=body)\n        return response",
        "begin_line": 367,
        "end_line": 377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage.store_response#379",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage.store_response(self, spider, request, response)",
        "snippet": "    def store_response(self, spider, request, response):\n        key = self._request_key(request)\n        data = {\n            'status': response.status,\n            'url': response.url,\n            'headers': dict(response.headers),\n            'body': response.body,\n        }\n        batch = self._leveldb.WriteBatch()\n        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))\n        batch.Put(key + b'_time', to_bytes(str(time())))\n        self.db.Write(batch)",
        "begin_line": 379,
        "end_line": 390,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage._read_data#392",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage._read_data(self, spider, request)",
        "snippet": "    def _read_data(self, spider, request):\n        key = self._request_key(request)\n        try:\n            ts = self.db.Get(key + b'_time')\n        except KeyError:\n            return  # not found or invalid entry\n\n        if 0 < self.expiration_secs < time() - float(ts):\n            return  # expired\n\n        try:\n            data = self.db.Get(key + b'_data')\n        except KeyError:\n            return  # invalid entry\n        else:\n            return pickle.loads(data)",
        "begin_line": 392,
        "end_line": 407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.LeveldbCacheStorage._request_key#409",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache.LeveldbCacheStorage",
        "signature": "scrapy.extensions.httpcache.LeveldbCacheStorage._request_key(self, request)",
        "snippet": "    def _request_key(self, request):\n        return to_bytes(request_fingerprint(request))",
        "begin_line": 409,
        "end_line": 410,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.parse_cachecontrol#414",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache",
        "signature": "scrapy.extensions.httpcache.parse_cachecontrol(header)",
        "snippet": "def parse_cachecontrol(header):\n    \"\"\"Parse Cache-Control header\n\n    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n    ...                                                 b'max-age': b'3600'}\n    True\n    >>> parse_cachecontrol(b'') == {}\n    True\n\n    \"\"\"\n    directives = {}\n    for directive in header.split(b','):\n        key, sep, val = directive.strip().partition(b'=')\n        if key:\n            directives[key.lower()] = val if sep else None\n    return directives",
        "begin_line": 414,
        "end_line": 431,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.httpcache.rfc1123_to_epoch#434",
        "src_path": "scrapy/extensions/httpcache.py",
        "class_name": "scrapy.extensions.httpcache",
        "signature": "scrapy.extensions.httpcache.rfc1123_to_epoch(date_str)",
        "snippet": "def rfc1123_to_epoch(date_str):\n    try:\n        date_str = to_unicode(date_str, encoding='ascii')\n        return mktime_tz(parsedate_tz(date_str))\n    except Exception:\n        return None",
        "begin_line": 434,
        "end_line": 439,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.__init__#17",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler#21",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n            raise NotConfigured\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 21,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.spider_closed#28",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        gc.collect()\n        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n        for cls, wdict in six.iteritems(live_refs):\n            if not wdict:\n                continue\n            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.__init__#16",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.__init__(self, maxdepth, stats, verbose_stats=False, prio=1)",
        "snippet": "    def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):\n        self.maxdepth = maxdepth\n        self.stats = stats\n        self.verbose_stats = verbose_stats\n        self.prio = prio",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.from_crawler#23",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        settings = crawler.settings\n        maxdepth = settings.getint('DEPTH_LIMIT')\n        verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n        prio = settings.getint('DEPTH_PRIORITY')\n        return cls(maxdepth, crawler.stats, verbose, prio)",
        "begin_line": 23,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware.process_spider_output#30",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value('request_depth_count/%s' % depth,\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True\n\n        # base case (depth=0)\n        if 'depth' not in response.meta:\n            response.meta['depth'] = 0\n            if self.verbose_stats:\n                self.stats.inc_value('request_depth_count/0', spider=spider)\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 30,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.depth.DepthMiddleware._filter#31",
        "src_path": "scrapy/spidermiddlewares/depth.py",
        "class_name": "scrapy.spidermiddlewares.depth.DepthMiddleware",
        "signature": "scrapy.spidermiddlewares.depth.DepthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request):\n                depth = response.meta['depth'] + 1\n                request.meta['depth'] = depth\n                if self.prio:\n                    request.priority -= depth * self.prio\n                if self.maxdepth and depth > self.maxdepth:\n                    logger.debug(\n                        \"Ignoring link (depth > %(maxdepth)d): %(requrl)s \",\n                        {'maxdepth': self.maxdepth, 'requrl': request.url},\n                        extra={'spider': spider}\n                    )\n                    return False\n                else:\n                    if self.verbose_stats:\n                        self.stats.inc_value('request_depth_count/%s' % depth,\n                                             spider=spider)\n                    self.stats.max_value('request_depth_max', depth,\n                                         spider=spider)\n            return True",
        "begin_line": 31,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.__init__#16",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.__init__(self, auth_encoding='latin-1')",
        "snippet": "    def __init__(self, auth_encoding='latin-1'):\n        self.auth_encoding = auth_encoding\n        self.proxies = {}\n        for type_, url in getproxies().items():\n            self.proxies[type_] = self._get_proxy(url, type_)",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.from_crawler#23",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('HTTPPROXY_ENABLED'):\n            raise NotConfigured\n        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n        return cls(auth_encoding)",
        "begin_line": 23,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._basic_auth_header#29",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._basic_auth_header(self, username, password)",
        "snippet": "    def _basic_auth_header(self, username, password):\n        user_pass = to_bytes(\n            '%s:%s' % (unquote(username), unquote(password)),\n            encoding=self.auth_encoding)\n        return base64.b64encode(user_pass)",
        "begin_line": 29,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._get_proxy#35",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._get_proxy(self, url, orig_type)",
        "snippet": "    def _get_proxy(self, url, orig_type):\n        proxy_type, user, password, hostport = _parse_proxy(url)\n        proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n\n        if user:\n            creds = self._basic_auth_header(user, password)\n        else:\n            creds = None\n\n        return creds, proxy_url",
        "begin_line": 35,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.process_request#46",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        # ignore if proxy is already set\n        if 'proxy' in request.meta:\n            if request.meta['proxy'] is None:\n                return\n            # extract credentials if present\n            creds, proxy_url = self._get_proxy(request.meta['proxy'], '')\n            request.meta['proxy'] = proxy_url\n            if creds and not request.headers.get('Proxy-Authorization'):\n                request.headers['Proxy-Authorization'] = b'Basic ' + creds\n            return\n        elif not self.proxies:\n            return\n\n        parsed = urlparse_cached(request)\n        scheme = parsed.scheme\n\n        # 'no_proxy' is only supported by http schemes\n        if scheme in ('http', 'https') and proxy_bypass(parsed.hostname):\n            return\n\n        if scheme in self.proxies:\n            self._set_proxy(request, scheme)",
        "begin_line": 46,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._set_proxy#70",
        "src_path": "scrapy/downloadermiddlewares/httpproxy.py",
        "class_name": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware._set_proxy(self, request, scheme)",
        "snippet": "    def _set_proxy(self, request, scheme):\n        creds, proxy = self.proxies[scheme]\n        request.meta['proxy'] = proxy\n        if creds:\n            request.headers['Proxy-Authorization'] = b'Basic ' + creds",
        "begin_line": 70,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.json_request.JsonRequest.__init__#17",
        "src_path": "scrapy/http/request/json_request.py",
        "class_name": "scrapy.http.request.json_request.JsonRequest",
        "signature": "scrapy.http.request.json_request.JsonRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n        dumps_kwargs.setdefault('sort_keys', True)\n        self._dumps_kwargs = dumps_kwargs\n\n        body_passed = kwargs.get('body', None) is not None\n        data = kwargs.pop('data', None)\n        data_passed = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn('Both body and data passed. data will be ignored')\n\n        elif not body_passed and data_passed:\n            kwargs['body'] = self._dumps(data)\n\n            if 'method' not in kwargs:\n                kwargs['method'] = 'POST'\n\n        super(JsonRequest, self).__init__(*args, **kwargs)\n        self.headers.setdefault('Content-Type', 'application/json')\n        self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')",
        "begin_line": 17,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1,
            "pseudo_dstar_susp": 0.014925373134328358,
            "pseudo_tarantula_susp": 0.1,
            "pseudo_op2_susp": 0.014925373134328358,
            "pseudo_barinel_susp": 0.1
        }
    },
    {
        "name": "scrapy.http.request.json_request.JsonRequest.replace#39",
        "src_path": "scrapy/http/request/json_request.py",
        "class_name": "scrapy.http.request.json_request.JsonRequest",
        "signature": "scrapy.http.request.json_request.JsonRequest.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        body_passed = kwargs.get('body', None) is not None\n        data = kwargs.pop('data', None)\n        data_passed = data is not None\n\n        if body_passed and data_passed:\n            warnings.warn('Both body and data passed. data will be ignored')\n\n        elif not body_passed and data_passed:\n            kwargs['body'] = self._dumps(data)\n\n        return super(JsonRequest, self).replace(*args, **kwargs)",
        "begin_line": 39,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.json_request.JsonRequest._dumps#52",
        "src_path": "scrapy/http/request/json_request.py",
        "class_name": "scrapy.http.request.json_request.JsonRequest",
        "signature": "scrapy.http.request.json_request.JsonRequest._dumps(self, data)",
        "snippet": "    def _dumps(self, data):\n        \"\"\"Convert to JSON \"\"\"\n        return json.dumps(data, **self._dumps_kwargs)",
        "begin_line": 52,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.__init__#19",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.from_crawler#23",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 23,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.process_spider_output#28",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        for x in result:\n            if isinstance(x, Request):\n                if x.dont_filter or self.should_follow(x, spider):\n                    yield x\n                else:\n                    domain = urlparse_cached(x).hostname\n                    if domain and domain not in self.domains_seen:\n                        self.domains_seen.add(domain)\n                        logger.debug(\n                            \"Filtered offsite request to %(domain)r: %(request)s\",\n                            {'domain': domain, 'request': x}, extra={'spider': spider})\n                        self.stats.inc_value('offsite/domains', spider=spider)\n                    self.stats.inc_value('offsite/filtered', spider=spider)\n            else:\n                yield x",
        "begin_line": 28,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.should_follow#45",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.should_follow(self, request, spider)",
        "snippet": "    def should_follow(self, request, spider):\n        regex = self.host_regex\n        # hostname can be None for wrong urls (like javascript links)\n        host = urlparse_cached(request).hostname or ''\n        return bool(regex.search(host))",
        "begin_line": 45,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00028121484814398203,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.get_host_regex#51",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.get_host_regex(self, spider)",
        "snippet": "    def get_host_regex(self, spider):\n        \"\"\"Override this method to implement a different offsite policy\"\"\"\n        allowed_domains = getattr(spider, 'allowed_domains', None)\n        if not allowed_domains:\n            return re.compile('')  # allow all by default\n        url_pattern = re.compile(\"^https?://.*$\")\n        for domain in allowed_domains:\n            if url_pattern.match(domain):\n                message = (\"allowed_domains accepts only domains, not URLs. \"\n                           \"Ignoring URL entry %s in allowed_domains.\" % domain)\n                warnings.warn(message, URLWarning)\n        domains = [re.escape(d) for d in allowed_domains if d is not None]\n        regex = r'^(.*\\.)?(%s)$' % '|'.join(domains)\n        return re.compile(regex)",
        "begin_line": 51,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.spider_opened#66",
        "src_path": "scrapy/spidermiddlewares/offsite.py",
        "class_name": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware",
        "signature": "scrapy.spidermiddlewares.offsite.OffsiteMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.host_regex = self.get_host_regex(spider)\n        self.domains_seen = set()",
        "begin_line": 66,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019076688286913393,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool(self.enabled_setting):\n            raise NotConfigured\n\n        self.max_redirect_times = settings.getint('REDIRECT_MAX_TIMES')\n        self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')",
        "begin_line": 17,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017677214071062401,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.from_crawler#25",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 25,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001771479185119575,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect#28",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect(self, redirected, request, spider, reason)",
        "snippet": "    def _redirect(self, redirected, request, spider, reason):\n        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)\n        redirects = request.meta.get('redirect_times', 0) + 1\n\n        if ttl and redirects <= self.max_redirect_times:\n            redirected.meta['redirect_times'] = redirects\n            redirected.meta['redirect_ttl'] = ttl - 1\n            redirected.meta['redirect_urls'] = request.meta.get('redirect_urls', []) + \\\n                [request.url]\n            redirected.meta['redirect_reasons'] = request.meta.get('redirect_reasons', []) + \\\n                [reason]\n            redirected.dont_filter = request.dont_filter\n            redirected.priority = request.priority + self.priority_adjust\n            logger.debug(\"Redirecting (%(reason)s) to %(redirected)s from %(request)s\",\n                         {'reason': reason, 'redirected': redirected, 'request': request},\n                         extra={'spider': spider})\n            return redirected\n        else:\n            logger.debug(\"Discarding %(request)s: max redirections reached\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest(\"max redirections reached\")",
        "begin_line": 28,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect_request_using_get#50",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.BaseRedirectMiddleware._redirect_request_using_get(self, request, redirect_url)",
        "snippet": "    def _redirect_request_using_get(self, request, redirect_url):\n        redirected = request.replace(url=redirect_url, method='GET', body='')\n        redirected.headers.pop('Content-Type', None)\n        redirected.headers.pop('Content-Length', None)\n        return redirected",
        "begin_line": 50,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware.process_response#62",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.RedirectMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if (request.meta.get('dont_redirect', False) or\n                response.status in getattr(spider, 'handle_httpstatus_list', []) or\n                response.status in request.meta.get('handle_httpstatus_list', []) or\n                request.meta.get('handle_httpstatus_all', False)):\n            return response\n\n        allowed_status = (301, 302, 303, 307, 308)\n        if 'Location' not in response.headers or response.status not in allowed_status:\n            return response\n\n        location = safe_url_string(response.headers['location'])\n\n        redirected_url = urljoin(request.url, location)\n\n        if response.status in (301, 307, 308) or request.method == 'HEAD':\n            redirected = request.replace(url=redirected_url)\n            return self._redirect(redirected, request, spider, response.status)\n\n        redirected = self._redirect_request_using_get(request, redirected_url)\n        return self._redirect(redirected, request, spider, response.status)",
        "begin_line": 62,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.__init__#89",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        super(MetaRefreshMiddleware, self).__init__(settings)\n        self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')\n        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',\n                                         settings.getint('METAREFRESH_MAXDELAY'))",
        "begin_line": 89,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001801477211313277,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.process_response#95",
        "src_path": "scrapy/downloadermiddlewares/redirect.py",
        "class_name": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware",
        "signature": "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \\\n                not isinstance(response, HtmlResponse):\n            return response\n\n        interval, url = get_meta_refresh(response,\n                                         ignore_tags=self._ignore_tags)\n        if url and interval < self._maxdelay:\n            redirected = self._redirect_request_using_get(request, url)\n            return self._redirect(redirected, request, spider, 'meta refresh')\n\n        return response",
        "begin_line": 95,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reactor.listen_tcp#3",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor",
        "signature": "scrapy.utils.reactor.listen_tcp(portrange, host, factory)",
        "snippet": "def listen_tcp(portrange, host, factory):\n    \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n    assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n    if not hasattr(portrange, '__iter__'):\n        return reactor.listenTCP(portrange, factory, interface=host)\n    if not portrange:\n        return reactor.listenTCP(0, factory, interface=host)\n    if len(portrange) == 1:\n        return reactor.listenTCP(portrange[0], factory, interface=host)\n    for x in range(portrange[0], portrange[1]+1):\n        try:\n            return reactor.listenTCP(x, factory, interface=host)\n        except error.CannotListenError:\n            if x == portrange[1]:\n                raise",
        "begin_line": 3,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.__init__#25",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.__init__(self, func, *a, **kw)",
        "snippet": "    def __init__(self, func, *a, **kw):\n        self._func = func\n        self._a = a\n        self._kw = kw\n        self._call = None",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.schedule#31",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.schedule(self, delay=0)",
        "snippet": "    def schedule(self, delay=0):\n        if self._call is None:\n            self._call = reactor.callLater(delay, self)",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020275750202757503,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.cancel#35",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.cancel(self)",
        "snippet": "    def cancel(self):\n        if self._call:\n            self._call.cancel()",
        "begin_line": 35,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reactor.CallLaterOnce.__call__#39",
        "src_path": "scrapy/utils/reactor.py",
        "class_name": "scrapy.utils.reactor.CallLaterOnce",
        "signature": "scrapy.utils.reactor.CallLaterOnce.__call__(self)",
        "snippet": "    def __call__(self):\n        self._call = None\n        return self._func(*self._a, **self._kw)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002069108214359611,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__new__#55",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__new__(metacls, name, bases, clsdict_)",
        "snippet": "        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__init__#61",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__init__(cls, name, bases, clsdict_)",
        "snippet": "        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__#77",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__(cls, inst)",
        "snippet": "        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__#81",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__(cls, sub)",
        "snippet": "        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)",
        "begin_line": 81,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__call__#95",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__call__(cls, *args, **kwargs)",
        "snippet": "        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate._clspath#120",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate._clspath(cls, forced=None)",
        "snippet": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "begin_line": 120,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.update_classpath#131",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.update_classpath(path)",
        "snippet": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path",
        "begin_line": 131,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.deprecate.method_is_overridden#142",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.method_is_overridden(subclass, base_class, method_name)",
        "snippet": "def method_is_overridden(subclass, base_class, method_name):\n    \"\"\"\n    Return True if a method named ``method_name`` of a ``base_class``\n    is overridden in a ``subclass``.\n\n    >>> class Base(object):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub1(Base):\n    ...     pass\n    >>> class Sub2(Base):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub3(Sub1):\n    ...     def foo(self):\n    ...         pass\n    >>> class Sub4(Sub2):\n    ...     pass\n    >>> method_is_overridden(Sub1, Base, 'foo')\n    False\n    >>> method_is_overridden(Sub2, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub3, Base, 'foo')\n    True\n    >>> method_is_overridden(Sub4, Base, 'foo')\n    True\n    \"\"\"\n    base_method = getattr(base_class, method_name)\n    sub_method = getattr(subclass, method_name)\n    return base_method.__code__ is not sub_method.__code__",
        "begin_line": 142,
        "end_line": 171,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000235626767200754,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.__init__#12",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.__init__(self, timeout=180)",
        "snippet": "    def __init__(self, timeout=180):\n        self._timeout = timeout",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001820830298616169,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.from_crawler#16",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        return o",
        "begin_line": 16,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001820830298616169,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.spider_opened#21",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self._timeout = getattr(spider, 'download_timeout', self._timeout)",
        "begin_line": 21,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001921598770176787,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.process_request#24",
        "src_path": "scrapy/downloadermiddlewares/downloadtimeout.py",
        "class_name": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware",
        "signature": "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if self._timeout:\n            request.meta.setdefault('download_timeout', self._timeout)",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019157088122605365,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_mimetype#43",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_mimetype(self, mimetype)",
        "snippet": "    def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = \"%s/*\" % mimetype.split('/')[0]\n            return self.classes.get(basetype, Response)",
        "begin_line": 43,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020903010033444816,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_type#53",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_type(self, content_type, content_encoding=None)",
        "snippet": "    def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)",
        "begin_line": 53,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_disposition#61",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_disposition(self, content_disposition)",
        "snippet": "    def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_native_str(content_disposition,\n                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n            filename = filename.strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response",
        "begin_line": 61,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_headers#70",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_headers(self, headers)",
        "snippet": "    def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls",
        "begin_line": 70,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_filename#83",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_filename(self, filename)",
        "snippet": "    def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response",
        "begin_line": 83,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00026014568158168577,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_body#91",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_body(self, body)",
        "snippet": "    def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if not binary_is_text(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')",
        "begin_line": 91,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_args#107",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_args(self, headers=None, url=None, filename=None, body=None)",
        "snippet": "    def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls",
        "begin_line": 107,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.defer_fail#10",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_fail(_failure)",
        "snippet": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go through readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d",
        "begin_line": 10,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002663115845539281,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.defer_succeed#21",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_succeed(result)",
        "snippet": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d",
        "begin_line": 21,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019076688286913393,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.defer_result#32",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_result(result)",
        "snippet": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)",
        "begin_line": 32,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.mustbe_deferred#40",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.mustbe_deferred(f, *args, **kw)",
        "snippet": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except Exception:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)",
        "begin_line": 40,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.parallel#56",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.parallel(iterable, count, callable, *args, **named)",
        "snippet": "def parallel(iterable, count, callable, *args, **named):\n    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n    using no more than ``count`` concurrent calls.\n\n    Taken from: https://jcalderone.livejournal.com/24285.html\n    \"\"\"\n    coop = task.Cooperator()\n    work = (callable(elem, *args, **named) for elem in iterable)\n    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])",
        "begin_line": 56,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022686025408348456,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain#66",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain(callbacks, input, *a, **kw)",
        "snippet": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d",
        "begin_line": 66,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain_both#74",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain_both(callbacks, errbacks, input, *a, **kw)",
        "snippet": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d",
        "begin_line": 74,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.process_parallel#86",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_parallel(callbacks, input, *a, **kw)",
        "snippet": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d",
        "begin_line": 86,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.defer.iter_errback#95",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.iter_errback(iterable, errback, *a, **kw)",
        "snippet": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except Exception:\n            errback(failure.Failure(), *a, **kw)",
        "begin_line": 95,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser.request_to_dict#11",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_to_dict(request, spider=None)",
        "snippet": "def request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n        'flags': request.flags,\n        'cb_kwargs': request.cb_kwargs,\n    }\n    if type(request) is not Request:\n        d['_class'] = request.__module__ + '.' + request.__class__.__name__\n    return d",
        "begin_line": 11,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser.request_from_dict#43",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_from_dict(d, spider=None)",
        "snippet": "def request_from_dict(d, spider=None):\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    cb = d['callback']\n    if cb and spider:\n        cb = _get_method(spider, cb)\n    eb = d['errback']\n    if eb and spider:\n        eb = _get_method(spider, eb)\n    request_cls = load_object(d['_class']) if '_class' in d else Request\n    return request_cls(\n        url=to_native_str(d['url']),\n        callback=cb,\n        errback=eb,\n        method=d['method'],\n        headers=d['headers'],\n        body=d['body'],\n        cookies=d['cookies'],\n        meta=d['meta'],\n        encoding=d['_encoding'],\n        priority=d['priority'],\n        dont_filter=d['dont_filter'],\n        flags=d.get('flags'),\n        cb_kwargs=d.get('cb_kwargs'),\n    )",
        "begin_line": 43,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser._is_private_method#73",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._is_private_method(name)",
        "snippet": "def _is_private_method(name):\n    return name.startswith('__') and not name.endswith('__')",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser._mangle_private_name#77",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._mangle_private_name(obj, func, name)",
        "snippet": "def _mangle_private_name(obj, func, name):\n    qualname = getattr(func, '__qualname__', None)\n    if qualname is None:\n        classname = obj.__class__.__name__.lstrip('_')\n        return '_%s%s' % (classname, name)\n    else:\n        splits = qualname.split('.')\n        return '_%s%s' % (splits[-2], splits[-1])",
        "begin_line": 77,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser._find_method#87",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._find_method(obj, func)",
        "snippet": "def _find_method(obj, func):\n    if obj:\n        try:\n            func_self = six.get_method_self(func)\n        except AttributeError:  # func has no __self__\n            pass\n        else:\n            if func_self is obj:\n                name = six.get_method_function(func).__name__\n                if _is_private_method(name):\n                    return _mangle_private_name(obj, func, name)\n                return name\n    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))",
        "begin_line": 87,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.reqser._get_method#102",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._get_method(obj, name)",
        "snippet": "def _get_method(obj, name):\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(\"Method %r not found in: %s\" % (name, obj))",
        "begin_line": 102,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.arg_to_iter#19",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.arg_to_iter(arg)",
        "snippet": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]",
        "begin_line": 19,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.load_object#33",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.load_object(path)",
        "snippet": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    object can be a class, function, variable or an instance.\n    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n    \"\"\"\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n\n    module, name = path[:dot], path[dot+1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n\n    return obj",
        "begin_line": 33,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.walk_modules#56",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.walk_modules(path)",
        "snippet": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods",
        "begin_line": 56,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.extract_regex#78",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.extract_regex(regex, text, encoding='utf-8')",
        "snippet": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n\n    if isinstance(regex, six.string_types):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except Exception:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, six.text_type):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]",
        "begin_line": 78,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.md5sum#102",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.md5sum(file)",
        "snippet": "def md5sum(file):\n    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n    whole content in memory.\n\n    >>> from io import BytesIO\n    >>> md5sum(BytesIO(b'file content to hash'))\n    '784406af91dd5a54fbb9c84c2236595a'\n    \"\"\"\n    m = hashlib.md5()\n    while True:\n        d = file.read(8096)\n        if not d:\n            break\n        m.update(d)\n    return m.hexdigest()",
        "begin_line": 102,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.rel_has_nofollow#119",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.rel_has_nofollow(rel)",
        "snippet": "def rel_has_nofollow(rel):\n    \"\"\"Return True if link rel attribute has nofollow type\"\"\"\n    return rel is not None and 'nofollow' in rel.split()",
        "begin_line": 119,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022972662531587412,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.create_instance#124",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.create_instance(objcls, settings, crawler, *args, **kwargs)",
        "snippet": "def create_instance(objcls, settings, crawler, *args, **kwargs):\n    \"\"\"Construct a class instance using its ``from_crawler`` or\n    ``from_settings`` constructors, if available.\n\n    At least one of ``settings`` and ``crawler`` needs to be different from\n    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n    tried.\n\n    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n\n    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n    \"\"\"\n    if settings is None:\n        if crawler is None:\n            raise ValueError(\"Specifiy at least one of settings and crawler.\")\n        settings = crawler.settings\n    if crawler and hasattr(objcls, 'from_crawler'):\n        return objcls.from_crawler(crawler, *args, **kwargs)\n    elif hasattr(objcls, 'from_settings'):\n        return objcls.from_settings(settings, *args, **kwargs)\n    else:\n        return objcls(*args, **kwargs)",
        "begin_line": 124,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.misc.set_environ#150",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.set_environ(**kwargs)",
        "snippet": "def set_environ(**kwargs):\n    \"\"\"Temporarily set environment variables inside the context manager and\n    fully restore previous environment afterwards\n    \"\"\"\n\n    original_env = {k: os.environ.get(k) for k in kwargs}\n    os.environ.update(kwargs)\n    try:\n        yield\n    finally:\n        for k, v in original_env.items():\n            if v is None:\n                del os.environ[k]\n            else:\n                os.environ[k] = v",
        "begin_line": 150,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.link.Link.__init__#18",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__init__(self, url, text='', fragment='', nofollow=False)",
        "snippet": "    def __init__(self, url, text='', fragment='', nofollow=False):\n        if not isinstance(url, str):\n            if six.PY2:\n                warnings.warn(\"Link urls must be str objects. \"\n                              \"Assuming utf-8 encoding (which could be wrong)\")\n                url = to_bytes(url, encoding='utf8')\n            else:\n                got = url.__class__.__name__\n                raise TypeError(\"Link urls must be str objects, got %s\" % got)\n        self.url = url\n        self.text = text\n        self.fragment = fragment\n        self.nofollow = nofollow",
        "begin_line": 18,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.link.Link.__eq__#32",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self.url == other.url and self.text == other.text and \\\n            self.fragment == other.fragment and self.nofollow == other.nofollow",
        "begin_line": 32,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002557544757033248,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.link.Link.__hash__#36",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)",
        "begin_line": 36,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002255299954894001,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.link.Link.__repr__#39",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n            (self.url, self.text, self.fragment, self.nofollow)",
        "begin_line": 39,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor.__init__#52",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor.__init__(self, link_extractor, allow, deny, allow_domains, deny_domains, restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text)",
        "snippet": "    def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                 restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n\n        self.link_extractor = link_extractor\n\n        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)\n                          for x in arg_to_iter(allow)]\n        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x)\n                         for x in arg_to_iter(deny)]\n\n        self.allow_domains = set(arg_to_iter(allow_domains))\n        self.deny_domains = set(arg_to_iter(deny_domains))\n\n        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n        self.restrict_xpaths += tuple(map(self._csstranslator.css_to_xpath,\n                                          arg_to_iter(restrict_css)))\n\n        self.canonicalize = canonicalize\n        if deny_extensions is None:\n            deny_extensions = IGNORED_EXTENSIONS\n        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}\n        self.restrict_text = [x if isinstance(x, _re_type) else re.compile(x)\n                              for x in arg_to_iter(restrict_text)]",
        "begin_line": 52,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024067388688327315,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._link_allowed#76",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._link_allowed(self, link)",
        "snippet": "    def _link_allowed(self, link):\n        if not _is_valid_url(link.url):\n            return False\n        if self.allow_res and not _matches(link.url, self.allow_res):\n            return False\n        if self.deny_res and _matches(link.url, self.deny_res):\n            return False\n        parsed_url = urlparse(link.url)\n        if self.allow_domains and not url_is_from_any_domain(parsed_url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n            return False\n        if self.deny_extensions and url_has_any_extension(parsed_url, self.deny_extensions):\n            return False\n        if self.restrict_text and not _matches(link.text, self.restrict_text):\n            return False\n        return True",
        "begin_line": 76,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor.matches#94",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor.matches(self, url)",
        "snippet": "    def matches(self, url):\n\n        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n            return False\n        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n            return False\n\n        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]\n        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n        return any(allowed) and not any(denied)",
        "begin_line": 94,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._process_links#105",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._process_links(self, links)",
        "snippet": "    def _process_links(self, links):\n        links = [x for x in links if self._link_allowed(x)]\n        if self.canonicalize:\n            for link in links:\n                link.url = canonicalize_url(link.url)\n        links = self.link_extractor._process_links(links)\n        return links",
        "begin_line": 105,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.__init__.FilteringLinkExtractor._extract_links#113",
        "src_path": "scrapy/linkextractors/__init__.py",
        "class_name": "scrapy.linkextractors.__init__.FilteringLinkExtractor",
        "signature": "scrapy.linkextractors.__init__.FilteringLinkExtractor._extract_links(self, *args, **kwargs)",
        "snippet": "    def _extract_links(self, *args, **kwargs):\n        return self.link_extractor._extract_links(*args, **kwargs)",
        "begin_line": 113,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022972662531587412,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.__init__#10",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.__init__(self, policy=None, check_expired_frequency=10000)",
        "snippet": "    def __init__(self, policy=None, check_expired_frequency=10000):\n        self.policy = policy or DefaultCookiePolicy()\n        self.jar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()\n        self.check_expired_frequency = check_expired_frequency\n        self.processed = 0",
        "begin_line": 10,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.extract_cookies#17",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.extract_cookies(self, response, request)",
        "snippet": "    def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.add_cookie_header#22",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.add_cookie_header(self, request)",
        "snippet": "    def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()",
        "begin_line": 22,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.make_cookies#73",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.make_cookies(self, response, request)",
        "snippet": "    def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.set_cookie_if_ok#81",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.set_cookie_if_ok(self, cookie, request)",
        "snippet": "    def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.potential_domain_matches#85",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies",
        "signature": "scrapy.http.cookies.potential_domain_matches(domain)",
        "snippet": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]",
        "begin_line": 85,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.acquire#105",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.acquire(self)",
        "snippet": "    def acquire(self):\n        pass",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.release#108",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.release(self)",
        "snippet": "    def release(self):\n        pass",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.__init__#118",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.__init__(self, request)",
        "snippet": "    def __init__(self, request):\n        self.request = request",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001801477211313277,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_full_url#121",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_full_url(self)",
        "snippet": "    def get_full_url(self):\n        return self.request.url",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_host#124",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_host(self)",
        "snippet": "    def get_host(self):\n        return urlparse_cached(self.request).netloc",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_type#127",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_type(self)",
        "snippet": "    def get_type(self):\n        return urlparse_cached(self.request).scheme",
        "begin_line": 127,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.is_unverifiable#130",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.is_unverifiable(self)",
        "snippet": "    def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)",
        "begin_line": 130,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_origin_req_host#140",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_origin_req_host(self)",
        "snippet": "    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname",
        "begin_line": 140,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.full_url#145",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.full_url(self)",
        "snippet": "    def full_url(self):\n        return self.get_full_url()",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.host#149",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.host(self)",
        "snippet": "    def host(self):\n        return self.get_host()",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.type#153",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.type(self)",
        "snippet": "    def type(self):\n        return self.get_type()",
        "begin_line": 153,
        "end_line": 154,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.unverifiable#157",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.unverifiable(self)",
        "snippet": "    def unverifiable(self):\n        return self.is_unverifiable()",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.origin_req_host#161",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.origin_req_host(self)",
        "snippet": "    def origin_req_host(self):\n        return self.get_origin_req_host()",
        "begin_line": 161,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.has_header#164",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.has_header(self, name)",
        "snippet": "    def has_header(self, name):\n        return name in self.request.headers",
        "begin_line": 164,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_header#167",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_header(self, name, default=None)",
        "snippet": "    def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')",
        "begin_line": 167,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.header_items#171",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.header_items(self)",
        "snippet": "    def header_items(self):\n        return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n            for k, v in self.request.headers.items()\n        ]",
        "begin_line": 171,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.add_unredirected_header#178",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.add_unredirected_header(self, name, value)",
        "snippet": "    def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)",
        "begin_line": 178,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.__init__#184",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.__init__(self, response)",
        "snippet": "    def __init__(self, response):\n        self.response = response",
        "begin_line": 184,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.info#187",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.info(self)",
        "snippet": "    def info(self):\n        return self",
        "begin_line": 187,
        "end_line": 188,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001832508704416346,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.get_all#191",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.get_all(self, name, default=None)",
        "snippet": "    def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]",
        "begin_line": 191,
        "end_line": 193,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018291567587342235,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__init__#19",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__init__(self, url, status=200, headers=None, body=b'', flags=None, request=None)",
        "snippet": "    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 19,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001546551190844417,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.meta#28",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.meta(self)",
        "snippet": "    def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_url#37",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015842839036755386,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_url#40",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n                type(url).__name__))",
        "begin_line": 40,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_body#49",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 49,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015669069257286117,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_body#52",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body",
        "begin_line": 52,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__str__#65",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%d %s>\" % (self.status, self.url)",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.copy#70",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()",
        "begin_line": 70,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.replace#74",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 74,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024242424242424242,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.urljoin#83",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)",
        "begin_line": 83,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.text#89",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.text(self)",
        "snippet": "    def text(self):\n        \"\"\"For subclasses of TextResponse, this will return the body\n        as text (unicode object in Python 2 and str in Python 3)\n        \"\"\"\n        raise AttributeError(\"Response content isn't text\")",
        "begin_line": 89,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.css#95",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.css(self, *a, **kw)",
        "snippet": "    def css(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")",
        "begin_line": 95,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.xpath#101",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.xpath(self, *a, **kw)",
        "snippet": "    def xpath(self, *a, **kw):\n        \"\"\"Shortcut method implemented only by responses whose content\n        is text (subclasses of TextResponse).\n        \"\"\"\n        raise NotSupported(\"Response content isn't text\")",
        "begin_line": 101,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.follow#107",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.follow(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None, cb_kwargs=None)",
        "snippet": "    def follow(self, url, callback=None, method='GET', headers=None, body=None,\n               cookies=None, meta=None, encoding='utf-8', priority=0,\n               dont_filter=False, errback=None, cb_kwargs=None):\n        # type: (...) -> Request\n        \"\"\"\n        Return a :class:`~.Request` instance to follow a link ``url``.\n        It accepts the same arguments as ``Request.__init__`` method,\n        but ``url`` can be a relative URL or a ``scrapy.link.Link`` object,\n        not only an absolute URL.\n        \n        :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` \n        method which supports selectors in addition to absolute/relative URLs\n        and Link objects.\n        \"\"\"\n        if isinstance(url, Link):\n            url = url.url\n        elif url is None:\n            raise ValueError(\"url can't be None\")\n        url = self.urljoin(url)\n        return Request(url, callback,\n                       method=method,\n                       headers=headers,\n                       body=body,\n                       cookies=cookies,\n                       meta=meta,\n                       encoding=encoding,\n                       priority=priority,\n                       dont_filter=dont_filter,\n                       errback=errback,\n                       cb_kwargs=cb_kwargs)",
        "begin_line": 107,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.__init__#12",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self._dump = crawler.settings.getbool('STATS_DUMP')\n        self._stats = {}",
        "begin_line": 12,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_value#16",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return self._stats.get(key, default)",
        "begin_line": 16,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018040772145047808,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_stats#19",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_stats(self, spider=None)",
        "snippet": "    def get_stats(self, spider=None):\n        return self._stats",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.set_value#22",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        self._stats[key] = value",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018758206715438003,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.inc_value#28",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count",
        "begin_line": 28,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001585791309863622,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.max_value#32",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        self._stats[key] = max(self._stats.setdefault(key, value), value)",
        "begin_line": 32,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.min_value#35",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        self._stats[key] = min(self._stats.setdefault(key, value), value)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.open_spider#41",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        pass",
        "begin_line": 41,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017537706068046299,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.close_spider#44",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.close_spider(self, spider, reason)",
        "snippet": "    def close_spider(self, spider, reason):\n        if self._dump:\n            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                        extra={'spider': spider})\n        self._persist_stats(self._stats, spider)",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017537706068046299,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector._persist_stats#50",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        pass",
        "begin_line": 50,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector.__init__#56",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        super(MemoryStatsCollector, self).__init__(crawler)\n        self.spider_stats = {}",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector._persist_stats#60",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        self.spider_stats[spider.name] = stats",
        "begin_line": 60,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000175530981218185,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.get_value#66",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return default",
        "begin_line": 66,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.set_value#69",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        pass",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.inc_value#75",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        pass",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.max_value#78",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        pass",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.min_value#81",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        pass",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.job.job_dir#3",
        "src_path": "scrapy/utils/job.py",
        "class_name": "scrapy.utils.job",
        "signature": "scrapy.utils.job.job_dir(settings)",
        "snippet": "def job_dir(settings):\n    path = settings['JOBDIR']\n    if path and not os.path.exists(path):\n        os.makedirs(path)\n    return path",
        "begin_line": 3,
        "end_line": 7,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.__init__#24",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module('resource')\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler = crawler\n        self.warned = False\n        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n        self.mail = MailSender.from_settings(crawler.settings)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)",
        "begin_line": 24,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.from_crawler#44",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 44,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.get_virtual_size#47",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.get_virtual_size(self)",
        "snippet": "    def get_virtual_size(self):\n        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != 'darwin':\n            # on Mac OS X ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size",
        "begin_line": 47,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.engine_started#54",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.engine_started(self)",
        "snippet": "    def engine_started(self):\n        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n        self.tasks = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)",
        "begin_line": 54,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.engine_stopped#69",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.engine_stopped(self)",
        "snippet": "    def engine_stopped(self):\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()",
        "begin_line": 69,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.update#74",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.update(self)",
        "snippet": "    def update(self):\n        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._check_limit#77",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._check_limit(self)",
        "snippet": "    def _check_limit(self):\n        if self.get_virtual_size() > self.limit:\n            self.crawler.stats.set_value('memusage/limit_reached', 1)\n            mem = self.limit/1024/1024\n            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n                         {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/limit_notified', 1)\n\n            open_spiders = self.crawler.engine.open_spiders\n            if open_spiders:\n                for spider in open_spiders:\n                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n            else:\n                self.crawler.stop()",
        "begin_line": 77,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._check_warning#96",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._check_warning(self)",
        "snippet": "    def _check_warning(self):\n        if self.warned: # warn only once\n            return\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value('memusage/warning_reached', 1)\n            mem = self.warning/1024/1024\n            logger.warning(\"Memory usage reached %(memusage)dM\",\n                           {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = \"%s warning: memory usage reached %dM at %s\" % \\\n                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/warning_notified', 1)\n            self.warned = True",
        "begin_line": 96,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._send_report#111",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._send_report(self, rcpts, subject)",
        "snippet": "    def _send_report(self, rcpts, subject):\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        stats = self.crawler.stats\n        s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n        s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n        s += \"Current memory usage           : %dM\\r\\n\" % (self.get_virtual_size()/1024/1024)\n\n        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)",
        "begin_line": 111,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.flatten#18",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.flatten(x)",
        "snippet": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))",
        "begin_line": 18,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002663115845539281,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.iflatten#38",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.iflatten(x)",
        "snippet": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in iflatten(el):\n                yield el_\n        else:\n            yield el",
        "begin_line": 38,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002663115845539281,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.is_listlike#50",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.is_listlike(x)",
        "snippet": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(six.moves.xrange(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))",
        "begin_line": 50,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017739932588256165,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.unique#74",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.unique(list_, key=lambda x: x)",
        "snippet": "def unique(list_, key=lambda x: x):\n    \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n    seen = set()\n    result = []\n    for item in list_:\n        seenkey = key(item)\n        if seenkey in seen:\n            continue\n        seen.add(seenkey)\n        result.append(item)\n    return result",
        "begin_line": 74,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.to_unicode#87",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_unicode(text, encoding=None, errors='strict')",
        "snippet": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object ``text``. If\n    ``text`` is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, six.text_type):\n        return text\n    if not isinstance(text, (bytes, six.text_type)):\n        raise TypeError('to_unicode must receive a bytes, str or unicode '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)",
        "begin_line": 87,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.to_bytes#100",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_bytes(text, encoding=None, errors='strict')",
        "snippet": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of ``text``. If ``text``\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, six.string_types):\n        raise TypeError('to_bytes must receive a unicode, str or bytes '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)",
        "begin_line": 100,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.to_native_str#113",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_native_str(text, encoding=None, errors='strict')",
        "snippet": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of ``text``\n    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n    if six.PY2:\n        return to_bytes(text, encoding, errors)\n    else:\n        return to_unicode(text, encoding, errors)",
        "begin_line": 113,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015556938394523958,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.re_rsearch#122",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.re_rsearch(pattern, text, chunk_size=1024)",
        "snippet": "def re_rsearch(pattern, text, chunk_size=1024):\n    \"\"\"\n    This function does a reverse search in a text using a regular expression\n    given in the attribute 'pattern'.\n    Since the re module does not provide this functionality, we have to find for\n    the expression into chunks of text extracted from the end (for the sake of efficiency).\n    At first, a chunk of 'chunk_size' kilobytes is extracted from the end, and searched for\n    the pattern. If the pattern is not found, another chunk is extracted, and another\n    search is performed.\n    This process continues until a match is found, or until the whole file is read.\n    In case the pattern wasn't found, None is returned, otherwise it returns a tuple containing\n    the start position of the match, and the ending (regarding the entire text).\n    \"\"\"\n    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)\n\n    if isinstance(pattern, six.string_types):\n        pattern = re.compile(pattern)\n\n    for chunk, offset in _chunk_iter():\n        matches = [match for match in pattern.finditer(chunk)]\n        if matches:\n            start, end = matches[-1].span()\n            return offset + start, offset + end\n    return None",
        "begin_line": 122,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python._chunk_iter#135",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python._chunk_iter()",
        "snippet": "    def _chunk_iter():\n        offset = len(text)\n        while True:\n            offset -= (chunk_size * 1024)\n            if offset <= 0:\n                break\n            yield (text[offset:], offset)\n        yield (text, 0)",
        "begin_line": 135,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.memoizemethod_noargs#155",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.memoizemethod_noargs(method)",
        "snippet": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n    return new_method",
        "begin_line": 155,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.new_method#161",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.new_method(self, *args, **kwargs)",
        "snippet": "    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]",
        "begin_line": 161,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.binary_is_text#179",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.binary_is_text(data)",
        "snippet": "def binary_is_text(data):\n    \"\"\" Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n    does not contain unprintable control characters.\n    \"\"\"\n    if not isinstance(data, bytes):\n        raise TypeError(\"data must be bytes, got '%s'\" % type(data).__name__)\n    return all(c not in _BINARYCHARS for c in data)",
        "begin_line": 179,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024067388688327315,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python._getargspec_py23#188",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python._getargspec_py23(func)",
        "snippet": "def _getargspec_py23(func):\n    \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n                                                        defaults)\n\n    Identical to inspect.getargspec() in python2, but uses\n    inspect.getfullargspec() for python3 behind the scenes to avoid\n    DeprecationWarning.\n\n    >>> def f(a, b=2, *ar, **kw):\n    ...     pass\n\n    >>> _getargspec_py23(f)\n    ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n    \"\"\"\n    if six.PY2:\n        return inspect.getargspec(func)\n\n    return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])",
        "begin_line": 188,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018635855385762206,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.get_func_args#208",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_func_args(func, stripself=False)",
        "snippet": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        func_args, _, _, _ = _getargspec_py23(func)\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n    if stripself:\n        func_args.pop(0)\n    return func_args",
        "begin_line": 208,
        "end_line": 232,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.get_spec#235",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_spec(func)",
        "snippet": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test(object):\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = _getargspec_py23(func)\n    elif hasattr(func, '__call__'):\n        spec = _getargspec_py23(func.__call__)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs",
        "begin_line": 235,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.equal_attributes#272",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.equal_attributes(obj1, obj2, attributes)",
        "snippet": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    temp1, temp2 = object(), object()\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if attr(obj1) != attr(obj2):\n                return False\n        elif getattr(obj1, attr, temp1) != getattr(obj2, attr, temp2):\n            return False\n    # all attributes equal\n    return True",
        "begin_line": 272,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__init__#292",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__init__(self, default_factory)",
        "snippet": "    def __init__(self, default_factory):\n        self.default_factory = default_factory\n        self._weakdict = weakref.WeakKeyDictionary()",
        "begin_line": 292,
        "end_line": 294,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__getitem__#296",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]",
        "begin_line": 296,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.without_none_values#347",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.without_none_values(iterable)",
        "snippet": "def without_none_values(iterable):\n    \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n\n    If ``iterable`` is a mapping, return a dictionary where all pairs that have\n    value ``None`` have been removed.\n    \"\"\"\n    try:\n        return {k: v for k, v in six.iteritems(iterable) if v is not None}\n    except AttributeError:\n        return type(iterable)((v for v in iterable if v is not None))",
        "begin_line": 347,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.global_object_name#359",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.global_object_name(obj)",
        "snippet": "def global_object_name(obj):\n    \"\"\"\n    Return full name of a global object.\n\n    >>> from scrapy import Request\n    >>> global_object_name(Request)\n    'scrapy.http.request.Request'\n    \"\"\"\n    return \"%s.%s\" % (obj.__module__, obj.__name__)",
        "begin_line": 359,
        "end_line": 367,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.garbage_collect#376",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.garbage_collect()",
        "snippet": "    def garbage_collect():\n        gc.collect()",
        "begin_line": 376,
        "end_line": 377,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.MutableChain.__init__#384",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.MutableChain",
        "signature": "scrapy.utils.python.MutableChain.__init__(self, *args)",
        "snippet": "    def __init__(self, *args):\n        self.data = chain(*args)",
        "begin_line": 384,
        "end_line": 385,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022114108801415304,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.MutableChain.extend#387",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.MutableChain",
        "signature": "scrapy.utils.python.MutableChain.extend(self, *iterables)",
        "snippet": "    def extend(self, *iterables):\n        self.data = chain(self.data, *iterables)",
        "begin_line": 387,
        "end_line": 388,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.MutableChain.__iter__#390",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.MutableChain",
        "signature": "scrapy.utils.python.MutableChain.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return self.data.__iter__()",
        "begin_line": 390,
        "end_line": 391,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002255299954894001,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.python.MutableChain.__next__#393",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.MutableChain",
        "signature": "scrapy.utils.python.MutableChain.__next__(self)",
        "snippet": "    def __next__(self):\n        return next(self.data)",
        "begin_line": 393,
        "end_line": 394,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.start_requests#8",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        self._postinit_reqs = super(InitSpider, self).start_requests()\n        return iterate_spider_output(self.init_request())",
        "begin_line": 8,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.initialized#12",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.initialized(self, response=None)",
        "snippet": "    def initialized(self, response=None):\n        \"\"\"This method must be set as the callback of your last initialization\n        request. See self.init_request() docstring for more info.\n        \"\"\"\n        return self.__dict__.pop('_postinit_reqs')",
        "begin_line": 12,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.init.InitSpider.init_request#18",
        "src_path": "scrapy/spiders/init.py",
        "class_name": "scrapy.spiders.init.InitSpider",
        "signature": "scrapy.spiders.init.InitSpider.init_request(self)",
        "snippet": "    def init_request(self):\n        \"\"\"This function should return one initialization request, with the\n        self.initialized method as callback. When the self.initialized method\n        is called this spider is considered initialized. If you need to perform\n        several requests for initializing your spider, you can do so by using\n        different callbacks. The only requirement is that the final callback\n        (of the last initialization request) must be self.initialized.\n\n        The default implementation calls self.initialized immediately, and\n        means that no initialization is needed. This method should be\n        overridden only when you need to perform requests to initialize your\n        spider\n        \"\"\"\n        return self.initialized()",
        "begin_line": 18,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.__init__#22",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.__init__(self, concurrency, delay, randomize_delay)",
        "snippet": "    def __init__(self, concurrency, delay, randomize_delay):\n        self.concurrency = concurrency\n        self.delay = delay\n        self.randomize_delay = randomize_delay\n\n        self.active = set()\n        self.queue = deque()\n        self.transferring = set()\n        self.lastseen = 0\n        self.latercall = None",
        "begin_line": 22,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.free_transfer_slots#33",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.free_transfer_slots(self)",
        "snippet": "    def free_transfer_slots(self):\n        return self.concurrency - len(self.transferring)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.download_delay#36",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.download_delay(self)",
        "snippet": "    def download_delay(self):\n        if self.randomize_delay:\n            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n        return self.delay",
        "begin_line": 36,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Slot.close#41",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Slot",
        "signature": "scrapy.core.downloader.__init__.Slot.close(self)",
        "snippet": "    def close(self):\n        if self.latercall and self.latercall.active():\n            self.latercall.cancel()",
        "begin_line": 41,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__._get_concurrency_delay#61",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__",
        "signature": "scrapy.core.downloader.__init__._get_concurrency_delay(concurrency, spider, settings)",
        "snippet": "def _get_concurrency_delay(concurrency, spider, settings):\n    delay = settings.getfloat('DOWNLOAD_DELAY')\n    if hasattr(spider, 'download_delay'):\n        delay = spider.download_delay\n\n    if hasattr(spider, 'max_concurrent_requests'):\n        concurrency = spider.max_concurrent_requests\n\n    return concurrency, delay",
        "begin_line": 61,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.__init__#76",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.settings = crawler.settings\n        self.signals = crawler.signals\n        self.slots = {}\n        self.active = set()\n        self.handlers = DownloadHandlers(crawler)\n        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n        self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n        self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n        self._slot_gc_loop.start(60)",
        "begin_line": 76,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.fetch#90",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.fetch(self, request, spider)",
        "snippet": "    def fetch(self, request, spider):\n        def _deactivate(response):\n            self.active.remove(request)\n            return response\n\n        self.active.add(request)\n        dfd = self.middleware.download(self._enqueue_request, request, spider)\n        return dfd.addBoth(_deactivate)",
        "begin_line": 90,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._deactivate#91",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._deactivate(response)",
        "snippet": "        def _deactivate(response):\n            self.active.remove(request)\n            return response",
        "begin_line": 91,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.needs_backout#99",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.needs_backout(self)",
        "snippet": "    def needs_backout(self):\n        return len(self.active) >= self.total_concurrency",
        "begin_line": 99,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002069108214359611,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._get_slot#102",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._get_slot(self, request, spider)",
        "snippet": "    def _get_slot(self, request, spider):\n        key = self._get_slot_key(request, spider)\n        if key not in self.slots:\n            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n\n        return key, self.slots[key]",
        "begin_line": 102,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._get_slot_key#111",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._get_slot_key(self, request, spider)",
        "snippet": "    def _get_slot_key(self, request, spider):\n        if self.DOWNLOAD_SLOT in request.meta:\n            return request.meta[self.DOWNLOAD_SLOT]\n\n        key = urlparse_cached(request).hostname or ''\n        if self.ip_concurrency:\n            key = dnscache.get(key, key)\n\n        return key",
        "begin_line": 111,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._enqueue_request#121",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._enqueue_request(self, request, spider)",
        "snippet": "    def _enqueue_request(self, request, spider):\n        key, slot = self._get_slot(request, spider)\n        request.meta[self.DOWNLOAD_SLOT] = key\n\n        def _deactivate(response):\n            slot.active.remove(request)\n            return response\n\n        slot.active.add(request)\n        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n                                    request=request,\n                                    spider=spider)\n        deferred = defer.Deferred().addBoth(_deactivate)\n        slot.queue.append((request, deferred))\n        self._process_queue(spider, slot)\n        return deferred",
        "begin_line": 121,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._deactivate#125",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._deactivate(response)",
        "snippet": "        def _deactivate(response):\n            slot.active.remove(request)\n            return response",
        "begin_line": 125,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._process_queue#138",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._process_queue(self, spider, slot)",
        "snippet": "    def _process_queue(self, spider, slot):\n        if slot.latercall and slot.latercall.active():\n            return\n\n        # Delay queue processing if a download_delay is configured\n        now = time()\n        delay = slot.download_delay()\n        if delay:\n            penalty = delay - now + slot.lastseen\n            if penalty > 0:\n                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n                return\n\n        # Process enqueued requests if there are free slots to transfer for this slot\n        while slot.queue and slot.free_transfer_slots() > 0:\n            slot.lastseen = now\n            request, deferred = slot.queue.popleft()\n            dfd = self._download(slot, request, spider)\n            dfd.chainDeferred(deferred)\n            # prevent burst if inter-request delays were configured\n            if delay:\n                self._process_queue(spider, slot)\n                break",
        "begin_line": 138,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._download#162",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._download(self, slot, request, spider)",
        "snippet": "    def _download(self, slot, request, spider):\n        # The order is very important for the following deferreds. Do not change!\n\n        # 1. Create the download deferred\n        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n\n        # 2. Notify response_downloaded listeners about the recent download\n        # before querying queue for next request\n        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response\n        dfd.addCallback(_downloaded)\n\n        # 3. After response arrives,  remove the request from transferring\n        # state to free up the transferring slot so it can be used by the\n        # following requests (perhaps those which came from the downloader\n        # middleware itself)\n        slot.transferring.add(request)\n\n        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            return _\n\n        return dfd.addBoth(finish_transferring)",
        "begin_line": 162,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._downloaded#170",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._downloaded(response)",
        "snippet": "        def _downloaded(response):\n            self.signals.send_catch_log(signal=signals.response_downloaded,\n                                        response=response,\n                                        request=request,\n                                        spider=spider)\n            return response",
        "begin_line": 170,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022366360993066427,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.finish_transferring#184",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.finish_transferring(_)",
        "snippet": "        def finish_transferring(_):\n            slot.transferring.remove(request)\n            self._process_queue(spider, slot)\n            return _",
        "begin_line": 184,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader.close#191",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader.close(self)",
        "snippet": "    def close(self):\n        self._slot_gc_loop.stop()\n        for slot in six.itervalues(self.slots):\n            slot.close()",
        "begin_line": 191,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.__init__.Downloader._slot_gc#196",
        "src_path": "scrapy/core/downloader/__init__.py",
        "class_name": "scrapy.core.downloader.__init__.Downloader",
        "signature": "scrapy.core.downloader.__init__.Downloader._slot_gc(self, age=60)",
        "snippet": "    def _slot_gc(self, age=60):\n        mintime = time() - age\n        for key, slot in list(self.slots.items()):\n            if not slot.active and slot.lastseen + slot.delay < mintime:\n                self.slots.pop(key).close()",
        "begin_line": 196,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.versions.scrapy_components_versions#15",
        "src_path": "scrapy/utils/versions.py",
        "class_name": "scrapy.utils.versions",
        "signature": "scrapy.utils.versions.scrapy_components_versions()",
        "snippet": "def scrapy_components_versions():\n    lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n    libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n\n    return [\n        (\"Scrapy\", scrapy.__version__),\n        (\"lxml\", lxml_version),\n        (\"libxml2\", libxml2_version),\n        (\"cssselect\", cssselect.__version__),\n        (\"parsel\", parsel.__version__),\n        (\"w3lib\", w3lib.__version__),\n        (\"Twisted\", twisted.version.short()),\n        (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n        (\"pyOpenSSL\", get_openssl_version()),\n        (\"cryptography\", cryptography.__version__),\n        (\"Platform\",  platform.platform()),\n    ]",
        "begin_line": 15,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.setUp#10",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.setUp(self)",
        "snippet": "    def setUp(self):\n        super(SiteTest, self).setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = \"http://localhost:%d/\" % self.site.getHost().port",
        "begin_line": 10,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000249500998003992,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.tearDown#15",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.tearDown(self)",
        "snippet": "    def tearDown(self):\n        super(SiteTest, self).tearDown()\n        self.site.stopListening()",
        "begin_line": 15,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000249500998003992,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testsite.SiteTest.url#19",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.SiteTest",
        "signature": "scrapy.utils.testsite.SiteTest.url(self, path)",
        "snippet": "    def url(self, path):\n        return urljoin(self.baseurl, path)",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002557544757033248,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testsite.NoMetaRefreshRedirect.render#24",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite.NoMetaRefreshRedirect",
        "signature": "scrapy.utils.testsite.NoMetaRefreshRedirect.render(self, request)",
        "snippet": "    def render(self, request):\n        content = util.Redirect.render(self, request)\n        return content.replace(b'http-equiv=\\\"refresh\\\"',\n            b'http-no-equiv=\\\"do-not-refresh-me\\\"')",
        "begin_line": 24,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.testsite.test_site#30",
        "src_path": "scrapy/utils/testsite.py",
        "class_name": "scrapy.utils.testsite",
        "signature": "scrapy.utils.testsite.test_site()",
        "snippet": "def test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)",
        "begin_line": 30,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000249500998003992,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.__init__#25",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.__init__(self, item=None, selector=None, response=None, parent=None, **context)",
        "snippet": "    def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n        self.selector = selector\n        context.update(selector=selector, response=response)\n        if item is None:\n            item = self.default_item_class()\n        self.context = context\n        self.parent = parent\n        self._local_item = context['item'] = item\n        self._local_values = defaultdict(list)\n        # Preprocess values if item built from dict\n        # Values need to be added to item._values if added them from dict (not with add_values)\n        for field_name, value in item.items():\n            self._values[field_name] = self._process_input_value(field_name, value)",
        "begin_line": 25,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._values#42",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._values(self)",
        "snippet": "    def _values(self):\n        if self.parent is not None:\n            return self.parent._values\n        else:\n            return self._local_values",
        "begin_line": 42,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.item#49",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.item(self)",
        "snippet": "    def item(self):\n        if self.parent is not None:\n            return self.parent.item\n        else:\n            return self._local_item",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.nested_xpath#55",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.nested_xpath(self, xpath, **context)",
        "snippet": "    def nested_xpath(self, xpath, **context):\n        selector = self.selector.xpath(xpath)\n        context.update(selector=selector)\n        subloader = self.__class__(\n            item=self.item, parent=self, **context\n        )\n        return subloader",
        "begin_line": 55,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.nested_css#63",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.nested_css(self, css, **context)",
        "snippet": "    def nested_css(self, css, **context):\n        selector = self.selector.css(css)\n        context.update(selector=selector)\n        subloader = self.__class__(\n            item=self.item, parent=self, **context\n        )\n        return subloader",
        "begin_line": 63,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_value#71",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def add_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._add_value(k, v)\n        else:\n            self._add_value(field_name, value)",
        "begin_line": 71,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_value#81",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def replace_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._replace_value(k, v)\n        else:\n            self._replace_value(field_name, value)",
        "begin_line": 81,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._add_value#91",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._add_value(self, field_name, value)",
        "snippet": "    def _add_value(self, field_name, value):\n        value = arg_to_iter(value)\n        processed_value = self._process_input_value(field_name, value)\n        if processed_value:\n            self._values[field_name] += arg_to_iter(processed_value)",
        "begin_line": 91,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023413720440177945,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._replace_value#97",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._replace_value(self, field_name, value)",
        "snippet": "    def _replace_value(self, field_name, value):\n        self._values.pop(field_name, None)\n        self._add_value(field_name, value)",
        "begin_line": 97,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_value#101",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_value(self, value, *processors, **kw)",
        "snippet": "    def get_value(self, value, *processors, **kw):\n        regex = kw.get('re', None)\n        if regex:\n            value = arg_to_iter(value)\n            value = flatten(extract_regex(regex, x) for x in value)\n\n        for proc in processors:\n            if value is None:\n                break\n            _proc = proc\n            proc = wrap_loader_context(proc, self.context)\n            try:\n                value = proc(value)\n            except Exception as e:\n                raise ValueError(\"Error with processor %s value=%r error='%s: %s'\" %\n                                 (_proc.__class__.__name__, value,\n                                  type(e).__name__, str(e)))\n        return value",
        "begin_line": 101,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.load_item#120",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.load_item(self)",
        "snippet": "    def load_item(self):\n        item = self.item\n        for field_name in tuple(self._values):\n            value = self.get_output_value(field_name)\n            if value is not None:\n                item[field_name] = value\n\n        return item",
        "begin_line": 120,
        "end_line": 127,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_value#129",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_value(self, field_name)",
        "snippet": "    def get_output_value(self, field_name):\n        proc = self.get_output_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        try:\n            return proc(self._values[field_name])\n        except Exception as e:\n            raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n                (field_name, self._values[field_name], type(e).__name__, str(e)))",
        "begin_line": 129,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_collected_values#138",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_collected_values(self, field_name)",
        "snippet": "    def get_collected_values(self, field_name):\n        return self._values[field_name]",
        "begin_line": 138,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_input_processor#141",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_input_processor(self, field_name)",
        "snippet": "    def get_input_processor(self, field_name):\n        proc = getattr(self, '%s_in' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'input_processor', \\\n                self.default_input_processor)\n        return proc",
        "begin_line": 141,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002617801047120419,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_processor#148",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_processor(self, field_name)",
        "snippet": "    def get_output_processor(self, field_name):\n        proc = getattr(self, '%s_out' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'output_processor', \\\n                self.default_output_processor)\n        return proc",
        "begin_line": 148,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024067388688327315,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._process_input_value#155",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._process_input_value(self, field_name, value)",
        "snippet": "    def _process_input_value(self, field_name, value):\n        proc = self.get_input_processor(field_name)\n        _proc = proc\n        proc = wrap_loader_context(proc, self.context)\n        try:\n            return proc(value)\n        except Exception as e:\n            raise ValueError(\n                \"Error with input processor %s: field=%r value=%r \"\n                \"error='%s: %s'\" % (_proc.__class__.__name__, field_name,\n                                    value, type(e).__name__, str(e)))",
        "begin_line": 155,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_item_field_attr#167",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_item_field_attr(self, field_name, key, default=None)",
        "snippet": "    def _get_item_field_attr(self, field_name, key, default=None):\n        if isinstance(self.item, Item):\n            value = self.item.fields[field_name].get(key, default)\n        else:\n            value = default\n        return value",
        "begin_line": 167,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._check_selector_method#174",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._check_selector_method(self)",
        "snippet": "    def _check_selector_method(self):\n        if self.selector is None:\n            raise RuntimeError(\"To use XPath or CSS selectors, \"\n                \"%s must be instantiated with a selector \"\n                \"or a response\" % self.__class__.__name__)",
        "begin_line": 174,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_xpath#180",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def add_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 180,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_xpath#184",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def replace_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 184,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_xpath#188",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_xpath(self, xpath, *processors, **kw)",
        "snippet": "    def get_xpath(self, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 188,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_xpathvalues#192",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_xpathvalues(self, xpaths, **kw)",
        "snippet": "    def _get_xpathvalues(self, xpaths, **kw):\n        self._check_selector_method()\n        xpaths = arg_to_iter(xpaths)\n        return flatten(self.selector.xpath(xpath).getall() for xpath in xpaths)",
        "begin_line": 192,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_css#197",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def add_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 197,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_css#201",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def replace_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 201,
        "end_line": 203,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_css#205",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_css(self, css, *processors, **kw)",
        "snippet": "    def get_css(self, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 205,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_cssvalues#209",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_cssvalues(self, csss, **kw)",
        "snippet": "    def _get_cssvalues(self, csss, **kw):\n        self._check_selector_method()\n        csss = arg_to_iter(csss)\n        return flatten(self.selector.css(css).getall() for css in csss)",
        "begin_line": 209,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.__init__#22",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.__init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs)",
        "snippet": "    def __init__(self, method=SSL.SSLv23_METHOD, tls_verbose_logging=False, tls_ciphers=None, *args, **kwargs):\n        super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)\n        self._ssl_method = method\n        self.tls_verbose_logging = tls_verbose_logging\n        if tls_ciphers:\n            self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n        else:\n            self.tls_ciphers = DEFAULT_CIPHERS",
        "begin_line": 22,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.from_settings#32",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs)",
        "snippet": "    def from_settings(cls, settings, method=SSL.SSLv23_METHOD, *args, **kwargs):\n        tls_verbose_logging = settings.getbool('DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING')\n        tls_ciphers = settings['DOWNLOADER_CLIENT_TLS_CIPHERS']\n        return cls(method=method, tls_verbose_logging=tls_verbose_logging, tls_ciphers=tls_ciphers, *args, **kwargs)",
        "begin_line": 32,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016730801405387318,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getCertificateOptions#37",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getCertificateOptions(self)",
        "snippet": "    def getCertificateOptions(self):\n        # setting verify=True will require you to provide CAs\n        # to verify against; in other words: it's not that simple\n\n        # backward-compatible SSL/TLS method:\n        #\n        # * this will respect `method` attribute in often recommended\n        #   `ScrapyClientContextFactory` subclass\n        #   (https://github.com/scrapy/scrapy/issues/1429#issuecomment-131782133)\n        #\n        # * getattr() for `_ssl_method` attribute for context factories\n        #   not calling super(..., self).__init__\n        return CertificateOptions(verify=False,\n                    method=getattr(self, 'method',\n                                   getattr(self, '_ssl_method', None)),\n                    fixBrokenPeers=True,\n                    acceptableCiphers=self.tls_ciphers)",
        "begin_line": 37,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017467248908296942,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getContext#57",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.getContext(self, hostname=None, port=None)",
        "snippet": "    def getContext(self, hostname=None, port=None):\n        return self.getCertificateOptions().getContext()",
        "begin_line": 57,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017467248908296942,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.creatorForNetloc#60",
        "src_path": "scrapy/core/downloader/contextfactory.py",
        "class_name": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory",
        "signature": "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory.creatorForNetloc(self, hostname, port)",
        "snippet": "    def creatorForNetloc(self, hostname, port):\n        return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext(),\n                                      verbose_logging=self.tls_verbose_logging)",
        "begin_line": 60,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001762114537444934,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.__init__#37",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        if not settings.getbool('RETRY_ENABLED'):\n            raise NotConfigured\n        self.max_retry_times = settings.getint('RETRY_TIMES')\n        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))\n        self.priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')",
        "begin_line": 37,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.from_crawler#45",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler.settings)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001792435920415845,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_response#48",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_retry', False):\n            return response\n        if response.status in self.retry_http_codes:\n            reason = response_status_message(response.status)\n            return self._retry(request, reason, spider) or response\n        return response",
        "begin_line": 48,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_exception#56",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n                and not request.meta.get('dont_retry', False):\n            return self._retry(request, exception, spider)",
        "begin_line": 56,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.retry.RetryMiddleware._retry#61",
        "src_path": "scrapy/downloadermiddlewares/retry.py",
        "class_name": "scrapy.downloadermiddlewares.retry.RetryMiddleware",
        "signature": "scrapy.downloadermiddlewares.retry.RetryMiddleware._retry(self, request, reason, spider)",
        "snippet": "    def _retry(self, request, reason, spider):\n        retries = request.meta.get('retry_times', 0) + 1\n\n        retry_times = self.max_retry_times\n\n        if 'max_retry_times' in request.meta:\n            retry_times = request.meta['max_retry_times']\n\n        stats = spider.crawler.stats\n        if retries <= retry_times:\n            logger.debug(\"Retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                         {'request': request, 'retries': retries, 'reason': reason},\n                         extra={'spider': spider})\n            retryreq = request.copy()\n            retryreq.meta['retry_times'] = retries\n            retryreq.dont_filter = True\n            retryreq.priority = request.priority + self.priority_adjust\n\n            if isinstance(reason, Exception):\n                reason = global_object_name(reason.__class__)\n\n            stats.inc_value('retry/count')\n            stats.inc_value('retry/reason_count/%s' % reason)\n            return retryreq\n        else:\n            stats.inc_value('retry/max_reached')\n            logger.debug(\"Gave up retrying %(request)s (failed %(retries)d times): %(reason)s\",\n                         {'request': request, 'retries': retries, 'reason': reason},\n                         extra={'spider': spider})",
        "begin_line": 61,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.__init__#11",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):\n            raise NotConfigured\n\n        self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency = crawler.settings.getfloat(\"AUTOTHROTTLE_TARGET_CONCURRENCY\")\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)",
        "begin_line": 11,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.from_crawler#22",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.httpobj.urlparse_cached#8",
        "src_path": "scrapy/utils/httpobj.py",
        "class_name": "scrapy.utils.httpobj",
        "signature": "scrapy.utils.httpobj.urlparse_cached(request_or_response)",
        "snippet": "def urlparse_cached(request_or_response):\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]",
        "begin_line": 8,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017140898183064793,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.common.newsetter#2",
        "src_path": "scrapy/http/common.py",
        "class_name": "scrapy.http.common",
        "signature": "scrapy.http.common.newsetter(self, value)",
        "snippet": "    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n        raise AttributeError(msg)",
        "begin_line": 2,
        "end_line": 5,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.__init__#11",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.__init__(self, jobdir=None)",
        "snippet": "    def __init__(self, jobdir=None):\n        self.jobdir = jobdir",
        "begin_line": 11,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.from_crawler#15",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        jobdir = job_dir(crawler.settings)\n        if not jobdir:\n            raise NotConfigured\n\n        obj = cls(jobdir)\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj",
        "begin_line": 15,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_closed#25",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=2)",
        "begin_line": 25,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_opened#30",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.statefn#38",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.statefn(self)",
        "snippet": "    def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.__init__#28",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.__init__(self, **kwargs)",
        "snippet": "    def __init__(self, **kwargs):\n        self._configure(kwargs)",
        "begin_line": 28,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00028121484814398203,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter._configure#31",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter._configure(self, options, dont_fail=False)",
        "snippet": "    def _configure(self, options, dont_fail=False):\n        \"\"\"Configure the exporter by poping options from the ``options`` dict.\n        If dont_fail is set, it won't raise an exception on unexpected options\n        (useful for using with keyword arguments in subclasses constructors)\n        \"\"\"\n        self.encoding = options.pop('encoding', None)\n        self.fields_to_export = options.pop('fields_to_export', None)\n        self.export_empty_fields = options.pop('export_empty_fields', False)\n        self.indent = options.pop('indent', None)\n        if not dont_fail and options:\n            raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))",
        "begin_line": 31,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.export_item#43",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        raise NotImplementedError",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.serialize_field#46",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', lambda x: x)\n        return serializer(value)",
        "begin_line": 46,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.start_exporting#50",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        pass",
        "begin_line": 50,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002477700693756194,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter.finish_exporting#53",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        pass",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002477700693756194,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.BaseItemExporter._get_serialized_fields#56",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.BaseItemExporter",
        "signature": "scrapy.exporters.BaseItemExporter._get_serialized_fields(self, item, default_value=None, include_empty=None)",
        "snippet": "    def _get_serialized_fields(self, item, default_value=None, include_empty=None):\n        \"\"\"Return the fields to export as an iterable of tuples\n        (name, serialized_value)\n        \"\"\"\n        if include_empty is None:\n            include_empty = self.export_empty_fields\n        if self.fields_to_export is None:\n            if include_empty and not isinstance(item, dict):\n                field_iter = six.iterkeys(item.fields)\n            else:\n                field_iter = six.iterkeys(item)\n        else:\n            if include_empty:\n                field_iter = self.fields_to_export\n            else:\n                field_iter = (x for x in self.fields_to_export if x in item)\n\n        for field_name in field_iter:\n            if field_name in item:\n                field = {} if isinstance(item, dict) else item.fields[field_name]\n                value = self.serialize_field(field, field_name, item[field_name])\n            else:\n                value = default_value\n\n            yield field_name, value",
        "begin_line": 56,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonLinesItemExporter.__init__#85",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonLinesItemExporter",
        "signature": "scrapy.exporters.JsonLinesItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        self.file = file\n        kwargs.setdefault('ensure_ascii', not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**kwargs)",
        "begin_line": 85,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonLinesItemExporter.export_item#91",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonLinesItemExporter",
        "signature": "scrapy.exporters.JsonLinesItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict) + '\\n'\n        self.file.write(to_bytes(data, self.encoding))",
        "begin_line": 91,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.__init__#99",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        self.file = file\n        # there is a small difference between the behaviour or JsonItemExporter.indent\n        # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n        # the addition of newlines everywhere\n        json_indent = self.indent if self.indent is not None and self.indent > 0 else None\n        kwargs.setdefault('indent', json_indent)\n        kwargs.setdefault('ensure_ascii', not self.encoding)\n        self.encoder = ScrapyJSONEncoder(**kwargs)\n        self.first_item = True",
        "begin_line": 99,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter._beautify_newline#111",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter._beautify_newline(self)",
        "snippet": "    def _beautify_newline(self):\n        if self.indent is not None:\n            self.file.write(b'\\n')",
        "begin_line": 111,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.start_exporting#115",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        self.file.write(b\"[\")\n        self._beautify_newline()",
        "begin_line": 115,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.finish_exporting#119",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        self._beautify_newline()\n        self.file.write(b\"]\")",
        "begin_line": 119,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.JsonItemExporter.export_item#123",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.JsonItemExporter",
        "signature": "scrapy.exporters.JsonItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        if self.first_item:\n            self.first_item = False\n        else:\n            self.file.write(b',')\n            self._beautify_newline()\n        itemdict = dict(self._get_serialized_fields(item))\n        data = self.encoder.encode(itemdict)\n        self.file.write(to_bytes(data, self.encoding))",
        "begin_line": 123,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.__init__#136",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self.item_element = kwargs.pop('item_element', 'item')\n        self.root_element = kwargs.pop('root_element', 'items')\n        self._configure(kwargs)\n        if not self.encoding:\n            self.encoding = 'utf-8'\n        self.xg = XMLGenerator(file, encoding=self.encoding)",
        "begin_line": 136,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._beautify_newline#144",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._beautify_newline(self, new_item=False)",
        "snippet": "    def _beautify_newline(self, new_item=False):\n        if self.indent is not None and (self.indent > 0 or new_item):\n            self._xg_characters('\\n')",
        "begin_line": 144,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._beautify_indent#148",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._beautify_indent(self, depth=1)",
        "snippet": "    def _beautify_indent(self, depth=1):\n        if self.indent:\n            self._xg_characters(' ' * self.indent * depth)",
        "begin_line": 148,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.start_exporting#152",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.start_exporting(self)",
        "snippet": "    def start_exporting(self):\n        self.xg.startDocument()\n        self.xg.startElement(self.root_element, {})\n        self._beautify_newline(new_item=True)",
        "begin_line": 152,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.export_item#157",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        self._beautify_indent(depth=1)\n        self.xg.startElement(self.item_element, {})\n        self._beautify_newline()\n        for name, value in self._get_serialized_fields(item, default_value=''):\n            self._export_xml_field(name, value, depth=2)\n        self._beautify_indent(depth=1)\n        self.xg.endElement(self.item_element)\n        self._beautify_newline(new_item=True)",
        "begin_line": 157,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter.finish_exporting#167",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter.finish_exporting(self)",
        "snippet": "    def finish_exporting(self):\n        self.xg.endElement(self.root_element)\n        self.xg.endDocument()",
        "begin_line": 167,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._export_xml_field#171",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._export_xml_field(self, name, serialized_value, depth)",
        "snippet": "    def _export_xml_field(self, name, serialized_value, depth):\n        self._beautify_indent(depth=depth)\n        self.xg.startElement(name, {})\n        if hasattr(serialized_value, 'items'):\n            self._beautify_newline()\n            for subname, value in serialized_value.items():\n                self._export_xml_field(subname, value, depth=depth+1)\n            self._beautify_indent(depth=depth)\n        elif is_listlike(serialized_value):\n            self._beautify_newline()\n            for value in serialized_value:\n                self._export_xml_field('value', value, depth=depth+1)\n            self._beautify_indent(depth=depth)\n        elif isinstance(serialized_value, six.text_type):\n            self._xg_characters(serialized_value)\n        else:\n            self._xg_characters(str(serialized_value))\n        self.xg.endElement(name)\n        self._beautify_newline()",
        "begin_line": 171,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.XmlItemExporter._xg_characters#197",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.XmlItemExporter",
        "signature": "scrapy.exporters.XmlItemExporter._xg_characters(self, serialized_value)",
        "snippet": "        def _xg_characters(self, serialized_value):\n            if not isinstance(serialized_value, six.text_type):\n                serialized_value = serialized_value.decode(self.encoding)\n            return self.xg.characters(serialized_value)",
        "begin_line": 197,
        "end_line": 200,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.__init__#208",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.__init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs)",
        "snippet": "    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n        self._configure(kwargs, dont_fail=True)\n        if not self.encoding:\n            self.encoding = 'utf-8'\n        self.include_headers_line = include_headers_line\n        self.stream = io.TextIOWrapper(\n            file,\n            line_buffering=False,\n            write_through=True,\n            encoding=self.encoding,\n            newline='' # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n        ) if six.PY3 else file\n        self.csv_writer = csv.writer(self.stream, **kwargs)\n        self._headers_not_written = True\n        self._join_multivalued = join_multivalued",
        "begin_line": 208,
        "end_line": 222,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002663115845539281,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.serialize_field#224",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._join_if_needed)\n        return serializer(value)",
        "begin_line": 224,
        "end_line": 226,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._join_if_needed#228",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._join_if_needed(self, value)",
        "snippet": "    def _join_if_needed(self, value):\n        if isinstance(value, (list, tuple)):\n            try:\n                return self._join_multivalued.join(value)\n            except TypeError:  # list in value may not contain strings\n                pass\n        return value",
        "begin_line": 228,
        "end_line": 234,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter.export_item#236",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        if self._headers_not_written:\n            self._headers_not_written = False\n            self._write_headers_and_set_fields_to_export(item)\n\n        fields = self._get_serialized_fields(item, default_value='',\n                                             include_empty=True)\n        values = list(self._build_row(x for _, x in fields))\n        self.csv_writer.writerow(values)",
        "begin_line": 236,
        "end_line": 244,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002933411557641537,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._build_row#246",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._build_row(self, values)",
        "snippet": "    def _build_row(self, values):\n        for s in values:\n            try:\n                yield to_native_str(s, self.encoding)\n            except TypeError:\n                yield s",
        "begin_line": 246,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.CsvItemExporter._write_headers_and_set_fields_to_export#253",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.CsvItemExporter",
        "signature": "scrapy.exporters.CsvItemExporter._write_headers_and_set_fields_to_export(self, item)",
        "snippet": "    def _write_headers_and_set_fields_to_export(self, item):\n        if self.include_headers_line:\n            if not self.fields_to_export:\n                if isinstance(item, dict):\n                    # for dicts try using fields of the first item\n                    self.fields_to_export = list(item.keys())\n                else:\n                    # use fields declared in Item\n                    self.fields_to_export = list(item.fields.keys())\n            row = list(self._build_row(self.fields_to_export))\n            self.csv_writer.writerow(row)",
        "begin_line": 253,
        "end_line": 263,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PickleItemExporter.__init__#268",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PickleItemExporter",
        "signature": "scrapy.exporters.PickleItemExporter.__init__(self, file, protocol=2, **kwargs)",
        "snippet": "    def __init__(self, file, protocol=2, **kwargs):\n        self._configure(kwargs)\n        self.file = file\n        self.protocol = protocol",
        "begin_line": 268,
        "end_line": 271,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PickleItemExporter.export_item#273",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PickleItemExporter",
        "signature": "scrapy.exporters.PickleItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        d = dict(self._get_serialized_fields(item))\n        pickle.dump(d, self.file, self.protocol)",
        "begin_line": 273,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.MarshalItemExporter.__init__#287",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.MarshalItemExporter",
        "signature": "scrapy.exporters.MarshalItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs)\n        self.file = file",
        "begin_line": 287,
        "end_line": 289,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.MarshalItemExporter.export_item#291",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.MarshalItemExporter",
        "signature": "scrapy.exporters.MarshalItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        marshal.dump(dict(self._get_serialized_fields(item)), self.file)",
        "begin_line": 291,
        "end_line": 292,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PprintItemExporter.__init__#297",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PprintItemExporter",
        "signature": "scrapy.exporters.PprintItemExporter.__init__(self, file, **kwargs)",
        "snippet": "    def __init__(self, file, **kwargs):\n        self._configure(kwargs)\n        self.file = file",
        "begin_line": 297,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PprintItemExporter.export_item#301",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PprintItemExporter",
        "signature": "scrapy.exporters.PprintItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        itemdict = dict(self._get_serialized_fields(item))\n        self.file.write(to_bytes(pprint.pformat(itemdict) + '\\n'))",
        "begin_line": 301,
        "end_line": 303,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._configure#315",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._configure(self, options, dont_fail=False)",
        "snippet": "    def _configure(self, options, dont_fail=False):\n        self.binary = options.pop('binary', True)\n        super(PythonItemExporter, self)._configure(options, dont_fail)\n        if self.binary:\n            warnings.warn(\n                \"PythonItemExporter will drop support for binary export in the future\",\n                ScrapyDeprecationWarning)\n        if not self.encoding:\n            self.encoding = 'utf-8'",
        "begin_line": 315,
        "end_line": 323,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter.serialize_field#325",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter.serialize_field(self, field, name, value)",
        "snippet": "    def serialize_field(self, field, name, value):\n        serializer = field.get('serializer', self._serialize_value)\n        return serializer(value)",
        "begin_line": 325,
        "end_line": 327,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._serialize_value#329",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._serialize_value(self, value)",
        "snippet": "    def _serialize_value(self, value):\n        if isinstance(value, BaseItem):\n            return self.export_item(value)\n        if isinstance(value, dict):\n            return dict(self._serialize_dict(value))\n        if is_listlike(value):\n            return [self._serialize_value(v) for v in value]\n        encode_func = to_bytes if self.binary else to_unicode\n        if isinstance(value, (six.text_type, bytes)):\n            return encode_func(value, encoding=self.encoding)\n        return value",
        "begin_line": 329,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter._serialize_dict#341",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter._serialize_dict(self, value)",
        "snippet": "    def _serialize_dict(self, value):\n        for key, val in six.iteritems(value):\n            key = to_bytes(key) if self.binary else key\n            yield key, self._serialize_value(val)",
        "begin_line": 341,
        "end_line": 344,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.exporters.PythonItemExporter.export_item#346",
        "src_path": "scrapy/exporters.py",
        "class_name": "scrapy.exporters.PythonItemExporter",
        "signature": "scrapy.exporters.PythonItemExporter.export_item(self, item)",
        "snippet": "    def export_item(self, item):\n        result = dict(self._get_serialized_fields(item))\n        if self.binary:\n            result = dict(self._serialize_dict(result))\n        return result",
        "begin_line": 346,
        "end_line": 350,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.request.request_fingerprint#19",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_fingerprint(request, include_headers=None)",
        "snippet": "def request_fingerprint(request, include_headers=None):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (ie. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accessible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    \"\"\"\n    if include_headers:\n        include_headers = tuple(to_bytes(h.lower())\n                                 for h in sorted(include_headers))\n    cache = _fingerprint_cache.setdefault(request, {})\n    if include_headers not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url)))\n        fp.update(request.body or b'')\n        if include_headers:\n            for hdr in include_headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[include_headers] = fp.hexdigest()\n    return cache[include_headers]",
        "begin_line": 19,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.request.request_authenticate#65",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_authenticate(request, username, password)",
        "snippet": "def request_authenticate(request, username, password):\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)",
        "begin_line": 65,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.request.request_httprepr#72",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_httprepr(request)",
        "snippet": "def request_httprepr(request):\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname or b'') + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s",
        "begin_line": 72,
        "end_line": 86,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.request.referer_str#89",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.referer_str(request)",
        "snippet": "def referer_str(request):\n    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n    referrer = request.headers.get('Referer')\n    if referrer is None:\n        return referrer\n    return to_native_str(referrer, errors='replace')",
        "begin_line": 89,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.http.decode_chunked_transfer#20",
        "src_path": "scrapy/utils/http.py",
        "class_name": "scrapy.utils.http",
        "signature": "scrapy.utils.http.decode_chunked_transfer(chunked_body)",
        "snippet": "def decode_chunked_transfer(chunked_body):\n    \"\"\"Parsed body received with chunked transfer encoding, and return the\n    decoded body.\n\n    For more info see:\n    https://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n    \"\"\"\n    body, h, t = '', '', chunked_body\n    while t:\n        h, t = t.split('\\r\\n', 1)\n        if h == '0':\n            break\n        size = int(h, 16)\n        body += t[:size]\n        t = t[size+2:]\n    return body",
        "begin_line": 20,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._get_mwlist_from_settings#22",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(\n            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))",
        "begin_line": 22,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._add_middleware#26",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'process_request'):\n            self.methods['process_request'].append(mw.process_request)\n        if hasattr(mw, 'process_response'):\n            self.methods['process_response'].appendleft(mw.process_response)\n        if hasattr(mw, 'process_exception'):\n            self.methods['process_exception'].appendleft(mw.process_exception)",
        "begin_line": 26,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.download#34",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.download(self, download_func, request, spider)",
        "snippet": "    def download(self, download_func, request, spider):\n        @defer.inlineCallbacks\n        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield method(request=request, spider=spider)\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, response.__class__.__name__))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue((yield download_func(request=request, spider=spider)))\n\n        @defer.inlineCallbacks\n        def process_response(response):\n            assert response is not None, 'Received None in process_response'\n            if isinstance(response, Request):\n                defer.returnValue(response)\n\n            for method in self.methods['process_response']:\n                response = yield method(request=request, response=response, spider=spider)\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                if isinstance(response, Request):\n                    defer.returnValue(response)\n            defer.returnValue(response)\n\n        @defer.inlineCallbacks\n        def process_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_exception']:\n                response = yield method(request=request, exception=exception, spider=spider)\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue(_failure)\n\n        deferred = mustbe_deferred(process_request, request)\n        deferred.addErrback(process_exception)\n        deferred.addCallback(process_response)\n        return deferred",
        "begin_line": 34,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_request#36",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_request(request)",
        "snippet": "        def process_request(request):\n            for method in self.methods['process_request']:\n                response = yield method(request=request, spider=spider)\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, response.__class__.__name__))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue((yield download_func(request=request, spider=spider)))",
        "begin_line": 36,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_response#47",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_response(response)",
        "snippet": "        def process_response(response):\n            assert response is not None, 'Received None in process_response'\n            if isinstance(response, Request):\n                defer.returnValue(response)\n\n            for method in self.methods['process_response']:\n                response = yield method(request=request, response=response, spider=spider)\n                if not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                if isinstance(response, Request):\n                    defer.returnValue(response)\n            defer.returnValue(response)",
        "begin_line": 47,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_exception#62",
        "src_path": "scrapy/core/downloader/middleware.py",
        "class_name": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager",
        "signature": "scrapy.core.downloader.middleware.DownloaderMiddlewareManager.process_exception(_failure)",
        "snippet": "        def process_exception(_failure):\n            exception = _failure.value\n            for method in self.methods['process_exception']:\n                response = yield method(request=request, exception=exception, spider=spider)\n                if response is not None and not isinstance(response, (Response, Request)):\n                    raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                                         (six.get_method_self(method).__class__.__name__, type(response)))\n                if response:\n                    defer.returnValue(response)\n            defer.returnValue(_failure)",
        "begin_line": 62,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.boto.is_botocore#9",
        "src_path": "scrapy/utils/boto.py",
        "class_name": "scrapy.utils.boto",
        "signature": "scrapy.utils.boto.is_botocore()",
        "snippet": "def is_botocore():\n    try:\n        import botocore\n        return True\n    except ImportError:\n        if six.PY2:\n            try:\n                import boto\n                return False\n            except ImportError:\n                raise NotConfigured('missing botocore or boto library')\n        else:\n            raise NotConfigured('missing botocore library')",
        "begin_line": 9,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002702702702702703,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.project.inside_project#17",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.inside_project()",
        "snippet": "def inside_project():\n    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n    if scrapy_module is not None:\n        try:\n            import_module(scrapy_module)\n        except ImportError as exc:\n            warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n        else:\n            return True\n    return bool(closest_scrapy_cfg())",
        "begin_line": 17,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.project.project_data_dir#29",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.project_data_dir(project='default')",
        "snippet": "def project_data_dir(project='default'):\n    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n    if not inside_project():\n        raise NotConfigured(\"Not inside a project\")\n    cfg = get_config()\n    if cfg.has_option(DATADIR_CFG_SECTION, project):\n        d = cfg.get(DATADIR_CFG_SECTION, project)\n    else:\n        scrapy_cfg = closest_scrapy_cfg()\n        if not scrapy_cfg:\n            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n    if not exists(d):\n        os.makedirs(d)\n    return d",
        "begin_line": 29,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.project.data_path#46",
        "src_path": "scrapy/utils/project.py",
        "class_name": "scrapy.utils.project",
        "signature": "scrapy.utils.project.data_path(path, createdir=False)",
        "snippet": "def data_path(path, createdir=False):\n    \"\"\"\n    Return the given path joined with the .scrapy data directory.\n    If given an absolute path, return it unmodified.\n    \"\"\"\n    if not isabs(path):\n        if inside_project():\n            path = join(project_data_dir(), path)\n        else:\n            path = join('.scrapy', path)\n    if createdir and not exists(path):\n        os.makedirs(path)\n    return path",
        "begin_line": 46,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml._nons#24",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml",
        "signature": "scrapy.linkextractors.lxmlhtml._nons(tag)",
        "snippet": "def _nons(tag):\n    if isinstance(tag, six.string_types):\n        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE)+1] == XHTML_NAMESPACE:\n            return tag.split('}')[-1]\n    return tag",
        "begin_line": 24,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor.__init__#32",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor.__init__(self, tag='a', attr='href', process=None, unique=False, strip=True, canonicalized=False)",
        "snippet": "    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n                 strip=True, canonicalized=False):\n        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n        self.process_attr = process if callable(process) else lambda v: v\n        self.unique = unique\n        self.strip = strip\n        if canonicalized:\n            self.link_key = lambda link: link.url\n        else:\n            self.link_key = lambda link: canonicalize_url(link.url,\n                                                          keep_fragments=True)",
        "begin_line": 32,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._iter_links#45",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._iter_links(self, document)",
        "snippet": "    def _iter_links(self, document):\n        for el in document.iter(etree.Element):\n            if not self.scan_tag(_nons(el.tag)):\n                continue\n            attribs = el.attrib\n            for attrib in attribs:\n                if not self.scan_attr(attrib):\n                    continue\n                yield (el, attrib, attribs[attrib])",
        "begin_line": 45,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._extract_links#55",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._extract_links(self, selector, response_url, response_encoding, base_url)",
        "snippet": "    def _extract_links(self, selector, response_url, response_encoding, base_url):\n        links = []\n        # hacky way to get the underlying lxml parsed document\n        for el, attr, attr_val in self._iter_links(selector.root):\n            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n            try:\n                if self.strip:\n                    attr_val = strip_html5_whitespace(attr_val)\n                attr_val = urljoin(base_url, attr_val)\n            except ValueError:\n                continue  # skipping bogus links\n            else:\n                url = self.process_attr(attr_val)\n                if url is None:\n                    continue\n            url = to_native_str(url, encoding=response_encoding)\n            # to fix relative links after process_value\n            url = urljoin(response_url, url)\n            link = Link(url, _collect_string_content(el) or u'',\n                        nofollow=rel_has_nofollow(el.get('rel')))\n            links.append(link)\n        return self._deduplicate_if_needed(links)",
        "begin_line": 55,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._process_links#82",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._process_links(self, links)",
        "snippet": "    def _process_links(self, links):\n        \"\"\" Normalize and filter extracted links\n\n        The subclass should override it if neccessary\n        \"\"\"\n        return self._deduplicate_if_needed(links)",
        "begin_line": 82,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022972662531587412,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._deduplicate_if_needed#89",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlParserLinkExtractor._deduplicate_if_needed(self, links)",
        "snippet": "    def _deduplicate_if_needed(self, links):\n        if self.unique:\n            return unique_list(links, key=self.link_key)\n        return links",
        "begin_line": 89,
        "end_line": 92,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.__init__#97",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.__init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True, process_value=None, deny_extensions=None, restrict_css=(), strip=True, restrict_text=None)",
        "snippet": "    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n                 tags=('a', 'area'), attrs=('href',), canonicalize=False,\n                 unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n                 strip=True, restrict_text=None):\n        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n        tag_func = lambda x: x in tags\n        attr_func = lambda x: x in attrs\n        lx = LxmlParserLinkExtractor(\n            tag=tag_func,\n            attr=attr_func,\n            unique=unique,\n            process=process_value,\n            strip=strip,\n            canonicalized=canonicalize\n        )\n\n        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n                                                allow_domains=allow_domains, deny_domains=deny_domains,\n                                                restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n                                                restrict_text=restrict_text)",
        "begin_line": 97,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024067388688327315,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links#119",
        "src_path": "scrapy/linkextractors/lxmlhtml.py",
        "class_name": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor",
        "signature": "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links(self, response)",
        "snippet": "    def extract_links(self, response):\n        base_url = get_base_url(response)\n        if self.restrict_xpaths:\n            docs = [subdoc\n                    for x in self.restrict_xpaths\n                    for subdoc in response.xpath(x)]\n        else:\n            docs = [response.selector]\n        all_links = []\n        for doc in docs:\n            links = self._extract_links(doc, response.url, response.encoding, base_url)\n            all_links.extend(self._process_links(links))\n        return unique_list(all_links)",
        "begin_line": 119,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.__init__#44",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured(\n                'TELNETCONSOLE_ENABLED setting is True but required twisted '\n                'modules failed to import:\\n' + _TWISTED_CONCH_TRACEBACK)\n        self.crawler = crawler\n        self.noisy = False\n        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n        self.host = crawler.settings['TELNETCONSOLE_HOST']\n        self.username = crawler.settings['TELNETCONSOLE_USERNAME']\n        self.password = crawler.settings['TELNETCONSOLE_PASSWORD']\n\n        if not self.password:\n            self.password = binascii.hexlify(os.urandom(8)).decode('utf8')\n            logger.info('Telnet Password: %s', self.password)\n\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)",
        "begin_line": 44,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016570008285004143,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.from_crawler#66",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 66,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.start_listening#69",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.start_listening(self)",
        "snippet": "    def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.info(\"Telnet console listening on %(host)s:%(port)d\",\n                    {'host': h.host, 'port': h.port},\n                    extra={'crawler': self.crawler})",
        "begin_line": 69,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.stop_listening#76",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.stop_listening(self)",
        "snippet": "    def stop_listening(self):\n        self.port.stopListening()",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.TelnetConsole.protocol#79",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.TelnetConsole",
        "signature": "scrapy.extensions.telnet.TelnetConsole.protocol(self)",
        "snippet": "    def protocol(self):\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (credentials.username == self.username.encode('utf8') and\n                        credentials.checkPassword(self.password.encode('utf8'))):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(\n            telnet.AuthenticatingTelnetProtocol,\n            Portal()\n        )",
        "begin_line": 79,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.Portal.protocol#79",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.Portal",
        "signature": "scrapy.extensions.telnet.Portal.protocol(self)",
        "snippet": "    def protocol(self):\n        class Portal:\n            \"\"\"An implementation of IPortal\"\"\"\n            @defers\n            def login(self_, credentials, mind, *interfaces):\n                if not (credentials.username == self.username.encode('utf8') and\n                        credentials.checkPassword(self.password.encode('utf8'))):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)\n\n        return telnet.TelnetTransport(\n            telnet.AuthenticatingTelnetProtocol,\n            Portal()\n        )",
        "begin_line": 79,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.telnet.Portal.login#83",
        "src_path": "scrapy/extensions/telnet.py",
        "class_name": "scrapy.extensions.telnet.Portal",
        "signature": "scrapy.extensions.telnet.Portal.login(self_, credentials, mind, *interfaces)",
        "snippet": "            def login(self_, credentials, mind, *interfaces):\n                if not (credentials.username == self.username.encode('utf8') and\n                        credentials.checkPassword(self.password.encode('utf8'))):\n                    raise ValueError(\"Invalid credentials\")\n\n                protocol = telnet.TelnetBootstrapProtocol(\n                    insults.ServerProtocol,\n                    manhole.Manhole,\n                    self._get_telnet_vars()\n                )\n                return (interfaces[0], protocol, lambda: None)",
        "begin_line": 83,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.Slot.__init__#26",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.__init__(self, start_requests, close_if_idle, nextcall, scheduler)",
        "snippet": "    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n        self.closing = False\n        self.inprogress = set() # requests in progress\n        self.start_requests = iter(start_requests)\n        self.close_if_idle = close_if_idle\n        self.nextcall = nextcall\n        self.scheduler = scheduler\n        self.heartbeat = task.LoopingCall(nextcall.schedule)",
        "begin_line": 26,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.Slot.add_request#35",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.add_request(self, request)",
        "snippet": "    def add_request(self, request):\n        self.inprogress.add(request)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.Slot.remove_request#38",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.remove_request(self, request)",
        "snippet": "    def remove_request(self, request):\n        self.inprogress.remove(request)\n        self._maybe_fire_closing()",
        "begin_line": 38,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.Slot.close#42",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot.close(self)",
        "snippet": "    def close(self):\n        self.closing = defer.Deferred()\n        self._maybe_fire_closing()\n        return self.closing",
        "begin_line": 42,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.Slot._maybe_fire_closing#47",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.Slot",
        "signature": "scrapy.core.engine.Slot._maybe_fire_closing(self)",
        "snippet": "    def _maybe_fire_closing(self):\n        if self.closing and not self.inprogress:\n            if self.nextcall:\n                self.nextcall.cancel()\n                if self.heartbeat.running:\n                    self.heartbeat.stop()\n            self.closing.callback(None)",
        "begin_line": 47,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020275750202757503,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.__init__#58",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.__init__(self, crawler, spider_closed_callback)",
        "snippet": "    def __init__(self, crawler, spider_closed_callback):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        self.signals = crawler.signals\n        self.logformatter = crawler.logformatter\n        self.slot = None\n        self.spider = None\n        self.running = False\n        self.paused = False\n        self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n        downloader_cls = load_object(self.settings['DOWNLOADER'])\n        self.downloader = downloader_cls(crawler)\n        self.scraper = Scraper(crawler)\n        self._spider_closed_callback = spider_closed_callback",
        "begin_line": 58,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.start#74",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.start(self)",
        "snippet": "    def start(self):\n        \"\"\"Start the execution engine\"\"\"\n        assert not self.running, \"Engine already running\"\n        self.start_time = time()\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n        self.running = True\n        self._closewait = defer.Deferred()\n        yield self._closewait",
        "begin_line": 74,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.stop#83",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.stop(self)",
        "snippet": "    def stop(self):\n        \"\"\"Stop the execution engine gracefully\"\"\"\n        assert self.running, \"Engine not running\"\n        self.running = False\n        dfd = self._close_all_spiders()\n        return dfd.addBoth(lambda _: self._finish_stopping_engine())",
        "begin_line": 83,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.close#90",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.close(self)",
        "snippet": "    def close(self):\n        \"\"\"Close the execution engine gracefully.\n\n        If it has already been started, stop it. In all cases, close all spiders\n        and the downloader.\n        \"\"\"\n        if self.running:\n            # Will also close spiders and downloader\n            return self.stop()\n        elif self.open_spiders:\n            # Will also close downloader\n            return self._close_all_spiders()\n        else:\n            return defer.succeed(self.downloader.close())",
        "begin_line": 90,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._next_request#113",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._next_request(self, spider)",
        "snippet": "    def _next_request(self, spider):\n        slot = self.slot\n        if not slot:\n            return\n\n        if self.paused:\n            return\n\n        while not self._needs_backout(spider):\n            if not self._next_request_from_scheduler(spider):\n                break\n\n        if slot.start_requests and not self._needs_backout(spider):\n            try:\n                request = next(slot.start_requests)\n            except StopIteration:\n                slot.start_requests = None\n            except Exception:\n                slot.start_requests = None\n                logger.error('Error while obtaining start requests',\n                             exc_info=True, extra={'spider': spider})\n            else:\n                self.crawl(request, spider)\n\n        if self.spider_is_idle(spider) and slot.close_if_idle:\n            self._spider_idle(spider)",
        "begin_line": 113,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._needs_backout#140",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._needs_backout(self, spider)",
        "snippet": "    def _needs_backout(self, spider):\n        slot = self.slot\n        return not self.running \\\n            or slot.closing \\\n            or self.downloader.needs_backout() \\\n            or self.scraper.slot.needs_backout()",
        "begin_line": 140,
        "end_line": 145,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002069108214359611,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._next_request_from_scheduler#147",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._next_request_from_scheduler(self, spider)",
        "snippet": "    def _next_request_from_scheduler(self, spider):\n        slot = self.slot\n        request = slot.scheduler.next_request()\n        if not request:\n            return\n        d = self._download(request, spider)\n        d.addBoth(self._handle_downloader_output, request, spider)\n        d.addErrback(lambda f: logger.info('Error while handling downloader output',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.remove_request(request))\n        d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        d.addBoth(lambda _: slot.nextcall.schedule())\n        d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                           exc_info=failure_to_exc_info(f),\n                                           extra={'spider': spider}))\n        return d",
        "begin_line": 147,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._handle_downloader_output#167",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._handle_downloader_output(self, response, request, spider)",
        "snippet": "    def _handle_downloader_output(self, response, request, spider):\n        assert isinstance(response, (Request, Response, Failure)), response\n        # downloader middleware can return requests (for example, redirects)\n        if isinstance(response, Request):\n            self.crawl(response, spider)\n            return\n        # response is a Response or Failure\n        d = self.scraper.enqueue_scrape(response, request, spider)\n        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d",
        "begin_line": 167,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.spider_is_idle#180",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.spider_is_idle(self, spider)",
        "snippet": "    def spider_is_idle(self, spider):\n        if not self.scraper.slot.is_idle():\n            # scraper is not idle\n            return False\n\n        if self.downloader.active:\n            # downloader has pending requests\n            return False\n\n        if self.slot.start_requests is not None:\n            # not all start requests are handled\n            return False\n\n        if self.slot.scheduler.has_pending_requests():\n            # scheduler has pending requests\n            return False\n\n        return True",
        "begin_line": 180,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.open_spiders#200",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.open_spiders(self)",
        "snippet": "    def open_spiders(self):\n        return [self.spider] if self.spider else []",
        "begin_line": 200,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.has_capacity#203",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.has_capacity(self)",
        "snippet": "    def has_capacity(self):\n        \"\"\"Does the engine have capacity to handle more spiders\"\"\"\n        return not bool(self.slot)",
        "begin_line": 203,
        "end_line": 205,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.crawl#207",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.crawl(self, request, spider)",
        "snippet": "    def crawl(self, request, spider):\n        assert spider in self.open_spiders, \\\n            \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n        self.schedule(request, spider)\n        self.slot.nextcall.schedule()",
        "begin_line": 207,
        "end_line": 211,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.schedule#213",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.schedule(self, request, spider)",
        "snippet": "    def schedule(self, request, spider):\n        self.signals.send_catch_log(signal=signals.request_scheduled,\n                request=request, spider=spider)\n        if not self.slot.scheduler.enqueue_request(request):\n            self.signals.send_catch_log(signal=signals.request_dropped,\n                                        request=request, spider=spider)",
        "begin_line": 213,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.download#220",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.download(self, request, spider)",
        "snippet": "    def download(self, request, spider):\n        d = self._download(request, spider)\n        d.addBoth(self._downloaded, self.slot, request, spider)\n        return d",
        "begin_line": 220,
        "end_line": 223,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._downloaded#225",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._downloaded(self, response, slot, request, spider)",
        "snippet": "    def _downloaded(self, response, slot, request, spider):\n        slot.remove_request(request)\n        return self.download(response, spider) \\\n                if isinstance(response, Request) else response",
        "begin_line": 225,
        "end_line": 228,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._download#230",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._download(self, request, spider)",
        "snippet": "    def _download(self, request, spider):\n        slot = self.slot\n        slot.add_request(request)\n        def _on_success(response):\n            assert isinstance(response, (Response, Request))\n            if isinstance(response, Response):\n                response.request = request  # tie request to response received\n                logkws = self.logformatter.crawled(request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(signal=signals.response_received,\n                    response=response, request=request, spider=spider)\n            return response\n\n        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _\n\n        dwld = self.downloader.fetch(request, spider)\n        dwld.addCallbacks(_on_success)\n        dwld.addBoth(_on_complete)\n        return dwld",
        "begin_line": 230,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._on_success#233",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._on_success(response)",
        "snippet": "        def _on_success(response):\n            assert isinstance(response, (Response, Request))\n            if isinstance(response, Response):\n                response.request = request  # tie request to response received\n                logkws = self.logformatter.crawled(request, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                self.signals.send_catch_log(signal=signals.response_received,\n                    response=response, request=request, spider=spider)\n            return response",
        "begin_line": 233,
        "end_line": 242,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002255299954894001,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._on_complete#244",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._on_complete(_)",
        "snippet": "        def _on_complete(_):\n            slot.nextcall.schedule()\n            return _",
        "begin_line": 244,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.open_spider#254",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.open_spider(self, spider, start_requests=(), close_if_idle=True)",
        "snippet": "    def open_spider(self, spider, start_requests=(), close_if_idle=True):\n        assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n            spider.name\n        logger.info(\"Spider opened\", extra={'spider': spider})\n        nextcall = CallLaterOnce(self._next_request, spider)\n        scheduler = self.scheduler_cls.from_crawler(self.crawler)\n        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)\n        slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n        self.slot = slot\n        self.spider = spider\n        yield scheduler.open(spider)\n        yield self.scraper.open_spider(spider)\n        self.crawler.stats.open_spider(spider)\n        yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)\n        slot.nextcall.schedule()\n        slot.heartbeat.start(5)",
        "begin_line": 254,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020275750202757503,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._spider_idle#271",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._spider_idle(self, spider)",
        "snippet": "    def _spider_idle(self, spider):\n        \"\"\"Called when a spider gets idle. This function is called when there\n        are no remaining pages to download or schedule. It can be called\n        multiple times. If some extension raises a DontCloseSpider exception\n        (in the spider_idle signal handler) the spider is not closed until the\n        next loop and this function is guaranteed to be called (at least) once\n        again for this spider.\n        \"\"\"\n        res = self.signals.send_catch_log(signal=signals.spider_idle, \\\n            spider=spider, dont_log=DontCloseSpider)\n        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) \\\n                for _, x in res):\n            return\n\n        if self.spider_is_idle(spider):\n            self.close_spider(spider, reason='finished')",
        "begin_line": 271,
        "end_line": 286,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.close_spider#288",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.close_spider(self, spider, reason='cancelled')",
        "snippet": "    def close_spider(self, spider, reason='cancelled'):\n        \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n        slot = self.slot\n        if slot.closing:\n            return slot.closing\n        logger.info(\"Closing spider (%(reason)s)\",\n                    {'reason': reason},\n                    extra={'spider': spider})\n\n        dfd = slot.close()\n\n        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback\n\n        dfd.addBoth(lambda _: self.downloader.close())\n        dfd.addErrback(log_failure('Downloader close failure'))\n\n        dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n        dfd.addErrback(log_failure('Scraper close failure'))\n\n        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n        dfd.addErrback(log_failure('Scheduler close failure'))\n\n        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(\n            signal=signals.spider_closed, spider=spider, reason=reason))\n        dfd.addErrback(log_failure('Error while sending spider_close signal'))\n\n        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))\n        dfd.addErrback(log_failure('Stats close failure'))\n\n        dfd.addBoth(lambda _: logger.info(\"Spider closed (%(reason)s)\",\n                                          {'reason': reason},\n                                          extra={'spider': spider}))\n\n        dfd.addBoth(lambda _: setattr(self, 'slot', None))\n        dfd.addErrback(log_failure('Error while unassigning slot'))\n\n        dfd.addBoth(lambda _: setattr(self, 'spider', None))\n        dfd.addErrback(log_failure('Error while unassigning spider'))\n\n        dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n        return dfd",
        "begin_line": 288,
        "end_line": 337,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.log_failure#300",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.log_failure(msg)",
        "snippet": "        def log_failure(msg):\n            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )\n            return errback",
        "begin_line": 300,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine.errback#301",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine.errback(failure)",
        "snippet": "            def errback(failure):\n                logger.error(\n                    msg,\n                    exc_info=failure_to_exc_info(failure),\n                    extra={'spider': spider}\n                )",
        "begin_line": 301,
        "end_line": 306,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._close_all_spiders#339",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._close_all_spiders(self)",
        "snippet": "    def _close_all_spiders(self):\n        dfds = [self.close_spider(s, reason='shutdown') for s in self.open_spiders]\n        dlist = defer.DeferredList(dfds)\n        return dlist",
        "begin_line": 339,
        "end_line": 342,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.engine.ExecutionEngine._finish_stopping_engine#345",
        "src_path": "scrapy/core/engine.py",
        "class_name": "scrapy.core.engine.ExecutionEngine",
        "signature": "scrapy.core.engine.ExecutionEngine._finish_stopping_engine(self)",
        "snippet": "    def _finish_stopping_engine(self):\n        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n        self._closewait.callback(None)",
        "begin_line": 345,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.test.skip_if_no_boto#29",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.skip_if_no_boto()",
        "snippet": "def skip_if_no_boto():\n    try:\n        is_botocore()\n    except NotConfigured as e:\n        raise SkipTest(e)",
        "begin_line": 29,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.test.get_crawler#64",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_crawler(spidercls=None, settings_dict=None)",
        "snippet": "def get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(settings_dict)\n    return runner.create_crawler(spidercls or Spider)",
        "begin_line": 64,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017094017094017094,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.test.get_pythonpath#75",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_pythonpath()",
        "snippet": "def get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')",
        "begin_line": 75,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017739932588256165,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.test.get_testenv#81",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_testenv()",
        "snippet": "def get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env",
        "begin_line": 81,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017739932588256165,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.test.assert_samelines#89",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.assert_samelines(testcase, text1, text2, msg=None)",
        "snippet": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)",
        "begin_line": 89,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.__init__#18",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self._crawler = crawler\n        self._schemes = {}  # stores acceptable schemes on instancing\n        self._handlers = {}  # stores instanced handlers for schemes\n        self._notconfigured = {}  # remembers failed handlers\n        handlers = without_none_values(\n            crawler.settings.getwithbase('DOWNLOAD_HANDLERS'))\n        for scheme, clspath in six.iteritems(handlers):\n            self._schemes[scheme] = clspath\n            self._load_handler(scheme, skip_lazy=True)\n\n        crawler.signals.connect(self._close, signals.engine_stopped)",
        "begin_line": 18,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._get_handler#31",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._get_handler(self, scheme)",
        "snippet": "    def _get_handler(self, scheme):\n        \"\"\"Lazy-load the downloadhandler for a scheme\n        only on the first request for that scheme.\n        \"\"\"\n        if scheme in self._handlers:\n            return self._handlers[scheme]\n        if scheme in self._notconfigured:\n            return None\n        if scheme not in self._schemes:\n            self._notconfigured[scheme] = 'no handler available for that scheme'\n            return None\n\n        return self._load_handler(scheme)",
        "begin_line": 31,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._load_handler#45",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._load_handler(self, scheme, skip_lazy=False)",
        "snippet": "    def _load_handler(self, scheme, skip_lazy=False):\n        path = self._schemes[scheme]\n        try:\n            dhcls = load_object(path)\n            if skip_lazy and getattr(dhcls, 'lazy', True):\n                return None\n            dh = dhcls(self._crawler.settings)\n        except NotConfigured as ex:\n            self._notconfigured[scheme] = str(ex)\n            return None\n        except Exception as ex:\n            logger.error('Loading \"%(clspath)s\" for scheme \"%(scheme)s\"',\n                         {\"clspath\": path, \"scheme\": scheme},\n                         exc_info=True, extra={'crawler': self._crawler})\n            self._notconfigured[scheme] = str(ex)\n            return None\n        else:\n            self._handlers[scheme] = dh\n            return dh",
        "begin_line": 45,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.download_request#65",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        scheme = urlparse_cached(request).scheme\n        handler = self._get_handler(scheme)\n        if not handler:\n            raise NotSupported(\"Unsupported URL scheme '%s': %s\" %\n                               (scheme, self._notconfigured[scheme]))\n        return handler.download_request(request, spider)",
        "begin_line": 65,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._close#74",
        "src_path": "scrapy/core/downloader/handlers/__init__.py",
        "class_name": "scrapy.core.downloader.handlers.__init__.DownloadHandlers",
        "signature": "scrapy.core.downloader.handlers.__init__.DownloadHandlers._close(self, *_a, **_kw)",
        "snippet": "    def _close(self, *_a, **_kw):\n        for dh in self._handlers.values():\n            if hasattr(dh, 'close'):\n                yield dh.close()",
        "begin_line": 74,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020475020475020476,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw._isiterable#17",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw",
        "signature": "scrapy.core.spidermw._isiterable(possible_iterator)",
        "snippet": "def _isiterable(possible_iterator):\n    return hasattr(possible_iterator, '__iter__')",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002199736031676199,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager._get_mwlist_from_settings#26",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))",
        "begin_line": 26,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018925056775170325,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager._add_middleware#29",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n        if hasattr(mw, 'process_spider_input'):\n            self.methods['process_spider_input'].append(mw.process_spider_input)\n        if hasattr(mw, 'process_start_requests'):\n            self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n        self.methods['process_spider_output'].appendleft(getattr(mw, 'process_spider_output', None))\n        self.methods['process_spider_exception'].appendleft(getattr(mw, 'process_spider_exception', None))",
        "begin_line": 29,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response#38",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.scrape_response(self, scrape_func, response, request, spider)",
        "snippet": "    def scrape_response(self, scrape_func, response, request, spider):\n        fname = lambda f:'%s.%s' % (\n                six.get_method_self(f).__class__.__name__,\n                six.get_method_function(f).__name__)\n\n        def process_spider_input(response):\n            for method in self.methods['process_spider_input']:\n                try:\n                    result = method(response=response, spider=spider)\n                    if result is not None:\n                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}' \\\n                                             .format(fname(method), type(result)))\n                except _InvalidOutput:\n                    raise\n                except Exception:\n                    return scrape_func(Failure(), request, spider)\n            return scrape_func(response, request, spider)\n\n        def process_spider_exception(_failure, start_index=0):\n            exception = _failure.value\n            # don't handle _InvalidOutput exception\n            if isinstance(exception, _InvalidOutput):\n                return _failure\n            method_list = islice(self.methods['process_spider_exception'], start_index, None)\n            for method_index, method in enumerate(method_list, start=start_index):\n                if method is None:\n                    continue\n                result = method(response=response, exception=exception, spider=spider)\n                if _isiterable(result):\n                    # stop exception handling by handing control over to the\n                    # process_spider_output chain if an iterable has been returned\n                    return process_spider_output(result, method_index+1)\n                elif result is None:\n                    continue\n                else:\n                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n                                         .format(fname(method), type(result)))\n            return _failure\n\n        def process_spider_output(result, start_index=0):\n            # items in this iterable do not need to go through the process_spider_output\n            # chain, they went through it already from the process_spider_exception method\n            recovered = MutableChain()\n\n            def evaluate_iterable(iterable, index):\n                try:\n                    for r in iterable:\n                        yield r\n                except Exception as ex:\n                    exception_result = process_spider_exception(Failure(ex), index+1)\n                    if isinstance(exception_result, Failure):\n                        raise\n                    recovered.extend(exception_result)\n\n            method_list = islice(self.methods['process_spider_output'], start_index, None)\n            for method_index, method in enumerate(method_list, start=start_index):\n                if method is None:\n                    continue\n                # the following might fail directly if the output value is not a generator\n                try:\n                    result = method(response=response, result=result, spider=spider)\n                except Exception as ex:\n                    exception_result = process_spider_exception(Failure(ex), method_index+1)\n                    if isinstance(exception_result, Failure):\n                        raise\n                    return exception_result\n                if _isiterable(result):\n                    result = evaluate_iterable(result, method_index)\n                else:\n                    raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n                                         .format(fname(method), type(result)))\n\n            return chain(result, recovered)\n\n        dfd = mustbe_deferred(process_spider_input, response)\n        dfd.addCallbacks(callback=process_spider_output, errback=process_spider_exception)\n        return dfd",
        "begin_line": 38,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_input#43",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_input(response)",
        "snippet": "        def process_spider_input(response):\n            for method in self.methods['process_spider_input']:\n                try:\n                    result = method(response=response, spider=spider)\n                    if result is not None:\n                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}' \\\n                                             .format(fname(method), type(result)))\n                except _InvalidOutput:\n                    raise\n                except Exception:\n                    return scrape_func(Failure(), request, spider)\n            return scrape_func(response, request, spider)",
        "begin_line": 43,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_exception#56",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_exception(_failure, start_index=0)",
        "snippet": "        def process_spider_exception(_failure, start_index=0):\n            exception = _failure.value\n            # don't handle _InvalidOutput exception\n            if isinstance(exception, _InvalidOutput):\n                return _failure\n            method_list = islice(self.methods['process_spider_exception'], start_index, None)\n            for method_index, method in enumerate(method_list, start=start_index):\n                if method is None:\n                    continue\n                result = method(response=response, exception=exception, spider=spider)\n                if _isiterable(result):\n                    # stop exception handling by handing control over to the\n                    # process_spider_output chain if an iterable has been returned\n                    return process_spider_output(result, method_index+1)\n                elif result is None:\n                    continue\n                else:\n                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n                                         .format(fname(method), type(result)))\n            return _failure",
        "begin_line": 56,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_output#77",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_spider_output(result, start_index=0)",
        "snippet": "        def process_spider_output(result, start_index=0):\n            # items in this iterable do not need to go through the process_spider_output\n            # chain, they went through it already from the process_spider_exception method\n            recovered = MutableChain()\n\n            def evaluate_iterable(iterable, index):\n                try:\n                    for r in iterable:\n                        yield r\n                except Exception as ex:\n                    exception_result = process_spider_exception(Failure(ex), index+1)\n                    if isinstance(exception_result, Failure):\n                        raise\n                    recovered.extend(exception_result)\n\n            method_list = islice(self.methods['process_spider_output'], start_index, None)\n            for method_index, method in enumerate(method_list, start=start_index):\n                if method is None:\n                    continue\n                # the following might fail directly if the output value is not a generator\n                try:\n                    result = method(response=response, result=result, spider=spider)\n                except Exception as ex:\n                    exception_result = process_spider_exception(Failure(ex), method_index+1)\n                    if isinstance(exception_result, Failure):\n                        raise\n                    return exception_result\n                if _isiterable(result):\n                    result = evaluate_iterable(result, method_index)\n                else:\n                    raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n                                         .format(fname(method), type(result)))\n\n            return chain(result, recovered)",
        "begin_line": 77,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.evaluate_iterable#82",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.evaluate_iterable(iterable, index)",
        "snippet": "            def evaluate_iterable(iterable, index):\n                try:\n                    for r in iterable:\n                        yield r\n                except Exception as ex:\n                    exception_result = process_spider_exception(Failure(ex), index+1)\n                    if isinstance(exception_result, Failure):\n                        raise\n                    recovered.extend(exception_result)",
        "begin_line": 82,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.spidermw.SpiderMiddlewareManager.process_start_requests#116",
        "src_path": "scrapy/core/spidermw.py",
        "class_name": "scrapy.core.spidermw.SpiderMiddlewareManager",
        "signature": "scrapy.core.spidermw.SpiderMiddlewareManager.process_start_requests(self, start_requests, spider)",
        "snippet": "    def process_start_requests(self, start_requests, spider):\n        return self._process_chain('process_start_requests', start_requests, spider)",
        "begin_line": 116,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore.__init__#45",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore.__init__(self, basedir)",
        "snippet": "    def __init__(self, basedir):\n        if '://' in basedir:\n            basedir = basedir.split('://', 1)[1]\n        self.basedir = basedir\n        self._mkdir(self.basedir)\n        self.created_directories = defaultdict(set)",
        "begin_line": 45,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002535496957403651,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore.persist_file#52",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore.persist_file(self, path, buf, info, meta=None, headers=None)",
        "snippet": "    def persist_file(self, path, buf, info, meta=None, headers=None):\n        absolute_path = self._get_filesystem_path(path)\n        self._mkdir(os.path.dirname(absolute_path), info)\n        with open(absolute_path, 'wb') as f:\n            f.write(buf.getvalue())",
        "begin_line": 52,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore.stat_file#58",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore.stat_file(self, path, info)",
        "snippet": "    def stat_file(self, path, info):\n        absolute_path = self._get_filesystem_path(path)\n        try:\n            last_modified = os.path.getmtime(absolute_path)\n        except os.error:\n            return {}\n\n        with open(absolute_path, 'rb') as f:\n            checksum = md5sum(f)\n\n        return {'last_modified': last_modified, 'checksum': checksum}",
        "begin_line": 58,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore._get_filesystem_path#70",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore._get_filesystem_path(self, path)",
        "snippet": "    def _get_filesystem_path(self, path):\n        path_comps = path.split('/')\n        return os.path.join(self.basedir, *path_comps)",
        "begin_line": 70,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FSFilesStore._mkdir#74",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FSFilesStore",
        "signature": "scrapy.pipelines.files.FSFilesStore._mkdir(self, dirname, domain=None)",
        "snippet": "    def _mkdir(self, dirname, domain=None):\n        seen = self.created_directories[domain] if domain else set()\n        if dirname not in seen:\n            if not os.path.exists(dirname):\n                os.makedirs(dirname)\n            seen.add(dirname)",
        "begin_line": 74,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.S3FilesStore.__init__#96",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.S3FilesStore",
        "signature": "scrapy.pipelines.files.S3FilesStore.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.is_botocore = is_botocore()\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3',\n                aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY,\n                endpoint_url=self.AWS_ENDPOINT_URL,\n                region_name=self.AWS_REGION_NAME,\n                use_ssl=self.AWS_USE_SSL,\n                verify=self.AWS_VERIFY\n            )\n        else:\n            from boto.s3.connection import S3Connection\n            self.S3Connection = S3Connection\n        assert uri.startswith('s3://')\n        self.bucket, self.prefix = uri[5:].split('/', 1)",
        "begin_line": 96,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.__init__#296",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.__init__(self, store_uri, download_func=None, settings=None)",
        "snippet": "    def __init__(self, store_uri, download_func=None, settings=None):\n        if not store_uri:\n            raise NotConfigured\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        cls_name = \"FilesPipeline\"\n        self.store = self._get_store(store_uri)\n        resolve = functools.partial(self._key_for_pipe,\n                                    base_class_name=cls_name,\n                                    settings=settings)\n        self.expires = settings.getint(\n            resolve('FILES_EXPIRES'), self.EXPIRES\n        )\n        if not hasattr(self, \"FILES_URLS_FIELD\"):\n            self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n        if not hasattr(self, \"FILES_RESULT_FIELD\"):\n            self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n        self.files_urls_field = settings.get(\n            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD\n        )\n        self.files_result_field = settings.get(\n            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD\n        )\n\n        super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)",
        "begin_line": 296,
        "end_line": 322,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.from_settings#325",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        s3store = cls.STORE_SCHEMES['s3']\n        s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n        s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n        s3store.AWS_ENDPOINT_URL = settings['AWS_ENDPOINT_URL']\n        s3store.AWS_REGION_NAME = settings['AWS_REGION_NAME']\n        s3store.AWS_USE_SSL = settings['AWS_USE_SSL']\n        s3store.AWS_VERIFY = settings['AWS_VERIFY']\n        s3store.POLICY = settings['FILES_STORE_S3_ACL']\n\n        gcs_store = cls.STORE_SCHEMES['gs']\n        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n\n        store_uri = settings['FILES_STORE']\n        return cls(store_uri, settings=settings)",
        "begin_line": 325,
        "end_line": 340,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline._get_store#342",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline._get_store(self, uri)",
        "snippet": "    def _get_store(self, uri):\n        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n            scheme = 'file'\n        else:\n            scheme = urlparse(uri).scheme\n        store_cls = self.STORE_SCHEMES[scheme]\n        return store_cls(uri)",
        "begin_line": 342,
        "end_line": 348,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00031735956839098697,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.media_to_download#350",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum}\n\n        path = self.file_path(request, info=info)\n        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n        dfd.addCallbacks(_onsuccess, lambda _: None)\n        dfd.addErrback(\n            lambda f:\n            logger.error(self.__class__.__name__ + '.store.stat_file',\n                         exc_info=failure_to_exc_info(f),\n                         extra={'spider': info.spider})\n        )\n        return dfd",
        "begin_line": 350,
        "end_line": 386,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0003362474781439139,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline._onsuccess#351",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline._onsuccess(result)",
        "snippet": "        def _onsuccess(result):\n            if not result:\n                return  # returning None force download\n\n            last_modified = result.get('last_modified', None)\n            if not last_modified:\n                return  # returning None force download\n\n            age_seconds = time.time() - last_modified\n            age_days = age_seconds / 60 / 60 / 24\n            if age_days > self.expires:\n                return  # returning None force download\n\n            referer = referer_str(request)\n            logger.debug(\n                'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                'referred in <%(referer)s>',\n                {'medianame': self.MEDIA_NAME, 'request': request,\n                 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            self.inc_stats(info.spider, 'uptodate')\n\n            checksum = result.get('checksum', None)\n            return {'url': request.url, 'path': path, 'checksum': checksum}",
        "begin_line": 351,
        "end_line": 375,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.media_downloaded#401",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        referer = referer_str(request)\n\n        if response.status != 200:\n            logger.warning(\n                'File (code: %(status)s): Error downloading file from '\n                '%(request)s referred in <%(referer)s>',\n                {'status': response.status,\n                 'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('download-error')\n\n        if not response.body:\n            logger.warning(\n                'File (empty-content): Empty file from %(request)s referred '\n                'in <%(referer)s>: no-content',\n                {'request': request, 'referer': referer},\n                extra={'spider': info.spider}\n            )\n            raise FileException('empty-content')\n\n        status = 'cached' if 'cached' in response.flags else 'downloaded'\n        logger.debug(\n            'File (%(status)s): Downloaded file from %(request)s referred in '\n            '<%(referer)s>',\n            {'status': status, 'request': request, 'referer': referer},\n            extra={'spider': info.spider}\n        )\n        self.inc_stats(info.spider, status)\n\n        try:\n            path = self.file_path(request, response=response, info=info)\n            checksum = self.file_downloaded(response, request, info)\n        except FileException as exc:\n            logger.warning(\n                'File (error): Error processing file from %(request)s '\n                'referred in <%(referer)s>: %(errormsg)s',\n                {'request': request, 'referer': referer, 'errormsg': str(exc)},\n                extra={'spider': info.spider}, exc_info=True\n            )\n            raise\n        except Exception as exc:\n            logger.error(\n                'File (unknown-error): Error processing file from %(request)s '\n                'referred in <%(referer)s>',\n                {'request': request, 'referer': referer},\n                exc_info=True, extra={'spider': info.spider}\n            )\n            raise FileException(str(exc))\n\n        return {'url': request.url, 'path': path, 'checksum': checksum}",
        "begin_line": 401,
        "end_line": 452,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.inc_stats#454",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.inc_stats(self, spider, status)",
        "snippet": "    def inc_stats(self, spider, status):\n        spider.crawler.stats.inc_value('file_count', spider=spider)\n        spider.crawler.stats.inc_value('file_status_count/%s' % status, spider=spider)",
        "begin_line": 454,
        "end_line": 456,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.get_media_requests#459",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        return [Request(x) for x in item.get(self.files_urls_field, [])]",
        "begin_line": 459,
        "end_line": 460,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.file_downloaded#462",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.file_downloaded(self, response, request, info)",
        "snippet": "    def file_downloaded(self, response, request, info):\n        path = self.file_path(request, response=response, info=info)\n        buf = BytesIO(response.body)\n        checksum = md5sum(buf)\n        buf.seek(0)\n        self.store.persist_file(path, buf, info)\n        return checksum",
        "begin_line": 462,
        "end_line": 468,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.item_completed#470",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        if isinstance(item, dict) or self.files_result_field in item.fields:\n            item[self.files_result_field] = [x for ok, x in results if ok]\n        return item",
        "begin_line": 470,
        "end_line": 473,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.pipelines.files.FilesPipeline.file_path#475",
        "src_path": "scrapy/pipelines/files.py",
        "class_name": "scrapy.pipelines.files.FilesPipeline",
        "signature": "scrapy.pipelines.files.FilesPipeline.file_path(self, request, response=None, info=None)",
        "snippet": "    def file_path(self, request, response=None, info=None):\n        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n        media_ext = os.path.splitext(request.url)[1]\n        # Handles empty and wild extensions by trying to guess the\n        # mime type then extension or default to empty string otherwise\n        if media_ext not in mimetypes.types_map:\n            media_ext = ''\n            media_type = mimetypes.guess_type(request.url)[0]\n            if media_type:\n                media_ext = mimetypes.guess_extension(media_type)\n        return 'full/%s%s' % (media_guid, media_ext)",
        "begin_line": 475,
        "end_line": 485,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.failure_to_exc_info#20",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.failure_to_exc_info(failure)",
        "snippet": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000273224043715847,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.TopLevelFormatter.__init__#38",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.TopLevelFormatter",
        "signature": "scrapy.utils.log.TopLevelFormatter.__init__(self, loggers=None)",
        "snippet": "    def __init__(self, loggers=None):\n        self.loggers = loggers or []",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.TopLevelFormatter.filter#41",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.TopLevelFormatter",
        "signature": "scrapy.utils.log.TopLevelFormatter.filter(self, record)",
        "snippet": "    def filter(self, record):\n        if any(record.name.startswith(l + '.') for l in self.loggers):\n            record.name = record.name.split('.', 1)[0]\n        return True",
        "begin_line": 41,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.configure_logging#61",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.configure_logging(settings=None, install_root_handler=True)",
        "snippet": "def configure_logging(settings=None, install_root_handler=True):\n    \"\"\"\n    Initialize logging defaults for Scrapy.\n\n    :param settings: settings used to create and configure a handler for the\n        root logger (default: None).\n    :type settings: dict, :class:`~scrapy.settings.Settings` object or ``None``\n\n    :param install_root_handler: whether to install root logging handler\n        (default: True)\n    :type install_root_handler: bool\n\n    This function does:\n\n    - Route warnings and twisted logging through Python standard logging\n    - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n    - Route stdout to log if LOG_STDOUT setting is True\n\n    When ``install_root_handler`` is True (default), this function also\n    creates a handler for the root logger according to given settings\n    (see :ref:`topics-logging-settings`). You can override default options\n    using ``settings`` argument. When ``settings`` is empty or None, defaults\n    are used.\n    \"\"\"\n    if not sys.warnoptions:\n        # Route warnings through python logging\n        logging.captureWarnings(True)\n\n    observer = twisted_log.PythonLoggingObserver('twisted')\n    observer.start()\n\n    dictConfig(DEFAULT_LOGGING)\n\n    if isinstance(settings, dict) or settings is None:\n        settings = Settings(settings)\n\n    if settings.getbool('LOG_STDOUT'):\n        sys.stdout = StreamLogger(logging.getLogger('stdout'))\n\n    if install_root_handler:\n        install_scrapy_root_handler(settings)",
        "begin_line": 61,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.install_scrapy_root_handler#104",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.install_scrapy_root_handler(settings)",
        "snippet": "def install_scrapy_root_handler(settings):\n    global _scrapy_root_handler\n\n    if (_scrapy_root_handler is not None\n            and _scrapy_root_handler in logging.root.handlers):\n        logging.root.removeHandler(_scrapy_root_handler)\n    logging.root.setLevel(logging.NOTSET)\n    _scrapy_root_handler = _get_handler(settings)\n    logging.root.addHandler(_scrapy_root_handler)",
        "begin_line": 104,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016815200941651252,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.get_scrapy_root_handler#115",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.get_scrapy_root_handler()",
        "snippet": "def get_scrapy_root_handler():\n    return _scrapy_root_handler",
        "begin_line": 115,
        "end_line": 116,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log._get_handler#122",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log._get_handler(settings)",
        "snippet": "def _get_handler(settings):\n    \"\"\" Return a log handler object according to settings \"\"\"\n    filename = settings.get('LOG_FILE')\n    if filename:\n        encoding = settings.get('LOG_ENCODING')\n        handler = logging.FileHandler(filename, encoding=encoding)\n    elif settings.getbool('LOG_ENABLED'):\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.NullHandler()\n\n    formatter = logging.Formatter(\n        fmt=settings.get('LOG_FORMAT'),\n        datefmt=settings.get('LOG_DATEFORMAT')\n    )\n    handler.setFormatter(formatter)\n    handler.setLevel(settings.get('LOG_LEVEL'))\n    if settings.getbool('LOG_SHORT_NAMES'):\n        handler.addFilter(TopLevelFormatter(['scrapy']))\n    return handler",
        "begin_line": 122,
        "end_line": 141,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.log_scrapy_info#144",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.log_scrapy_info(settings)",
        "snippet": "def log_scrapy_info(settings):\n    logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n    logger.info(\"Versions: %(versions)s\",\n                {'versions': \", \".join(\"%s %s\" % (name, version)\n                    for name, version in scrapy_components_versions()\n                    if name != \"Scrapy\")})",
        "begin_line": 144,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.StreamLogger.__init__#159",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.StreamLogger",
        "signature": "scrapy.utils.log.StreamLogger.__init__(self, logger, log_level=logging.INFO)",
        "snippet": "    def __init__(self, logger, log_level=logging.INFO):\n        self.logger = logger\n        self.log_level = log_level\n        self.linebuf = ''",
        "begin_line": 159,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.StreamLogger.write#164",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.StreamLogger",
        "signature": "scrapy.utils.log.StreamLogger.write(self, buf)",
        "snippet": "    def write(self, buf):\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.log_level, line.rstrip())",
        "begin_line": 164,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.LogCounterHandler.__init__#176",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.LogCounterHandler",
        "signature": "scrapy.utils.log.LogCounterHandler.__init__(self, crawler, *args, **kwargs)",
        "snippet": "    def __init__(self, crawler, *args, **kwargs):\n        super(LogCounterHandler, self).__init__(*args, **kwargs)\n        self.crawler = crawler",
        "begin_line": 176,
        "end_line": 178,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.LogCounterHandler.emit#180",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.LogCounterHandler",
        "signature": "scrapy.utils.log.LogCounterHandler.emit(self, record)",
        "snippet": "    def emit(self, record):\n        sname = 'log_count/{}'.format(record.levelname)\n        self.crawler.stats.inc_value(sname)",
        "begin_line": 180,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00015895724050230488,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.log.logformatter_adapter#185",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.logformatter_adapter(logkws)",
        "snippet": "def logformatter_adapter(logkws):\n    \"\"\"\n    Helper that takes the dictionary output from the methods in LogFormatter\n    and adapts it into a tuple of positional arguments for logger.log calls,\n    handling backward compatibility as well.\n    \"\"\"\n    if not {'level', 'msg', 'args'} <= set(logkws):\n        warnings.warn('Missing keys in LogFormatter method',\n                      ScrapyDeprecationWarning)\n\n    if 'format' in logkws:\n        warnings.warn('`format` key in LogFormatter methods has been '\n                      'deprecated, use `msg` instead',\n                      ScrapyDeprecationWarning)\n\n    level = logkws.get('level', logging.INFO)\n    message = logkws.get('format', logkws.get('msg'))\n    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n    # play well in logger.log calls\n    args = logkws if not logkws.get('args') else logkws['args']\n\n    return (level, message, args)",
        "begin_line": 185,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002255299954894001,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.__init__#35",
        "src_path": "scrapy/core/downloader/handlers/s3.py",
        "class_name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.__init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, httpdownloadhandler=HTTPDownloadHandler, **kw)",
        "snippet": "    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n            httpdownloadhandler=HTTPDownloadHandler, **kw):\n\n        if not aws_access_key_id:\n            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n        if not aws_secret_access_key:\n            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n\n        # If no credentials could be found anywhere,\n        # consider this an anonymous connection request by default;\n        # unless 'anon' was set explicitly (True/False).\n        anon = kw.get('anon')\n        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n            kw['anon'] = True\n        self.anon = kw.get('anon')\n\n        self._signer = None\n        if is_botocore():\n            import botocore.auth\n            import botocore.credentials\n            kw.pop('anon', None)\n            if kw:\n                raise TypeError('Unexpected keyword arguments: %s' % kw)\n            if not self.anon:\n                SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n                self._signer = SignerCls(botocore.credentials.Credentials(\n                    aws_access_key_id, aws_secret_access_key))\n        else:\n            _S3Connection = _get_boto_connection()\n            try:\n                self.conn = _S3Connection(\n                    aws_access_key_id, aws_secret_access_key, **kw)\n            except Exception as ex:\n                raise NotConfigured(str(ex))\n\n        self._download_http = httpdownloadhandler(settings).download_request",
        "begin_line": 35,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.download_request#72",
        "src_path": "scrapy/core/downloader/handlers/s3.py",
        "class_name": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
        "signature": "scrapy.core.downloader.handlers.s3.S3DownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        p = urlparse_cached(request)\n        scheme = 'https' if request.meta.get('is_secure') else 'http'\n        bucket = p.hostname\n        path = p.path + '?' + p.query if p.query else p.path\n        url = '%s://%s.s3.amazonaws.com%s' % (scheme, bucket, path)\n        if self.anon:\n            request = request.replace(url=url)\n        elif self._signer is not None:\n            import botocore.awsrequest\n            awsrequest = botocore.awsrequest.AWSRequest(\n                method=request.method,\n                url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),\n                headers=request.headers.to_unicode_dict(),\n                data=request.body)\n            self._signer.add_auth(awsrequest)\n            request = request.replace(\n                url=url, headers=awsrequest.headers.items())\n        else:\n            signed_headers = self.conn.make_request(\n                    method=request.method,\n                    bucket=bucket,\n                    key=unquote(p.path),\n                    query_args=unquote(p.query),\n                    headers=request.headers,\n                    data=request.body)\n            request = request.replace(url=url, headers=signed_headers)\n        return self._download_http(request, spider)",
        "begin_line": 72,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_any_domain#19",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_any_domain(url, domains)",
        "snippet": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)",
        "begin_line": 19,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_spider#28",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_spider(url, spider)",
        "snippet": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url,\n        [spider.name] + list(getattr(spider, 'allowed_domains', [])))",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.url_has_any_extension#34",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_has_any_extension(url, extensions)",
        "snippet": "def url_has_any_extension(url, extensions):\n    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022972662531587412,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.parse_url#38",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.parse_url(url, encoding=None)",
        "snippet": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_unicode(url, encoding))",
        "begin_line": 38,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.escape_ajax#47",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.escape_ajax(url)",
        "snippet": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])",
        "begin_line": 47,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.043478260869565216,
            "pseudo_dstar_susp": 0.3333333333333333,
            "pseudo_tarantula_susp": 0.03333333333333333,
            "pseudo_op2_susp": 0.3333333333333333,
            "pseudo_barinel_susp": 0.03333333333333333
        }
    },
    {
        "name": "scrapy.utils.url.add_http_if_no_scheme#76",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.add_http_if_no_scheme(url)",
        "snippet": "def add_http_if_no_scheme(url):\n    \"\"\"Add http as the default scheme if it is missing from the url.\"\"\"\n    match = re.match(r\"^\\w+://\", url, flags=re.I)\n    if not match:\n        parts = urlparse(url)\n        scheme = \"http:\" if parts.netloc else \"http://\"\n        url = scheme + url\n\n    return url",
        "begin_line": 76,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002617801047120419,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.guess_scheme#87",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.guess_scheme(url)",
        "snippet": "def guess_scheme(url):\n    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or http:// otherwise.\"\"\"\n    parts = urlparse(url)\n    if parts.scheme:\n        return url\n    # Note: this does not match Windows filepath\n    if re.match(r'''^                   # start with...\n                    (\n                        \\.              # ...a single dot,\n                        (\n                            \\. | [^/\\.]+  # optionally followed by\n                        )?                # either a second dot or some characters\n                    )?      # optional match of \".\", \"..\" or \".blabla\"\n                    /       # at least one \"/\" for a file path,\n                    .       # and something after the \"/\"\n                    ''', parts.path, flags=re.VERBOSE):\n        return any_to_uri(url)\n    else:\n        return add_http_if_no_scheme(url)",
        "begin_line": 87,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.url.strip_url#108",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True)",
        "snippet": "def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):\n\n    \"\"\"Strip URL string from some of its components:\n\n    - ``strip_credentials`` removes \"user:password@\"\n    - ``strip_default_port`` removes \":80\" (resp. \":443\", \":21\")\n      from http:// (resp. https://, ftp://) URLs\n    - ``origin_only`` replaces path component with \"/\", also dropping\n      query and fragment components ; it also strips credentials\n    - ``strip_fragment`` drops any #fragment component\n    \"\"\"\n\n    parsed_url = urlparse(url)\n    netloc = parsed_url.netloc\n    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):\n        netloc = netloc.split('@')[-1]\n    if strip_default_port and parsed_url.port:\n        if (parsed_url.scheme, parsed_url.port) in (('http', 80),\n                                                    ('https', 443),\n                                                    ('ftp', 21)):\n            netloc = netloc.replace(':{p.port}'.format(p=parsed_url), '')\n    return urlunparse((\n        parsed_url.scheme,\n        netloc,\n        '/' if origin_only else parsed_url.path,\n        '' if origin_only else parsed_url.params,\n        '' if origin_only else parsed_url.query,\n        '' if strip_fragment else parsed_url.fragment\n    ))",
        "begin_line": 108,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002765486725663717,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler.__init__#30",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.__init__(self, spidercls, settings=None)",
        "snippet": "    def __init__(self, spidercls, settings=None):\n        if isinstance(spidercls, Spider):\n            raise ValueError(\n                'The spidercls argument must be a class, not an object')\n\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls = spidercls\n        self.settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n\n        self.signals = SignalManager(self)\n        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n\n        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n        logging.root.addHandler(handler)\n\n        d = dict(overridden_settings(self.settings))\n        logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n\n        if get_scrapy_root_handler() is not None:\n            # scrapy root handler already installed: update it with new settings\n            install_scrapy_root_handler(self.settings)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving __init__ scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n        self.logformatter = lf_cls.from_crawler(self)\n        self.extensions = ExtensionManager.from_crawler(self)\n\n        self.settings.freeze()\n        self.crawling = False\n        self.spider = None\n        self.engine = None",
        "begin_line": 30,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler.spiders#69",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.spiders(self)",
        "snippet": "    def spiders(self):\n        if not hasattr(self, '_spiders'):\n            warnings.warn(\"Crawler.spiders is deprecated, use \"\n                          \"CrawlerRunner.spider_loader or instantiate \"\n                          \"scrapy.spiderloader.SpiderLoader with your \"\n                          \"settings.\",\n                          category=ScrapyDeprecationWarning, stacklevel=2)\n            self._spiders = _get_spider_loader(self.settings.frozencopy())\n        return self._spiders",
        "begin_line": 69,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler.crawl#80",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.crawl(self, *args, **kwargs)",
        "snippet": "    def crawl(self, *args, **kwargs):\n        assert not self.crawling, \"Crawling already taking place\"\n        self.crawling = True\n\n        try:\n            self.spider = self._create_spider(*args, **kwargs)\n            self.engine = self._create_engine()\n            start_requests = iter(self.spider.start_requests())\n            yield self.engine.open_spider(self.spider, start_requests)\n            yield defer.maybeDeferred(self.engine.start)\n        except Exception:\n            # In Python 2 reraising an exception after yield discards\n            # the original traceback (see https://bugs.python.org/issue7563),\n            # so sys.exc_info() workaround is used.\n            # This workaround also works in Python 3, but it is not needed,\n            # and it is slower, so in Python 3 we use native `raise`.\n            if six.PY2:\n                exc_info = sys.exc_info()\n\n            self.crawling = False\n            if self.engine is not None:\n                yield self.engine.close()\n\n            if six.PY2:\n                six.reraise(*exc_info)\n            raise",
        "begin_line": 80,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler._create_spider#107",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler._create_spider(self, *args, **kwargs)",
        "snippet": "    def _create_spider(self, *args, **kwargs):\n        return self.spidercls.from_crawler(self, *args, **kwargs)",
        "begin_line": 107,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017140898183064793,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler._create_engine#110",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler._create_engine(self)",
        "snippet": "    def _create_engine(self):\n        return ExecutionEngine(self, lambda _: self.stop())",
        "begin_line": 110,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00020275750202757503,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.Crawler.stop#114",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.stop(self)",
        "snippet": "    def stop(self):\n        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n        fired when the crawler is stopped.\"\"\"\n        if self.crawling:\n            self.crawling = False\n            yield defer.maybeDeferred(self.engine.stop)",
        "begin_line": 114,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002069108214359611,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.__init__#141",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.__init__(self, settings=None)",
        "snippet": "    def __init__(self, settings=None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        self.settings = settings\n        self.spider_loader = _get_spider_loader(settings)\n        self._crawlers = set()\n        self._active = set()\n        self.bootstrap_failed = False",
        "begin_line": 141,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016784155756965425,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.spiders#151",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.spiders(self)",
        "snippet": "    def spiders(self):\n        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n                      \"CrawlerRunner.spider_loader.\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        return self.spider_loader",
        "begin_line": 151,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.crawl#157",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.crawl(self, crawler_or_spidercls, *args, **kwargs)",
        "snippet": "    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n        \"\"\"\n        Run a crawler with the provided arguments.\n\n        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n        keeping track of it so it can be stopped later.\n\n        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n        instance, this method will try to create one using this parameter as\n        the spider class given to it.\n\n        Returns a deferred that is fired when the crawling is finished.\n\n        :param crawler_or_spidercls: already created crawler, or a spider class\n            or spider's name inside the project to create it\n        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n            :class:`~scrapy.spiders.Spider` subclass or string\n\n        :param list args: arguments to initialize the spider\n\n        :param dict kwargs: keyword arguments to initialize the spider\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        crawler = self.create_crawler(crawler_or_spidercls)\n        return self._crawl(crawler, *args, **kwargs)",
        "begin_line": 157,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._crawl#186",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._crawl(self, crawler, *args, **kwargs)",
        "snippet": "    def _crawl(self, crawler, *args, **kwargs):\n        self.crawlers.add(crawler)\n        d = crawler.crawl(*args, **kwargs)\n        self._active.add(d)\n\n        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result\n\n        return d.addBoth(_done)",
        "begin_line": 186,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._done#191",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._done(result)",
        "snippet": "        def _done(result):\n            self.crawlers.discard(crawler)\n            self._active.discard(d)\n            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n            return result",
        "begin_line": 191,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002871088142405972,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.create_crawler#199",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.create_crawler(self, crawler_or_spidercls)",
        "snippet": "    def create_crawler(self, crawler_or_spidercls):\n        \"\"\"\n        Return a :class:`~scrapy.crawler.Crawler` object.\n\n        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n          is constructed for it.\n        * If ``crawler_or_spidercls`` is a string, this function finds\n          a spider with this name in a Scrapy project (using spider loader),\n          then creates a Crawler instance for it.\n        \"\"\"\n        if isinstance(crawler_or_spidercls, Spider):\n            raise ValueError(\n                'The crawler_or_spidercls argument cannot be a spider object, '\n                'it must be a spider class (or a Crawler object)')\n        if isinstance(crawler_or_spidercls, Crawler):\n            return crawler_or_spidercls\n        return self._create_crawler(crawler_or_spidercls)",
        "begin_line": 199,
        "end_line": 216,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._create_crawler#218",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._create_crawler(self, spidercls)",
        "snippet": "    def _create_crawler(self, spidercls):\n        if isinstance(spidercls, six.string_types):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)",
        "begin_line": 218,
        "end_line": 221,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.join#232",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.join(self)",
        "snippet": "    def join(self):\n        \"\"\"\n        join()\n\n        Returns a deferred that is fired when all managed :attr:`crawlers` have\n        completed their executions.\n        \"\"\"\n        while self._active:\n            yield defer.DeferredList(self._active)",
        "begin_line": 232,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler.CrawlerProcess.__init__#266",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerProcess",
        "signature": "scrapy.crawler.CrawlerProcess.__init__(self, settings=None, install_root_handler=True)",
        "snippet": "    def __init__(self, settings=None, install_root_handler=True):\n        super(CrawlerProcess, self).__init__(settings)\n        install_shutdown_handlers(self._signal_shutdown)\n        configure_logging(self.settings, install_root_handler)\n        log_scrapy_info(self.settings)",
        "begin_line": 266,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.crawler._get_spider_loader#334",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler",
        "signature": "scrapy.crawler._get_spider_loader(settings)",
        "snippet": "def _get_spider_loader(settings):\n    \"\"\" Get SpiderLoader instance from settings \"\"\"\n    cls_path = settings.get('SPIDER_LOADER_CLASS')\n    loader_cls = load_object(cls_path)\n    try:\n        verifyClass(ISpiderLoader, loader_cls)\n    except DoesNotImplement:\n        warnings.warn(\n            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n            'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n            'Please add all missing methods to avoid unexpected runtime errors.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n    return loader_cls.from_settings(settings.frozencopy())",
        "begin_line": 334,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016666666666666666,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__init__#25",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__init__(self, name=None, **kwargs)",
        "snippet": "    def __init__(self, name=None, **kwargs):\n        if name is not None:\n            self.name = name\n        elif not getattr(self, 'name', None):\n            raise ValueError(\"%s must have a name\" % type(self).__name__)\n        self.__dict__.update(kwargs)\n        if not hasattr(self, 'start_urls'):\n            self.start_urls = []",
        "begin_line": 25,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.logger#35",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.logger(self)",
        "snippet": "    def logger(self):\n        logger = logging.getLogger(self.name)\n        return logging.LoggerAdapter(logger, {'spider': self})",
        "begin_line": 35,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000249500998003992,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.log#39",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.log(self, message, level=logging.DEBUG, **kw)",
        "snippet": "    def log(self, message, level=logging.DEBUG, **kw):\n        \"\"\"Log the given message at the given log level\n\n        This helper wraps a log call to the logger within the spider, but you\n        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n        Python logger too.\n        \"\"\"\n        self.logger.log(level, message, **kw)",
        "begin_line": 39,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.from_crawler#49",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.from_crawler(cls, crawler, *args, **kwargs)",
        "snippet": "    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = cls(*args, **kwargs)\n        spider._set_crawler(crawler)\n        return spider",
        "begin_line": 49,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017032873445750298,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider._set_crawler#54",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider._set_crawler(self, crawler)",
        "snippet": "    def _set_crawler(self, crawler):\n        self.crawler = crawler\n        self.settings = crawler.settings\n        crawler.signals.connect(self.close, signals.spider_closed)",
        "begin_line": 54,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017032873445750298,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.start_requests#59",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.start_requests(self)",
        "snippet": "    def start_requests(self):\n        cls = self.__class__\n        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n            warnings.warn(\n                \"Spider.make_requests_from_url method is deprecated; it \"\n                \"won't be called in future Scrapy releases. Please \"\n                \"override Spider.start_requests method instead (see %s.%s).\" % (\n                    cls.__module__, cls.__name__\n                ),\n            )\n            for url in self.start_urls:\n                yield self.make_requests_from_url(url)\n        else:\n            for url in self.start_urls:\n                yield Request(url, dont_filter=True)",
        "begin_line": 59,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.parse#79",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.parse(self, response)",
        "snippet": "    def parse(self, response):\n        raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.update_settings#83",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.update_settings(cls, settings)",
        "snippet": "    def update_settings(cls, settings):\n        settings.setdict(cls.custom_settings or {}, priority='spider')",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016035920461834508,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.handles_request#87",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.handles_request(cls, request)",
        "snippet": "    def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)",
        "begin_line": 87,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.close#91",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.close(spider, reason)",
        "snippet": "    def close(spider, reason):\n        closed = getattr(spider, 'closed', None)\n        if callable(closed):\n            return closed(reason)",
        "begin_line": 91,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023730422401518748,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__str__#96",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))",
        "begin_line": 96,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.__init__#23",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop('formdata', None)\n        if formdata and kwargs.get('method') is None:\n            kwargs['method'] = 'POST'\n\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            querystr = _urlencode(items, self.encoding)\n            if self.method == 'POST':\n                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                self._set_body(querystr)\n            else:\n                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)",
        "begin_line": 23,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014705882352941176,
            "pseudo_dstar_susp": 0.012345679012345678,
            "pseudo_tarantula_susp": 0.041666666666666664,
            "pseudo_op2_susp": 0.012345679012345678,
            "pseudo_barinel_susp": 0.041666666666666664
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.from_response#40",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None, clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs)",
        "snippet": "    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n\n        kwargs.setdefault('encoding', response.encoding)\n\n        if formcss is not None:\n            from parsel.csstranslator import HTMLTranslator\n            formxpath = HTMLTranslator().css_to_xpath(formcss)\n\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n\n        method = kwargs.pop('method', form.method)\n        if method is not None:\n            method = method.upper()\n            if method not in cls.valid_form_methods:\n                method = 'GET'\n\n        return cls(url=url, method=method, formdata=formdata, **kwargs)",
        "begin_line": 40,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._get_form_url#62",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form_url(form, url)",
        "snippet": "def _get_form_url(form, url):\n    if url is None:\n        action = form.get('action')\n        if action is None:\n            return form.base_url\n        return urljoin(form.base_url, strip_html5_whitespace(action))\n    return urljoin(form.base_url, url)",
        "begin_line": 62,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._urlencode#71",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._urlencode(seq, enc)",
        "snippet": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)",
        "begin_line": 71,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022114108801415304,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._get_form#78",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form(response, formname, formid, formnumber, formxpath)",
        "snippet": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    root = create_root_node(response.text, lxml.html.HTMLParser,\n                            base_url=get_base_url(response))\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(\"No <form> element found in %s\" % response)\n\n    if formname is not None:\n        f = root.xpath('//form[@name=\"%s\"]' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath('//form[@id=\"%s\"]' % formid)\n        if f:\n            return f[0]\n\n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')\n        raise ValueError('No <form> element found with %s' % encoded)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(\"Form number %d not found in %s\" %\n                             (formnumber, response))\n        else:\n            return form",
        "begin_line": 78,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._get_inputs#122",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_inputs(form, formdata, dont_click, clickdata, response)",
        "snippet": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata_keys = dict(formdata or ()).keys()\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    if not formdata:\n        formdata = ()\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[not(@type) or @type['\n                        ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n                        ' and (../@checked or'\n                        '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n                        namespaces={\n                            \"re\": \"http://exslt.org/regular-expressions\"})\n    values = [(k, u'' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata_keys]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    if isinstance(formdata, dict):\n        formdata = formdata.items()\n\n    values.extend((k, v) for k, v in formdata if v is not None)\n    return values",
        "begin_line": 122,
        "end_line": 151,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._value#154",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._value(ele)",
        "snippet": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v",
        "begin_line": 154,
        "end_line": 159,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._select_value#162",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._select_value(ele, n, v)",
        "snippet": "def _select_value(ele, n, v):\n    multiple = ele.multiple\n    if v is None and not multiple:\n        # Match browser behaviour on simple select tag without options selected\n        # And for select tags wihout options\n        o = ele.value_options\n        return (n, o[0]) if o else (None, None)\n    elif v is not None and multiple:\n        # This is a workround to bug in lxml fixed 2.3.1\n        # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n        selected_options = ele.xpath('.//option[@selected]')\n        v = [(o.get('value') or o.text or u'').strip() for o in selected_options]\n    return n, v",
        "begin_line": 162,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.http.request.form._get_clickable#177",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_clickable(clickdata, form)",
        "snippet": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = [\n        el for el in form.xpath(\n            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n            namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n        ]\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.get('name'), el.get('value') or '')\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.get('name'), el.get('value') or '')\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u'.//*' + \\\n            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].get('name'), el[0].get('value') or '')\n    elif len(el) > 1:\n        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n                         \"in clickdata: %r\" % (el, clickdata))\n    else:\n        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))",
        "begin_line": 177,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.__init__#14",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.__init__(self, stats, interval=60.0)",
        "snippet": "    def __init__(self, stats, interval=60.0):\n        self.stats = stats\n        self.interval = interval\n        self.multiplier = 60.0 / self.interval\n        self.task = None",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.from_crawler#21",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n        if not interval:\n            raise NotConfigured\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 21,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_opened#30",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.pagesprev = 0\n        self.itemsprev = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)",
        "begin_line": 30,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019076688286913393,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.log#37",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.log(self, spider)",
        "snippet": "    def log(self, spider):\n        items = self.stats.get_value('item_scraped_count', 0)\n        pages = self.stats.get_value('response_received_count', 0)\n        irate = (items - self.itemsprev) * self.multiplier\n        prate = (pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = pages, items\n\n        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n        log_args = {'pages': pages, 'pagerate': prate,\n                    'items': items, 'itemrate': irate}\n        logger.info(msg, log_args, extra={'spider': spider})",
        "begin_line": 37,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019076688286913393,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_closed#50",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        if self.task and self.task.running:\n            self.task.stop()",
        "begin_line": 50,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019076688286913393,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extension.ExtensionManager._get_mwlist_from_settings#14",
        "src_path": "scrapy/extension.py",
        "class_name": "scrapy.extension.ExtensionManager",
        "signature": "scrapy.extension.ExtensionManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings.getwithbase('EXTENSIONS'))",
        "begin_line": 14,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.__init__#29",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.__init__(self, max_active_size=5000000)",
        "snippet": "    def __init__(self, max_active_size=5000000):\n        self.max_active_size = max_active_size\n        self.queue = deque()\n        self.active = set()\n        self.active_size = 0\n        self.itemproc_size = 0\n        self.closing = None",
        "begin_line": 29,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.add_response_request#37",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.add_response_request(self, response, request)",
        "snippet": "    def add_response_request(self, response, request):\n        deferred = defer.Deferred()\n        self.queue.append((response, request, deferred))\n        if isinstance(response, Response):\n            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size += self.MIN_RESPONSE_SIZE\n        return deferred",
        "begin_line": 37,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.next_response_request_deferred#46",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.next_response_request_deferred(self)",
        "snippet": "    def next_response_request_deferred(self):\n        response, request, deferred = self.queue.popleft()\n        self.active.add(request)\n        return response, request, deferred",
        "begin_line": 46,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.finish_response#51",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.finish_response(self, response, request)",
        "snippet": "    def finish_response(self, response, request):\n        self.active.remove(request)\n        if isinstance(response, Response):\n            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n        else:\n            self.active_size -= self.MIN_RESPONSE_SIZE",
        "begin_line": 51,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.is_idle#58",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.is_idle(self)",
        "snippet": "    def is_idle(self):\n        return not (self.queue or self.active)",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Slot.needs_backout#61",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Slot",
        "signature": "scrapy.core.scraper.Slot.needs_backout(self)",
        "snippet": "    def needs_backout(self):\n        return self.active_size > self.max_active_size",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002069108214359611,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.__init__#67",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.slot = None\n        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n        self.itemproc = itemproc_cls.from_crawler(crawler)\n        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n        self.crawler = crawler\n        self.signals = crawler.signals\n        self.logformatter = crawler.logformatter",
        "begin_line": 67,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019391118867558658,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.open_spider#78",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n        self.slot = Slot()\n        yield self.itemproc.open_spider(spider)",
        "begin_line": 78,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.close_spider#83",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        \"\"\"Close a spider being scraped and release its resources\"\"\"\n        slot = self.slot\n        slot.closing = defer.Deferred()\n        slot.closing.addCallback(self.itemproc.close_spider)\n        self._check_if_closing(spider, slot)\n        return slot.closing",
        "begin_line": 83,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.is_idle#91",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.is_idle(self)",
        "snippet": "    def is_idle(self):\n        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n        return not self.slot",
        "begin_line": 91,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._check_if_closing#95",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._check_if_closing(self, spider, slot)",
        "snippet": "    def _check_if_closing(self, spider, slot):\n        if slot.closing and slot.is_idle():\n            slot.closing.callback(spider)",
        "begin_line": 95,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019940179461615153,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.enqueue_scrape#99",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.enqueue_scrape(self, response, request, spider)",
        "snippet": "    def enqueue_scrape(self, response, request, spider):\n        slot = self.slot\n        dfd = slot.add_response_request(response, request)\n        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _\n        dfd.addBoth(finish_scraping)\n        dfd.addErrback(\n            lambda f: logger.error('Scraper bug processing %(request)s',\n                                   {'request': request},\n                                   exc_info=failure_to_exc_info(f),\n                                   extra={'spider': spider}))\n        self._scrape_next(spider, slot)\n        return dfd",
        "begin_line": 99,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.finish_scraping#102",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.finish_scraping(_)",
        "snippet": "        def finish_scraping(_):\n            slot.finish_response(response, request)\n            self._check_if_closing(spider, slot)\n            self._scrape_next(spider, slot)\n            return _",
        "begin_line": 102,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape_next#116",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape_next(self, spider, slot)",
        "snippet": "    def _scrape_next(self, spider, slot):\n        while slot.queue:\n            response, request, deferred = slot.next_response_request_deferred()\n            self._scrape(response, request, spider).chainDeferred(deferred)",
        "begin_line": 116,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape#121",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape(self, response, request, spider)",
        "snippet": "    def _scrape(self, response, request, spider):\n        \"\"\"Handle the downloaded response or failure through the spider\n        callback/errback\"\"\"\n        assert isinstance(response, (Response, Failure))\n\n        dfd = self._scrape2(response, request, spider) # returns spiders processed output\n        dfd.addErrback(self.handle_spider_error, request, response, spider)\n        dfd.addCallback(self.handle_spider_output, request, response, spider)\n        return dfd",
        "begin_line": 121,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._scrape2#131",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._scrape2(self, request_result, request, spider)",
        "snippet": "    def _scrape2(self, request_result, request, spider):\n        \"\"\"Handle the different cases of request's result been a Response or a\n        Failure\"\"\"\n        if not isinstance(request_result, Failure):\n            return self.spidermw.scrape_response(\n                self.call_spider, request_result, request, spider)\n        else:\n            dfd = self.call_spider(request_result, request, spider)\n            return dfd.addErrback(\n                self._log_download_errors, request_result, request, spider)",
        "begin_line": 131,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.call_spider#142",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.call_spider(self, result, request, spider)",
        "snippet": "    def call_spider(self, result, request, spider):\n        result.request = request\n        dfd = defer_result(result)\n        dfd.addCallbacks(callback=request.callback or spider.parse,\n                         errback=request.errback,\n                         callbackKeywords=request.cb_kwargs)\n        return dfd.addCallback(iterate_spider_output)",
        "begin_line": 142,
        "end_line": 148,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002134927412467976,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.handle_spider_error#150",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.handle_spider_error(self, _failure, request, response, spider)",
        "snippet": "    def handle_spider_error(self, _failure, request, response, spider):\n        exc = _failure.value\n        if isinstance(exc, CloseSpider):\n            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n            return\n        logger.error(\n            \"Spider error processing %(request)s (referer: %(referer)s)\",\n            {'request': request, 'referer': referer_str(request)},\n            exc_info=failure_to_exc_info(_failure),\n            extra={'spider': spider}\n        )\n        self.signals.send_catch_log(\n            signal=signals.spider_error,\n            failure=_failure, response=response,\n            spider=spider\n        )\n        self.crawler.stats.inc_value(\n            \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n            spider=spider\n        )",
        "begin_line": 150,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper.handle_spider_output#171",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper.handle_spider_output(self, result, request, response, spider)",
        "snippet": "    def handle_spider_output(self, result, request, response, spider):\n        if not result:\n            return defer_succeed(None)\n        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n        dfd = parallel(it, self.concurrent_items,\n            self._process_spidermw_output, request, response, spider)\n        return dfd",
        "begin_line": 171,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._process_spidermw_output#179",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._process_spidermw_output(self, output, request, response, spider)",
        "snippet": "    def _process_spidermw_output(self, output, request, response, spider):\n        \"\"\"Process each Request/Item (given in the output parameter) returned\n        from the given spider\n        \"\"\"\n        if isinstance(output, Request):\n            self.crawler.engine.crawl(request=output, spider=spider)\n        elif isinstance(output, (BaseItem, dict)):\n            self.slot.itemproc_size += 1\n            dfd = self.itemproc.process_item(output, spider)\n            dfd.addBoth(self._itemproc_finished, output, response, spider)\n            return dfd\n        elif output is None:\n            pass\n        else:\n            typename = type(output).__name__\n            logger.error('Spider must return Request, BaseItem, dict or None, '\n                         'got %(typename)r in %(request)s',\n                         {'request': request, 'typename': typename},\n                         extra={'spider': spider})",
        "begin_line": 179,
        "end_line": 197,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._log_download_errors#199",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._log_download_errors(self, spider_failure, download_failure, request, spider)",
        "snippet": "    def _log_download_errors(self, spider_failure, download_failure, request, spider):\n        \"\"\"Log and silence errors that come from the engine (typically download\n        errors that got propagated thru here)\n        \"\"\"\n        if (isinstance(download_failure, Failure) and\n                not download_failure.check(IgnoreRequest)):\n            if download_failure.frames:\n                logger.error('Error downloading %(request)s',\n                             {'request': request},\n                             exc_info=failure_to_exc_info(download_failure),\n                             extra={'spider': spider})\n            else:\n                errmsg = download_failure.getErrorMessage()\n                if errmsg:\n                    logger.error('Error downloading %(request)s: %(errmsg)s',\n                                 {'request': request, 'errmsg': errmsg},\n                                 extra={'spider': spider})\n\n        if spider_failure is not download_failure:\n            return spider_failure",
        "begin_line": 199,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.scraper.Scraper._itemproc_finished#220",
        "src_path": "scrapy/core/scraper.py",
        "class_name": "scrapy.core.scraper.Scraper",
        "signature": "scrapy.core.scraper.Scraper._itemproc_finished(self, output, item, response, spider)",
        "snippet": "    def _itemproc_finished(self, output, item, response, spider):\n        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n        \"\"\"\n        self.slot.itemproc_size -= 1\n        if isinstance(output, Failure):\n            ex = output.value\n            if isinstance(ex, DropItem):\n                logkws = self.logformatter.dropped(item, ex, response, spider)\n                if logkws is not None:\n                    logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_dropped, item=item, response=response,\n                    spider=spider, exception=output.value)\n            else:\n                logger.error('Error processing %(item)s', {'item': item},\n                             exc_info=failure_to_exc_info(output),\n                             extra={'spider': spider})\n                return self.signals.send_catch_log_deferred(\n                    signal=signals.item_error, item=item, response=response,\n                    spider=spider, failure=output)\n        else:\n            logkws = self.logformatter.scraped(output, response, spider)\n            if logkws is not None:\n                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n            return self.signals.send_catch_log_deferred(\n                signal=signals.item_scraped, item=output, response=response,\n                spider=spider)",
        "begin_line": 220,
        "end_line": 246,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl._identity#21",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl",
        "signature": "scrapy.spiders.crawl._identity(request, response)",
        "snippet": "def _identity(request, response):\n    return request",
        "begin_line": 21,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl._get_method#25",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl",
        "signature": "scrapy.spiders.crawl._get_method(method, spider)",
        "snippet": "def _get_method(method, spider):\n    if callable(method):\n        return method\n    elif isinstance(method, six.string_types):\n        return getattr(spider, method, None)",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004909180166912126,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.Rule.__init__#37",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.Rule",
        "signature": "scrapy.spiders.crawl.Rule.__init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)",
        "snippet": "    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n        self.link_extractor = link_extractor or _default_link_extractor\n        self.callback = callback\n        self.cb_kwargs = cb_kwargs or {}\n        self.process_links = process_links\n        self.process_request = process_request or _identity\n        self.process_request_argcount = None\n        self.follow = follow if follow is not None else not callback",
        "begin_line": 37,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.Rule._compile#46",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.Rule",
        "signature": "scrapy.spiders.crawl.Rule._compile(self, spider)",
        "snippet": "    def _compile(self, spider):\n        self.callback = _get_method(self.callback, spider)\n        self.process_links = _get_method(self.process_links, spider)\n        self.process_request = _get_method(self.process_request, spider)\n        self.process_request_argcount = len(get_func_args(self.process_request))\n        if self.process_request_argcount == 1:\n            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)",
        "begin_line": 46,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.Rule._process_request#55",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.Rule",
        "signature": "scrapy.spiders.crawl.Rule._process_request(self, request, response)",
        "snippet": "    def _process_request(self, request, response):\n        \"\"\"\n        Wrapper around the request processing function to maintain backward\n        compatibility with functions that do not take a Response object\n        \"\"\"\n        args = [request] if self.process_request_argcount == 1 else [request, response]\n        return self.process_request(*args)",
        "begin_line": 55,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.__init__#68",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.__init__(self, *a, **kw)",
        "snippet": "    def __init__(self, *a, **kw):\n        super(CrawlSpider, self).__init__(*a, **kw)\n        self._compile_rules()",
        "begin_line": 68,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00028121484814398203,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider._build_request#81",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider._build_request(self, rule, link)",
        "snippet": "    def _build_request(self, rule, link):\n        r = Request(url=link.url, callback=self._response_downloaded)\n        r.meta.update(rule=rule, link_text=link.text)\n        return r",
        "begin_line": 81,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider._requests_to_follow#86",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider._requests_to_follow(self, response)",
        "snippet": "    def _requests_to_follow(self, response):\n        if not isinstance(response, HtmlResponse):\n            return\n        seen = set()\n        for n, rule in enumerate(self._rules):\n            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n                     if lnk not in seen]\n            if links and rule.process_links:\n                links = rule.process_links(links)\n            for link in links:\n                seen.add(link)\n                request = self._build_request(n, link)\n                yield rule._process_request(request, response)",
        "begin_line": 86,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider._compile_rules#115",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider._compile_rules(self)",
        "snippet": "    def _compile_rules(self):\n        self._rules = [copy.copy(r) for r in self.rules]\n        for rule in self._rules:\n            rule._compile(self)",
        "begin_line": 115,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.spiders.crawl.CrawlSpider.from_crawler#121",
        "src_path": "scrapy/spiders/crawl.py",
        "class_name": "scrapy.spiders.crawl.CrawlSpider",
        "signature": "scrapy.spiders.crawl.CrawlSpider.from_crawler(cls, crawler, *args, **kwargs)",
        "snippet": "    def from_crawler(cls, crawler, *args, **kwargs):\n        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n        spider._follow_links = crawler.settings.getbool(\n            'CRAWLSPIDER_FOLLOW_LINKS', True)\n        return spider",
        "begin_line": 121,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.__init__#46",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol",
        "signature": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.__init__(self, filename=None)",
        "snippet": "    def __init__(self, filename=None):\n        self.__filename = filename\n        self.body = open(filename, \"wb\") if filename else BytesIO()\n        self.size = 0",
        "begin_line": 46,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.dataReceived#51",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol",
        "signature": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.dataReceived(self, data)",
        "snippet": "    def dataReceived(self, data):\n        self.body.write(data)\n        self.size += len(data)",
        "begin_line": 51,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.filename#56",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol",
        "signature": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.filename(self)",
        "snippet": "    def filename(self):\n        return self.__filename",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.close#59",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol",
        "signature": "scrapy.core.downloader.handlers.ftp.ReceivedDataProtocol.close(self)",
        "snippet": "    def close(self):\n        self.body.close() if self.filename else self.body.seek(0)",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.__init__#73",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.default_user = settings['FTP_USER']\n        self.default_password = settings['FTP_PASSWORD']\n        self.passive_mode = settings['FTP_PASSIVE_MODE']",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001801477211313277,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.download_request#78",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        parsed_url = urlparse_cached(request)\n        user = request.meta.get(\"ftp_user\", self.default_user)\n        password = request.meta.get(\"ftp_password\", self.default_password)\n        passive_mode = 1 if bool(request.meta.get(\"ftp_passive\",\n                                                  self.passive_mode)) else 0\n        creator = ClientCreator(reactor, FTPClient, user, password,\n            passive=passive_mode)\n        return creator.connectTCP(parsed_url.hostname, parsed_url.port or 21).addCallback(self.gotClient,\n                                request, unquote(parsed_url.path))",
        "begin_line": 78,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.gotClient#89",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler.gotClient(self, client, request, filepath)",
        "snippet": "    def gotClient(self, client, request, filepath):\n        self.client = client\n        protocol = ReceivedDataProtocol(request.meta.get(\"ftp_local_filename\"))\n        return client.retrieveFile(filepath, protocol)\\\n                .addCallbacks(callback=self._build_response,\n                        callbackArgs=(request, protocol),\n                        errback=self._failed,\n                        errbackArgs=(request,))",
        "begin_line": 89,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler._build_response#98",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler._build_response(self, result, request, protocol)",
        "snippet": "    def _build_response(self, result, request, protocol):\n        self.result = result\n        respcls = responsetypes.from_args(url=request.url)\n        protocol.close()\n        body = protocol.filename or protocol.body.read()\n        headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)",
        "begin_line": 98,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler._failed#106",
        "src_path": "scrapy/core/downloader/handlers/ftp.py",
        "class_name": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler._failed(self, result, request)",
        "snippet": "    def _failed(self, result, request):\n        message = result.getErrorMessage()\n        if result.type == CommandFailed:\n            m = _CODE_RE.search(message)\n            if m:\n                ftpcode = m.group()\n                httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                return Response(url=request.url, status=httpcode, body=to_bytes(message))\n        raise result.type(result.value)",
        "begin_line": 106,
        "end_line": 114,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.__init__#19",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.__init__(self, settings, stats)",
        "snippet": "    def __init__(self, settings, stats):\n        if not settings.getbool('HTTPCACHE_ENABLED'):\n            raise NotConfigured\n        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)\n        self.storage = load_object(settings['HTTPCACHE_STORAGE'])(settings)\n        self.ignore_missing = settings.getbool('HTTPCACHE_IGNORE_MISSING')\n        self.stats = stats",
        "begin_line": 19,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.from_crawler#28",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings, crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 28,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_opened#34",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.storage.open_spider(spider)",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_closed#37",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        self.storage.close_spider(spider)",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_request#40",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_cache', False):\n            return\n\n        # Skip uncacheable requests\n        if not self.policy.should_cache_request(request):\n            request.meta['_dont_cache'] = True  # flag as uncacheable\n            return\n\n        # Look for cached response and check if expired\n        cachedresponse = self.storage.retrieve_response(spider, request)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/miss', spider=spider)\n            if self.ignore_missing:\n                self.stats.inc_value('httpcache/ignore', spider=spider)\n                raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n            return  # first time request\n\n        # Return cached response only if not expired\n        cachedresponse.flags.append('cached')\n        if self.policy.is_cached_response_fresh(cachedresponse, request):\n            self.stats.inc_value('httpcache/hit', spider=spider)\n            return cachedresponse\n\n        # Keep a reference to cached response to avoid a second cache lookup on\n        # process_response hook\n        request.meta['cached_response'] = cachedresponse",
        "begin_line": 40,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_response#68",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_cache', False):\n            return response\n\n        # Skip cached responses and uncacheable requests\n        if 'cached' in response.flags or '_dont_cache' in request.meta:\n            request.meta.pop('_dont_cache', None)\n            return response\n\n        # RFC2616 requires origin server to set Date header,\n        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n        if 'Date' not in response.headers:\n            response.headers['Date'] = formatdate(usegmt=1)\n\n        # Do not validate first-hand responses\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is None:\n            self.stats.inc_value('httpcache/firsthand', spider=spider)\n            self._cache_response(spider, response, request, cachedresponse)\n            return response\n\n        if self.policy.is_cached_response_valid(cachedresponse, response, request):\n            self.stats.inc_value('httpcache/revalidate', spider=spider)\n            return cachedresponse\n\n        self.stats.inc_value('httpcache/invalidate', spider=spider)\n        self._cache_response(spider, response, request, cachedresponse)\n        return response",
        "begin_line": 68,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_exception#97",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        cachedresponse = request.meta.pop('cached_response', None)\n        if cachedresponse is not None and isinstance(exception, self.DOWNLOAD_EXCEPTIONS):\n            self.stats.inc_value('httpcache/errorrecovery', spider=spider)\n            return cachedresponse",
        "begin_line": 97,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware._cache_response#103",
        "src_path": "scrapy/downloadermiddlewares/httpcache.py",
        "class_name": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware",
        "signature": "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware._cache_response(self, spider, response, request, cachedresponse)",
        "snippet": "    def _cache_response(self, spider, response, request, cachedresponse):\n        if self.policy.should_cache_response(response, request):\n            self.stats.inc_value('httpcache/store', spider=spider)\n            self.storage.store_response(spider, request, response)\n        else:\n            self.stats.inc_value('httpcache/uncacheable', spider=spider)",
        "begin_line": 103,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00032562683165092806,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler.__init__#11",
        "src_path": "scrapy/core/downloader/handlers/datauri.py",
        "class_name": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        super(DataURIDownloadHandler, self).__init__()",
        "begin_line": 11,
        "end_line": 12,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler.download_request#15",
        "src_path": "scrapy/core/downloader/handlers/datauri.py",
        "class_name": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler",
        "signature": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler.download_request(self, request, spider)",
        "snippet": "    def download_request(self, request, spider):\n        uri = parse_data_uri(request.url)\n        respcls = responsetypes.from_mimetype(uri.media_type)\n\n        resp_kwargs = {}\n        if (issubclass(respcls, TextResponse) and\n                uri.media_type.split('/')[0] == 'text'):\n            charset = uri.media_type_parameters.get('charset')\n            resp_kwargs['encoding'] = charset\n\n        return respcls(url=request.url, body=uri.data, **resp_kwargs)",
        "begin_line": 15,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.tls.set_tlsext_host_name#34",
        "src_path": "scrapy/core/downloader/tls.py",
        "class_name": "scrapy.core.downloader.tls",
        "signature": "scrapy.core.downloader.tls.set_tlsext_host_name(connection, hostNameBytes)",
        "snippet": "    def set_tlsext_host_name(connection, hostNameBytes):\n        connection.set_tlsext_host_name(hostNameBytes)",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001762114537444934,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions.__init__#49",
        "src_path": "scrapy/core/downloader/tls.py",
        "class_name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions",
        "signature": "scrapy.core.downloader.tls.ScrapyClientTLSOptions.__init__(self, hostname, ctx, verbose_logging=False)",
        "snippet": "    def __init__(self, hostname, ctx, verbose_logging=False):\n        super(ScrapyClientTLSOptions, self).__init__(hostname, ctx)\n        self.verbose_logging = verbose_logging",
        "begin_line": 49,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001762114537444934,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions._identityVerifyingInfoCallback#53",
        "src_path": "scrapy/core/downloader/tls.py",
        "class_name": "scrapy.core.downloader.tls.ScrapyClientTLSOptions",
        "signature": "scrapy.core.downloader.tls.ScrapyClientTLSOptions._identityVerifyingInfoCallback(self, connection, where, ret)",
        "snippet": "    def _identityVerifyingInfoCallback(self, connection, where, ret):\n        if where & SSL.SSL_CB_HANDSHAKE_START:\n            set_tlsext_host_name(connection, self._hostnameBytes)\n        elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n            if self.verbose_logging:\n                if hasattr(connection, 'get_cipher_name'):  # requires pyOPenSSL 0.15\n                    if hasattr(connection, 'get_protocol_version_name'):  # requires pyOPenSSL 16.0.0\n                        logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                                     self._hostnameASCII,\n                                     connection.get_protocol_version_name(),\n                                     connection.get_cipher_name(),\n                                     )\n                    else:\n                        logger.debug('SSL connection to %s using cipher %s',\n                                     self._hostnameASCII,\n                                     connection.get_cipher_name(),\n                                     )\n                server_cert = connection.get_peer_certificate()\n                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                             x509name_to_string(server_cert.get_issuer()),\n                             x509name_to_string(server_cert.get_subject()),\n                             )\n                key_info = get_temp_key_info(connection._ssl)\n                if key_info:\n                    logger.debug('SSL temp key: %s', key_info)\n\n            try:\n                verifyHostname(connection, self._hostnameASCII)\n            except (CertificateError, VerificationError) as e:\n                logger.warning(\n                    'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n                        self._hostnameASCII, e))\n\n            except ValueError as e:\n                logger.warning(\n                    'Ignoring error while verifying certificate '\n                    'from host \"{}\" (exception: {})'.format(\n                        self._hostnameASCII, repr(e)))",
        "begin_line": 53,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.crawled#51",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.crawled(self, request, response, spider)",
        "snippet": "    def crawled(self, request, response, spider):\n        \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n        request_flags = ' %s' % str(request.flags) if request.flags else ''\n        response_flags = ' %s' % str(response.flags) if response.flags else ''\n        return {\n            'level': logging.DEBUG,\n            'msg': CRAWLEDMSG,\n            'args': {\n                'status': response.status,\n                'request': request,\n                'request_flags': request_flags,\n                'referer': referer_str(request),\n                'response_flags': response_flags,\n                # backward compatibility with Scrapy logformatter below 1.4 version\n                'flags': response_flags\n            }\n        }",
        "begin_line": 51,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00022114108801415304,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.scraped#69",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.scraped(self, item, response, spider)",
        "snippet": "    def scraped(self, item, response, spider):\n        \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n        if isinstance(response, Failure):\n            src = response.getErrorMessage()\n        else:\n            src = response\n        return {\n            'level': logging.DEBUG,\n            'msg': SCRAPEDMSG,\n            'args': {\n                'src': src,\n                'item': item,\n            }\n        }",
        "begin_line": 69,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002617801047120419,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.dropped#84",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.dropped(self, item, exception, response, spider)",
        "snippet": "    def dropped(self, item, exception, response, spider):\n        \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n        return {\n            'level': logging.WARNING,\n            'msg': DROPPEDMSG,\n            'args': {\n                'exception': exception,\n                'item': item,\n            }\n        }",
        "begin_line": 84,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0006476683937823834,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.from_crawler#96",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls()",
        "begin_line": 96,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00016385384237260363,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.__init__#9",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 9,
        "end_line": 10,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001824817518248175,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.from_crawler#13",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('DOWNLOADER_STATS'):\n            raise NotConfigured\n        return cls(crawler.stats)",
        "begin_line": 13,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00018463810930576072,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_request#18",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        self.stats.inc_value('downloader/request_count', spider=spider)\n        self.stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)\n        reqlen = len(request_httprepr(request))\n        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)",
        "begin_line": 18,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00019615535504119262,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_response#24",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        self.stats.inc_value('downloader/response_count', spider=spider)\n        self.stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)\n        reslen = len(response_httprepr(response))\n        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)\n        return response",
        "begin_line": 24,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00021843599825251202,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_exception#31",
        "src_path": "scrapy/downloadermiddlewares/stats.py",
        "class_name": "scrapy.downloadermiddlewares.stats.DownloaderStats",
        "signature": "scrapy.downloadermiddlewares.stats.DownloaderStats.process_exception(self, request, exception, spider)",
        "snippet": "    def process_exception(self, request, exception, spider):\n        ex_class = global_object_name(exception.__class__)\n        self.stats.inc_value('downloader/exception_count', spider=spider)\n        self.stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)",
        "begin_line": 31,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.push#15",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.push(self, obj)",
        "snippet": "        def push(self, obj):\n            s = serialize(obj)\n            super(SerializableQueue, self).push(s)",
        "begin_line": 15,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017307026652821047,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.pop#19",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.pop(self)",
        "snippet": "        def pop(self):\n            s = super(SerializableQueue, self).pop()\n            if s:\n                return deserialize(s)",
        "begin_line": 19,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0001734304543877905,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.squeues._pickle_serialize#27",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues",
        "signature": "scrapy.squeues._pickle_serialize(obj)",
        "snippet": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=2)\n    # Python <= 3.4 raises pickle.PicklingError here while\n    # 3.5 <= Python < 3.6 raises AttributeError and\n    # Python >= 3.6 raises TypeError\n    except (pickle.PicklingError, AttributeError, TypeError) as e:\n        raise ValueError(str(e))",
        "begin_line": 27,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.response.get_base_url#16",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_base_url(response)",
        "snippet": "def get_base_url(response):\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.text[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\n            response.encoding)\n    return _baseurl_cache[response]",
        "begin_line": 16,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017307026652821047,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.response.get_meta_refresh#26",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_meta_refresh(response, ignore_tags=('script', 'noscript'))",
        "snippet": "def get_meta_refresh(response, ignore_tags=('script', 'noscript')):\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.text[0:4096]\n        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n            response.encoding, ignore_tags=ignore_tags)\n    return _metaref_cache[response]",
        "begin_line": 26,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00023299161230195712,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.response.response_status_message#35",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_status_message(status)",
        "snippet": "def response_status_message(status):\n    \"\"\"Return status code plus status text descriptive message\n    \"\"\"\n    message = http.RESPONSES.get(int(status), \"Unknown Status\")\n    return '%s %s' % (status, to_native_str(message))",
        "begin_line": 35,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.response.response_httprepr#42",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_httprepr(response)",
        "snippet": "def response_httprepr(response):\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n        to_bytes(http.RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n    if response.headers:\n        s += response.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += response.body\n    return s",
        "begin_line": 42,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002199736031676199,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.utils.response.open_in_browser#56",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.open_in_browser(response, _openfunc=webbrowser.open)",
        "snippet": "def open_in_browser(response, _openfunc=webbrowser.open):\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = '<head><base href=\"%s\">' % response.url\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: %s\" %\n                        response.__class__.__name__)\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(\"file://%s\" % fname)",
        "begin_line": 56,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "conftest.chdir#19",
        "src_path": "conftest.py",
        "class_name": "conftest",
        "signature": "conftest.chdir(tmpdir)",
        "snippet": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()",
        "begin_line": 19,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013513513513513514,
            "pseudo_dstar_susp": 0.02127659574468085,
            "pseudo_tarantula_susp": 0.012195121951219513,
            "pseudo_op2_susp": 0.022222222222222223,
            "pseudo_barinel_susp": 0.012195121951219513
        }
    },
    {
        "name": "scrapy.selector.unified._st#16",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._st(response, st)",
        "snippet": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    return st",
        "begin_line": 16,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00017803097739006588,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.selector.unified._response_from_text#22",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._response_from_text(text, st)",
        "snippet": "def _response_from_text(text, st):\n    rt = XmlResponse if st == 'xml' else HtmlResponse\n    return rt(url='about:blank', encoding='utf-8',\n              body=to_bytes(text, 'utf-8'))",
        "begin_line": 22,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00024509803921568627,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.__init__#69",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.__init__(self, response=None, text=None, type=None, root=None, **kwargs)",
        "snippet": "    def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n        if not(response is None or text is None):\n           raise ValueError('%s.__init__() received both response and text'\n                            % self.__class__.__name__)\n\n        st = _st(response, type or self._default_type)\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            text = response.text\n            kwargs.setdefault('base_url', response.url)\n\n        self.response = response\n        super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)",
        "begin_line": 69,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.BlockingFeedStorage.open#49",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.BlockingFeedStorage",
        "signature": "scrapy.extensions.feedexport.BlockingFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        path = spider.crawler.settings['FEED_TEMPDIR']\n        if path and not os.path.isdir(path):\n            raise OSError('Not a Directory: ' + str(path))\n\n        return NamedTemporaryFile(prefix='feed-', dir=path)",
        "begin_line": 49,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.BlockingFeedStorage.store#56",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.BlockingFeedStorage",
        "signature": "scrapy.extensions.feedexport.BlockingFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        return threads.deferToThread(self._store_in_thread, file)",
        "begin_line": 56,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.000546448087431694,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__#66",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__(self, uri, _stdout=None)",
        "snippet": "    def __init__(self, uri, _stdout=None):\n        if not _stdout:\n            _stdout = sys.stdout if six.PY2 else sys.stdout.buffer\n        self._stdout = _stdout",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.open#71",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        return self._stdout",
        "begin_line": 71,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.store#74",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        pass",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.__init__#81",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0002992220227408737,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.open#84",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')",
        "begin_line": 84,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.store#90",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        file.close()",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00030826140567200987,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.S3FeedStorage.__init__#96",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.S3FeedStorage",
        "signature": "scrapy.extensions.feedexport.S3FeedStorage.__init__(self, uri, access_key=None, secret_key=None, acl=None)",
        "snippet": "    def __init__(self, uri, access_key=None, secret_key=None, acl=None):\n        # BEGIN Backward compatibility for initialising without keys (and\n        # without using from_crawler)\n        no_defaults = access_key is None and secret_key is None\n        if no_defaults:\n            from scrapy.utils.project import get_project_settings\n            settings = get_project_settings()\n            if 'AWS_ACCESS_KEY_ID' in settings or 'AWS_SECRET_ACCESS_KEY' in settings:\n                import warnings\n                from scrapy.exceptions import ScrapyDeprecationWarning\n                warnings.warn(\n                    \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n                    \"without AWS keys is deprecated. Please supply credentials or \"\n                    \"use the `from_crawler()` constructor.\",\n                    category=ScrapyDeprecationWarning,\n                    stacklevel=2\n                )\n                access_key = settings['AWS_ACCESS_KEY_ID']\n                secret_key = settings['AWS_SECRET_ACCESS_KEY']\n        # END Backward compatibility\n        u = urlparse(uri)\n        self.bucketname = u.hostname\n        self.access_key = u.username or access_key\n        self.secret_key = u.password or secret_key\n        self.is_botocore = is_botocore()\n        self.keyname = u.path[1:]  # remove first \"/\"\n        self.acl = acl\n        if self.is_botocore:\n            import botocore.session\n            session = botocore.session.get_session()\n            self.s3_client = session.create_client(\n                's3', aws_access_key_id=self.access_key,\n                aws_secret_access_key=self.secret_key)\n        else:\n            import boto\n            self.connect_s3 = boto.connect_s3",
        "begin_line": 96,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00039032006245121,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.S3FeedStorage.from_crawler#134",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.S3FeedStorage",
        "signature": "scrapy.extensions.feedexport.S3FeedStorage.from_crawler(cls, crawler, uri)",
        "snippet": "    def from_crawler(cls, crawler, uri):\n        return cls(\n            uri=uri,\n            access_key=crawler.settings['AWS_ACCESS_KEY_ID'],\n            secret_key=crawler.settings['AWS_SECRET_ACCESS_KEY'],\n            acl=crawler.settings['FEED_STORAGE_S3_ACL'] or None\n        )",
        "begin_line": 134,
        "end_line": 140,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.S3FeedStorage._store_in_thread#142",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.S3FeedStorage",
        "signature": "scrapy.extensions.feedexport.S3FeedStorage._store_in_thread(self, file)",
        "snippet": "    def _store_in_thread(self, file):\n        file.seek(0)\n        if self.is_botocore:\n            kwargs = {'ACL': self.acl} if self.acl else {}\n            self.s3_client.put_object(\n                Bucket=self.bucketname, Key=self.keyname, Body=file,\n                **kwargs)\n        else:\n            conn = self.connect_s3(self.access_key, self.secret_key)\n            bucket = conn.get_bucket(self.bucketname, validate=False)\n            key = bucket.new_key(self.keyname)\n            kwargs = {'policy': self.acl} if self.acl else {}\n            key.set_contents_from_file(file, **kwargs)\n            key.close()",
        "begin_line": 142,
        "end_line": 155,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FTPFeedStorage.__init__#160",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FTPFeedStorage",
        "signature": "scrapy.extensions.feedexport.FTPFeedStorage.__init__(self, uri, use_active_mode=False)",
        "snippet": "    def __init__(self, uri, use_active_mode=False):\n        u = urlparse(uri)\n        self.host = u.hostname\n        self.port = int(u.port or '21')\n        self.username = u.username\n        self.password = unquote(u.password)\n        self.path = u.path\n        self.use_active_mode = use_active_mode",
        "begin_line": 160,
        "end_line": 167,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.SpiderSlot.__init__#190",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.SpiderSlot",
        "signature": "scrapy.extensions.feedexport.SpiderSlot.__init__(self, file, exporter, storage, uri)",
        "snippet": "    def __init__(self, file, exporter, storage, uri):\n        self.file = file\n        self.exporter = exporter\n        self.storage = storage\n        self.uri = uri\n        self.itemcount = 0",
        "begin_line": 190,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.__init__#200",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.settings = settings\n        self.urifmt = settings['FEED_URI']\n        if not self.urifmt:\n            raise NotConfigured\n        self.format = settings['FEED_FORMAT'].lower()\n        self.export_encoding = settings['FEED_EXPORT_ENCODING']\n        self.storages = self._load_components('FEED_STORAGES')\n        self.exporters = self._load_components('FEED_EXPORTERS')\n        if not self._storage_supported(self.urifmt):\n            raise NotConfigured\n        if not self._exporter_supported(self.format):\n            raise NotConfigured\n        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n        self._exporting = False\n        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n        self.indent = None\n        if settings.get('FEED_EXPORT_INDENT') is not None:\n            self.indent = settings.getint('FEED_EXPORT_INDENT')\n        uripar = settings['FEED_URI_PARAMS']\n        self._uripar = load_object(uripar) if uripar else lambda x, y: None",
        "begin_line": 200,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0004201680672268908,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.from_crawler#223",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings)\n        o.crawler = crawler\n        crawler.signals.connect(o.open_spider, signals.spider_opened)\n        crawler.signals.connect(o.close_spider, signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n        return o",
        "begin_line": 223,
        "end_line": 229,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.open_spider#231",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        uri = self.urifmt % self._get_uri_params(spider)\n        storage = self._get_storage(uri)\n        file = storage.open(spider)\n        exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n            encoding=self.export_encoding, indent=self.indent)\n        if self.store_empty:\n            exporter.start_exporting()\n            self._exporting = True\n        self.slot = SpiderSlot(file, exporter, storage, uri)",
        "begin_line": 231,
        "end_line": 240,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0019011406844106464,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.close_spider#242",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.close_spider(self, spider)",
        "snippet": "    def close_spider(self, spider):\n        slot = self.slot\n        if not slot.itemcount and not self.store_empty:\n            # We need to call slot.storage.store nonetheless to get the file\n            # properly closed.\n            return defer.maybeDeferred(slot.storage.store, slot.file)\n        if self._exporting:\n            slot.exporter.finish_exporting()\n            self._exporting = False\n        logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n        log_args = {'format': self.format,\n                    'itemcount': slot.itemcount,\n                    'uri': slot.uri}\n        d = defer.maybeDeferred(slot.storage.store, slot.file)\n        d.addCallback(lambda _: logger.info(logfmt % \"Stored\", log_args,\n                                            extra={'spider': spider}))\n        d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': spider}))\n        return d",
        "begin_line": 242,
        "end_line": 261,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.0008517887563884157,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.item_scraped#263",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        slot = self.slot\n        if not self._exporting:\n            slot.exporter.start_exporting()\n            self._exporting = True\n        slot.exporter.export_item(item)\n        slot.itemcount += 1\n        return item",
        "begin_line": 263,
        "end_line": 270,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00045004500450045,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._load_components#272",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._load_components(self, setting_prefix)",
        "snippet": "    def _load_components(self, setting_prefix):\n        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n        d = {}\n        for k, v in conf.items():\n            try:\n                d[k] = load_object(v)\n            except NotConfigured:\n                pass\n        return d",
        "begin_line": 272,
        "end_line": 280,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._exporter_supported#282",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._exporter_supported(self, format)",
        "snippet": "    def _exporter_supported(self, format):\n        if format in self.exporters:\n            return True\n        logger.error(\"Unknown feed format: %(format)s\", {'format': format})",
        "begin_line": 282,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._storage_supported#287",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._storage_supported(self, uri)",
        "snippet": "    def _storage_supported(self, uri):\n        scheme = urlparse(uri).scheme\n        if scheme in self.storages:\n            try:\n                self._get_storage(uri)\n                return True\n            except NotConfigured as e:\n                logger.error(\"Disabled feed storage scheme: %(scheme)s. \"\n                             \"Reason: %(reason)s\",\n                             {'scheme': scheme, 'reason': str(e)})\n        else:\n            logger.error(\"Unknown feed storage scheme: %(scheme)s\",\n                         {'scheme': scheme})",
        "begin_line": 287,
        "end_line": 299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_instance#301",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_instance(self, objcls, *args, **kwargs)",
        "snippet": "    def _get_instance(self, objcls, *args, **kwargs):\n        return create_instance(\n            objcls, self.settings, getattr(self, 'crawler', None),\n            *args, **kwargs)",
        "begin_line": 301,
        "end_line": 304,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_exporter#306",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_exporter(self, *args, **kwargs)",
        "snippet": "    def _get_exporter(self, *args, **kwargs):\n        return self._get_instance(self.exporters[self.format], *args, **kwargs)",
        "begin_line": 306,
        "end_line": 307,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_storage#309",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_storage(self, uri)",
        "snippet": "    def _get_storage(self, uri):\n        return self._get_instance(self.storages[urlparse(uri).scheme], uri)",
        "begin_line": 309,
        "end_line": 310,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter._get_uri_params#312",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter._get_uri_params(self, spider)",
        "snippet": "    def _get_uri_params(self, spider):\n        params = {}\n        for k in dir(spider):\n            params[k] = getattr(spider, k)\n        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n        params['time'] = ts\n        self._uripar(params, spider)\n        return params",
        "begin_line": 312,
        "end_line": 319,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0003051571559353067,
            "pseudo_dstar_susp": 0.0003051571559353067,
            "pseudo_tarantula_susp": 0.0003051571559353067,
            "pseudo_op2_susp": 0.00035676061362825543,
            "pseudo_barinel_susp": 0.0003051571559353067
        }
    }
]