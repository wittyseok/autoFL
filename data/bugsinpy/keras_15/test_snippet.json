[
    {
        "name": "tests.keras.backend.reference_operations.wrapper#13",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.wrapper(*args, **kwargs)",
        "snippet": "    def wrapper(*args, **kwargs):\n        x = args[0]\n        w = args[1]\n        if x.ndim == 3:\n            w = np.flipud(w)\n            w = np.transpose(w, (1, 2, 0))\n            if kwargs['data_format'] == 'channels_last':\n                x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            w = np.fliplr(np.flipud(w))\n            w = np.transpose(w, (2, 3, 0, 1))\n            if kwargs['data_format'] == 'channels_last':\n                x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            w = np.flip(np.fliplr(np.flipud(w)), axis=2)\n            w = np.transpose(w, (3, 4, 0, 1, 2))\n            if kwargs['data_format'] == 'channels_last':\n                x = np.transpose(x, (0, 4, 1, 2, 3))\n\n        y = func(x, w, **kwargs)\n\n        if kwargs['data_format'] == 'channels_last':\n            if y.ndim == 3:\n                y = np.transpose(y, (0, 2, 1))\n            elif y.ndim == 4:\n                y = np.transpose(y, (0, 2, 3, 1))\n            else:\n                y = np.transpose(y, (0, 2, 3, 4, 1))\n\n        return y",
        "begin_line": 13,
        "end_line": 42,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.conv#48",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.conv(x, w, padding, data_format)",
        "snippet": "def conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[1]):\n            __y = []\n            for k in range(w.shape[0]):\n                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))\n            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))\n        y.append(_y)\n    y = np.array(y)\n    return y",
        "begin_line": 48,
        "end_line": 59,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.depthwise_conv#63",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.depthwise_conv(x, w, padding, data_format)",
        "snippet": "def depthwise_conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[0]):\n            __y = []\n            for k in range(w.shape[1]):\n                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))\n            _y.append(np.stack(__y, axis=0))\n        y.append(np.concatenate(_y, axis=0))\n    y = np.array(y)\n    return y",
        "begin_line": 63,
        "end_line": 74,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.separable_conv#77",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.separable_conv(x, w1, w2, padding, data_format)",
        "snippet": "def separable_conv(x, w1, w2, padding, data_format):\n    x2 = depthwise_conv(x, w1, padding=padding, data_format=data_format)\n    return conv(x2, w2, padding=padding, data_format=data_format)",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.pool#90",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.pool(x, pool_size, strides, padding, data_format, pool_mode)",
        "snippet": "def pool(x, pool_size, strides, padding, data_format, pool_mode):\n    if data_format == 'channels_last':\n        if x.ndim == 3:\n            x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            x = np.transpose(x, (0, 4, 1, 2, 3))\n\n    if padding == 'same':\n        pad = [(0, 0), (0, 0)] + [(s // 2, s // 2) for s in pool_size]\n        x = np.pad(x, pad, 'constant', constant_values=-np.inf)\n\n    # indexing trick\n    x = np.pad(x, [(0, 0), (0, 0)] + [(0, 1) for _ in pool_size],\n               'constant', constant_values=0)\n\n    if x.ndim == 3:\n        y = [x[:, :, k:k1:strides[0]]\n             for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0))]\n    elif x.ndim == 4:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                y.append(x[:, :, k:k1:strides[0], l:l1:strides[1]])\n    else:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                for (m, m1) in zip(range(pool_size[2]), range(-pool_size[2], 0)):\n                    y.append(x[:,\n                               :,\n                               k:k1:strides[0],\n                               l:l1:strides[1],\n                               m:m1:strides[2]])\n    y = np.stack(y, axis=-1)\n    if pool_mode == 'avg':\n        y = np.mean(np.ma.masked_invalid(y), axis=-1).data\n    elif pool_mode == 'max':\n        y = np.max(y, axis=-1)\n\n    if data_format == 'channels_last':\n        if y.ndim == 3:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.ndim == 4:\n            y = np.transpose(y, (0, 2, 3, 1))\n        else:\n            y = np.transpose(y, (0, 2, 3, 4, 1))\n\n    return y",
        "begin_line": 90,
        "end_line": 139,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.bias_add#146",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.bias_add(x, y, data_format)",
        "snippet": "def bias_add(x, y, data_format):\n    if data_format == 'channels_first':\n        if y.ndim > 1:\n            y = np.reshape(y, y.shape[::-1])\n        for _ in range(x.ndim - y.ndim - 1):\n            y = np.expand_dims(y, -1)\n    else:\n        for _ in range(x.ndim - y.ndim - 1):\n            y = np.expand_dims(y, 0)\n    return x + y",
        "begin_line": 146,
        "end_line": 155,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.rnn#158",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None)",
        "snippet": "def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n    w_i, w_h, w_o = w\n    h = []\n    o = []\n\n    if go_backwards:\n        t_list = range(x.shape[1] - 1, -1, -1)\n    else:\n        t_list = range(x.shape[1])\n\n    if mask is not None:\n        from keras import backend as K\n        np_mask = K.eval(mask)\n    else:\n        np_mask = None\n\n    for (i, t) in enumerate(t_list):\n        h_t = np.dot(x[:, t], w_i)\n\n        if w_h is not None:\n            prev = h[i - 1] if i > 0 else init\n            h_t1 = np.dot(prev, w_h)\n            if np_mask is not None:\n                h_t1[np_mask[:, t] == 0] = prev[np_mask[:, t] == 0]\n        else:\n            h_t1 = 0\n\n        o_t = h_t + h_t1\n        if w_o is not None:\n            o_t = np.dot(o_t, w_o)\n        o.append(o_t)\n\n        if np_mask is not None:\n            h_t = h_t * np_mask[:, t].reshape(-1, 1)\n        h.append(h_t + h_t1)\n\n    return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)",
        "begin_line": 158,
        "end_line": 194,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.in_train_phase#209",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.in_train_phase(x, alt, training=None)",
        "snippet": "def in_train_phase(x, alt, training=None):\n    if training is None:\n        training = learning_phase()\n\n    if training is 1 or training is True:\n        if callable(x):\n            return x()\n        else:\n            return x\n    else:\n        if callable(alt):\n            return alt()\n        else:\n            return alt",
        "begin_line": 209,
        "end_line": 222,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.in_test_phase#225",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.in_test_phase(x, alt, training=None)",
        "snippet": "def in_test_phase(x, alt, training=None):\n    return in_train_phase(alt, x, training=training)",
        "begin_line": 225,
        "end_line": 226,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.relu#229",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.relu(x, alpha=0.0, max_value=None)",
        "snippet": "def relu(x, alpha=0., max_value=None):\n    y = x * (x > 0) + alpha * x * (x < 0)\n    if max_value is not None:\n        y = np.minimum(y, max_value)\n    return y",
        "begin_line": 229,
        "end_line": 233,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.switch#236",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.switch(condition, then_expression, else_expression)",
        "snippet": "def switch(condition, then_expression, else_expression):\n    cond_float = condition.astype(floatx())\n    while cond_float.ndim < then_expression.ndim:\n        cond_float = cond_float[..., None]\n    return cond_float * then_expression + (1 - cond_float) * else_expression",
        "begin_line": 236,
        "end_line": 240,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.softplus#243",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.softplus(x)",
        "snippet": "def softplus(x):\n    return np.log(1. + np.exp(x))",
        "begin_line": 243,
        "end_line": 244,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.elu#247",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.elu(x, alpha=1.0)",
        "snippet": "def elu(x, alpha=1.):\n    return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0)",
        "begin_line": 247,
        "end_line": 248,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.sigmoid#251",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.sigmoid(x)",
        "snippet": "def sigmoid(x):\n    return 1. / (1. + np.exp(-x))",
        "begin_line": 251,
        "end_line": 252,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.hard_sigmoid#255",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.hard_sigmoid(x)",
        "snippet": "def hard_sigmoid(x):\n    y = 0.2 * x + 0.5\n    y = np.minimum(y, 1.)\n    y = np.maximum(y, 0.)\n    return y",
        "begin_line": 255,
        "end_line": 259,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.tanh#262",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.tanh(x)",
        "snippet": "def tanh(x):\n    return np.tanh(x)",
        "begin_line": 262,
        "end_line": 263,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.softmax#266",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.softmax(x, axis=-1)",
        "snippet": "def softmax(x, axis=-1):\n    y = np.exp(x - np.max(x, axis, keepdims=True))\n    return y / np.sum(y, axis, keepdims=True)",
        "begin_line": 266,
        "end_line": 268,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.l2_normalize#271",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.l2_normalize(x, axis=-1)",
        "snippet": "def l2_normalize(x, axis=-1):\n    y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True)\n    return x / np.sqrt(y)",
        "begin_line": 271,
        "end_line": 273,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.binary_crossentropy#276",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.binary_crossentropy(target, output, from_logits=False)",
        "snippet": "def binary_crossentropy(target, output, from_logits=False):\n    if not from_logits:\n        output = np.clip(output, 1e-7, 1 - 1e-7)\n        output = np.log(output / (1 - output))\n    return (target * -np.log(sigmoid(output)) +\n            (1 - target) * -np.log(1 - sigmoid(output)))",
        "begin_line": 276,
        "end_line": 281,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.categorical_crossentropy#284",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.categorical_crossentropy(target, output, from_logits=False)",
        "snippet": "def categorical_crossentropy(target, output, from_logits=False):\n    if from_logits:\n        output = softmax(output)\n    else:\n        output /= output.sum(axis=-1, keepdims=True)\n    output = np.clip(output, 1e-7, 1 - 1e-7)\n    return np.sum(target * -np.log(output), axis=-1, keepdims=False)",
        "begin_line": 284,
        "end_line": 290,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.max#293",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.max(x, axis=None, keepdims=False)",
        "snippet": "def max(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return np.max(x, axis=axis, keepdims=keepdims)",
        "begin_line": 293,
        "end_line": 296,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.min#299",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.min(x, axis=None, keepdims=False)",
        "snippet": "def min(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return np.min(x, axis=axis, keepdims=keepdims)",
        "begin_line": 299,
        "end_line": 302,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.mean#305",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.mean(x, axis=None, keepdims=False)",
        "snippet": "def mean(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return np.mean(x, axis=axis, keepdims=keepdims)",
        "begin_line": 305,
        "end_line": 308,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.var#311",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.var(x, axis=None, keepdims=False)",
        "snippet": "def var(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return np.var(x, axis=axis, keepdims=keepdims)",
        "begin_line": 311,
        "end_line": 314,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.std#317",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.std(x, axis=None, keepdims=False)",
        "snippet": "def std(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return np.std(x, axis=axis, keepdims=keepdims)",
        "begin_line": 317,
        "end_line": 320,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.logsumexp#323",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.logsumexp(x, axis=None, keepdims=False)",
        "snippet": "def logsumexp(x, axis=None, keepdims=False):\n    if isinstance(axis, list):\n        axis = tuple(axis)\n    return sp.misc.logsumexp(x, axis=axis, keepdims=keepdims)",
        "begin_line": 323,
        "end_line": 326,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.cumsum#341",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.cumsum(x, axis=0)",
        "snippet": "def cumsum(x, axis=0):\n    return np.cumsum(x, axis=axis)",
        "begin_line": 341,
        "end_line": 342,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.cumprod#345",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.cumprod(x, axis=0)",
        "snippet": "def cumprod(x, axis=0):\n    return np.cumprod(x, axis=axis)",
        "begin_line": 345,
        "end_line": 346,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.concatenate#383",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.concatenate(tensors, axis=-1)",
        "snippet": "def concatenate(tensors, axis=-1):\n    return np.concatenate(tensors, axis)",
        "begin_line": 383,
        "end_line": 384,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.permute_dimensions#387",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.permute_dimensions(x, pattern)",
        "snippet": "def permute_dimensions(x, pattern):\n    return np.transpose(x, pattern)",
        "begin_line": 387,
        "end_line": 388,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.reshape#391",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.reshape(x, shape)",
        "snippet": "def reshape(x, shape):\n    return np.reshape(x, shape)",
        "begin_line": 391,
        "end_line": 392,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.repeat_elements#395",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.repeat_elements(x, rep, axis)",
        "snippet": "def repeat_elements(x, rep, axis):\n    return np.repeat(x, rep, axis=axis)",
        "begin_line": 395,
        "end_line": 396,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.repeat#399",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.repeat(x, n)",
        "snippet": "def repeat(x, n):\n    y = np.expand_dims(x, 1)\n    y = np.repeat(y, n, axis=1)\n    return y",
        "begin_line": 399,
        "end_line": 402,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.arange#405",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.arange(start, stop=None, step=1, dtype='int32')",
        "snippet": "def arange(start, stop=None, step=1, dtype='int32'):\n    return np.arange(start, stop, step, dtype)",
        "begin_line": 405,
        "end_line": 406,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.flatten#409",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.flatten(x)",
        "snippet": "def flatten(x):\n    return np.reshape(x, (-1,))",
        "begin_line": 409,
        "end_line": 410,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.batch_flatten#413",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.batch_flatten(x)",
        "snippet": "def batch_flatten(x):\n    return np.reshape(x, (x.shape[0], -1))",
        "begin_line": 413,
        "end_line": 414,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.eval#417",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.eval(x)",
        "snippet": "def eval(x):\n    return x",
        "begin_line": 417,
        "end_line": 418,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.dtype#421",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.dtype(x)",
        "snippet": "def dtype(x):\n    return x.dtype.name",
        "begin_line": 421,
        "end_line": 422,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.constant#425",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.constant(value, dtype=None, shape=None, name=None)",
        "snippet": "def constant(value, dtype=None, shape=None, name=None):\n    if dtype is None:\n        dtype = floatx()\n    if shape is None:\n        shape = ()\n    np_value = value * np.ones(shape)\n    np_value.astype(dtype)\n    return np_value",
        "begin_line": 425,
        "end_line": 432,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.print_tensor#435",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.print_tensor(x, message='')",
        "snippet": "def print_tensor(x, message=''):\n    print(x, message)\n    return x",
        "begin_line": 435,
        "end_line": 437,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.eye#440",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.eye(size, dtype=None, name=None)",
        "snippet": "def eye(size, dtype=None, name=None):\n    return np.eye(size, dtype=dtype)",
        "begin_line": 440,
        "end_line": 441,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.dot#444",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.dot(x, y)",
        "snippet": "def dot(x, y):\n    return np.dot(x, y)",
        "begin_line": 444,
        "end_line": 445,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.transpose#448",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.transpose(x)",
        "snippet": "def transpose(x):\n    return np.transpose(x)",
        "begin_line": 448,
        "end_line": 449,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.reverse#452",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.reverse(x, axes)",
        "snippet": "def reverse(x, axes):\n    if isinstance(axes, int):\n        axes = [axes]\n    for a in axes:\n        x = np.flip(x, a)\n    return x",
        "begin_line": 452,
        "end_line": 457,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.variable#460",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.variable(value, dtype=None, name=None, constraint=None)",
        "snippet": "def variable(value, dtype=None, name=None, constraint=None):\n    if constraint is not None:\n        raise TypeError(\"Constraint must be None when \"\n                        \"using the NumPy backend.\")\n    return np.array(value, dtype)",
        "begin_line": 460,
        "end_line": 464,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.greater_equal#479",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.greater_equal(x, y)",
        "snippet": "def greater_equal(x, y):\n    return x >= y",
        "begin_line": 479,
        "end_line": 480,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.random_uniform_variable#499",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)",
        "snippet": "def random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None):\n    return (high - low) * np.random.random(shape).astype(dtype) + low",
        "begin_line": 499,
        "end_line": 500,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.random_normal_variable#503",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)",
        "snippet": "def random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None):\n    return scale * np.random.randn(*shape).astype(dtype) + mean",
        "begin_line": 503,
        "end_line": 504,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.resize_images#507",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.resize_images(x, height_factor, width_factor, data_format)",
        "snippet": "def resize_images(x, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        x = repeat_elements(x, height_factor, axis=2)\n        x = repeat_elements(x, width_factor, axis=3)\n    elif data_format == 'channels_last':\n        x = repeat_elements(x, height_factor, axis=1)\n        x = repeat_elements(x, width_factor, axis=2)\n    return x",
        "begin_line": 507,
        "end_line": 514,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.resize_volumes#517",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.resize_volumes(x, depth_factor, height_factor, width_factor, data_format)",
        "snippet": "def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n    if data_format == 'channels_first':\n        x = repeat_elements(x, depth_factor, axis=2)\n        x = repeat_elements(x, height_factor, axis=3)\n        x = repeat_elements(x, width_factor, axis=4)\n    elif data_format == 'channels_last':\n        x = repeat_elements(x, depth_factor, axis=1)\n        x = repeat_elements(x, height_factor, axis=2)\n        x = repeat_elements(x, width_factor, axis=3)\n    return x",
        "begin_line": 517,
        "end_line": 526,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TerminateOnNaN#38",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TerminateOnNaN()",
        "snippet": "def test_TerminateOnNaN():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    cbks = [callbacks.TerminateOnNaN()]\n    model = Sequential()\n    initializer = initializers.Constant(value=1e5)\n    for _ in range(5):\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu',\n                        kernel_initializer=initializer))\n    model.add(Dense(num_classes, activation='linear'))\n    model.compile(loss='mean_squared_error',\n                  optimizer='rmsprop')\n\n    # case 1 fit\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n    loss = history.history['loss']\n    assert len(loss) == 1\n    assert loss[0] == np.inf\n\n    # case 2 fit_generator\n    def data_generator():\n        max_batch_index = len(X_train) // batch_size\n        i = 0\n        while 1:\n            yield (X_train[i * batch_size: (i + 1) * batch_size],\n                   y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n    history = model.fit_generator(data_generator(),\n                                  len(X_train),\n                                  validation_data=(X_test, y_test),\n                                  callbacks=cbks,\n                                  epochs=20)\n    loss = history.history['loss']\n    assert len(loss) == 1\n    assert loss[0] == np.inf or np.isnan(loss[0])",
        "begin_line": 38,
        "end_line": 81,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#66",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator()",
        "snippet": "    def data_generator():\n        max_batch_index = len(X_train) // batch_size\n        i = 0\n        while 1:\n            yield (X_train[i * batch_size: (i + 1) * batch_size],\n                   y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 66,
        "end_line": 73,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_stop_training_csv#85",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_stop_training_csv(tmpdir)",
        "snippet": "def test_stop_training_csv(tmpdir):\n    np.random.seed(1337)\n    fp = str(tmpdir / 'test.csv')\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    cbks = [callbacks.TerminateOnNaN(), callbacks.CSVLogger(fp)]\n    model = Sequential()\n    for _ in range(5):\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='linear'))\n    model.compile(loss='mean_squared_error',\n                  optimizer='rmsprop')\n\n    def data_generator():\n        i = 0\n        max_batch_index = len(X_train) // batch_size\n        tot = 0\n        while 1:\n            if tot > 3 * len(X_train):\n                yield (np.ones([batch_size, input_dim]) * np.nan,\n                       np.ones([batch_size, num_classes]) * np.nan)\n            else:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            tot += 1\n            i = i % max_batch_index\n\n    history = model.fit_generator(data_generator(),\n                                  len(X_train) // batch_size,\n                                  validation_data=(X_test, y_test),\n                                  callbacks=cbks,\n                                  epochs=20)\n    loss = history.history['loss']\n    assert len(loss) > 1\n    assert loss[-1] == np.inf or np.isnan(loss[-1])\n\n    values = []\n    with open(fp) as f:\n        for x in reader(f):\n            values.append(x)\n\n    assert 'nan' in values[-1], 'The last epoch was not logged.'\n    os.remove(fp)",
        "begin_line": 85,
        "end_line": 134,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#104",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator()",
        "snippet": "    def data_generator():\n        i = 0\n        max_batch_index = len(X_train) // batch_size\n        tot = 0\n        while 1:\n            if tot > 3 * len(X_train):\n                yield (np.ones([batch_size, input_dim]) * np.nan,\n                       np.ones([batch_size, num_classes]) * np.nan)\n            else:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            tot += 1\n            i = i % max_batch_index",
        "begin_line": 104,
        "end_line": 117,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ModelCheckpoint#138",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ModelCheckpoint(tmpdir)",
        "snippet": "def test_ModelCheckpoint(tmpdir):\n    np.random.seed(1337)\n    filepath = str(tmpdir / 'checkpoint.h5')\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    # case 1\n    monitor = 'val_loss'\n    save_best_only = False\n    mode = 'auto'\n\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 2\n    mode = 'min'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 3\n    mode = 'max'\n    monitor = 'val_acc'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 4\n    save_best_only = True\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n    assert os.path.isfile(filepath)\n    os.remove(filepath)\n\n    # case 5\n    save_best_only = False\n    period = 2\n    mode = 'auto'\n    filepath = 'checkpoint.{epoch:02d}.h5'\n    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n                                      save_best_only=save_best_only, mode=mode,\n                                      period=period)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n    assert os.path.isfile(filepath.format(epoch=2))\n    assert os.path.isfile(filepath.format(epoch=4))\n    assert not os.path.exists(filepath.format(epoch=1))\n    assert not os.path.exists(filepath.format(epoch=3))\n    os.remove(filepath.format(epoch=2))\n    os.remove(filepath.format(epoch=4))\n    assert not tmpdir.listdir()",
        "begin_line": 138,
        "end_line": 211,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping#215",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping()",
        "snippet": "def test_EarlyStopping():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    mode = 'max'\n    monitor = 'val_acc'\n    patience = 0\n    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)\n\n    mode = 'auto'\n    monitor = 'val_acc'\n    patience = 2\n    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n    history = model.fit(X_train, y_train, batch_size=batch_size,\n                        validation_data=(X_test, y_test), callbacks=cbks, epochs=20)",
        "begin_line": 215,
        "end_line": 242,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_reuse#246",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_reuse()",
        "snippet": "def test_EarlyStopping_reuse():\n    np.random.seed(1337)\n    patience = 3\n    data = np.random.random((100, 1))\n    labels = np.where(data > 0.5, 1, 0)\n    model = Sequential((\n        Dense(1, input_dim=1, activation='relu'),\n        Dense(1, activation='sigmoid'),\n    ))\n    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n    stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n    weights = model.get_weights()\n\n    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n    assert len(hist.epoch) >= patience\n\n    # This should allow training to go for at least `patience` epochs\n    model.set_weights(weights)\n    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n    assert len(hist.epoch) >= patience",
        "begin_line": 246,
        "end_line": 265,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_patience#269",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_patience()",
        "snippet": "def test_EarlyStopping_patience():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n        def get_weights(self):\n            return []\n\n        def set_weights(self, weights):\n            pass\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n\n    # Should stop after epoch 3,\n    # as the loss has not improved after patience=2 epochs.\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    assert epochs_trained == 3",
        "begin_line": 269,
        "end_line": 297,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_patience#269",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_patience()",
        "snippet": "def test_EarlyStopping_patience():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n        def get_weights(self):\n            return []\n\n        def set_weights(self, weights):\n            pass\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n\n    # Should stop after epoch 3,\n    # as the loss has not improved after patience=2 epochs.\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    assert epochs_trained == 3",
        "begin_line": 269,
        "end_line": 297,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#271",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False",
        "begin_line": 271,
        "end_line": 272,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.get_weights#274",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.get_weights(self)",
        "snippet": "        def get_weights(self):\n            return []",
        "begin_line": 274,
        "end_line": 275,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weights#277",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weights(self, weights)",
        "snippet": "        def set_weights(self, weights):\n            pass",
        "begin_line": 277,
        "end_line": 278,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_baseline#301",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_baseline()",
        "snippet": "def test_EarlyStopping_baseline():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n        def get_weights(self):\n            return []\n\n        def set_weights(self, weights):\n            pass\n\n    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75,\n                                             patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained\n\n    acc_levels = [0.55, 0.76, 0.81, 0.81]\n    baseline_met = baseline_tester(acc_levels)\n    acc_levels = [0.55, 0.74, 0.81, 0.81]\n    baseline_not_met = baseline_tester(acc_levels)\n\n    # All epochs should run because baseline was met in second epoch\n    assert baseline_met == 4\n    # Baseline was not met by second epoch and should stop\n    assert baseline_not_met == 2",
        "begin_line": 301,
        "end_line": 333,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_baseline#301",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_baseline()",
        "snippet": "def test_EarlyStopping_baseline():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n\n        def get_weights(self):\n            return []\n\n        def set_weights(self, weights):\n            pass\n\n    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75,\n                                             patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained\n\n    acc_levels = [0.55, 0.76, 0.81, 0.81]\n    baseline_met = baseline_tester(acc_levels)\n    acc_levels = [0.55, 0.74, 0.81, 0.81]\n    baseline_not_met = baseline_tester(acc_levels)\n\n    # All epochs should run because baseline was met in second epoch\n    assert baseline_met == 4\n    # Baseline was not met by second epoch and should stop\n    assert baseline_not_met == 2",
        "begin_line": 301,
        "end_line": 333,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#303",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False",
        "begin_line": 303,
        "end_line": 304,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.get_weights#306",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.get_weights(self)",
        "snippet": "        def get_weights(self):\n            return []",
        "begin_line": 306,
        "end_line": 307,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weights#309",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weights(self, weights)",
        "snippet": "        def set_weights(self, weights):\n            pass",
        "begin_line": 309,
        "end_line": 310,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.baseline_tester#312",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.baseline_tester(acc_levels)",
        "snippet": "    def baseline_tester(acc_levels):\n        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75,\n                                             patience=2)\n        early_stop.model = DummyModel()\n        epochs_trained = 0\n        early_stop.on_train_begin()\n        for epoch in range(len(acc_levels)):\n            epochs_trained += 1\n            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n            if early_stop.model.stop_training:\n                break\n        return epochs_trained",
        "begin_line": 312,
        "end_line": 323,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_final_weights#337",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_final_weights()",
        "snippet": "def test_EarlyStopping_final_weights():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n            self.weights = -1\n\n        def get_weights(self):\n            return self.weights\n\n        def set_weights(self, weights):\n            self.weights = weights\n\n        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.model.set_weight_to_epoch(epoch=epoch)\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    # The best configuration is in the epoch 2 (loss = 0.1000),\n    # so with patience=2 we need to end up at epoch 4\n    assert early_stop.model.get_weights() == 4",
        "begin_line": 337,
        "end_line": 370,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_final_weights#337",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_final_weights()",
        "snippet": "def test_EarlyStopping_final_weights():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n            self.weights = -1\n\n        def get_weights(self):\n            return self.weights\n\n        def set_weights(self, weights):\n            self.weights = weights\n\n        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n    early_stop.model = DummyModel()\n\n    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.model.set_weight_to_epoch(epoch=epoch)\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    # The best configuration is in the epoch 2 (loss = 0.1000),\n    # so with patience=2 we need to end up at epoch 4\n    assert early_stop.model.get_weights() == 4",
        "begin_line": 337,
        "end_line": 370,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#339",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False\n            self.weights = -1",
        "begin_line": 339,
        "end_line": 341,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.get_weights#343",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.get_weights(self)",
        "snippet": "        def get_weights(self):\n            return self.weights",
        "begin_line": 343,
        "end_line": 344,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weights#346",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weights(self, weights)",
        "snippet": "        def set_weights(self, weights):\n            self.weights = weights",
        "begin_line": 346,
        "end_line": 347,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weight_to_epoch#349",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weight_to_epoch(self, epoch)",
        "snippet": "        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch",
        "begin_line": 349,
        "end_line": 350,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_EarlyStopping_final_weights_when_restoring_model_weights#374",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_EarlyStopping_final_weights_when_restoring_model_weights()",
        "snippet": "def test_EarlyStopping_final_weights_when_restoring_model_weights():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n            self.weights = -1\n\n        def get_weights(self):\n            return self.weights\n\n        def set_weights(self, weights):\n            self.weights = weights\n\n        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2,\n                                         restore_best_weights=True)\n    early_stop.model = DummyModel()\n\n    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n\n    # The best configuration is in the epoch 2 (loss = 0.1000).\n\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.model.set_weight_to_epoch(epoch=epoch)\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    # The best configuration is in epoch 2 (loss = 0.1000),\n    # and while patience = 2, we're restoring the best weights,\n    # so we end up at the epoch with the best weights, i.e. epoch 2\n    assert early_stop.model.get_weights() == 2",
        "begin_line": 374,
        "end_line": 411,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_final_weights_when_restoring_model_weights#374",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_EarlyStopping_final_weights_when_restoring_model_weights()",
        "snippet": "def test_EarlyStopping_final_weights_when_restoring_model_weights():\n    class DummyModel(object):\n        def __init__(self):\n            self.stop_training = False\n            self.weights = -1\n\n        def get_weights(self):\n            return self.weights\n\n        def set_weights(self, weights):\n            self.weights = weights\n\n        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch\n\n    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2,\n                                         restore_best_weights=True)\n    early_stop.model = DummyModel()\n\n    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n\n    # The best configuration is in the epoch 2 (loss = 0.1000).\n\n    epochs_trained = 0\n    early_stop.on_train_begin()\n\n    for epoch in range(len(losses)):\n        epochs_trained += 1\n        early_stop.model.set_weight_to_epoch(epoch=epoch)\n        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n\n        if early_stop.model.stop_training:\n            break\n\n    # The best configuration is in epoch 2 (loss = 0.1000),\n    # and while patience = 2, we're restoring the best weights,\n    # so we end up at the epoch with the best weights, i.e. epoch 2\n    assert early_stop.model.get_weights() == 2",
        "begin_line": 374,
        "end_line": 411,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#376",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.stop_training = False\n            self.weights = -1",
        "begin_line": 376,
        "end_line": 378,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.get_weights#380",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.get_weights(self)",
        "snippet": "        def get_weights(self):\n            return self.weights",
        "begin_line": 380,
        "end_line": 381,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weights#383",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weights(self, weights)",
        "snippet": "        def set_weights(self, weights):\n            self.weights = weights",
        "begin_line": 383,
        "end_line": 384,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.set_weight_to_epoch#386",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.set_weight_to_epoch(self, epoch)",
        "snippet": "        def set_weight_to_epoch(self, epoch):\n            self.weights = epoch",
        "begin_line": 386,
        "end_line": 387,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_LearningRateScheduler#415",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_LearningRateScheduler()",
        "snippet": "def test_LearningRateScheduler():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n    assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()",
        "begin_line": 415,
        "end_line": 434,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau#438",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau()",
        "snippet": "def test_ReduceLROnPlateau():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model\n\n    model = make_model()\n\n    # This should reduce the LR after the first epoch (due to high epsilon).\n    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                        min_delta=10, patience=1, cooldown=5)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n    assert_allclose(float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n\n    model = make_model()\n    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                        min_delta=0, patience=1, cooldown=5)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5, verbose=2)\n    assert_allclose(float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())",
        "begin_line": 438,
        "end_line": 473,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.make_model#448",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.make_model()",
        "snippet": "    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model",
        "begin_line": 448,
        "end_line": 457,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau_patience#477",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 477,
        "end_line": 498,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyOptimizer.test_ReduceLROnPlateau_patience#477",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyOptimizer",
        "signature": "tests.keras.test_callbacks.DummyOptimizer.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 477,
        "end_line": 498,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyOptimizer.__init__#479",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyOptimizer",
        "signature": "tests.keras.test_callbacks.DummyOptimizer.__init__(self)",
        "snippet": "        def __init__(self):\n            self.lr = K.variable(1.0)",
        "begin_line": 479,
        "end_line": 480,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.test_ReduceLROnPlateau_patience#477",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.test_ReduceLROnPlateau_patience()",
        "snippet": "def test_ReduceLROnPlateau_patience():\n    class DummyOptimizer(object):\n        def __init__(self):\n            self.lr = K.variable(1.0)\n\n    class DummyModel(object):\n        def __init__(self):\n            self.optimizer = DummyOptimizer()\n\n    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                    patience=2)\n    reduce_on_plateau.model = DummyModel()\n\n    losses = [0.0860, 0.1096, 0.1040]\n    lrs = []\n\n    for epoch in range(len(losses)):\n        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n\n    # The learning rates should be 1.0 except the last one\n    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0",
        "begin_line": 477,
        "end_line": 498,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyModel.__init__#483",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyModel",
        "signature": "tests.keras.test_callbacks.DummyModel.__init__(self)",
        "snippet": "        def __init__(self):\n            self.optimizer = DummyOptimizer()",
        "begin_line": 483,
        "end_line": 484,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_ReduceLROnPlateau_backwards_compatibility#502",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_ReduceLROnPlateau_backwards_compatibility()",
        "snippet": "def test_ReduceLROnPlateau_backwards_compatibility():\n    import warnings\n    with warnings.catch_warnings(record=True) as ws:\n        reduce_on_plateau = callbacks.ReduceLROnPlateau(epsilon=1e-13)\n        # Check if warnings are disabled\n        if os.environ.get(\"PYTHONWARNINGS\") != \"ignore\":\n            assert \"`epsilon` argument is deprecated\" in str(ws[0].message)\n    assert not hasattr(reduce_on_plateau, 'epsilon')\n    assert hasattr(reduce_on_plateau, 'min_delta')\n    assert reduce_on_plateau.min_delta == 1e-13",
        "begin_line": 502,
        "end_line": 511,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_CSVLogger#515",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_CSVLogger(tmpdir)",
        "snippet": "def test_CSVLogger(tmpdir):\n    np.random.seed(1337)\n    filepath = str(tmpdir / 'log.tsv')\n    sep = '\\t'\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model\n\n    # case 1, create new file with defined separator\n    model = make_model()\n    cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n\n    assert os.path.isfile(filepath)\n    with open(filepath) as csvfile:\n        dialect = Sniffer().sniff(csvfile.read())\n    assert dialect.delimiter == sep\n    del model\n    del cbks\n\n    # case 2, append data to existing file, skip header\n    model = make_model()\n    cbks = [callbacks.CSVLogger(filepath, separator=sep, append=True)]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n\n    # case 3, reuse of CSVLogger object\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n\n    import re\n    with open(filepath) as csvfile:\n        list_lines = csvfile.readlines()\n        for line in list_lines:\n            assert line.count(sep) == 4\n        assert len(list_lines) == 5\n        output = \" \".join(list_lines)\n        assert len(re.findall('epoch', output)) == 1\n\n    os.remove(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 515,
        "end_line": 571,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.make_model#527",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.make_model()",
        "snippet": "    def make_model():\n        np.random.seed(1337)\n        model = Sequential()\n        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n        model.add(Dense(num_classes, activation='softmax'))\n\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizers.SGD(lr=0.1),\n                      metrics=['accuracy'])\n        return model",
        "begin_line": 527,
        "end_line": 536,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard#575",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard(tmpdir)",
        "snippet": "def test_TensorBoard(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim,),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    class DummyStatefulMetric(Layer):\n\n        def __init__(self, name='dummy_stateful_metric', **kwargs):\n            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.state = K.variable(value=0, dtype='int32')\n\n        def reset_states(self):\n            pass\n\n        def __call__(self, y_true, y_pred):\n            return self.state\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy', DummyStatefulMetric()])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=X_test,\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit(X_train, y_train, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n              epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test),\n              callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0,\n                                                    embeddings_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=(X_test, y_test),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 575,
        "end_line": 659,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#588",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 588,
        "end_line": 603,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyStatefulMetric.test_TensorBoard#575",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyStatefulMetric",
        "signature": "tests.keras.test_callbacks.DummyStatefulMetric.test_TensorBoard(tmpdir)",
        "snippet": "def test_TensorBoard(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim,),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    class DummyStatefulMetric(Layer):\n\n        def __init__(self, name='dummy_stateful_metric', **kwargs):\n            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.state = K.variable(value=0, dtype='int32')\n\n        def reset_states(self):\n            pass\n\n        def __call__(self, y_true, y_pred):\n            return self.state\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy', DummyStatefulMetric()])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=X_test,\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit(X_train, y_train, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n              epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test),\n              callbacks=callbacks_factory(histogram_freq=0), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0,\n                                                    embeddings_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=(X_test, y_test),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 575,
        "end_line": 659,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyStatefulMetric.__init__#607",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyStatefulMetric",
        "signature": "tests.keras.test_callbacks.DummyStatefulMetric.__init__(self, name='dummy_stateful_metric', **kwargs)",
        "snippet": "        def __init__(self, name='dummy_stateful_metric', **kwargs):\n            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.state = K.variable(value=0, dtype='int32')",
        "begin_line": 607,
        "end_line": 610,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyStatefulMetric.reset_states#612",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyStatefulMetric",
        "signature": "tests.keras.test_callbacks.DummyStatefulMetric.reset_states(self)",
        "snippet": "        def reset_states(self):\n            pass",
        "begin_line": 612,
        "end_line": 613,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.DummyStatefulMetric.__call__#615",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.DummyStatefulMetric",
        "signature": "tests.keras.test_callbacks.DummyStatefulMetric.__call__(self, y_true, y_pred)",
        "snippet": "        def __call__(self, y_true, y_pred):\n            return self.state",
        "begin_line": 615,
        "end_line": 616,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#628",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq, embeddings_freq=1)",
        "snippet": "    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=X_test,\n                                      batch_size=5)]",
        "begin_line": 628,
        "end_line": 635,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_histogram_freq_must_have_validation_data#665",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir)",
        "snippet": "def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim,),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    inp = Input((input_dim,))\n    hidden = Dense(num_hidden, activation='relu')(inp)\n    hidden = Dropout(0.1)(hidden)\n    output = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=inp, outputs=output)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=X_test,\n                                      batch_size=5)]\n\n    # fit without validation data should raise ValueError if histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  callbacks=callbacks_factory(histogram_freq=1), epochs=3)\n    assert 'validation_data must be provided' in str(raised_exception.value)\n\n    # fit generator without validation data should raise ValueError if\n    # histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                            callbacks=callbacks_factory(histogram_freq=1))\n    assert 'validation_data must be provided' in str(raised_exception.value)\n\n    # fit generator with validation data generator should raise ValueError if\n    # histogram_freq > 0\n    with pytest.raises(ValueError) as raised_exception:\n        model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                            validation_data=data_generator(False),\n                            validation_steps=1,\n                            callbacks=callbacks_factory(histogram_freq=1))\n    assert 'validation_data must be provided' in str(raised_exception.value)",
        "begin_line": 665,
        "end_line": 734,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#678",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 678,
        "end_line": 693,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#705",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq, embeddings_freq=1)",
        "snippet": "    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=X_test,\n                                      batch_size=5)]",
        "begin_line": 705,
        "end_line": 712,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_multi_input_output#738",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_multi_input_output(tmpdir)",
        "snippet": "def test_TensorBoard_multi_input_output(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(\n        num_train=train_samples,\n        num_test=test_samples,\n        input_shape=(input_dim, input_dim),\n        classification=True,\n        num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n            else:\n                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n            i += 1\n            i = i % max_batch_index\n\n    inp1 = Input((input_dim, input_dim))\n    inp2 = Input((input_dim, input_dim))\n    inp_3d = add([inp1, inp2])\n    inp_2d = GlobalAveragePooling1D()(inp_3d)\n    # test a layer with a list of output tensors\n    inp_pair = Lambda(lambda x: x)([inp_3d, inp_2d])\n    hidden = dot(inp_pair, axes=-1)\n    hidden = Dense(num_hidden, activation='relu')(hidden)\n    hidden = Dropout(0.1)(hidden)\n    output1 = Dense(num_classes, activation='softmax')(hidden)\n    output2 = Dense(num_classes, activation='softmax')(hidden)\n    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # we must generate new callbacks for each test, as they aren't stateless\n    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=[X_test] * 2,\n                                      batch_size=5)]\n\n    # fit without validation data\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              callbacks=callbacks_factory(histogram_freq=0, embeddings_freq=0),\n              epochs=3)\n\n    # fit with validation data and accuracy\n    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n              validation_data=([X_test] * 2, [y_test] * 2),\n              callbacks=callbacks_factory(histogram_freq=1), epochs=2)\n\n    # fit generator without validation data\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        callbacks=callbacks_factory(histogram_freq=0,\n                                                    embeddings_freq=0))\n\n    # fit generator with validation data and accuracy\n    model.fit_generator(data_generator(True), len(X_train), epochs=2,\n                        validation_data=([X_test] * 2, [y_test] * 2),\n                        callbacks=callbacks_factory(histogram_freq=1))\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 738,
        "end_line": 816,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#751",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                # simulate multi-input/output models\n                yield ([X_train[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_train[i * batch_size: (i + 1) * batch_size]] * 2)\n            else:\n                yield ([X_test[i * batch_size: (i + 1) * batch_size]] * 2,\n                       [y_test[i * batch_size: (i + 1) * batch_size]] * 2)\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 751,
        "end_line": 766,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.callbacks_factory#785",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.callbacks_factory(histogram_freq, embeddings_freq=1)",
        "snippet": "    def callbacks_factory(histogram_freq, embeddings_freq=1):\n        return [callbacks.TensorBoard(log_dir=filepath,\n                                      histogram_freq=histogram_freq,\n                                      write_images=True, write_grads=True,\n                                      embeddings_freq=embeddings_freq,\n                                      embeddings_layer_names=['dense_1'],\n                                      embeddings_data=[X_test] * 2,\n                                      batch_size=5)]",
        "begin_line": 785,
        "end_line": 792,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_convnet#820",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_convnet(tmpdir)",
        "snippet": "def test_TensorBoard_convnet(tmpdir):\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    input_shape = (16, 16, 3)\n    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n                                                         num_test=200,\n                                                         input_shape=input_shape,\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_train = np_utils.to_categorical(y_train)\n    y_test = np_utils.to_categorical(y_test)\n\n    model = Sequential([\n        Conv2D(filters=8, kernel_size=3,\n               activation='relu',\n               input_shape=input_shape),\n        MaxPooling2D(pool_size=2),\n        Conv2D(filters=4, kernel_size=(3, 3),\n               activation='relu', padding='same'),\n        GlobalAveragePooling2D(),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1,\n                                write_images=True, write_grads=True,\n                                batch_size=16)\n    cbks = [tsb]\n    model.summary()\n    history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n                        validation_data=(x_test, y_test),\n                        callbacks=cbks,\n                        verbose=0)\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 820,
        "end_line": 857,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_display_float_from_logs#861",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_display_float_from_logs(tmpdir)",
        "snippet": "def test_TensorBoard_display_float_from_logs(tmpdir):\n    filepath = str(tmpdir / 'logs')\n\n    input_shape = (3,)\n    (x_train, y_train), _ = get_test_data(num_train=10,\n                                          num_test=0,\n                                          input_shape=input_shape,\n                                          classification=True,\n                                          num_classes=num_classes)\n    y_train = np_utils.to_categorical(y_train)\n\n    model = Sequential([\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop')\n\n    class CustomCallback(callbacks.Callback):\n\n        def on_epoch_end(self, epoch, logs=None):\n            logs['test'] = 0.\n\n    tsb = callbacks.TensorBoard(log_dir=filepath,\n                                batch_size=16)\n    cbks = [CustomCallback(), tsb]\n    model.fit(x_train, y_train, epochs=2, batch_size=16,\n              callbacks=cbks,\n              verbose=0)\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 861,
        "end_line": 891,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.CustomCallback.test_TensorBoard_display_float_from_logs#861",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.CustomCallback",
        "signature": "tests.keras.test_callbacks.CustomCallback.test_TensorBoard_display_float_from_logs(tmpdir)",
        "snippet": "def test_TensorBoard_display_float_from_logs(tmpdir):\n    filepath = str(tmpdir / 'logs')\n\n    input_shape = (3,)\n    (x_train, y_train), _ = get_test_data(num_train=10,\n                                          num_test=0,\n                                          input_shape=input_shape,\n                                          classification=True,\n                                          num_classes=num_classes)\n    y_train = np_utils.to_categorical(y_train)\n\n    model = Sequential([\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop')\n\n    class CustomCallback(callbacks.Callback):\n\n        def on_epoch_end(self, epoch, logs=None):\n            logs['test'] = 0.\n\n    tsb = callbacks.TensorBoard(log_dir=filepath,\n                                batch_size=16)\n    cbks = [CustomCallback(), tsb]\n    model.fit(x_train, y_train, epochs=2, batch_size=16,\n              callbacks=cbks,\n              verbose=0)\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 861,
        "end_line": 891,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.CustomCallback.on_epoch_end#880",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks.CustomCallback",
        "signature": "tests.keras.test_callbacks.CustomCallback.on_epoch_end(self, epoch, logs=None)",
        "snippet": "        def on_epoch_end(self, epoch, logs=None):\n            logs['test'] = 0.",
        "begin_line": 880,
        "end_line": 881,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_CallbackValData#895",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_CallbackValData()",
        "snippet": "def test_CallbackValData():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbk = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=[cbk], epochs=1)\n\n    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index\n\n    cbk2 = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n    model.fit_generator(data_generator(True), len(X_train), epochs=1,\n                        validation_data=(X_test, y_test),\n                        callbacks=[cbk2])\n\n    # callback validation data should always have x, y, and sample weights\n    assert len(cbk.validation_data) == len(cbk2.validation_data) == 3\n    assert cbk.validation_data[0] is cbk2.validation_data[0]\n    assert cbk.validation_data[1] is cbk2.validation_data[1]\n    assert cbk.validation_data[2].shape == cbk2.validation_data[2].shape",
        "begin_line": 895,
        "end_line": 940,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.data_generator#915",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.data_generator(train)",
        "snippet": "    def data_generator(train):\n        if train:\n            max_batch_index = len(X_train) // batch_size\n        else:\n            max_batch_index = len(X_test) // batch_size\n        i = 0\n        while 1:\n            if train:\n                yield (X_train[i * batch_size: (i + 1) * batch_size],\n                       y_train[i * batch_size: (i + 1) * batch_size])\n            else:\n                yield (X_test[i * batch_size: (i + 1) * batch_size],\n                       y_test[i * batch_size: (i + 1) * batch_size])\n            i += 1\n            i = i % max_batch_index",
        "begin_line": 915,
        "end_line": 929,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_LambdaCallback#944",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_LambdaCallback()",
        "snippet": "def test_LambdaCallback():\n    np.random.seed(1337)\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    # Start an arbitrary process that should run during model training and\n    # be terminated after training has completed.\n    def f():\n        while True:\n            pass\n\n    p = multiprocessing.Process(target=f)\n    p.start()\n    cleanup_callback = callbacks.LambdaCallback(\n        on_train_end=lambda logs: p.terminate())\n\n    cbks = [cleanup_callback]\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n    p.join()\n    assert not p.is_alive()",
        "begin_line": 944,
        "end_line": 975,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.f#962",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.f()",
        "snippet": "    def f():\n        while True:\n            pass",
        "begin_line": 962,
        "end_line": 964,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.test_TensorBoard_with_ReduceLROnPlateau#979",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.test_TensorBoard_with_ReduceLROnPlateau(tmpdir)",
        "snippet": "def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n    import shutil\n    np.random.seed(np.random.randint(1, 1e7))\n    filepath = str(tmpdir / 'logs')\n\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='sgd',\n                  metrics=['accuracy'])\n\n    cbks = [\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=4,\n            verbose=1),\n        callbacks.TensorBoard(\n            log_dir=filepath)]\n\n    model.fit(X_train, y_train, batch_size=batch_size,\n              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n\n    assert os.path.isdir(filepath)\n    shutil.rmtree(filepath)\n    assert not tmpdir.listdir()",
        "begin_line": 979,
        "end_line": 1013,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.tests_RemoteMonitor#1017",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.tests_RemoteMonitor()",
        "snippet": "def tests_RemoteMonitor():\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    cbks = [callbacks.RemoteMonitor()]\n\n    with patch('requests.post'):\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)",
        "begin_line": 1017,
        "end_line": 1035,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.test_callbacks.tests_RemoteMonitorWithJsonPayload#1039",
        "src_path": "tests/keras/test_callbacks.py",
        "class_name": "tests.keras.test_callbacks",
        "signature": "tests.keras.test_callbacks.tests_RemoteMonitorWithJsonPayload()",
        "snippet": "def tests_RemoteMonitorWithJsonPayload():\n    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n                                                         num_test=test_samples,\n                                                         input_shape=(input_dim,),\n                                                         classification=True,\n                                                         num_classes=num_classes)\n    y_test = np_utils.to_categorical(y_test)\n    y_train = np_utils.to_categorical(y_train)\n    model = Sequential()\n    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    cbks = [callbacks.RemoteMonitor(send_as_json=True)]\n\n    with patch('requests.post'):\n        model.fit(X_train, y_train, batch_size=batch_size,\n                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)",
        "begin_line": 1039,
        "end_line": 1057,
        "comment": "",
        "is_bug": false
    }
]