[
    {
        "name": "tests.keras.legacy.conftest.clear_session_after_test#6",
        "src_path": "tests/keras/legacy/conftest.py",
        "class_name": "tests.keras.legacy.conftest",
        "signature": "tests.keras.legacy.conftest.clear_session_after_test()",
        "snippet": "def clear_session_after_test():\n    \"\"\"This wrapper runs for all the tests in the legacy directory (recursively).\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', message=r'(.+) Keras 2 ',\n                                category=UserWarning)\n        yield",
        "begin_line": 6,
        "end_line": 12,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.conftest.clear_session_after_test#6",
        "src_path": "tests/conftest.py",
        "class_name": "tests.conftest",
        "signature": "tests.conftest.clear_session_after_test()",
        "snippet": "def clear_session_after_test():\n    \"\"\"Test wrapper to clean up after TensorFlow and CNTK tests.\n\n    This wrapper runs for all the tests in the keras test suite.\n    \"\"\"\n    yield\n    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n        K.clear_session()",
        "begin_line": 6,
        "end_line": 13,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.get_test_data#16",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.get_test_data()",
        "snippet": "def get_test_data():\n    np.random.seed(1337)\n    (x_train, y_train), _ = test_utils.get_test_data(num_train=1000,\n                                                     num_test=200,\n                                                     input_shape=(10,),\n                                                     classification=True,\n                                                     num_classes=num_classes)\n    y_train = to_categorical(y_train)\n    return x_train, y_train",
        "begin_line": 16,
        "end_line": 24,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test._test_optimizer#27",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test._test_optimizer(optimizer, target=0.75)",
        "snippet": "def _test_optimizer(optimizer, target=0.75):\n    x_train, y_train = get_test_data()\n\n    model = Sequential()\n    model.add(Dense(10, input_shape=(x_train.shape[1],)))\n    model.add(Activation('relu'))\n    model.add(Dense(y_train.shape[1]))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n\n    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n    assert history.history['acc'][-1] >= target\n    config = optimizers.serialize(optimizer)\n    optim = optimizers.deserialize(config)\n    new_config = optimizers.serialize(optim)\n    new_config['class_name'] = new_config['class_name'].lower()\n    assert config == new_config\n\n    # Test constraints.\n    model = Sequential()\n    dense = Dense(10,\n                  input_shape=(x_train.shape[1],),\n                  kernel_constraint=lambda x: 0. * x + 1.,\n                  bias_constraint=lambda x: 0. * x + 2.,)\n    model.add(dense)\n    model.add(Activation('relu'))\n    model.add(Dense(y_train.shape[1]))\n    model.add(Activation('softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    model.train_on_batch(x_train[:10], y_train[:10])\n    kernel, bias = dense.get_weights()\n    assert_allclose(kernel, 1.)\n    assert_allclose(bias, 2.)",
        "begin_line": 27,
        "end_line": 63,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_no_grad#69",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_no_grad()",
        "snippet": "def test_no_grad():\n    inp = Input([3])\n    x = Dense(10)(inp)\n    x = Lambda(lambda l: 1.0 * K.reshape(K.cast(K.argmax(l), 'float32'), [-1, 1]),\n               output_shape=lambda x: [x[0], 1])(x)\n    mod = Model(inp, x)\n    mod.compile('sgd', 'mse')\n    with pytest.raises(ValueError):\n        mod.fit(np.zeros([10, 3]), np.zeros([10, 1], np.float32),\n                batch_size=10, epochs=10)",
        "begin_line": 69,
        "end_line": 78,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_sgd#81",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_sgd()",
        "snippet": "def test_sgd():\n    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n    _test_optimizer(sgd)",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_rmsprop#86",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_rmsprop()",
        "snippet": "def test_rmsprop():\n    _test_optimizer(optimizers.RMSprop())\n    _test_optimizer(optimizers.RMSprop(decay=1e-3))",
        "begin_line": 86,
        "end_line": 88,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_adagrad#91",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_adagrad()",
        "snippet": "def test_adagrad():\n    _test_optimizer(optimizers.Adagrad())\n    _test_optimizer(optimizers.Adagrad(decay=1e-3))",
        "begin_line": 91,
        "end_line": 93,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_adadelta#96",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_adadelta()",
        "snippet": "def test_adadelta():\n    _test_optimizer(optimizers.Adadelta(), target=0.6)\n    _test_optimizer(optimizers.Adadelta(decay=1e-3), target=0.6)",
        "begin_line": 96,
        "end_line": 98,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_adam#101",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_adam()",
        "snippet": "def test_adam():\n    _test_optimizer(optimizers.Adam())\n    _test_optimizer(optimizers.Adam(decay=1e-3))",
        "begin_line": 101,
        "end_line": 103,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_adamax#106",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_adamax()",
        "snippet": "def test_adamax():\n    _test_optimizer(optimizers.Adamax())\n    _test_optimizer(optimizers.Adamax(decay=1e-3))",
        "begin_line": 106,
        "end_line": 108,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_nadam#111",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_nadam()",
        "snippet": "def test_nadam():\n    _test_optimizer(optimizers.Nadam())",
        "begin_line": 111,
        "end_line": 112,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_adam_amsgrad#115",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_adam_amsgrad()",
        "snippet": "def test_adam_amsgrad():\n    _test_optimizer(optimizers.Adam(amsgrad=True))\n    _test_optimizer(optimizers.Adam(amsgrad=True, decay=1e-3))",
        "begin_line": 115,
        "end_line": 117,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_clipnorm#120",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_clipnorm()",
        "snippet": "def test_clipnorm():\n    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n    _test_optimizer(sgd)",
        "begin_line": 120,
        "end_line": 122,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_clipvalue#125",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_clipvalue()",
        "snippet": "def test_clipvalue():\n    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n    _test_optimizer(sgd)",
        "begin_line": 125,
        "end_line": 127,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_tfoptimizer#132",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_tfoptimizer()",
        "snippet": "def test_tfoptimizer():\n    from keras import constraints\n    from tensorflow import train\n    optimizer = optimizers.TFOptimizer(train.AdamOptimizer())\n    model = Sequential()\n    model.add(Dense(num_classes, input_shape=(3,),\n                    kernel_constraint=constraints.MaxNorm(1)))\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n              epochs=1, batch_size=5, verbose=0)\n    # not supported\n    with pytest.raises(NotImplementedError):\n        optimizer.weights\n    with pytest.raises(NotImplementedError):\n        optimizer.get_config()\n    with pytest.raises(NotImplementedError):\n        optimizer.from_config(None)",
        "begin_line": 132,
        "end_line": 148,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer#153",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test",
        "signature": "tests.keras.optimizers_test.test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer()",
        "snippet": "def test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer():\n    from keras import constraints\n    from tensorflow import train\n\n    class MyTfOptimizer(train.Optimizer):\n        wrapping_optimizer = train.AdamOptimizer()\n\n        def compute_gradients(self, loss, **kwargs):\n            return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)\n\n        def apply_gradients(self, grads_and_vars, **kwargs):\n            return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n                                                           **kwargs)\n    my_tf_optimizer = MyTfOptimizer(use_locking=False, name='MyTfOptimizer')\n    optimizer = optimizers.TFOptimizer(my_tf_optimizer)\n    model = Sequential()\n    model.add(Dense(num_classes, input_shape=(3,),\n                    kernel_constraint=constraints.MaxNorm(1)))\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n              epochs=1, batch_size=5, verbose=0)",
        "begin_line": 153,
        "end_line": 173,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.MyTfOptimizer.test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer#153",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test.MyTfOptimizer",
        "signature": "tests.keras.optimizers_test.MyTfOptimizer.test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer()",
        "snippet": "def test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer():\n    from keras import constraints\n    from tensorflow import train\n\n    class MyTfOptimizer(train.Optimizer):\n        wrapping_optimizer = train.AdamOptimizer()\n\n        def compute_gradients(self, loss, **kwargs):\n            return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)\n\n        def apply_gradients(self, grads_and_vars, **kwargs):\n            return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n                                                           **kwargs)\n    my_tf_optimizer = MyTfOptimizer(use_locking=False, name='MyTfOptimizer')\n    optimizer = optimizers.TFOptimizer(my_tf_optimizer)\n    model = Sequential()\n    model.add(Dense(num_classes, input_shape=(3,),\n                    kernel_constraint=constraints.MaxNorm(1)))\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n              epochs=1, batch_size=5, verbose=0)",
        "begin_line": 153,
        "end_line": 173,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.MyTfOptimizer.compute_gradients#160",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test.MyTfOptimizer",
        "signature": "tests.keras.optimizers_test.MyTfOptimizer.compute_gradients(self, loss, **kwargs)",
        "snippet": "        def compute_gradients(self, loss, **kwargs):\n            return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)",
        "begin_line": 160,
        "end_line": 161,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.optimizers_test.MyTfOptimizer.apply_gradients#163",
        "src_path": "tests/keras/optimizers_test.py",
        "class_name": "tests.keras.optimizers_test.MyTfOptimizer",
        "signature": "tests.keras.optimizers_test.MyTfOptimizer.apply_gradients(self, grads_and_vars, **kwargs)",
        "snippet": "        def apply_gradients(self, grads_and_vars, **kwargs):\n            return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n                                                           **kwargs)",
        "begin_line": 163,
        "end_line": 165,
        "comment": "",
        "is_bug": false
    }
]