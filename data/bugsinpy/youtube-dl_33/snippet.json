[
    {
        "name": "youtube_dl.extractor.youporn.YouPornIE._real_extract#37",
        "src_path": "youtube_dl/extractor/youporn.py",
        "class_name": "youtube_dl.extractor.youporn.YouPornIE",
        "signature": "youtube_dl.extractor.youporn.YouPornIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('videoid')\n        url = mobj.group('proto') + 'www.' + mobj.group('url')\n\n        req = compat_urllib_request.Request(url)\n        req.add_header('Cookie', 'age_verified=1')\n        webpage = self._download_webpage(req, video_id)\n        age_limit = self._rta_search(webpage)\n\n        # Get JSON parameters\n        json_params = self._search_regex(r'var currentVideo = new Video\\((.*)\\);', webpage, 'JSON parameters')\n        try:\n            params = json.loads(json_params)\n        except:\n            raise ExtractorError(u'Invalid JSON')\n\n        self.report_extraction(video_id)\n        try:\n            video_title = params['title']\n            upload_date = unified_strdate(params['release_date_f'])\n            video_description = params['description']\n            video_uploader = params['submitted_by']\n            thumbnail = params['thumbnails'][0]['image']\n        except KeyError:\n            raise ExtractorError('Missing JSON parameter: ' + sys.exc_info()[1])\n\n        # Get all of the links from the page\n        DOWNLOAD_LIST_RE = r'(?s)<ul class=\"downloadList\">(?P<download_list>.*?)</ul>'\n        download_list_html = self._search_regex(DOWNLOAD_LIST_RE,\n            webpage, 'download list').strip()\n        LINK_RE = r'<a href=\"([^\"]+)\">'\n        links = re.findall(LINK_RE, download_list_html)\n\n        # Get all encrypted links\n        encrypted_links = re.findall(r'var encryptedQuality[0-9]{3}URL = \\'([a-zA-Z0-9+/]+={0,2})\\';', webpage)\n        for encrypted_link in encrypted_links:\n            link = aes_decrypt_text(encrypted_link, video_title, 32).decode('utf-8')\n            links.append(link)\n        \n        formats = []\n        for link in links:\n            # A link looks like this:\n            # http://cdn1.download.youporn.phncdn.com/201210/31/8004515/480p_370k_8004515/YouPorn%20-%20Nubile%20Films%20The%20Pillow%20Fight.mp4?nvb=20121113051249&nva=20121114051249&ir=1200&sr=1200&hash=014b882080310e95fb6a0\n            # A path looks like this:\n            # /201210/31/8004515/480p_370k_8004515/YouPorn%20-%20Nubile%20Films%20The%20Pillow%20Fight.mp4\n            video_url = unescapeHTML(link)\n            path = compat_urllib_parse_urlparse(video_url).path\n            format_parts = path.split('/')[4].split('_')[:2]\n\n            dn = compat_urllib_parse_urlparse(video_url).netloc.partition('.')[0]\n\n            resolution = format_parts[0]\n            height = int(resolution[:-len('p')])\n            bitrate = int(format_parts[1][:-len('k')])\n            format = '-'.join(format_parts) + '-' + dn\n\n            formats.append({\n                'url': video_url,\n                'format': format,\n                'format_id': format,\n                'height': height,\n                'tbr': bitrate,\n                'resolution': resolution,\n            })\n\n        self._sort_formats(formats)\n\n        if not formats:\n            raise ExtractorError(u'ERROR: no known formats available for video')\n        \n        return {\n            'id': video_id,\n            'uploader': video_uploader,\n            'upload_date': upload_date,\n            'title': video_title,\n            'thumbnail': thumbnail,\n            'description': video_description,\n            'age_limit': age_limit,\n            'formats': formats,\n        }",
        "begin_line": 37,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011198208286674132,
            "pseudo_dstar_susp": 0.0010559662090813093,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010559662090813093,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.__init__#12",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.__init__(self, code)",
        "snippet": "    def __init__(self, code):\n        self.code = code\n        self._functions = {}\n        self._objects = {}",
        "begin_line": 12,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.interpret_statement#17",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.interpret_statement(self, stmt, local_vars, allow_recursion=20)",
        "snippet": "    def interpret_statement(self, stmt, local_vars, allow_recursion=20):\n        if allow_recursion < 0:\n            raise ExtractorError('Recursion limit reached')\n\n        if stmt.startswith('var '):\n            stmt = stmt[len('var '):]\n        ass_m = re.match(r'^(?P<out>[a-z]+)(?:\\[(?P<index>[^\\]]+)\\])?' +\n                         r'=(?P<expr>.*)$', stmt)\n        if ass_m:\n            if ass_m.groupdict().get('index'):\n                def assign(val):\n                    lvar = local_vars[ass_m.group('out')]\n                    idx = self.interpret_expression(\n                        ass_m.group('index'), local_vars, allow_recursion)\n                    assert isinstance(idx, int)\n                    lvar[idx] = val\n                    return val\n                expr = ass_m.group('expr')\n            else:\n                def assign(val):\n                    local_vars[ass_m.group('out')] = val\n                    return val\n                expr = ass_m.group('expr')\n        elif stmt.startswith('return '):\n            assign = lambda v: v\n            expr = stmt[len('return '):]\n        else:\n            # Try interpreting it as an expression\n            expr = stmt\n            assign = lambda v: v\n\n        v = self.interpret_expression(expr, local_vars, allow_recursion)\n        return assign(v)",
        "begin_line": 17,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.assign#27",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.assign(val)",
        "snippet": "                def assign(val):\n                    lvar = local_vars[ass_m.group('out')]\n                    idx = self.interpret_expression(\n                        ass_m.group('index'), local_vars, allow_recursion)\n                    assert isinstance(idx, int)\n                    lvar[idx] = val\n                    return val",
        "begin_line": 27,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.assign#36",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.assign(val)",
        "snippet": "                def assign(val):\n                    local_vars[ass_m.group('out')] = val\n                    return val",
        "begin_line": 36,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.interpret_expression#51",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.interpret_expression(self, expr, local_vars, allow_recursion)",
        "snippet": "    def interpret_expression(self, expr, local_vars, allow_recursion):\n        if expr.isdigit():\n            return int(expr)\n\n        if expr.isalpha():\n            return local_vars[expr]\n\n        try:\n            return json.loads(expr)\n        except ValueError:\n            pass\n\n        m = re.match(\n            r'^(?P<var>[a-zA-Z0-9_]+)\\.(?P<member>[^(]+)(?:\\(+(?P<args>[^()]*)\\))?$',\n            expr)\n        if m:\n            variable = m.group('var')\n            member = m.group('member')\n            arg_str = m.group('args')\n\n            if variable in local_vars:\n                obj = local_vars[variable]\n            else:\n                if variable not in self._objects:\n                    self._objects[variable] = self.extract_object(variable)\n                obj = self._objects[variable]\n\n            if arg_str is None:\n                # Member access\n                if member == 'length':\n                    return len(obj)\n                return obj[member]\n\n            assert expr.endswith(')')\n            # Function call\n            if arg_str == '':\n                argvals = tuple()\n            else:\n                argvals = tuple([\n                    self.interpret_expression(v, local_vars, allow_recursion)\n                    for v in arg_str.split(',')])\n\n            if member == 'split':\n                assert argvals == ('',)\n                return list(obj)\n            if member == 'join':\n                assert len(argvals) == 1\n                return argvals[0].join(obj)\n            if member == 'reverse':\n                assert len(argvals) == 0\n                obj.reverse()\n                return obj\n            if member == 'slice':\n                assert len(argvals) == 1\n                return obj[argvals[0]:]\n            if member == 'splice':\n                assert isinstance(obj, list)\n                index, howMany = argvals\n                res = []\n                for i in range(index, min(index + howMany, len(obj))):\n                    res.append(obj.pop(index))\n                return res\n\n            return obj[member](argvals)\n\n        m = re.match(\n            r'^(?P<in>[a-z]+)\\[(?P<idx>.+)\\]$', expr)\n        if m:\n            val = local_vars[m.group('in')]\n            idx = self.interpret_expression(\n                m.group('idx'), local_vars, allow_recursion - 1)\n            return val[idx]\n\n        m = re.match(r'^(?P<a>.+?)(?P<op>[%])(?P<b>.+?)$', expr)\n        if m:\n            a = self.interpret_expression(\n                m.group('a'), local_vars, allow_recursion)\n            b = self.interpret_expression(\n                m.group('b'), local_vars, allow_recursion)\n            return a % b\n\n        m = re.match(\n            r'^(?P<func>[a-zA-Z$]+)\\((?P<args>[a-z0-9,]+)\\)$', expr)\n        if m:\n            fname = m.group('func')\n            argvals = tuple([\n                int(v) if v.isdigit() else local_vars[v]\n                for v in m.group('args').split(',')])\n            if fname not in self._functions:\n                self._functions[fname] = self.extract_function(fname)\n            return self._functions[fname](argvals)\n        raise ExtractorError('Unsupported JS expression %r' % expr)",
        "begin_line": 51,
        "end_line": 142,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.extract_object#144",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.extract_object(self, objname)",
        "snippet": "    def extract_object(self, objname):\n        obj = {}\n        obj_m = re.search(\n            (r'(?:var\\s+)?%s\\s*=\\s*\\{' % re.escape(objname)) +\n            r'\\s*(?P<fields>([a-zA-Z$0-9]+\\s*:\\s*function\\(.*?\\)\\s*\\{.*?\\})*)' +\n            r'\\}\\s*;',\n            self.code)\n        fields = obj_m.group('fields')\n        # Currently, it only supports function definitions\n        fields_m = re.finditer(\n            r'(?P<key>[a-zA-Z$0-9]+)\\s*:\\s*function'\n            r'\\((?P<args>[a-z,]+)\\){(?P<code>[^}]+)}',\n            fields)\n        for f in fields_m:\n            argnames = f.group('args').split(',')\n            obj[f.group('key')] = self.build_function(argnames, f.group('code'))\n\n        return obj",
        "begin_line": 144,
        "end_line": 161,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006195786864931846,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.extract_function#163",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.extract_function(self, funcname)",
        "snippet": "    def extract_function(self, funcname):\n        func_m = re.search(\n            (r'(?:function %s|[{;]%s\\s*=\\s*function)' % (\n                re.escape(funcname), re.escape(funcname))) +\n            r'\\((?P<args>[a-z,]+)\\){(?P<code>[^}]+)}',\n            self.code)\n        if func_m is None:\n            raise ExtractorError('Could not find JS function %r' % funcname)\n        argnames = func_m.group('args').split(',')\n\n        return self.build_function(argnames, func_m.group('code'))",
        "begin_line": 163,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.build_function#175",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.build_function(self, argnames, code)",
        "snippet": "    def build_function(self, argnames, code):\n        def resf(args):\n            local_vars = dict(zip(argnames, args))\n            for stmt in code.split(';'):\n                res = self.interpret_statement(stmt, local_vars)\n            return res\n        return resf",
        "begin_line": 175,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.jsinterp.JSInterpreter.resf#176",
        "src_path": "youtube_dl/jsinterp.py",
        "class_name": "youtube_dl.jsinterp.JSInterpreter",
        "signature": "youtube_dl.jsinterp.JSInterpreter.resf(args)",
        "snippet": "        def resf(args):\n            local_vars = dict(zip(argnames, args))\n            for stmt in code.split(';'):\n                res = self.interpret_statement(stmt, local_vars)\n            return res",
        "begin_line": 176,
        "end_line": 180,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0005963029218843172,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.extractor.subtitles.SubtitlesInfoExtractor._have_to_download_any_subtitles#11",
        "src_path": "youtube_dl/extractor/subtitles.py",
        "class_name": "youtube_dl.extractor.subtitles.SubtitlesInfoExtractor",
        "signature": "youtube_dl.extractor.subtitles.SubtitlesInfoExtractor._have_to_download_any_subtitles(self)",
        "snippet": "    def _have_to_download_any_subtitles(self):\n        return any([self._downloader.params.get('writesubtitles', False),\n                    self._downloader.params.get('writeautomaticsub')])",
        "begin_line": 11,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011198208286674132,
            "pseudo_dstar_susp": 0.0010559662090813093,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010559662090813093,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.academicearth.AcademicEarthCourseIE._real_extract#20",
        "src_path": "youtube_dl/extractor/academicearth.py",
        "class_name": "youtube_dl.extractor.academicearth.AcademicEarthCourseIE",
        "signature": "youtube_dl.extractor.academicearth.AcademicEarthCourseIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        m = re.match(self._VALID_URL, url)\n        playlist_id = m.group('id')\n\n        webpage = self._download_webpage(url, playlist_id)\n        title = self._html_search_regex(\n            r'<h1 class=\"playlist-name\"[^>]*?>(.*?)</h1>', webpage, u'title')\n        description = self._html_search_regex(\n            r'<p class=\"excerpt\"[^>]*?>(.*?)</p>',\n            webpage, u'description', fatal=False)\n        urls = re.findall(\n            r'<li class=\"lecture-preview\">\\s*?<a target=\"_blank\" href=\"([^\"]+)\">',\n            webpage)\n        entries = [self.url_result(u) for u in urls]\n\n        return {\n            '_type': 'playlist',\n            'id': playlist_id,\n            'title': title,\n            'description': description,\n            'entries': entries,\n        }",
        "begin_line": 20,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0031446540880503146,
            "pseudo_dstar_susp": 0.003236245954692557,
            "pseudo_tarantula_susp": 0.002004008016032064,
            "pseudo_op2_susp": 0.003236245954692557,
            "pseudo_barinel_susp": 0.002004008016032064
        }
    },
    {
        "name": "youtube_dl.extractor.arte.ArteTVFutureIE._real_extract#220",
        "src_path": "youtube_dl/extractor/arte.py",
        "class_name": "youtube_dl.extractor.arte.ArteTVFutureIE",
        "signature": "youtube_dl.extractor.arte.ArteTVFutureIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        anchor_id, lang = self._extract_url_info(url)\n        webpage = self._download_webpage(url, anchor_id)\n        row = get_element_by_id(anchor_id, webpage)\n        return self._extract_from_webpage(row, anchor_id, lang)",
        "begin_line": 220,
        "end_line": 224,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0018018018018018018,
            "pseudo_dstar_susp": 0.0014534883720930232,
            "pseudo_tarantula_susp": 0.0014684287812041115,
            "pseudo_op2_susp": 0.0014534883720930232,
            "pseudo_barinel_susp": 0.0014792899408284023
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.__init__#48",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.__init__(self, ydl, params)",
        "snippet": "    def __init__(self, ydl, params):\n        \"\"\"Create a FileDownloader object with the given options.\"\"\"\n        self.ydl = ydl\n        self._progress_hooks = []\n        self.params = params",
        "begin_line": 48,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0027624309392265192,
            "pseudo_dstar_susp": 0.0031446540880503146,
            "pseudo_tarantula_susp": 0.0009182736455463728,
            "pseudo_op2_susp": 0.0031446540880503146,
            "pseudo_barinel_susp": 0.0009182736455463728
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.format_seconds#55",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.format_seconds(seconds)",
        "snippet": "    def format_seconds(seconds):\n        (mins, secs) = divmod(seconds, 60)\n        (hours, mins) = divmod(mins, 60)\n        if hours > 99:\n            return '--:--:--'\n        if hours == 0:\n            return '%02d:%02d' % (mins, secs)\n        else:\n            return '%02d:%02d:%02d' % (hours, mins, secs)",
        "begin_line": 55,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026954177897574125,
            "pseudo_dstar_susp": 0.0028089887640449437,
            "pseudo_tarantula_susp": 0.001098901098901099,
            "pseudo_op2_susp": 0.0028089887640449437,
            "pseudo_barinel_susp": 0.001098901098901099
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.calc_eta#78",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.calc_eta(start, now, total, current)",
        "snippet": "    def calc_eta(start, now, total, current):\n        if total is None:\n            return None\n        dif = now - start\n        if current == 0 or dif < 0.001: # One millisecond\n            return None\n        rate = float(current) / dif\n        return int((float(total) - float(current)) / rate)",
        "begin_line": 78,
        "end_line": 85,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00196078431372549,
            "pseudo_dstar_susp": 0.0023752969121140144,
            "pseudo_tarantula_susp": 0.0009174311926605505,
            "pseudo_op2_susp": 0.0023752969121140144,
            "pseudo_barinel_susp": 0.0009174311926605505
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.format_eta#88",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.format_eta(eta)",
        "snippet": "    def format_eta(eta):\n        if eta is None:\n            return '--:--'\n        return FileDownloader.format_seconds(eta)",
        "begin_line": 88,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002150537634408602,
            "pseudo_dstar_susp": 0.002380952380952381,
            "pseudo_tarantula_susp": 0.0010683760683760685,
            "pseudo_op2_susp": 0.002380952380952381,
            "pseudo_barinel_susp": 0.0010683760683760685
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.calc_speed#94",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.calc_speed(start, now, bytes)",
        "snippet": "    def calc_speed(start, now, bytes):\n        dif = now - start\n        if bytes == 0 or dif < 0.001: # One millisecond\n            return None\n        return float(bytes) / dif",
        "begin_line": 94,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002352941176470588,
            "pseudo_dstar_susp": 0.002570694087403599,
            "pseudo_tarantula_susp": 0.001059322033898305,
            "pseudo_op2_susp": 0.002570694087403599,
            "pseudo_barinel_susp": 0.001059322033898305
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.best_block_size#107",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.best_block_size(elapsed_time, bytes)",
        "snippet": "    def best_block_size(elapsed_time, bytes):\n        new_min = max(bytes / 2.0, 1.0)\n        new_max = min(max(bytes * 2.0, 1.0), 4194304) # Do not surpass 4 MB\n        if elapsed_time < 0.001:\n            return int(new_max)\n        rate = bytes / elapsed_time\n        if rate > new_max:\n            return int(new_max)\n        if rate < new_min:\n            return int(new_min)\n        return int(rate)",
        "begin_line": 107,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002531645569620253,
            "pseudo_dstar_susp": 0.002824858757062147,
            "pseudo_tarantula_susp": 0.0009398496240601503,
            "pseudo_op2_susp": 0.002824858757062147,
            "pseudo_barinel_susp": 0.0009398496240601503
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.slow_down#147",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.slow_down(self, start_time, byte_counter)",
        "snippet": "    def slow_down(self, start_time, byte_counter):\n        \"\"\"Sleep if the download speed is over the rate limit.\"\"\"\n        rate_limit = self.params.get('ratelimit', None)\n        if rate_limit is None or byte_counter == 0:\n            return\n        now = time.time()\n        elapsed = now - start_time\n        if elapsed <= 0.0:\n            return\n        speed = float(byte_counter) / elapsed\n        if speed > rate_limit:\n            time.sleep((byte_counter - rate_limit * (now - start_time)) / rate_limit)",
        "begin_line": 147,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0023584905660377358,
            "pseudo_dstar_susp": 0.00273224043715847,
            "pseudo_tarantula_susp": 0.000980392156862745,
            "pseudo_op2_susp": 0.00273224043715847,
            "pseudo_barinel_susp": 0.000980392156862745
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.temp_name#160",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.temp_name(self, filename)",
        "snippet": "    def temp_name(self, filename):\n        \"\"\"Returns a temporary filename for the given filename.\"\"\"\n        if self.params.get('nopart', False) or filename == u'-' or \\\n                (os.path.exists(encodeFilename(filename)) and not os.path.isfile(encodeFilename(filename))):\n            return filename\n        return filename + u'.part'",
        "begin_line": 160,
        "end_line": 165,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001968503937007874,
            "pseudo_dstar_susp": 0.0024154589371980675,
            "pseudo_tarantula_susp": 0.0009784735812133072,
            "pseudo_op2_susp": 0.0024154589371980675,
            "pseudo_barinel_susp": 0.0009784735812133072
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.undo_temp_name#167",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.undo_temp_name(self, filename)",
        "snippet": "    def undo_temp_name(self, filename):\n        if filename.endswith(u'.part'):\n            return filename[:-len(u'.part')]\n        return filename",
        "begin_line": 167,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002551020408163265,
            "pseudo_dstar_susp": 0.002785515320334262,
            "pseudo_tarantula_susp": 0.0011025358324145535,
            "pseudo_op2_susp": 0.002785515320334262,
            "pseudo_barinel_susp": 0.0011025358324145535
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.try_utime#180",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.try_utime(self, filename, last_modified_hdr)",
        "snippet": "    def try_utime(self, filename, last_modified_hdr):\n        \"\"\"Try to set the last-modified time of the given file.\"\"\"\n        if last_modified_hdr is None:\n            return\n        if not os.path.isfile(encodeFilename(filename)):\n            return\n        timestr = last_modified_hdr\n        if timestr is None:\n            return\n        filetime = timeconvert(timestr)\n        if filetime is None:\n            return filetime\n        # Ignore obviously invalid dates\n        if filetime == 0:\n            return\n        try:\n            os.utime(filename, (time.time(), filetime))\n        except:\n            pass\n        return filetime",
        "begin_line": 180,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001402524544179523,
            "pseudo_dstar_susp": 0.002028397565922921,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.002028397565922921,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader._report_progress_status#205",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader._report_progress_status(self, msg, is_last_line=False)",
        "snippet": "    def _report_progress_status(self, msg, is_last_line=False):\n        fullmsg = u'[download] ' + msg\n        if self.params.get('progress_with_newline', False):\n            self.to_screen(fullmsg)\n        else:\n            if os.name == 'nt':\n                prev_len = getattr(self, '_report_progress_prev_line_length',\n                                   0)\n                if prev_len > len(fullmsg):\n                    fullmsg += u' ' * (prev_len - len(fullmsg))\n                self._report_progress_prev_line_length = len(fullmsg)\n                clear_line = u'\\r'\n            else:\n                clear_line = (u'\\r\\x1b[K' if sys.stderr.isatty() else u'\\r')\n            self.to_screen(clear_line + fullmsg, skip_eol=not is_last_line)\n        self.to_console_title(u'youtube-dl ' + msg)",
        "begin_line": 205,
        "end_line": 220,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002320185614849188,
            "pseudo_dstar_susp": 0.002583979328165375,
            "pseudo_tarantula_susp": 0.0010845986984815619,
            "pseudo_op2_susp": 0.002583979328165375,
            "pseudo_barinel_susp": 0.0010845986984815619
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.report_progress#222",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.report_progress(self, percent, data_len_str, speed, eta)",
        "snippet": "    def report_progress(self, percent, data_len_str, speed, eta):\n        \"\"\"Report download progress.\"\"\"\n        if self.params.get('noprogress', False):\n            return\n        if eta is not None:\n            eta_str = self.format_eta(eta)\n        else:\n            eta_str = 'Unknown ETA'\n        if percent is not None:\n            percent_str = self.format_percent(percent)\n        else:\n            percent_str = 'Unknown %'\n        speed_str = self.format_speed(speed)\n\n        msg = (u'%s of %s at %s ETA %s' %\n               (percent_str, data_len_str, speed_str, eta_str))\n        self._report_progress_status(msg)",
        "begin_line": 222,
        "end_line": 238,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026455026455026454,
            "pseudo_dstar_susp": 0.0027100271002710027,
            "pseudo_tarantula_susp": 0.001177856301531213,
            "pseudo_op2_susp": 0.0027100271002710027,
            "pseudo_barinel_susp": 0.001177856301531213
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.download#278",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.download(self, filename, info_dict)",
        "snippet": "    def download(self, filename, info_dict):\n        \"\"\"Download to a filename using the info from info_dict\n        Return True on success and False otherwise\n        \"\"\"\n        # Check file already present\n        if self.params.get('continuedl', False) and os.path.isfile(encodeFilename(filename)) and not self.params.get('nopart', False):\n            self.report_file_already_downloaded(filename)\n            self._hook_progress({\n                'filename': filename,\n                'status': 'finished',\n                'total_bytes': os.path.getsize(encodeFilename(filename)),\n            })\n            return True\n\n        return self.real_download(filename, info_dict)",
        "begin_line": 278,
        "end_line": 292,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013477088948787063,
            "pseudo_dstar_susp": 0.0016501650165016502,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0016501650165016502,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader._hook_progress#298",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader._hook_progress(self, status)",
        "snippet": "    def _hook_progress(self, status):\n        for ph in self._progress_hooks:\n            ph(status)",
        "begin_line": 298,
        "end_line": 300,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.downloader.common.FileDownloader.add_progress_hook#302",
        "src_path": "youtube_dl/downloader/common.py",
        "class_name": "youtube_dl.downloader.common.FileDownloader",
        "signature": "youtube_dl.downloader.common.FileDownloader.add_progress_hook(self, ph)",
        "snippet": "    def add_progress_hook(self, ph):\n        \"\"\" ph gets called on download progress, with a dictionary with the entries\n        * filename: The final filename\n        * status: One of \"downloading\" and \"finished\"\n\n        It can also have some of the following entries:\n\n        * downloaded_bytes: Bytes on disks\n        * total_bytes: Total bytes, None if unknown\n        * tmpfilename: The filename we're currently writing to\n        * eta: The estimated time in seconds, None if unknown\n        * speed: The download speed in bytes/second, None if unknown\n\n        Hooks are guaranteed to be called at least once (with status \"finished\")\n        if the download is successful.\n        \"\"\"\n        self._progress_hooks.append(ph)",
        "begin_line": 302,
        "end_line": 318,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013477088948787063,
            "pseudo_dstar_susp": 0.0016501650165016502,
            "pseudo_tarantula_susp": 0.0009115770282588879,
            "pseudo_op2_susp": 0.0016501650165016502,
            "pseudo_barinel_susp": 0.0009115770282588879
        }
    },
    {
        "name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._real_extract#59",
        "src_path": "youtube_dl/extractor/ceskatelevize.py",
        "class_name": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE",
        "signature": "youtube_dl.extractor.ceskatelevize.CeskaTelevizeIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url = url.replace('/porady/', '/ivysilani/').replace('/video/', '')\n\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n\n        webpage = self._download_webpage(url, video_id)\n\n        NOT_AVAILABLE_STRING = 'This content is not available at your territory due to limited copyright.'\n        if '%s</p>' % NOT_AVAILABLE_STRING in webpage:\n            raise ExtractorError(NOT_AVAILABLE_STRING, expected=True)\n\n        typ = self._html_search_regex(r'getPlaylistUrl\\(\\[\\{\"type\":\"(.+?)\",\"id\":\".+?\"\\}\\],', webpage, 'type')\n        episode_id = self._html_search_regex(r'getPlaylistUrl\\(\\[\\{\"type\":\".+?\",\"id\":\"(.+?)\"\\}\\],', webpage, 'episode_id')\n\n        data = {\n            'playlist[0][type]': typ,\n            'playlist[0][id]': episode_id,\n            'requestUrl': compat_urllib_parse_urlparse(url).path,\n            'requestSource': 'iVysilani',\n        }\n\n        req = compat_urllib_request.Request('http://www.ceskatelevize.cz/ivysilani/ajax/get-playlist-url',\n                                            data=compat_urllib_parse.urlencode(data))\n\n        req.add_header('Content-type', 'application/x-www-form-urlencoded')\n        req.add_header('x-addr', '127.0.0.1')\n        req.add_header('X-Requested-With', 'XMLHttpRequest')\n        req.add_header('Referer', url)\n\n        playlistpage = self._download_json(req, video_id)\n\n        req = compat_urllib_request.Request(compat_urllib_parse.unquote(playlistpage['url']))\n        req.add_header('Referer', url)\n\n        playlist = self._download_xml(req, video_id)\n        \n        formats = []\n        for i in playlist.find('smilRoot/body'):\n            if 'AD' not in i.attrib['id']:\n                base_url = i.attrib['base']\n                parsedurl = compat_urllib_parse_urlparse(base_url)\n                duration = i.attrib['duration']\n\n                for video in i.findall('video'):\n                    if video.attrib['label'] != 'AD':\n                        format_id = video.attrib['label']\n                        play_path = video.attrib['src']\n                        vbr = int(video.attrib['system-bitrate'])\n\n                        formats.append({\n                            'format_id': format_id,\n                            'url': base_url,\n                            'vbr': vbr,\n                            'play_path': play_path,\n                            'app': parsedurl.path[1:] + '?' + parsedurl.query,\n                            'rtmp_live': True,\n                            'ext': 'flv',\n                        })\n\n        self._sort_formats(formats)\n\n        return {\n            'id': episode_id,\n            'title': self._html_search_regex(r'<title>(.+?) \u2014 iVys\u00edl\u00e1n\u00ed \u2014 \u010cesk\u00e1 televize</title>', webpage, 'title'),\n            'duration': float(duration),\n            'formats': formats,\n        }",
        "begin_line": 59,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0030864197530864196,
            "pseudo_dstar_susp": 0.0029239766081871343,
            "pseudo_tarantula_susp": 0.0013831258644536654,
            "pseudo_op2_susp": 0.0029239766081871343,
            "pseudo_barinel_susp": 0.0013831258644536654
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.__init__#192",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.__init__(self, params=None, auto_init=True)",
        "snippet": "    def __init__(self, params=None, auto_init=True):\n        \"\"\"Create a FileDownloader object with the given options.\"\"\"\n        if params is None:\n            params = {}\n        self._ies = []\n        self._ies_instances = {}\n        self._pps = []\n        self._progress_hooks = []\n        self._download_retcode = 0\n        self._num_downloads = 0\n        self._screen_file = [sys.stdout, sys.stderr][params.get('logtostderr', False)]\n        self._err_file = sys.stderr\n        self.params = params\n        self.cache = Cache(self)\n\n        if params.get('bidi_workaround', False):\n            try:\n                import pty\n                master, slave = pty.openpty()\n                width = get_term_width()\n                if width is None:\n                    width_args = []\n                else:\n                    width_args = ['-w', str(width)]\n                sp_kwargs = dict(\n                    stdin=subprocess.PIPE,\n                    stdout=slave,\n                    stderr=self._err_file)\n                try:\n                    self._output_process = subprocess.Popen(\n                        ['bidiv'] + width_args, **sp_kwargs\n                    )\n                except OSError:\n                    self._output_process = subprocess.Popen(\n                        ['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)\n                self._output_channel = os.fdopen(master, 'rb')\n            except OSError as ose:\n                if ose.errno == 2:\n                    self.report_warning('Could not find fribidi executable, ignoring --bidi-workaround . Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')\n                else:\n                    raise\n\n        if (sys.version_info >= (3,) and sys.platform != 'win32' and\n                sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']\n                and not params.get('restrictfilenames', False)):\n            # On Python 3, the Unicode filesystem API will throw errors (#1474)\n            self.report_warning(\n                'Assuming --restrict-filenames since file system encoding '\n                'cannot encode all characters. '\n                'Set the LC_ALL environment variable to fix this.')\n            self.params['restrictfilenames'] = True\n\n        if '%(stitle)s' in self.params.get('outtmpl', ''):\n            self.report_warning('%(stitle)s is deprecated. Use the %(title)s and the --restrict-filenames flag(which also secures %(uploader)s et al) instead.')\n\n        self._setup_opener()\n\n        if auto_init:\n            self.print_debug_header()\n            self.add_default_info_extractors()",
        "begin_line": 192,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 1.0,
            "pseudo_dstar_susp": 1.0,
            "pseudo_tarantula_susp": 0.0023923444976076554,
            "pseudo_op2_susp": 1.0,
            "pseudo_barinel_susp": 0.0023923444976076554
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_info_extractor#253",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_info_extractor(self, ie)",
        "snippet": "    def add_info_extractor(self, ie):\n        \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\n        self._ies.append(ie)\n        self._ies_instances[ie.ie_key()] = ie\n        ie.set_downloader(self)",
        "begin_line": 253,
        "end_line": 257,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.058823529411764705,
            "pseudo_dstar_susp": 0.058823529411764705,
            "pseudo_tarantula_susp": 0.0024449877750611247,
            "pseudo_op2_susp": 0.058823529411764705,
            "pseudo_barinel_susp": 0.0024449877750611247
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.get_info_extractor#259",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.get_info_extractor(self, ie_key)",
        "snippet": "    def get_info_extractor(self, ie_key):\n        \"\"\"\n        Get an instance of an IE with name ie_key, it will try to get one from\n        the _ies list, if there's no instance it will create a new one and add\n        it to the extractor list.\n        \"\"\"\n        ie = self._ies_instances.get(ie_key)\n        if ie is None:\n            ie = get_info_extractor(ie_key)()\n            self.add_info_extractor(ie)\n        return ie",
        "begin_line": 259,
        "end_line": 269,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00205761316872428,
            "pseudo_dstar_susp": 0.0016778523489932886,
            "pseudo_tarantula_susp": 0.001557632398753894,
            "pseudo_op2_susp": 0.0016778523489932886,
            "pseudo_barinel_susp": 0.001557632398753894
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_default_info_extractors#271",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_default_info_extractors(self)",
        "snippet": "    def add_default_info_extractors(self):\n        \"\"\"\n        Add the InfoExtractors returned by gen_extractors to the end of the list\n        \"\"\"\n        for ie in gen_extractors():\n            self.add_info_extractor(ie)",
        "begin_line": 271,
        "end_line": 276,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010101010101010102,
            "pseudo_dstar_susp": 0.0125,
            "pseudo_tarantula_susp": 0.001697792869269949,
            "pseudo_op2_susp": 0.0125,
            "pseudo_barinel_susp": 0.001697792869269949
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_progress_hook#283",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_progress_hook(self, ph)",
        "snippet": "    def add_progress_hook(self, ph):\n        \"\"\"Add the progress hook (currently only for the file downloader)\"\"\"\n        self._progress_hooks.append(ph)",
        "begin_line": 283,
        "end_line": 285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005434782608695652,
            "pseudo_dstar_susp": 0.005319148936170213,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.005319148936170213,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._bidi_workaround#287",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._bidi_workaround(self, message)",
        "snippet": "    def _bidi_workaround(self, message):\n        if not hasattr(self, '_output_channel'):\n            return message\n\n        assert hasattr(self, '_output_process')\n        assert isinstance(message, compat_str)\n        line_count = message.count('\\n') + 1\n        self._output_process.stdin.write((message + '\\n').encode('utf-8'))\n        self._output_process.stdin.flush()\n        res = ''.join(self._output_channel.readline().decode('utf-8')\n                       for _ in range(line_count))\n        return res[:-len('\\n')]",
        "begin_line": 287,
        "end_line": 298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005747126436781609,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_screen#300",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_screen(self, message, skip_eol=False)",
        "snippet": "    def to_screen(self, message, skip_eol=False):\n        \"\"\"Print message to stdout if not in quiet mode.\"\"\"\n        return self.to_stdout(message, skip_eol, check_quiet=True)",
        "begin_line": 300,
        "end_line": 302,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005747126436781609,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._write_string#304",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._write_string(self, s, out=None)",
        "snippet": "    def _write_string(self, s, out=None):\n        write_string(s, out=out, encoding=self.params.get('encoding'))",
        "begin_line": 304,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013157894736842105,
            "pseudo_dstar_susp": 0.015873015873015872,
            "pseudo_tarantula_susp": 0.0019723865877712033,
            "pseudo_op2_susp": 0.015873015873015872,
            "pseudo_barinel_susp": 0.0019723865877712033
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_stdout#307",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_stdout(self, message, skip_eol=False, check_quiet=False)",
        "snippet": "    def to_stdout(self, message, skip_eol=False, check_quiet=False):\n        \"\"\"Print message to stdout if not in quiet mode.\"\"\"\n        if self.params.get('logger'):\n            self.params['logger'].debug(message)\n        elif not check_quiet or not self.params.get('quiet', False):\n            message = self._bidi_workaround(message)\n            terminator = ['\\n', ''][skip_eol]\n            output = message + terminator\n\n            self._write_string(output, self._screen_file)",
        "begin_line": 307,
        "end_line": 316,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005747126436781609,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_stderr#318",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_stderr(self, message)",
        "snippet": "    def to_stderr(self, message):\n        \"\"\"Print message to stderr.\"\"\"\n        assert isinstance(message, compat_str)\n        if self.params.get('logger'):\n            self.params['logger'].error(message)\n        else:\n            message = self._bidi_workaround(message)\n            output = message + '\\n'\n            self._write_string(output, self._err_file)",
        "begin_line": 318,
        "end_line": 326,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.to_console_title#328",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.to_console_title(self, message)",
        "snippet": "    def to_console_title(self, message):\n        if not self.params.get('consoletitle', False):\n            return\n        if os.name == 'nt' and ctypes.windll.kernel32.GetConsoleWindow():\n            # c_wchar_p() might not be necessary if `message` is\n            # already of type unicode()\n            ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))\n        elif 'TERM' in os.environ:\n            self._write_string('\\033]0;%s\\007' % message, self._screen_file)",
        "begin_line": 328,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00117096018735363,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.trouble#362",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.trouble(self, message=None, tb=None)",
        "snippet": "    def trouble(self, message=None, tb=None):\n        \"\"\"Determine action to take when a download problem appears.\n\n        Depending on if the downloader has been configured to ignore\n        download errors or not, this method may throw an exception or\n        not when errors are found, after printing the message.\n\n        tb, if given, is additional traceback information.\n        \"\"\"\n        if message is not None:\n            self.to_stderr(message)\n        if self.params.get('verbose'):\n            if tb is None:\n                if sys.exc_info()[0]:  # if .trouble has been called from an except block\n                    tb = ''\n                    if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                        tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))\n                    tb += compat_str(traceback.format_exc())\n                else:\n                    tb_data = traceback.format_list(traceback.extract_stack())\n                    tb = ''.join(tb_data)\n            self.to_stderr(tb)\n        if not self.params.get('ignoreerrors', False):\n            if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                exc_info = sys.exc_info()[1].exc_info\n            else:\n                exc_info = sys.exc_info()\n            raise DownloadError(message, exc_info)\n        self._download_retcode = 1",
        "begin_line": 362,
        "end_line": 390,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004807692307692308,
            "pseudo_dstar_susp": 0.004807692307692308,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.004807692307692308,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.report_error#409",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.report_error(self, message, tb=None)",
        "snippet": "    def report_error(self, message, tb=None):\n        '''\n        Do the same as trouble, but prefixes the message with 'ERROR:', colored\n        in red if stderr is a tty file.\n        '''\n        if self._err_file.isatty() and os.name != 'nt':\n            _msg_header = '\\033[0;31mERROR:\\033[0m'\n        else:\n            _msg_header = 'ERROR:'\n        error_message = '%s %s' % (_msg_header, message)\n        self.trouble(error_message, tb)",
        "begin_line": 409,
        "end_line": 419,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004807692307692308,
            "pseudo_dstar_susp": 0.004807692307692308,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.004807692307692308,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.prepare_filename#428",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.prepare_filename(self, info_dict)",
        "snippet": "    def prepare_filename(self, info_dict):\n        \"\"\"Generate the output filename.\"\"\"\n        try:\n            template_dict = dict(info_dict)\n\n            template_dict['epoch'] = int(time.time())\n            autonumber_size = self.params.get('autonumber_size')\n            if autonumber_size is None:\n                autonumber_size = 5\n            autonumber_templ = '%0' + str(autonumber_size) + 'd'\n            template_dict['autonumber'] = autonumber_templ % self._num_downloads\n            if template_dict.get('playlist_index') is not None:\n                template_dict['playlist_index'] = '%0*d' % (len(str(template_dict['n_entries'])), template_dict['playlist_index'])\n            if template_dict.get('resolution') is None:\n                if template_dict.get('width') and template_dict.get('height'):\n                    template_dict['resolution'] = '%dx%d' % (template_dict['width'], template_dict['height'])\n                elif template_dict.get('height'):\n                    template_dict['resolution'] = '%sp' % template_dict['height']\n                elif template_dict.get('width'):\n                    template_dict['resolution'] = '?x%d' % template_dict['width']\n\n            sanitize = lambda k, v: sanitize_filename(\n                compat_str(v),\n                restricted=self.params.get('restrictfilenames'),\n                is_id=(k == 'id'))\n            template_dict = dict((k, sanitize(k, v))\n                                 for k, v in template_dict.items()\n                                 if v is not None)\n            template_dict = collections.defaultdict(lambda: 'NA', template_dict)\n\n            outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n            tmpl = compat_expanduser(outtmpl)\n            filename = tmpl % template_dict\n            return filename\n        except ValueError as err:\n            self.report_error('Error in output template: ' + str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n            return None",
        "begin_line": 428,
        "end_line": 464,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004484304932735426,
            "pseudo_dstar_susp": 0.004484304932735426,
            "pseudo_tarantula_susp": 0.0021231422505307855,
            "pseudo_op2_susp": 0.004484304932735426,
            "pseudo_barinel_susp": 0.0021231422505307855
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._match_entry#466",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._match_entry(self, info_dict)",
        "snippet": "    def _match_entry(self, info_dict):\n        \"\"\" Returns None iff the file should be downloaded \"\"\"\n\n        video_title = info_dict.get('title', info_dict.get('id', 'video'))\n        if 'title' in info_dict:\n            # This can happen when we're just evaluating the playlist\n            title = info_dict['title']\n            matchtitle = self.params.get('matchtitle', False)\n            if matchtitle:\n                if not re.search(matchtitle, title, re.IGNORECASE):\n                    return '\"' + title + '\" title did not match pattern \"' + matchtitle + '\"'\n            rejecttitle = self.params.get('rejecttitle', False)\n            if rejecttitle:\n                if re.search(rejecttitle, title, re.IGNORECASE):\n                    return '\"' + title + '\" title matched reject pattern \"' + rejecttitle + '\"'\n        date = info_dict.get('upload_date', None)\n        if date is not None:\n            dateRange = self.params.get('daterange', DateRange())\n            if date not in dateRange:\n                return '%s upload date is not in range %s' % (date_from_str(date).isoformat(), dateRange)\n        view_count = info_dict.get('view_count', None)\n        if view_count is not None:\n            min_views = self.params.get('min_views')\n            if min_views is not None and view_count < min_views:\n                return 'Skipping %s, because it has not reached minimum view count (%d/%d)' % (video_title, view_count, min_views)\n            max_views = self.params.get('max_views')\n            if max_views is not None and view_count > max_views:\n                return 'Skipping %s, because it has exceeded the maximum view count (%d/%d)' % (video_title, view_count, max_views)\n        age_limit = self.params.get('age_limit')\n        if age_limit is not None:\n            actual_age_limit = info_dict.get('age_limit')\n            if actual_age_limit is None:\n                actual_age_limit = 0\n            if age_limit < actual_age_limit:\n                return 'Skipping \"' + title + '\" because it is age restricted'\n        if self.in_download_archive(info_dict):\n            return '%s has already been recorded in archive' % video_title\n        return None",
        "begin_line": 466,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0023094688221709007,
            "pseudo_dstar_susp": 0.002512562814070352,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.002512562814070352,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_extra_info#506",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_extra_info(info_dict, extra_info)",
        "snippet": "    def add_extra_info(info_dict, extra_info):\n        '''Set the keys from extra_info in info dict if they are missing'''\n        for key, value in extra_info.items():\n            info_dict.setdefault(key, value)",
        "begin_line": 506,
        "end_line": 509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029069767441860465,
            "pseudo_dstar_susp": 0.0029850746268656717,
            "pseudo_tarantula_susp": 0.0011198208286674132,
            "pseudo_op2_susp": 0.0029850746268656717,
            "pseudo_barinel_susp": 0.0011198208286674132
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.extract_info#511",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.extract_info(self, url, download=True, ie_key=None, extra_info={}, process=True)",
        "snippet": "    def extract_info(self, url, download=True, ie_key=None, extra_info={},\n                     process=True):\n        '''\n        Returns a list with a dictionary for each video we find.\n        If 'download', also downloads the videos.\n        extra_info is a dict containing the extra values to add to each result\n         '''\n\n        if ie_key:\n            ies = [self.get_info_extractor(ie_key)]\n        else:\n            ies = self._ies\n\n        for ie in ies:\n            if not ie.suitable(url):\n                continue\n\n            if not ie.working():\n                self.report_warning('The program functionality for this site has been marked as broken, '\n                                    'and will probably not work.')\n\n            try:\n                ie_result = ie.extract(url)\n                if ie_result is None: # Finished already (backwards compatibility; listformats and friends should be moved here)\n                    break\n                if isinstance(ie_result, list):\n                    # Backwards compatibility: old IE result format\n                    ie_result = {\n                        '_type': 'compat_list',\n                        'entries': ie_result,\n                    }\n                self.add_default_extra_info(ie_result, ie, url)\n                if process:\n                    return self.process_ie_result(ie_result, download, extra_info)\n                else:\n                    return ie_result\n            except ExtractorError as de: # An error we somewhat expected\n                self.report_error(compat_str(de), de.format_traceback())\n                break\n            except MaxDownloadsReached:\n                raise\n            except Exception as e:\n                if self.params.get('ignoreerrors', False):\n                    self.report_error(compat_str(e), tb=compat_str(traceback.format_exc()))\n                    break\n                else:\n                    raise\n        else:\n            self.report_error('no suitable InfoExtractor for URL %s' % url)",
        "begin_line": 511,
        "end_line": 559,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005747126436781609,
            "pseudo_dstar_susp": 0.0056179775280898875,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0056179775280898875,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.add_default_extra_info#561",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.add_default_extra_info(self, ie_result, ie, url)",
        "snippet": "    def add_default_extra_info(self, ie_result, ie, url):\n        self.add_extra_info(ie_result, {\n            'extractor': ie.IE_NAME,\n            'webpage_url': url,\n            'webpage_url_basename': url_basename(url),\n            'extractor_key': ie.ie_key(),\n        })",
        "begin_line": 561,
        "end_line": 567,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0029585798816568047,
            "pseudo_dstar_susp": 0.00303951367781155,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.00303951367781155,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_ie_result#569",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_ie_result(self, ie_result, download=True, extra_info={})",
        "snippet": "    def process_ie_result(self, ie_result, download=True, extra_info={}):\n        \"\"\"\n        Take the result of the ie(may be modified) and resolve all unresolved\n        references (URLs, playlist items).\n\n        It will also download the videos if 'download'.\n        Returns the resolved ie_result.\n        \"\"\"\n\n        result_type = ie_result.get('_type', 'video')\n\n        if result_type in ('url', 'url_transparent'):\n            extract_flat = self.params.get('extract_flat', False)\n            if ((extract_flat == 'in_playlist' and 'playlist' in extra_info) or\n                    extract_flat is True):\n                if self.params.get('forcejson', False):\n                    self.to_stdout(json.dumps(ie_result))\n                return ie_result\n\n        if result_type == 'video':\n            self.add_extra_info(ie_result, extra_info)\n            return self.process_video_result(ie_result, download=download)\n        elif result_type == 'url':\n            # We have to add extra_info to the results because it may be\n            # contained in a playlist\n            return self.extract_info(ie_result['url'],\n                                     download,\n                                     ie_key=ie_result.get('ie_key'),\n                                     extra_info=extra_info)\n        elif result_type == 'url_transparent':\n            # Use the information from the embedding page\n            info = self.extract_info(\n                ie_result['url'], ie_key=ie_result.get('ie_key'),\n                extra_info=extra_info, download=False, process=False)\n\n            def make_result(embedded_info):\n                new_result = ie_result.copy()\n                for f in ('_type', 'url', 'ext', 'player_url', 'formats',\n                          'entries', 'ie_key', 'duration',\n                          'subtitles', 'annotations', 'format',\n                          'thumbnail', 'thumbnails'):\n                    if f in new_result:\n                        del new_result[f]\n                    if f in embedded_info:\n                        new_result[f] = embedded_info[f]\n                return new_result\n            new_result = make_result(info)\n\n            assert new_result.get('_type') != 'url_transparent'\n            if new_result.get('_type') == 'compat_list':\n                new_result['entries'] = [\n                    make_result(e) for e in new_result['entries']]\n\n            return self.process_ie_result(\n                new_result, download=download, extra_info=extra_info)\n        elif result_type == 'playlist':\n            # We process each entry in the playlist\n            playlist = ie_result.get('title', None) or ie_result.get('id', None)\n            self.to_screen('[download] Downloading playlist: %s' % playlist)\n\n            playlist_results = []\n\n            playliststart = self.params.get('playliststart', 1) - 1\n            playlistend = self.params.get('playlistend', None)\n            # For backwards compatibility, interpret -1 as whole list\n            if playlistend == -1:\n                playlistend = None\n\n            if isinstance(ie_result['entries'], list):\n                n_all_entries = len(ie_result['entries'])\n                entries = ie_result['entries'][playliststart:playlistend]\n                n_entries = len(entries)\n                self.to_screen(\n                    \"[%s] playlist %s: Collected %d video ids (downloading %d of them)\" %\n                    (ie_result['extractor'], playlist, n_all_entries, n_entries))\n            else:\n                assert isinstance(ie_result['entries'], PagedList)\n                entries = ie_result['entries'].getslice(\n                    playliststart, playlistend)\n                n_entries = len(entries)\n                self.to_screen(\n                    \"[%s] playlist %s: Downloading %d videos\" %\n                    (ie_result['extractor'], playlist, n_entries))\n\n            for i, entry in enumerate(entries, 1):\n                self.to_screen('[download] Downloading video #%s of %s' % (i, n_entries))\n                extra = {\n                    'n_entries': n_entries,\n                    'playlist': playlist,\n                    'playlist_index': i + playliststart,\n                    'extractor': ie_result['extractor'],\n                    'webpage_url': ie_result['webpage_url'],\n                    'webpage_url_basename': url_basename(ie_result['webpage_url']),\n                    'extractor_key': ie_result['extractor_key'],\n                }\n\n                reason = self._match_entry(entry)\n                if reason is not None:\n                    self.to_screen('[download] ' + reason)\n                    continue\n\n                entry_result = self.process_ie_result(entry,\n                                                      download=download,\n                                                      extra_info=extra)\n                playlist_results.append(entry_result)\n            ie_result['entries'] = playlist_results\n            return ie_result\n        elif result_type == 'compat_list':\n            def _fixup(r):\n                self.add_extra_info(r,\n                    {\n                        'extractor': ie_result['extractor'],\n                        'webpage_url': ie_result['webpage_url'],\n                        'webpage_url_basename': url_basename(ie_result['webpage_url']),\n                        'extractor_key': ie_result['extractor_key'],\n                    })\n                return r\n            ie_result['entries'] = [\n                self.process_ie_result(_fixup(r), download, extra_info)\n                for r in ie_result['entries']\n            ]\n            return ie_result\n        else:\n            raise Exception('Invalid result type: %s' % result_type)",
        "begin_line": 569,
        "end_line": 692,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002898550724637681,
            "pseudo_dstar_susp": 0.002976190476190476,
            "pseudo_tarantula_susp": 0.001440922190201729,
            "pseudo_op2_susp": 0.002976190476190476,
            "pseudo_barinel_susp": 0.001440922190201729
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.select_format#694",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.select_format(self, format_spec, available_formats)",
        "snippet": "    def select_format(self, format_spec, available_formats):\n        if format_spec == 'best' or format_spec is None:\n            return available_formats[-1]\n        elif format_spec == 'worst':\n            return available_formats[0]\n        elif format_spec == 'bestaudio':\n            audio_formats = [\n                f for f in available_formats\n                if f.get('vcodec') == 'none']\n            if audio_formats:\n                return audio_formats[-1]\n        elif format_spec == 'worstaudio':\n            audio_formats = [\n                f for f in available_formats\n                if f.get('vcodec') == 'none']\n            if audio_formats:\n                return audio_formats[0]\n        elif format_spec == 'bestvideo':\n            video_formats = [\n                f for f in available_formats\n                if f.get('acodec') == 'none']\n            if video_formats:\n                return video_formats[-1]\n        elif format_spec == 'worstvideo':\n            video_formats = [\n                f for f in available_formats\n                if f.get('acodec') == 'none']\n            if video_formats:\n                return video_formats[0]\n        else:\n            extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a']\n            if format_spec in extensions:\n                filter_f = lambda f: f['ext'] == format_spec\n            else:\n                filter_f = lambda f: f['format_id'] == format_spec\n            matches = list(filter(filter_f, available_formats))\n            if matches:\n                return matches[-1]\n        return None",
        "begin_line": 694,
        "end_line": 732,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016233766233766235,
            "pseudo_dstar_susp": 0.0021929824561403508,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0021929824561403508,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_video_result#734",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_video_result(self, info_dict, download=True)",
        "snippet": "    def process_video_result(self, info_dict, download=True):\n        assert info_dict.get('_type', 'video') == 'video'\n\n        if 'id' not in info_dict:\n            raise ExtractorError('Missing \"id\" field in extractor result')\n        if 'title' not in info_dict:\n            raise ExtractorError('Missing \"title\" field in extractor result')\n\n        if 'playlist' not in info_dict:\n            # It isn't part of a playlist\n            info_dict['playlist'] = None\n            info_dict['playlist_index'] = None\n\n        thumbnails = info_dict.get('thumbnails')\n        if thumbnails:\n            thumbnails.sort(key=lambda t: (\n                t.get('width'), t.get('height'), t.get('url')))\n            for t in thumbnails:\n                if 'width' in t and 'height' in t:\n                    t['resolution'] = '%dx%d' % (t['width'], t['height'])\n\n        if thumbnails and 'thumbnail' not in info_dict:\n            info_dict['thumbnail'] = thumbnails[-1]['url']\n\n        if 'display_id' not in info_dict and 'id' in info_dict:\n            info_dict['display_id'] = info_dict['id']\n\n        if info_dict.get('upload_date') is None and info_dict.get('timestamp') is not None:\n            upload_date = datetime.datetime.utcfromtimestamp(\n                info_dict['timestamp'])\n            info_dict['upload_date'] = upload_date.strftime('%Y%m%d')\n\n        # This extractors handle format selection themselves\n        if info_dict['extractor'] in ['Youku']:\n            if download:\n                self.process_info(info_dict)\n            return info_dict\n\n        # We now pick which formats have to be downloaded\n        if info_dict.get('formats') is None:\n            # There's only one format available\n            formats = [info_dict]\n        else:\n            formats = info_dict['formats']\n\n        if not formats:\n            raise ExtractorError('No video formats found!')\n\n        # We check that all the formats have the format and format_id fields\n        for i, format in enumerate(formats):\n            if 'url' not in format:\n                raise ExtractorError('Missing \"url\" key in result (index %d)' % i)\n\n            if format.get('format_id') is None:\n                format['format_id'] = compat_str(i)\n            if format.get('format') is None:\n                format['format'] = '{id} - {res}{note}'.format(\n                    id=format['format_id'],\n                    res=self.format_resolution(format),\n                    note=' ({0})'.format(format['format_note']) if format.get('format_note') is not None else '',\n                )\n            # Automatically determine file extension if missing\n            if 'ext' not in format:\n                format['ext'] = determine_ext(format['url']).lower()\n\n        format_limit = self.params.get('format_limit', None)\n        if format_limit:\n            formats = list(takewhile_inclusive(\n                lambda f: f['format_id'] != format_limit, formats\n            ))\n\n        # TODO Central sorting goes here\n\n        if formats[0] is not info_dict:\n            # only set the 'formats' fields if the original info_dict list them\n            # otherwise we end up with a circular reference, the first (and unique)\n            # element in the 'formats' field in info_dict is info_dict itself,\n            # wich can't be exported to json\n            info_dict['formats'] = formats\n        if self.params.get('listformats', None):\n            self.list_formats(info_dict)\n            return\n\n        req_format = self.params.get('format')\n        if req_format is None:\n            req_format = 'best'\n        formats_to_download = []\n        # The -1 is for supporting YoutubeIE\n        if req_format in ('-1', 'all'):\n            formats_to_download = formats\n        else:\n            for rfstr in req_format.split(','):\n                # We can accept formats requested in the format: 34/5/best, we pick\n                # the first that is available, starting from left\n                req_formats = rfstr.split('/')\n                for rf in req_formats:\n                    if re.match(r'.+?\\+.+?', rf) is not None:\n                        # Two formats have been requested like '137+139'\n                        format_1, format_2 = rf.split('+')\n                        formats_info = (self.select_format(format_1, formats),\n                            self.select_format(format_2, formats))\n                        if all(formats_info):\n                            selected_format = {\n                                'requested_formats': formats_info,\n                                'format': rf,\n                                'ext': formats_info[0]['ext'],\n                            }\n                        else:\n                            selected_format = None\n                    else:\n                        selected_format = self.select_format(rf, formats)\n                    if selected_format is not None:\n                        formats_to_download.append(selected_format)\n                        break\n        if not formats_to_download:\n            raise ExtractorError('requested format not available',\n                                 expected=True)\n\n        if download:\n            if len(formats_to_download) > 1:\n                self.to_screen('[info] %s: downloading video in %s formats' % (info_dict['id'], len(formats_to_download)))\n            for format in formats_to_download:\n                new_info = dict(info_dict)\n                new_info.update(format)\n                self.process_info(new_info)\n        # We update the info dict with the best quality format (backwards compatibility)\n        info_dict.update(formats_to_download[-1])\n        return info_dict",
        "begin_line": 734,
        "end_line": 861,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001652892561983471,
            "pseudo_dstar_susp": 0.0022026431718061676,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0022026431718061676,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.process_info#863",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.process_info(self, info_dict)",
        "snippet": "    def process_info(self, info_dict):\n        \"\"\"Process a single resolved IE result.\"\"\"\n\n        assert info_dict.get('_type', 'video') == 'video'\n\n        max_downloads = self.params.get('max_downloads')\n        if max_downloads is not None:\n            if self._num_downloads >= int(max_downloads):\n                raise MaxDownloadsReached()\n\n        info_dict['fulltitle'] = info_dict['title']\n        if len(info_dict['title']) > 200:\n            info_dict['title'] = info_dict['title'][:197] + '...'\n\n        # Keep for backwards compatibility\n        info_dict['stitle'] = info_dict['title']\n\n        if 'format' not in info_dict:\n            info_dict['format'] = info_dict['ext']\n\n        reason = self._match_entry(info_dict)\n        if reason is not None:\n            self.to_screen('[download] ' + reason)\n            return\n\n        self._num_downloads += 1\n\n        filename = self.prepare_filename(info_dict)\n\n        # Forced printings\n        if self.params.get('forcetitle', False):\n            self.to_stdout(info_dict['fulltitle'])\n        if self.params.get('forceid', False):\n            self.to_stdout(info_dict['id'])\n        if self.params.get('forceurl', False):\n            # For RTMP URLs, also include the playpath\n            self.to_stdout(info_dict['url'] + info_dict.get('play_path', ''))\n        if self.params.get('forcethumbnail', False) and info_dict.get('thumbnail') is not None:\n            self.to_stdout(info_dict['thumbnail'])\n        if self.params.get('forcedescription', False) and info_dict.get('description') is not None:\n            self.to_stdout(info_dict['description'])\n        if self.params.get('forcefilename', False) and filename is not None:\n            self.to_stdout(filename)\n        if self.params.get('forceduration', False) and info_dict.get('duration') is not None:\n            self.to_stdout(formatSeconds(info_dict['duration']))\n        if self.params.get('forceformat', False):\n            self.to_stdout(info_dict['format'])\n        if self.params.get('forcejson', False):\n            info_dict['_filename'] = filename\n            self.to_stdout(json.dumps(info_dict))\n        if self.params.get('dump_single_json', False):\n            info_dict['_filename'] = filename\n\n        # Do nothing else if in simulate mode\n        if self.params.get('simulate', False):\n            return\n\n        if filename is None:\n            return\n\n        try:\n            dn = os.path.dirname(encodeFilename(filename))\n            if dn and not os.path.exists(dn):\n                os.makedirs(dn)\n        except (OSError, IOError) as err:\n            self.report_error('unable to create directory ' + compat_str(err))\n            return\n\n        if self.params.get('writedescription', False):\n            descfn = filename + '.description'\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(descfn)):\n                self.to_screen('[info] Video description is already present')\n            else:\n                try:\n                    self.to_screen('[info] Writing video description to: ' + descfn)\n                    with io.open(encodeFilename(descfn), 'w', encoding='utf-8') as descfile:\n                        descfile.write(info_dict['description'])\n                except (KeyError, TypeError):\n                    self.report_warning('There\\'s no description to write.')\n                except (OSError, IOError):\n                    self.report_error('Cannot write description file ' + descfn)\n                    return\n\n        if self.params.get('writeannotations', False):\n            annofn = filename + '.annotations.xml'\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(annofn)):\n                self.to_screen('[info] Video annotations are already present')\n            else:\n                try:\n                    self.to_screen('[info] Writing video annotations to: ' + annofn)\n                    with io.open(encodeFilename(annofn), 'w', encoding='utf-8') as annofile:\n                        annofile.write(info_dict['annotations'])\n                except (KeyError, TypeError):\n                    self.report_warning('There are no annotations to write.')\n                except (OSError, IOError):\n                    self.report_error('Cannot write annotations file: ' + annofn)\n                    return\n\n        subtitles_are_requested = any([self.params.get('writesubtitles', False),\n                                       self.params.get('writeautomaticsub')])\n\n        if subtitles_are_requested and 'subtitles' in info_dict and info_dict['subtitles']:\n            # subtitles download errors are already managed as troubles in relevant IE\n            # that way it will silently go on when used with unsupporting IE\n            subtitles = info_dict['subtitles']\n            sub_format = self.params.get('subtitlesformat', 'srt')\n            for sub_lang in subtitles.keys():\n                sub = subtitles[sub_lang]\n                if sub is None:\n                    continue\n                try:\n                    sub_filename = subtitles_filename(filename, sub_lang, sub_format)\n                    if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(sub_filename)):\n                        self.to_screen('[info] Video subtitle %s.%s is already_present' % (sub_lang, sub_format))\n                    else:\n                        self.to_screen('[info] Writing video subtitles to: ' + sub_filename)\n                        with io.open(encodeFilename(sub_filename), 'w', encoding='utf-8') as subfile:\n                                subfile.write(sub)\n                except (OSError, IOError):\n                    self.report_error('Cannot write subtitles file ' + sub_filename)\n                    return\n\n        if self.params.get('writeinfojson', False):\n            infofn = os.path.splitext(filename)[0] + '.info.json'\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(infofn)):\n                self.to_screen('[info] Video description metadata is already present')\n            else:\n                self.to_screen('[info] Writing video description metadata as JSON to: ' + infofn)\n                try:\n                    write_json_file(info_dict, encodeFilename(infofn))\n                except (OSError, IOError):\n                    self.report_error('Cannot write metadata to JSON file ' + infofn)\n                    return\n\n        if self.params.get('writethumbnail', False):\n            if info_dict.get('thumbnail') is not None:\n                thumb_format = determine_ext(info_dict['thumbnail'], 'jpg')\n                thumb_filename = os.path.splitext(filename)[0] + '.' + thumb_format\n                if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(thumb_filename)):\n                    self.to_screen('[%s] %s: Thumbnail is already present' %\n                                   (info_dict['extractor'], info_dict['id']))\n                else:\n                    self.to_screen('[%s] %s: Downloading thumbnail ...' %\n                                   (info_dict['extractor'], info_dict['id']))\n                    try:\n                        uf = self.urlopen(info_dict['thumbnail'])\n                        with open(thumb_filename, 'wb') as thumbf:\n                            shutil.copyfileobj(uf, thumbf)\n                        self.to_screen('[%s] %s: Writing thumbnail to: %s' %\n                            (info_dict['extractor'], info_dict['id'], thumb_filename))\n                    except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n                        self.report_warning('Unable to download thumbnail \"%s\": %s' %\n                            (info_dict['thumbnail'], compat_str(err)))\n\n        if not self.params.get('skip_download', False):\n            if self.params.get('nooverwrites', False) and os.path.exists(encodeFilename(filename)):\n                success = True\n            else:\n                try:\n                    def dl(name, info):\n                        fd = get_suitable_downloader(info)(self, self.params)\n                        for ph in self._progress_hooks:\n                            fd.add_progress_hook(ph)\n                        if self.params.get('verbose'):\n                            self.to_stdout('[debug] Invoking downloader on %r' % info.get('url'))\n                        return fd.download(name, info)\n                    if info_dict.get('requested_formats') is not None:\n                        downloaded = []\n                        success = True\n                        merger = FFmpegMergerPP(self, not self.params.get('keepvideo'))\n                        if not merger._executable:\n                            postprocessors = []\n                            self.report_warning('You have requested multiple '\n                                'formats but ffmpeg or avconv are not installed.'\n                                ' The formats won\\'t be merged')\n                        else:\n                            postprocessors = [merger]\n                        for f in info_dict['requested_formats']:\n                            new_info = dict(info_dict)\n                            new_info.update(f)\n                            fname = self.prepare_filename(new_info)\n                            fname = prepend_extension(fname, 'f%s' % f['format_id'])\n                            downloaded.append(fname)\n                            partial_success = dl(fname, new_info)\n                            success = success and partial_success\n                        info_dict['__postprocessors'] = postprocessors\n                        info_dict['__files_to_merge'] = downloaded\n                    else:\n                        # Just a single file\n                        success = dl(filename, info_dict)\n                except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n                    self.report_error('unable to download video data: %s' % str(err))\n                    return\n                except (OSError, IOError) as err:\n                    raise UnavailableVideoError(err)\n                except (ContentTooShortError, ) as err:\n                    self.report_error('content too short (expected %s bytes and served %s)' % (err.expected, err.downloaded))\n                    return\n\n            if success:\n                try:\n                    self.post_process(filename, info_dict)\n                except (PostProcessingError) as err:\n                    self.report_error('postprocessing: %s' % str(err))\n                    return\n\n        self.record_download_archive(info_dict)",
        "begin_line": 863,
        "end_line": 1069,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0017825311942959,
            "pseudo_dstar_susp": 0.002232142857142857,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.002232142857142857,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.dl#1022",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.dl(name, info)",
        "snippet": "                    def dl(name, info):\n                        fd = get_suitable_downloader(info)(self, self.params)\n                        for ph in self._progress_hooks:\n                            fd.add_progress_hook(ph)\n                        if self.params.get('verbose'):\n                            self.to_stdout('[debug] Invoking downloader on %r' % info.get('url'))\n                        return fd.download(name, info)",
        "begin_line": 1022,
        "end_line": 1028,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013477088948787063,
            "pseudo_dstar_susp": 0.0016501650165016502,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0016501650165016502,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.download#1071",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.download(self, url_list)",
        "snippet": "    def download(self, url_list):\n        \"\"\"Download a given list of URLs.\"\"\"\n        outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n        if (len(url_list) > 1 and\n                '%' not in outtmpl\n                and self.params.get('max_downloads') != 1):\n            raise SameFileError(outtmpl)\n\n        for url in url_list:\n            try:\n                #It also downloads the videos\n                res = self.extract_info(url)\n            except UnavailableVideoError:\n                self.report_error('unable to download video')\n            except MaxDownloadsReached:\n                self.to_screen('[info] Maximum number of downloaded files reached.')\n                raise\n            else:\n                if self.params.get('dump_single_json', False):\n                    self.to_stdout(json.dumps(res))\n\n        return self._download_retcode",
        "begin_line": 1071,
        "end_line": 1092,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011534025374855825,
            "pseudo_dstar_susp": 0.0010905125408942203,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010905125408942203,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.post_process#1108",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.post_process(self, filename, ie_info)",
        "snippet": "    def post_process(self, filename, ie_info):\n        \"\"\"Run all the postprocessors on the given file.\"\"\"\n        info = dict(ie_info)\n        info['filepath'] = filename\n        keep_video = None\n        pps_chain = []\n        if ie_info.get('__postprocessors') is not None:\n            pps_chain.extend(ie_info['__postprocessors'])\n        pps_chain.extend(self._pps)\n        for pp in pps_chain:\n            try:\n                keep_video_wish, new_info = pp.run(info)\n                if keep_video_wish is not None:\n                    if keep_video_wish:\n                        keep_video = keep_video_wish\n                    elif keep_video is None:\n                        # No clear decision yet, let IE decide\n                        keep_video = keep_video_wish\n            except PostProcessingError as e:\n                self.report_error(e.msg)\n        if keep_video is False and not self.params.get('keepvideo', False):\n            try:\n                self.to_screen('Deleting original file %s (pass -k to keep)' % filename)\n                os.remove(encodeFilename(filename))\n            except (IOError, OSError):\n                self.report_warning('Unable to remove downloaded video file')",
        "begin_line": 1108,
        "end_line": 1133,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00117096018735363,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.in_download_archive#1146",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.in_download_archive(self, info_dict)",
        "snippet": "    def in_download_archive(self, info_dict):\n        fn = self.params.get('download_archive')\n        if fn is None:\n            return False\n\n        vid_id = self._make_archive_id(info_dict)\n        if vid_id is None:\n            return False  # Incomplete video information\n\n        try:\n            with locked_file(fn, 'r', encoding='utf-8') as archive_file:\n                for line in archive_file:\n                    if line.strip() == vid_id:\n                        return True\n        except IOError as ioe:\n            if ioe.errno != errno.ENOENT:\n                raise\n        return False",
        "begin_line": 1146,
        "end_line": 1163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022624434389140274,
            "pseudo_dstar_susp": 0.0024630541871921183,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0024630541871921183,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.record_download_archive#1165",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.record_download_archive(self, info_dict)",
        "snippet": "    def record_download_archive(self, info_dict):\n        fn = self.params.get('download_archive')\n        if fn is None:\n            return\n        vid_id = self._make_archive_id(info_dict)\n        assert vid_id\n        with locked_file(fn, 'a', encoding='utf-8') as archive_file:\n            archive_file.write(vid_id + '\\n')",
        "begin_line": 1165,
        "end_line": 1172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001287001287001287,
            "pseudo_dstar_susp": 0.0015384615384615385,
            "pseudo_tarantula_susp": 0.0008620689655172414,
            "pseudo_op2_susp": 0.0015384615384615385,
            "pseudo_barinel_susp": 0.0008620689655172414
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.format_resolution#1175",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.format_resolution(format, default='unknown')",
        "snippet": "    def format_resolution(format, default='unknown'):\n        if format.get('vcodec') == 'none':\n            return 'audio only'\n        if format.get('resolution') is not None:\n            return format['resolution']\n        if format.get('height') is not None:\n            if format.get('width') is not None:\n                res = '%sx%s' % (format['width'], format['height'])\n            else:\n                res = '%sp' % format['height']\n        elif format.get('width') is not None:\n            res = '?x%d' % format['width']\n        else:\n            res = default\n        return res",
        "begin_line": 1175,
        "end_line": 1189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014792899408284023,
            "pseudo_dstar_susp": 0.001869158878504673,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001869158878504673,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._format_note#1191",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._format_note(self, fdict)",
        "snippet": "    def _format_note(self, fdict):\n        res = ''\n        if fdict.get('ext') in ['f4f', 'f4m']:\n            res += '(unsupported) '\n        if fdict.get('format_note') is not None:\n            res += fdict['format_note'] + ' '\n        if fdict.get('tbr') is not None:\n            res += '%4dk ' % fdict['tbr']\n        if fdict.get('container') is not None:\n            if res:\n                res += ', '\n            res += '%s container' % fdict['container']\n        if (fdict.get('vcodec') is not None and\n                fdict.get('vcodec') != 'none'):\n            if res:\n                res += ', '\n            res += fdict['vcodec']\n            if fdict.get('vbr') is not None:\n                res += '@'\n        elif fdict.get('vbr') is not None and fdict.get('abr') is not None:\n            res += 'video@'\n        if fdict.get('vbr') is not None:\n            res += '%4dk' % fdict['vbr']\n        if fdict.get('acodec') is not None:\n            if res:\n                res += ', '\n            if fdict['acodec'] == 'none':\n                res += 'video only'\n            else:\n                res += '%-5s' % fdict['acodec']\n        elif fdict.get('abr') is not None:\n            if res:\n                res += ', '\n            res += 'audio'\n        if fdict.get('abr') is not None:\n            res += '@%3dk' % fdict['abr']\n        if fdict.get('asr') is not None:\n            res += ' (%5dHz)' % fdict['asr']\n        if fdict.get('filesize') is not None:\n            if res:\n                res += ', '\n            res += format_bytes(fdict['filesize'])\n        elif fdict.get('filesize_approx') is not None:\n            if res:\n                res += ', '\n            res += '~' + format_bytes(fdict['filesize_approx'])\n        return res",
        "begin_line": 1191,
        "end_line": 1237,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.urlopen#1262",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.urlopen(self, req)",
        "snippet": "    def urlopen(self, req):\n        \"\"\" Start an HTTP download \"\"\"\n\n        # According to RFC 3986, URLs can not contain non-ASCII characters, however this is not\n        # always respected by websites, some tend to give out URLs with non percent-encoded\n        # non-ASCII characters (see telemb.py, ard.py [#3412])\n        # urllib chokes on URLs with non-ASCII characters (see http://bugs.python.org/issue3991)\n        # To work around aforementioned issue we will replace request's original URL with\n        # percent-encoded one\n        req_is_string = isinstance(req, basestring if sys.version_info < (3, 0) else compat_str)\n        url = req if req_is_string else req.get_full_url()\n        url_escaped = escape_url(url)\n\n        # Substitute URL if any change after escaping\n        if url != url_escaped:\n            if req_is_string:\n                req = url_escaped\n            else:\n                req = compat_urllib_request.Request(\n                    url_escaped, data=req.data, headers=req.headers,\n                    origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n\n        return self._opener.open(req, timeout=self._socket_timeout)",
        "begin_line": 1262,
        "end_line": 1284,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0196078431372549,
            "pseudo_dstar_susp": 0.00980392156862745,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.00980392156862745,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.print_debug_header#1286",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.print_debug_header(self)",
        "snippet": "    def print_debug_header(self):\n        if not self.params.get('verbose'):\n            return\n\n        if type('') is not compat_str:\n            # Python 2.6 on SLES11 SP1 (https://github.com/rg3/youtube-dl/issues/3326)\n            self.report_warning(\n                'Your Python is broken! Update to a newer and supported version')\n\n        encoding_str = (\n            '[debug] Encodings: locale %s, fs %s, out %s, pref %s\\n' % (\n                locale.getpreferredencoding(),\n                sys.getfilesystemencoding(),\n                sys.stdout.encoding,\n                self.get_encoding()))\n        write_string(encoding_str, encoding=None)\n\n        self._write_string('[debug] youtube-dl version ' + __version__ + '\\n')\n        try:\n            sp = subprocess.Popen(\n                ['git', 'rev-parse', '--short', 'HEAD'],\n                stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                cwd=os.path.dirname(os.path.abspath(__file__)))\n            out, err = sp.communicate()\n            out = out.decode().strip()\n            if re.match('[0-9a-f]+', out):\n                self._write_string('[debug] Git HEAD: ' + out + '\\n')\n        except:\n            try:\n                sys.exc_clear()\n            except:\n                pass\n        self._write_string('[debug] Python version %s - %s\\n' % (\n            platform.python_version(), platform_name()))\n\n        exe_versions = FFmpegPostProcessor.get_versions()\n        exe_str = ', '.join(\n            '%s %s' % (exe, v)\n            for exe, v in sorted(exe_versions.items())\n            if v\n        )\n        if not exe_str:\n            exe_str = 'none'\n        self._write_string('[debug] exe versions: %s\\n' % exe_str)\n\n        proxy_map = {}\n        for handler in self._opener.handlers:\n            if hasattr(handler, 'proxies'):\n                proxy_map.update(handler.proxies)\n        self._write_string('[debug] Proxy map: ' + compat_str(proxy_map) + '\\n')",
        "begin_line": 1286,
        "end_line": 1335,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010101010101010102,
            "pseudo_dstar_susp": 0.0125,
            "pseudo_tarantula_susp": 0.0018181818181818182,
            "pseudo_op2_susp": 0.0125,
            "pseudo_barinel_susp": 0.0018181818181818182
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL._setup_opener#1337",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL._setup_opener(self)",
        "snippet": "    def _setup_opener(self):\n        timeout_val = self.params.get('socket_timeout')\n        self._socket_timeout = 600 if timeout_val is None else float(timeout_val)\n\n        opts_cookiefile = self.params.get('cookiefile')\n        opts_proxy = self.params.get('proxy')\n\n        if opts_cookiefile is None:\n            self.cookiejar = compat_cookiejar.CookieJar()\n        else:\n            self.cookiejar = compat_cookiejar.MozillaCookieJar(\n                opts_cookiefile)\n            if os.access(opts_cookiefile, os.R_OK):\n                self.cookiejar.load()\n\n        cookie_processor = compat_urllib_request.HTTPCookieProcessor(\n            self.cookiejar)\n        if opts_proxy is not None:\n            if opts_proxy == '':\n                proxies = {}\n            else:\n                proxies = {'http': opts_proxy, 'https': opts_proxy}\n        else:\n            proxies = compat_urllib_request.getproxies()\n            # Set HTTPS proxy to HTTP one if given (https://github.com/rg3/youtube-dl/issues/805)\n            if 'http' in proxies and 'https' not in proxies:\n                proxies['https'] = proxies['http']\n        proxy_handler = compat_urllib_request.ProxyHandler(proxies)\n\n        debuglevel = 1 if self.params.get('debug_printtraffic') else 0\n        https_handler = make_HTTPS_handler(\n            self.params.get('nocheckcertificate', False), debuglevel=debuglevel)\n        ydlh = YoutubeDLHandler(debuglevel=debuglevel)\n        opener = compat_urllib_request.build_opener(\n            https_handler, proxy_handler, cookie_processor, ydlh)\n        # Delete the default user-agent header, which would otherwise apply in\n        # cases where our custom HTTP handler doesn't come into play\n        # (See https://github.com/rg3/youtube-dl/issues/1309 for details)\n        opener.addheaders = []\n        self._opener = opener",
        "begin_line": 1337,
        "end_line": 1376,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010101010101010102,
            "pseudo_dstar_susp": 0.0125,
            "pseudo_tarantula_susp": 0.001697792869269949,
            "pseudo_op2_susp": 0.0125,
            "pseudo_barinel_susp": 0.001697792869269949
        }
    },
    {
        "name": "youtube_dl.YoutubeDL.YoutubeDL.get_encoding#1388",
        "src_path": "youtube_dl/YoutubeDL.py",
        "class_name": "youtube_dl.YoutubeDL.YoutubeDL",
        "signature": "youtube_dl.YoutubeDL.YoutubeDL.get_encoding(self)",
        "snippet": "    def get_encoding(self):\n        encoding = self.params.get('encoding')\n        if encoding is None:\n            encoding = preferredencoding()\n        return encoding",
        "begin_line": 1388,
        "end_line": 1392,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007518796992481203,
            "pseudo_dstar_susp": 0.007142857142857143,
            "pseudo_tarantula_susp": 0.0018181818181818182,
            "pseudo_op2_susp": 0.007142857142857143,
            "pseudo_barinel_susp": 0.0018181818181818182
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.__init__#159",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.__init__(self, downloader=None)",
        "snippet": "    def __init__(self, downloader=None):\n        \"\"\"Constructor. Receives an optional downloader.\"\"\"\n        self._ready = False\n        self.set_downloader(downloader)",
        "begin_line": 159,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.022727272727272728,
            "pseudo_dstar_susp": 0.037037037037037035,
            "pseudo_tarantula_susp": 0.001422475106685633,
            "pseudo_op2_susp": 0.037037037037037035,
            "pseudo_barinel_susp": 0.001422475106685633
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.suitable#165",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        \"\"\"Receives a URL and returns True if suitable for this IE.\"\"\"\n\n        # This does not use has/getattr intentionally - we want to know whether\n        # we have cached the regexp for *this* class, whereas getattr would also\n        # match the superclass\n        if '_VALID_URL_RE' not in cls.__dict__:\n            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n        return cls._VALID_URL_RE.match(url) is not None",
        "begin_line": 165,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.005405405405405406,
            "pseudo_dstar_susp": 0.02,
            "pseudo_tarantula_susp": 0.0013315579227696406,
            "pseudo_op2_susp": 0.02,
            "pseudo_barinel_susp": 0.0013315579227696406
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._match_id#176",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._match_id(cls, url)",
        "snippet": "    def _match_id(cls, url):\n        if '_VALID_URL_RE' not in cls.__dict__:\n            cls._VALID_URL_RE = re.compile(cls._VALID_URL)\n        m = cls._VALID_URL_RE.match(url)\n        assert m\n        return m.group('id')",
        "begin_line": 176,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0033003300330033004,
            "pseudo_dstar_susp": 0.0033112582781456954,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0033112582781456954,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.working#184",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.working(cls)",
        "snippet": "    def working(cls):\n        \"\"\"Getter method for _WORKING.\"\"\"\n        return cls._WORKING",
        "begin_line": 184,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006134969325153374,
            "pseudo_dstar_susp": 0.03125,
            "pseudo_tarantula_susp": 0.001277139208173691,
            "pseudo_op2_susp": 0.03125,
            "pseudo_barinel_susp": 0.001277139208173691
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.initialize#188",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.initialize(self)",
        "snippet": "    def initialize(self):\n        \"\"\"Initializes an instance (authentication, etc).\"\"\"\n        if not self._ready:\n            self._real_initialize()\n            self._ready = True",
        "begin_line": 188,
        "end_line": 192,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.008771929824561403,
            "pseudo_dstar_susp": 0.029411764705882353,
            "pseudo_tarantula_susp": 0.0014492753623188406,
            "pseudo_op2_susp": 0.029411764705882353,
            "pseudo_barinel_susp": 0.0014492753623188406
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.set_downloader#199",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.set_downloader(self, downloader)",
        "snippet": "    def set_downloader(self, downloader):\n        \"\"\"Sets the downloader for this IE.\"\"\"\n        self._downloader = downloader",
        "begin_line": 199,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.000572737686139748,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.ie_key#212",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.ie_key(cls)",
        "snippet": "    def ie_key(cls):\n        \"\"\"A string for getting the InfoExtractor with get_info_extractor\"\"\"\n        return cls.__name__[:-2]",
        "begin_line": 212,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.027777777777777776,
            "pseudo_dstar_susp": 0.023809523809523808,
            "pseudo_tarantula_susp": 0.002127659574468085,
            "pseudo_op2_susp": 0.023809523809523808,
            "pseudo_barinel_susp": 0.002127659574468085
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.IE_NAME#217",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.IE_NAME(self)",
        "snippet": "    def IE_NAME(self):\n        return type(self).__name__[:-2]",
        "begin_line": 217,
        "end_line": 218,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0035335689045936395,
            "pseudo_dstar_susp": 0.0036363636363636364,
            "pseudo_tarantula_susp": 0.0013440860215053765,
            "pseudo_op2_susp": 0.0036363636363636364,
            "pseudo_barinel_susp": 0.0013440860215053765
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._request_webpage#220",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._request_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True)",
        "snippet": "    def _request_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True):\n        \"\"\" Returns the response handle \"\"\"\n        if note is None:\n            self.report_download_webpage(video_id)\n        elif note is not False:\n            if video_id is None:\n                self.to_screen('%s' % (note,))\n            else:\n                self.to_screen('%s: %s' % (video_id, note))\n        try:\n            return self._downloader.urlopen(url_or_request)\n        except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:\n            if errnote is False:\n                return False\n            if errnote is None:\n                errnote = 'Unable to download webpage'\n            errmsg = '%s: %s' % (errnote, compat_str(err))\n            if fatal:\n                raise ExtractorError(errmsg, sys.exc_info()[2], cause=err)\n            else:\n                self._downloader.report_warning(errmsg)\n                return False",
        "begin_line": 220,
        "end_line": 241,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.03225806451612903,
            "pseudo_dstar_susp": 0.02857142857142857,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.02857142857142857,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_webpage_handle#243",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_webpage_handle(self, url_or_request, video_id, note=None, errnote=None, fatal=True)",
        "snippet": "    def _download_webpage_handle(self, url_or_request, video_id, note=None, errnote=None, fatal=True):\n        \"\"\" Returns a tuple (page content as string, URL handle) \"\"\"\n        # Strip hashes from the URL (#1038)\n        if isinstance(url_or_request, (compat_str, str)):\n            url_or_request = url_or_request.partition('#')[0]\n\n        urlh = self._request_webpage(url_or_request, video_id, note, errnote, fatal)\n        if urlh is False:\n            assert not fatal\n            return False\n        content = self._webpage_read_content(urlh, url_or_request, video_id, note, errnote, fatal)\n        return (content, urlh)",
        "begin_line": 243,
        "end_line": 254,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02564102564102564,
            "pseudo_dstar_susp": 0.018867924528301886,
            "pseudo_tarantula_susp": 0.0024271844660194173,
            "pseudo_op2_susp": 0.018867924528301886,
            "pseudo_barinel_susp": 0.0024271844660194173
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._webpage_read_content#256",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._webpage_read_content(self, urlh, url_or_request, video_id, note=None, errnote=None, fatal=True)",
        "snippet": "    def _webpage_read_content(self, urlh, url_or_request, video_id, note=None, errnote=None, fatal=True):\n        content_type = urlh.headers.get('Content-Type', '')\n        webpage_bytes = urlh.read()\n        m = re.match(r'[a-zA-Z0-9_.-]+/[a-zA-Z0-9_.-]+\\s*;\\s*charset=(.+)', content_type)\n        if m:\n            encoding = m.group(1)\n        else:\n            m = re.search(br'<meta[^>]+charset=[\\'\"]?([^\\'\")]+)[ /\\'\">]',\n                          webpage_bytes[:1024])\n            if m:\n                encoding = m.group(1).decode('ascii')\n            elif webpage_bytes.startswith(b'\\xff\\xfe'):\n                encoding = 'utf-16'\n            else:\n                encoding = 'utf-8'\n        if self._downloader.params.get('dump_intermediate_pages', False):\n            try:\n                url = url_or_request.get_full_url()\n            except AttributeError:\n                url = url_or_request\n            self.to_screen('Dumping request to ' + url)\n            dump = base64.b64encode(webpage_bytes).decode('ascii')\n            self._downloader.to_screen(dump)\n        if self._downloader.params.get('write_pages', False):\n            try:\n                url = url_or_request.get_full_url()\n            except AttributeError:\n                url = url_or_request\n            basen = '%s_%s' % (video_id, url)\n            if len(basen) > 240:\n                h = '___' + hashlib.md5(basen.encode('utf-8')).hexdigest()\n                basen = basen[:240 - len(h)] + h\n            raw_filename = basen + '.dump'\n            filename = sanitize_filename(raw_filename, restricted=True)\n            self.to_screen('Saving request to ' + filename)\n            # Working around MAX_PATH limitation on Windows (see\n            # http://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx)\n            if os.name == 'nt':\n                absfilepath = os.path.abspath(filename)\n                if len(absfilepath) > 259:\n                    filename = '\\\\\\\\?\\\\' + absfilepath\n            with open(filename, 'wb') as outf:\n                outf.write(webpage_bytes)\n\n        try:\n            content = webpage_bytes.decode(encoding, 'replace')\n        except LookupError:\n            content = webpage_bytes.decode('utf-8', 'replace')\n\n        if ('<title>Access to this site is blocked</title>' in content and\n                'Websense' in content[:512]):\n            msg = 'Access to this webpage has been blocked by Websense filtering software in your network.'\n            blocked_iframe = self._html_search_regex(\n                r'<iframe src=\"([^\"]+)\"', content,\n                'Websense information URL', default=None)\n            if blocked_iframe:\n                msg += ' Visit %s for more details' % blocked_iframe\n            raise ExtractorError(msg, expected=True)\n\n        return content",
        "begin_line": 256,
        "end_line": 315,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003745318352059925,
            "pseudo_dstar_susp": 0.0037313432835820895,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0037313432835820895,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_webpage#317",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True)",
        "snippet": "    def _download_webpage(self, url_or_request, video_id, note=None, errnote=None, fatal=True):\n        \"\"\" Returns the data of the page as a string \"\"\"\n        res = self._download_webpage_handle(url_or_request, video_id, note, errnote, fatal)\n        if res is False:\n            return res\n        else:\n            content, _ = res\n            return content",
        "begin_line": 317,
        "end_line": 324,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006329113924050633,
            "pseudo_dstar_susp": 0.005917159763313609,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.005917159763313609,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_xml#326",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_xml(self, url_or_request, video_id, note='Downloading XML', errnote='Unable to download XML', transform_source=None, fatal=True)",
        "snippet": "    def _download_xml(self, url_or_request, video_id,\n                      note='Downloading XML', errnote='Unable to download XML',\n                      transform_source=None, fatal=True):\n        \"\"\"Return the xml as an xml.etree.ElementTree.Element\"\"\"\n        xml_string = self._download_webpage(\n            url_or_request, video_id, note, errnote, fatal=fatal)\n        if xml_string is False:\n            return xml_string\n        if transform_source:\n            xml_string = transform_source(xml_string)\n        return xml.etree.ElementTree.fromstring(xml_string.encode('utf-8'))",
        "begin_line": 326,
        "end_line": 336,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021598272138228943,
            "pseudo_dstar_susp": 0.002336448598130841,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.002336448598130841,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._download_json#338",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._download_json(self, url_or_request, video_id, note='Downloading JSON metadata', errnote='Unable to download JSON metadata', transform_source=None, fatal=True)",
        "snippet": "    def _download_json(self, url_or_request, video_id,\n                       note='Downloading JSON metadata',\n                       errnote='Unable to download JSON metadata',\n                       transform_source=None,\n                       fatal=True):\n        json_string = self._download_webpage(\n            url_or_request, video_id, note, errnote, fatal=fatal)\n        if (not fatal) and json_string is False:\n            return None\n        if transform_source:\n            json_string = transform_source(json_string)\n        try:\n            return json.loads(json_string)\n        except ValueError as ve:\n            errmsg = '%s: Failed to parse JSON ' % video_id\n            if fatal:\n                raise ExtractorError(errmsg, cause=ve)\n            else:\n                self.report_warning(errmsg + str(ve))",
        "begin_line": 338,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003194888178913738,
            "pseudo_dstar_susp": 0.0031847133757961785,
            "pseudo_tarantula_susp": 0.0031446540880503146,
            "pseudo_op2_susp": 0.0031847133757961785,
            "pseudo_barinel_susp": 0.0031446540880503146
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_warning#358",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_warning(self, msg, video_id=None)",
        "snippet": "    def report_warning(self, msg, video_id=None):\n        idstr = '' if video_id is None else '%s: ' % video_id\n        self._downloader.report_warning(\n            '[%s] %s%s' % (self.IE_NAME, idstr, msg))",
        "begin_line": 358,
        "end_line": 361,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.to_screen#363",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.to_screen(self, msg)",
        "snippet": "    def to_screen(self, msg):\n        \"\"\"Print msg to screen, prefixing it with '[ie_name]'\"\"\"\n        self._downloader.to_screen('[%s] %s' % (self.IE_NAME, msg))",
        "begin_line": 363,
        "end_line": 365,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015151515151515152,
            "pseudo_dstar_susp": 0.008264462809917356,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.008264462809917356,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_extraction#367",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_extraction(self, id_or_name)",
        "snippet": "    def report_extraction(self, id_or_name):\n        \"\"\"Report information extraction.\"\"\"\n        self.to_screen('%s: Extracting information' % id_or_name)",
        "begin_line": 367,
        "end_line": 369,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0030959752321981426,
            "pseudo_dstar_susp": 0.0028735632183908046,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0028735632183908046,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.report_download_webpage#371",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.report_download_webpage(self, video_id)",
        "snippet": "    def report_download_webpage(self, video_id):\n        \"\"\"Report webpage download.\"\"\"\n        self.to_screen('%s: Downloading webpage' % video_id)",
        "begin_line": 371,
        "end_line": 373,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0025575447570332483,
            "pseudo_dstar_susp": 0.00228310502283105,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.00228310502283105,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.url_result#385",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.url_result(url, ie=None, video_id=None)",
        "snippet": "    def url_result(url, ie=None, video_id=None):\n        \"\"\"Returns a url that points to a page that should be processed\"\"\"\n        #TODO: ie should be the class used for getting the info\n        video_info = {'_type': 'url',\n                      'url': url,\n                      'ie_key': ie}\n        if video_id is not None:\n            video_info['id'] = video_id\n        return video_info",
        "begin_line": 385,
        "end_line": 393,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0025906735751295338,
            "pseudo_dstar_susp": 0.002403846153846154,
            "pseudo_tarantula_susp": 0.0014064697609001407,
            "pseudo_op2_susp": 0.002403846153846154,
            "pseudo_barinel_susp": 0.0014044943820224719
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.playlist_result#395",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.playlist_result(entries, playlist_id=None, playlist_title=None)",
        "snippet": "    def playlist_result(entries, playlist_id=None, playlist_title=None):\n        \"\"\"Returns a playlist\"\"\"\n        video_info = {'_type': 'playlist',\n                      'entries': entries}\n        if playlist_id:\n            video_info['id'] = playlist_id\n        if playlist_title:\n            video_info['title'] = playlist_title\n        return video_info",
        "begin_line": 395,
        "end_line": 403,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013054830287206266,
            "pseudo_dstar_susp": 0.0012195121951219512,
            "pseudo_tarantula_susp": 0.0013477088948787063,
            "pseudo_op2_susp": 0.0012195121951219512,
            "pseudo_barinel_susp": 0.001349527665317139
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._search_regex#405",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._search_regex(self, pattern, string, name, default=_NO_DEFAULT, fatal=True, flags=0)",
        "snippet": "    def _search_regex(self, pattern, string, name, default=_NO_DEFAULT, fatal=True, flags=0):\n        \"\"\"\n        Perform a regex search on the given string, using a single or a list of\n        patterns returning the first matching group.\n        In case of failure return a default value or raise a WARNING or a\n        RegexNotFoundError, depending on fatal, specifying the field name.\n        \"\"\"\n        if isinstance(pattern, (str, compat_str, compiled_regex_type)):\n            mobj = re.search(pattern, string, flags)\n        else:\n            for p in pattern:\n                mobj = re.search(p, string, flags)\n                if mobj:\n                    break\n\n        if os.name != 'nt' and sys.stderr.isatty():\n            _name = '\\033[0;34m%s\\033[0m' % name\n        else:\n            _name = name\n\n        if mobj:\n            # return the first matching group\n            return next(g for g in mobj.groups() if g is not None)\n        elif default is not _NO_DEFAULT:\n            return default\n        elif fatal:\n            raise RegexNotFoundError('Unable to extract %s' % _name)\n        else:\n            self._downloader.report_warning('unable to extract %s; '\n                'please report this issue on http://yt-dl.org/bug' % _name)\n            return None",
        "begin_line": 405,
        "end_line": 435,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0034965034965034965,
            "pseudo_dstar_susp": 0.0034965034965034965,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0034965034965034965,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._html_search_regex#437",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._html_search_regex(self, pattern, string, name, default=_NO_DEFAULT, fatal=True, flags=0)",
        "snippet": "    def _html_search_regex(self, pattern, string, name, default=_NO_DEFAULT, fatal=True, flags=0):\n        \"\"\"\n        Like _search_regex, but strips HTML tags and unescapes entities.\n        \"\"\"\n        res = self._search_regex(pattern, string, name, default, fatal, flags)\n        if res:\n            return clean_html(res).strip()\n        else:\n            return res",
        "begin_line": 437,
        "end_line": 445,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0027247956403269754,
            "pseudo_dstar_susp": 0.0026954177897574125,
            "pseudo_tarantula_susp": 0.0028011204481792717,
            "pseudo_op2_susp": 0.0026954177897574125,
            "pseudo_barinel_susp": 0.0028011204481792717
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._get_login_info#447",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._get_login_info(self)",
        "snippet": "    def _get_login_info(self):\n        \"\"\"\n        Get the the login info as (username, password)\n        It will look in the netrc file using the _NETRC_MACHINE value\n        If there's no info available, return (None, None)\n        \"\"\"\n        if self._downloader is None:\n            return (None, None)\n\n        username = None\n        password = None\n        downloader_params = self._downloader.params\n\n        # Attempt to use provided username and password or .netrc data\n        if downloader_params.get('username', None) is not None:\n            username = downloader_params['username']\n            password = downloader_params['password']\n        elif downloader_params.get('usenetrc', False):\n            try:\n                info = netrc.netrc().authenticators(self._NETRC_MACHINE)\n                if info is not None:\n                    username = info[0]\n                    password = info[2]\n                else:\n                    raise netrc.NetrcParseError('No authenticators for %s' % self._NETRC_MACHINE)\n            except (IOError, netrc.NetrcParseError) as err:\n                self._downloader.report_warning('parsing .netrc: %s' % compat_str(err))\n        \n        return (username, password)",
        "begin_line": 447,
        "end_line": 475,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0031746031746031746,
            "pseudo_dstar_susp": 0.0029411764705882353,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0029411764705882353,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_regexes#495",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_regexes(prop)",
        "snippet": "    def _og_regexes(prop):\n        content_re = r'content=(?:\"([^>]+?)\"|\\'([^>]+?)\\')'\n        property_re = r'(?:name|property)=[\\'\"]og:%s[\\'\"]' % re.escape(prop)\n        template = r'<meta[^>]+?%s[^>]+?%s'\n        return [\n            template % (property_re, content_re),\n            template % (content_re, property_re),\n        ]",
        "begin_line": 495,
        "end_line": 502,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026246719160104987,
            "pseudo_dstar_susp": 0.002364066193853428,
            "pseudo_tarantula_susp": 0.0024691358024691358,
            "pseudo_op2_susp": 0.002364066193853428,
            "pseudo_barinel_susp": 0.0024813895781637717
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_property#504",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_property(self, prop, html, name=None, **kargs)",
        "snippet": "    def _og_search_property(self, prop, html, name=None, **kargs):\n        if name is None:\n            name = 'OpenGraph %s' % prop\n        escaped = self._search_regex(self._og_regexes(prop), html, name, flags=re.DOTALL, **kargs)\n        if escaped is None:\n            return None\n        return unescapeHTML(escaped)",
        "begin_line": 504,
        "end_line": 510,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00234192037470726,
            "pseudo_dstar_susp": 0.0017793594306049821,
            "pseudo_tarantula_susp": 0.002386634844868735,
            "pseudo_op2_susp": 0.0017793594306049821,
            "pseudo_barinel_susp": 0.002386634844868735
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_thumbnail#512",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_thumbnail(self, html, **kargs)",
        "snippet": "    def _og_search_thumbnail(self, html, **kargs):\n        return self._og_search_property('image', html, 'thumbnail url', fatal=False, **kargs)",
        "begin_line": 512,
        "end_line": 513,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001141552511415525,
            "pseudo_dstar_susp": 0.0011325028312570782,
            "pseudo_tarantula_susp": 0.0011148272017837235,
            "pseudo_op2_susp": 0.0011325028312570782,
            "pseudo_barinel_susp": 0.0011148272017837235
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_description#515",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_description(self, html, **kargs)",
        "snippet": "    def _og_search_description(self, html, **kargs):\n        return self._og_search_property('description', html, fatal=False, **kargs)",
        "begin_line": 515,
        "end_line": 516,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00141643059490085,
            "pseudo_dstar_susp": 0.001379310344827586,
            "pseudo_tarantula_susp": 0.001375515818431912,
            "pseudo_op2_susp": 0.001379310344827586,
            "pseudo_barinel_susp": 0.001375515818431912
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_title#518",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_title(self, html, **kargs)",
        "snippet": "    def _og_search_title(self, html, **kargs):\n        return self._og_search_property('title', html, **kargs)",
        "begin_line": 518,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._og_search_video_url#521",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._og_search_video_url(self, html, name='video url', secure=True, **kargs)",
        "snippet": "    def _og_search_video_url(self, html, name='video url', secure=True, **kargs):\n        regexes = self._og_regexes('video') + self._og_regexes('video:url')\n        if secure:\n            regexes = self._og_regexes('video:secure_url') + regexes\n        return self._html_search_regex(regexes, html, name, **kargs)",
        "begin_line": 521,
        "end_line": 525,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._html_search_meta#530",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._html_search_meta(self, name, html, display_name=None, fatal=False, **kwargs)",
        "snippet": "    def _html_search_meta(self, name, html, display_name=None, fatal=False, **kwargs):\n        if display_name is None:\n            display_name = name\n        return self._html_search_regex(\n            r'''(?ix)<meta\n                    (?=[^>]+(?:itemprop|name|property)=[\"\\']?%s[\"\\']?)\n                    [^>]+content=[\"\\']([^\"\\']+)[\"\\']''' % re.escape(name),\n            html, display_name, fatal=fatal, **kwargs)",
        "begin_line": 530,
        "end_line": 537,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00205761316872428,
            "pseudo_dstar_susp": 0.0016778523489932886,
            "pseudo_tarantula_susp": 0.001557632398753894,
            "pseudo_op2_susp": 0.0016778523489932886,
            "pseudo_barinel_susp": 0.001557632398753894
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._rta_search#542",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._rta_search(self, html)",
        "snippet": "    def _rta_search(self, html):\n        # See http://www.rtalabel.org/index.php?content=howtofaq#single\n        if re.search(r'(?ix)<meta\\s+name=\"rating\"\\s+'\n                     r'     content=\"RTA-5042-1996-1400-1577-RTA\"',\n                     html):\n            return 18\n        return 0",
        "begin_line": 542,
        "end_line": 548,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0025252525252525255,
            "pseudo_dstar_susp": 0.0023094688221709007,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0023094688221709007,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._sort_formats#570",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._sort_formats(self, formats)",
        "snippet": "    def _sort_formats(self, formats):\n        if not formats:\n            raise ExtractorError('No video formats found')\n\n        def _formats_key(f):\n            # TODO remove the following workaround\n            from ..utils import determine_ext\n            if not f.get('ext') and 'url' in f:\n                f['ext'] = determine_ext(f['url'])\n\n            preference = f.get('preference')\n            if preference is None:\n                proto = f.get('protocol')\n                if proto is None:\n                    proto = compat_urllib_parse_urlparse(f.get('url', '')).scheme\n\n                preference = 0 if proto in ['http', 'https'] else -0.1\n                if f.get('ext') in ['f4f', 'f4m']:  # Not yet supported\n                    preference -= 0.5\n\n            if f.get('vcodec') == 'none':  # audio only\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['aac', 'mp3', 'm4a', 'webm', 'ogg', 'opus']\n                else:\n                    ORDER = ['webm', 'opus', 'ogg', 'mp3', 'aac', 'm4a']\n                ext_preference = 0\n                try:\n                    audio_ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    audio_ext_preference = -1\n            else:\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['flv', 'mp4', 'webm']\n                else:\n                    ORDER = ['webm', 'flv', 'mp4']\n                try:\n                    ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    ext_preference = -1\n                audio_ext_preference = 0\n\n            return (\n                preference,\n                f.get('quality') if f.get('quality') is not None else -1,\n                f.get('height') if f.get('height') is not None else -1,\n                f.get('width') if f.get('width') is not None else -1,\n                ext_preference,\n                f.get('tbr') if f.get('tbr') is not None else -1,\n                f.get('vbr') if f.get('vbr') is not None else -1,\n                f.get('abr') if f.get('abr') is not None else -1,\n                audio_ext_preference,\n                f.get('filesize') if f.get('filesize') is not None else -1,\n                f.get('filesize_approx') if f.get('filesize_approx') is not None else -1,\n                f.get('source_preference') if f.get('source_preference') is not None else -1,\n                f.get('format_id'),\n            )\n        formats.sort(key=_formats_key)",
        "begin_line": 570,
        "end_line": 626,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012422360248447205,
            "pseudo_dstar_susp": 0.001440922190201729,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.001440922190201729,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._formats_key#574",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._formats_key(f)",
        "snippet": "        def _formats_key(f):\n            # TODO remove the following workaround\n            from ..utils import determine_ext\n            if not f.get('ext') and 'url' in f:\n                f['ext'] = determine_ext(f['url'])\n\n            preference = f.get('preference')\n            if preference is None:\n                proto = f.get('protocol')\n                if proto is None:\n                    proto = compat_urllib_parse_urlparse(f.get('url', '')).scheme\n\n                preference = 0 if proto in ['http', 'https'] else -0.1\n                if f.get('ext') in ['f4f', 'f4m']:  # Not yet supported\n                    preference -= 0.5\n\n            if f.get('vcodec') == 'none':  # audio only\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['aac', 'mp3', 'm4a', 'webm', 'ogg', 'opus']\n                else:\n                    ORDER = ['webm', 'opus', 'ogg', 'mp3', 'aac', 'm4a']\n                ext_preference = 0\n                try:\n                    audio_ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    audio_ext_preference = -1\n            else:\n                if self._downloader.params.get('prefer_free_formats'):\n                    ORDER = ['flv', 'mp4', 'webm']\n                else:\n                    ORDER = ['webm', 'flv', 'mp4']\n                try:\n                    ext_preference = ORDER.index(f['ext'])\n                except ValueError:\n                    ext_preference = -1\n                audio_ext_preference = 0\n\n            return (\n                preference,\n                f.get('quality') if f.get('quality') is not None else -1,\n                f.get('height') if f.get('height') is not None else -1,\n                f.get('width') if f.get('width') is not None else -1,\n                ext_preference,\n                f.get('tbr') if f.get('tbr') is not None else -1,\n                f.get('vbr') if f.get('vbr') is not None else -1,\n                f.get('abr') if f.get('abr') is not None else -1,\n                audio_ext_preference,\n                f.get('filesize') if f.get('filesize') is not None else -1,\n                f.get('filesize_approx') if f.get('filesize_approx') is not None else -1,\n                f.get('source_preference') if f.get('source_preference') is not None else -1,\n                f.get('format_id'),\n            )",
        "begin_line": 574,
        "end_line": 625,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011961722488038277,
            "pseudo_dstar_susp": 0.0013089005235602095,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0013089005235602095,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor.http_scheme#628",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor.http_scheme(self)",
        "snippet": "    def http_scheme(self):\n        \"\"\" Either \"http:\" or \"https:\", depending on the user's preferences \"\"\"\n        return (\n            'http:'\n            if self._downloader.params.get('prefer_insecure', False)\n            else 'https:')",
        "begin_line": 628,
        "end_line": 633,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010504201680672268,
            "pseudo_dstar_susp": 0.001075268817204301,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001075268817204301,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._proto_relative_url#635",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._proto_relative_url(self, url, scheme=None)",
        "snippet": "    def _proto_relative_url(self, url, scheme=None):\n        if url is None:\n            return url\n        if url.startswith('//'):\n            if scheme is None:\n                scheme = self.http_scheme()\n            return scheme + url\n        else:\n            return url",
        "begin_line": 635,
        "end_line": 643,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001226993865030675,
            "pseudo_dstar_susp": 0.001160092807424594,
            "pseudo_tarantula_susp": 0.0013477088948787063,
            "pseudo_op2_susp": 0.001160092807424594,
            "pseudo_barinel_susp": 0.001349527665317139
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_f4m_formats#652",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_f4m_formats(self, manifest_url, video_id)",
        "snippet": "    def _extract_f4m_formats(self, manifest_url, video_id):\n        manifest = self._download_xml(\n            manifest_url, video_id, 'Downloading f4m manifest',\n            'Unable to download f4m manifest')\n\n        formats = []\n        media_nodes = manifest.findall('{http://ns.adobe.com/f4m/1.0}media')\n        for i, media_el in enumerate(media_nodes):\n            tbr = int_or_none(media_el.attrib.get('bitrate'))\n            format_id = 'f4m-%d' % (i if tbr is None else tbr)\n            formats.append({\n                'format_id': format_id,\n                'url': manifest_url,\n                'ext': 'flv',\n                'tbr': tbr,\n                'width': int_or_none(media_el.attrib.get('width')),\n                'height': int_or_none(media_el.attrib.get('height')),\n            })\n        self._sort_formats(formats)\n\n        return formats",
        "begin_line": 652,
        "end_line": 672,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010615711252653928,
            "pseudo_dstar_susp": 0.0010351966873706005,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010351966873706005,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.InfoExtractor._extract_m3u8_formats#674",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.InfoExtractor",
        "signature": "youtube_dl.extractor.common.InfoExtractor._extract_m3u8_formats(self, m3u8_url, video_id, ext=None, entry_protocol='m3u8', preference=None)",
        "snippet": "    def _extract_m3u8_formats(self, m3u8_url, video_id, ext=None,\n                              entry_protocol='m3u8', preference=None):\n\n        formats = [{\n            'format_id': 'm3u8-meta',\n            'url': m3u8_url,\n            'ext': ext,\n            'protocol': 'm3u8',\n            'preference': -1,\n            'resolution': 'multiple',\n            'format_note': 'Quality selection URL',\n        }]\n\n        format_url = lambda u: (\n            u\n            if re.match(r'^https?://', u)\n            else compat_urlparse.urljoin(m3u8_url, u))\n\n        m3u8_doc = self._download_webpage(\n            m3u8_url, video_id,\n            note='Downloading m3u8 information',\n            errnote='Failed to download m3u8 information')\n        last_info = None\n        kv_rex = re.compile(\n            r'(?P<key>[a-zA-Z_-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)')\n        for line in m3u8_doc.splitlines():\n            if line.startswith('#EXT-X-STREAM-INF:'):\n                last_info = {}\n                for m in kv_rex.finditer(line):\n                    v = m.group('val')\n                    if v.startswith('\"'):\n                        v = v[1:-1]\n                    last_info[m.group('key')] = v\n            elif line.startswith('#') or not line.strip():\n                continue\n            else:\n                if last_info is None:\n                    formats.append({'url': format_url(line)})\n                    continue\n                tbr = int_or_none(last_info.get('BANDWIDTH'), scale=1000)\n\n                f = {\n                    'format_id': 'm3u8-%d' % (tbr if tbr else len(formats)),\n                    'url': format_url(line.strip()),\n                    'tbr': tbr,\n                    'ext': ext,\n                    'protocol': entry_protocol,\n                    'preference': preference,\n                }\n                codecs = last_info.get('CODECS')\n                if codecs:\n                    # TODO: looks like video codec is not always necessarily goes first\n                    va_codecs = codecs.split(',')\n                    if va_codecs[0]:\n                        f['vcodec'] = va_codecs[0].partition('.')[0]\n                    if len(va_codecs) > 1 and va_codecs[1]:\n                        f['acodec'] = va_codecs[1].partition('.')[0]\n                resolution = last_info.get('RESOLUTION')\n                if resolution:\n                    width_str, height_str = resolution.split('x')\n                    f['width'] = int(width_str)\n                    f['height'] = int(height_str)\n                formats.append(f)\n                last_info = {}\n        self._sort_formats(formats)\n        return formats",
        "begin_line": 674,
        "end_line": 739,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010615711252653928,
            "pseudo_dstar_susp": 0.0010351966873706005,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010351966873706005,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor._make_valid_url#778",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor._make_valid_url(cls)",
        "snippet": "    def _make_valid_url(cls):\n        return r'%s(?P<prefix>|[1-9][0-9]*|all):(?P<query>[\\s\\S]+)' % cls._SEARCH_KEY",
        "begin_line": 778,
        "end_line": 779,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009225092250922509,
            "pseudo_dstar_susp": 0.0010706638115631692,
            "pseudo_tarantula_susp": 0.0007733952049497294,
            "pseudo_op2_susp": 0.0010706638115631692,
            "pseudo_barinel_susp": 0.0007733952049497294
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor.suitable#782",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return re.match(cls._make_valid_url(), url) is not None",
        "begin_line": 782,
        "end_line": 783,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009225092250922509,
            "pseudo_dstar_susp": 0.0010706638115631692,
            "pseudo_tarantula_susp": 0.0007733952049497294,
            "pseudo_op2_susp": 0.0010706638115631692,
            "pseudo_barinel_susp": 0.0007733952049497294
        }
    },
    {
        "name": "youtube_dl.extractor.common.SearchInfoExtractor._real_extract#785",
        "src_path": "youtube_dl/extractor/common.py",
        "class_name": "youtube_dl.extractor.common.SearchInfoExtractor",
        "signature": "youtube_dl.extractor.common.SearchInfoExtractor._real_extract(self, query)",
        "snippet": "    def _real_extract(self, query):\n        mobj = re.match(self._make_valid_url(), query)\n        if mobj is None:\n            raise ExtractorError('Invalid search query \"%s\"' % query)\n\n        prefix = mobj.group('prefix')\n        query = mobj.group('query')\n        if prefix == '':\n            return self._get_n_results(query, 1)\n        elif prefix == 'all':\n            return self._get_n_results(query, self._MAX_RESULTS)\n        else:\n            n = int(prefix)\n            if n <= 0:\n                raise ExtractorError('invalid download number %s for query \"%s\"' % (n, query))\n            elif n > self._MAX_RESULTS:\n                self._downloader.report_warning('%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n))\n                n = self._MAX_RESULTS\n            return self._get_n_results(query, n)",
        "begin_line": 785,
        "end_line": 803,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.mtv.MTVIE._real_extract#235",
        "src_path": "youtube_dl/extractor/mtv.py",
        "class_name": "youtube_dl.extractor.mtv.MTVIE",
        "signature": "youtube_dl.extractor.mtv.MTVIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('videoid')\n        uri = mobj.groupdict().get('mgid')\n        if uri is None:\n            webpage = self._download_webpage(url, video_id)\n    \n            # Some videos come from Vevo.com\n            m_vevo = re.search(r'isVevoVideo = true;.*?vevoVideoId = \"(.*?)\";',\n                               webpage, re.DOTALL)\n            if m_vevo:\n                vevo_id = m_vevo.group(1);\n                self.to_screen('Vevo video detected: %s' % vevo_id)\n                return self.url_result('vevo:%s' % vevo_id, ie='Vevo')\n    \n            uri = self._html_search_regex(r'/uri/(.*?)\\?', webpage, 'uri')\n        return self._get_videos_info(uri)",
        "begin_line": 235,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0018796992481203006,
            "pseudo_dstar_susp": 0.001519756838905775,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001519756838905775,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.bliptv.BlipTVIE._real_extract#70",
        "src_path": "youtube_dl/extractor/bliptv.py",
        "class_name": "youtube_dl.extractor.bliptv.BlipTVIE",
        "signature": "youtube_dl.extractor.bliptv.BlipTVIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        lookup_id = mobj.group('lookup_id')\n\n        # See https://github.com/rg3/youtube-dl/issues/857\n        if lookup_id:\n            info_page = self._download_webpage(\n                'http://blip.tv/play/%s.x?p=1' % lookup_id, lookup_id, 'Resolving lookup id')\n            video_id = self._search_regex(r'data-episode-id=\"([0-9]+)', info_page, 'video_id')\n        else:\n            video_id = mobj.group('id')\n\n        rss = self._download_xml('http://blip.tv/rss/flash/%s' % video_id, video_id, 'Downloading video RSS')\n\n        def blip(s):\n            return '{http://blip.tv/dtd/blip/1.0}%s' % s\n\n        def media(s):\n            return '{http://search.yahoo.com/mrss/}%s' % s\n\n        def itunes(s):\n            return '{http://www.itunes.com/dtds/podcast-1.0.dtd}%s' % s\n\n        item = rss.find('channel/item')\n\n        video_id = item.find(blip('item_id')).text\n        title = item.find('./title').text\n        description = clean_html(compat_str(item.find(blip('puredescription')).text))\n        timestamp = parse_iso8601(item.find(blip('datestamp')).text)\n        uploader = item.find(blip('user')).text\n        uploader_id = item.find(blip('userid')).text\n        duration = int(item.find(blip('runtime')).text)\n        media_thumbnail = item.find(media('thumbnail'))\n        thumbnail = media_thumbnail.get('url') if media_thumbnail is not None else item.find(itunes('image')).text\n        categories = [category.text for category in item.findall('category')]\n\n        formats = []\n        subtitles = {}\n\n        media_group = item.find(media('group'))\n        for media_content in media_group.findall(media('content')):\n            url = media_content.get('url')\n            role = media_content.get(blip('role'))\n            msg = self._download_webpage(\n                url + '?showplayer=20140425131715&referrer=http://blip.tv&mask=7&skin=flashvars&view=url',\n                video_id, 'Resolving URL for %s' % role)\n            real_url = compat_urlparse.parse_qs(msg)['message'][0]\n\n            media_type = media_content.get('type')\n            if media_type == 'text/srt' or url.endswith('.srt'):\n                LANGS = {\n                    'english': 'en',\n                }\n                lang = role.rpartition('-')[-1].strip().lower()\n                langcode = LANGS.get(lang, lang)\n                subtitles[langcode] = url\n            elif media_type.startswith('video/'):\n                formats.append({\n                    'url': real_url,\n                    'format_id': role,\n                    'format_note': media_type,\n                    'vcodec': media_content.get(blip('vcodec')),\n                    'acodec': media_content.get(blip('acodec')),\n                    'filesize': media_content.get('filesize'),\n                    'width': int(media_content.get('width')),\n                    'height': int(media_content.get('height')),\n                })\n        self._sort_formats(formats)\n\n        # subtitles\n        video_subtitles = self.extract_subtitles(video_id, subtitles)\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, subtitles)\n            return\n\n        return {\n            'id': video_id,\n            'title': title,\n            'description': description,\n            'timestamp': timestamp,\n            'uploader': uploader,\n            'uploader_id': uploader_id,\n            'duration': duration,\n            'thumbnail': thumbnail,\n            'categories': categories,\n            'formats': formats,\n            'subtitles': video_subtitles,\n        }",
        "begin_line": 70,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.walla.WallaIE._real_extract#36",
        "src_path": "youtube_dl/extractor/walla.py",
        "class_name": "youtube_dl.extractor.walla.WallaIE",
        "signature": "youtube_dl.extractor.walla.WallaIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        display_id = mobj.group('display_id')\n\n        video = self._download_xml(\n            'http://video2.walla.co.il/?w=null/null/%s/@@/video/flv_pl' % video_id,\n            display_id)\n\n        item = video.find('./items/item')\n\n        title = xpath_text(item, './title', 'title')\n        description = xpath_text(item, './synopsis', 'description')\n        thumbnail = xpath_text(item, './preview_pic', 'thumbnail')\n        duration = int_or_none(xpath_text(item, './duration', 'duration'))\n\n        subtitles = {}\n        for subtitle in item.findall('./subtitles/subtitle'):\n            lang = xpath_text(subtitle, './title')\n            subtitles[self._SUBTITLE_LANGS.get(lang, lang)] = xpath_text(subtitle, './src')\n\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, subtitles)\n            return\n\n        subtitles = self.extract_subtitles(video_id, subtitles)\n\n        formats = []\n        for quality in item.findall('./qualities/quality'):\n            format_id = xpath_text(quality, './title')\n            fmt = {\n                'url': 'rtmp://wafla.walla.co.il/vod',\n                'play_path': xpath_text(quality, './src'),\n                'player_url': 'http://isc.walla.co.il/w9/swf/video_swf/vod/WallaMediaPlayerAvod.swf',\n                'page_url': url,\n                'ext': 'flv',\n                'format_id': xpath_text(quality, './title'),\n            }\n            m = re.search(r'^(?P<height>\\d+)[Pp]', format_id)\n            if m:\n                fmt['height'] = int(m.group('height'))\n            formats.append(fmt)\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'display_id': display_id,\n            'title': title,\n            'description': description,\n            'thumbnail': thumbnail,\n            'duration': duration,\n            'formats': formats,\n            'subtitles': subtitles,\n        }",
        "begin_line": 36,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.udemy.UdemyCourseIE.suitable#135",
        "src_path": "youtube_dl/extractor/udemy.py",
        "class_name": "youtube_dl.extractor.udemy.UdemyCourseIE",
        "signature": "youtube_dl.extractor.udemy.UdemyCourseIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return False if UdemyIE.suitable(url) else super(UdemyCourseIE, cls).suitable(url)",
        "begin_line": 135,
        "end_line": 136,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000819672131147541,
            "pseudo_dstar_susp": 0.0009852216748768472,
            "pseudo_tarantula_susp": 0.0007716049382716049,
            "pseudo_op2_susp": 0.0009852216748768472,
            "pseudo_barinel_susp": 0.0007716049382716049
        }
    },
    {
        "name": "youtube_dl.extractor.channel9.Channel9IE._real_extract#255",
        "src_path": "youtube_dl/extractor/channel9.py",
        "class_name": "youtube_dl.extractor.channel9.Channel9IE",
        "signature": "youtube_dl.extractor.channel9.Channel9IE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        content_path = mobj.group('contentpath')\n\n        webpage = self._download_webpage(url, content_path, 'Downloading web page')\n\n        page_type_m = re.search(r'<meta name=\"Search.PageType\" content=\"(?P<pagetype>[^\"]+)\"/>', webpage)\n        if page_type_m is None:\n            raise ExtractorError('Search.PageType not found, don\\'t know how to process this page', expected=True)\n\n        page_type = page_type_m.group('pagetype')\n        if page_type == 'List':         # List page, may contain list of 'item'-like objects\n            return self._extract_list(content_path)\n        elif page_type == 'Entry.Item': # Any 'item'-like page, may contain downloadable content\n            return self._extract_entry_item(webpage, content_path)\n        elif page_type == 'Session':    # Event session page, may contain downloadable content\n            return self._extract_session(webpage, content_path)\n        else:\n            raise ExtractorError('Unexpected Search.PageType %s' % page_type, expected=True)",
        "begin_line": 255,
        "end_line": 273,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._extract_info#92",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._extract_info(self, webpage)",
        "snippet": "    def _extract_info(self, webpage):\n        info_json = self._search_regex(r'q\\(\"\\w+.init\",({.+})\\)</script>',\n            webpage, 'info json')\n        return json.loads(info_json)",
        "begin_line": 92,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012330456226880395,
            "pseudo_dstar_susp": 0.0011467889908256881,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011467889908256881,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._real_extract#97",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        m = re.match(self._VALID_URL, url, re.VERBOSE)\n        if m.group('type') == 'embed':\n            desktop_url = m.group('proto') + 'www' + m.group('urlmain')\n            return self.url_result(desktop_url, 'TED')\n        name = m.group('name')\n        if m.group('type_talk'):\n            return self._talk_info(url, name)\n        elif m.group('type_watch'):\n            return self._watch_info(url, name)\n        else:\n            return self._playlist_videos_info(url, name)",
        "begin_line": 97,
        "end_line": 108,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012330456226880395,
            "pseudo_dstar_susp": 0.0011467889908256881,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011467889908256881,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.ted.TEDIE._talk_info#127",
        "src_path": "youtube_dl/extractor/ted.py",
        "class_name": "youtube_dl.extractor.ted.TEDIE",
        "signature": "youtube_dl.extractor.ted.TEDIE._talk_info(self, url, video_name)",
        "snippet": "    def _talk_info(self, url, video_name):\n        webpage = self._download_webpage(url, video_name)\n        self.report_extraction(video_name)\n\n        talk_info = self._extract_info(webpage)['talks'][0]\n\n        if talk_info.get('external') is not None:\n            self.to_screen('Found video from %s' % talk_info['external']['service'])\n            return {\n                '_type': 'url',\n                'url': talk_info['external']['uri'],\n            }\n\n        formats = [{\n            'url': format_url,\n            'format_id': format_id,\n            'format': format_id,\n        } for (format_id, format_url) in talk_info['nativeDownloads'].items() if format_url is not None]\n        if formats:\n            for f in formats:\n                finfo = self._NATIVE_FORMATS.get(f['format_id'])\n                if finfo:\n                    f.update(finfo)\n        else:\n            # Use rtmp downloads\n            formats = [{\n                'format_id': f['name'],\n                'url': talk_info['streamer'],\n                'play_path': f['file'],\n                'ext': 'flv',\n                'width': f['width'],\n                'height': f['height'],\n                'tbr': f['bitrate'],\n            } for f in talk_info['resources']['rtmp']]\n        self._sort_formats(formats)\n\n        video_id = compat_str(talk_info['id'])\n        # subtitles\n        video_subtitles = self.extract_subtitles(video_id, talk_info)\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, talk_info)\n            return\n\n        thumbnail = talk_info['thumb']\n        if not thumbnail.startswith('http'):\n            thumbnail = 'http://' + thumbnail\n        return {\n            'id': video_id,\n            'title': talk_info['title'].strip(),\n            'uploader': talk_info['speaker'],\n            'thumbnail': thumbnail,\n            'description': self._og_search_description(webpage),\n            'subtitles': video_subtitles,\n            'formats': formats,\n        }",
        "begin_line": 127,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012330456226880395,
            "pseudo_dstar_susp": 0.0011467889908256881,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011467889908256881,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.sina.SinaIE.suitable#43",
        "src_path": "youtube_dl/extractor/sina.py",
        "class_name": "youtube_dl.extractor.sina.SinaIE",
        "signature": "youtube_dl.extractor.sina.SinaIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        return re.match(cls._VALID_URL, url, flags=re.VERBOSE) is not None",
        "begin_line": 43,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007692307692307692,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "youtube_dl.extractor.allocine.AllocineIE._real_extract#50",
        "src_path": "youtube_dl/extractor/allocine.py",
        "class_name": "youtube_dl.extractor.allocine.AllocineIE",
        "signature": "youtube_dl.extractor.allocine.AllocineIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        typ = mobj.group('typ')\n        display_id = mobj.group('id')\n\n        webpage = self._download_webpage(url, display_id)\n\n        if typ == 'film':\n            video_id = self._search_regex(r'href=\"/video/player_gen_cmedia=([0-9]+).+\"', webpage, 'video id')\n        else:\n            player = self._search_regex(r'data-player=\\'([^\\']+)\\'>', webpage, 'data player')\n\n            player_data = json.loads(player)\n            video_id = compat_str(player_data['refMedia'])\n\n        xml = self._download_xml('http://www.allocine.fr/ws/AcVisiondataV4.ashx?media=%s' % video_id, display_id)\n\n        video = xml.find('.//AcVisionVideo').attrib\n        quality = qualities(['ld', 'md', 'hd'])\n\n        formats = []\n        for k, v in video.items():\n            if re.match(r'.+_path', k):\n                format_id = k.split('_')[0]\n                formats.append({\n                    'format_id': format_id,\n                    'quality': quality(format_id),\n                    'url': v,\n                    'ext': determine_ext(v),\n                })\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': video['videoTitle'],\n            'thumbnail': self._og_search_thumbnail(webpage),\n            'formats': formats,\n            'description': self._og_search_description(webpage),\n        }",
        "begin_line": 50,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0028089887640449437,
            "pseudo_dstar_susp": 0.0027624309392265192,
            "pseudo_tarantula_susp": 0.0012836970474967907,
            "pseudo_op2_susp": 0.0027624309392265192,
            "pseudo_barinel_susp": 0.0012836970474967907
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE.report_following_redirect#411",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE.report_following_redirect(self, new_url)",
        "snippet": "    def report_following_redirect(self, new_url):\n        \"\"\"Report information extraction.\"\"\"\n        self._downloader.to_screen('[redirect] Following redirect to %s' % new_url)",
        "begin_line": 411,
        "end_line": 413,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE._extract_camtasia#434",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE._extract_camtasia(self, url, video_id, webpage)",
        "snippet": "    def _extract_camtasia(self, url, video_id, webpage):\n        \"\"\" Returns None if no camtasia video can be found. \"\"\"\n\n        camtasia_cfg = self._search_regex(\n            r'fo\\.addVariable\\(\\s*\"csConfigFile\",\\s*\"([^\"]+)\"\\s*\\);',\n            webpage, 'camtasia configuration file', default=None)\n        if camtasia_cfg is None:\n            return None\n\n        title = self._html_search_meta('DC.title', webpage, fatal=True)\n\n        camtasia_url = compat_urlparse.urljoin(url, camtasia_cfg)\n        camtasia_cfg = self._download_xml(\n            camtasia_url, video_id,\n            note='Downloading camtasia configuration',\n            errnote='Failed to download camtasia configuration')\n        fileset_node = camtasia_cfg.find('./playlist/array/fileset')\n\n        entries = []\n        for n in fileset_node.getchildren():\n            url_n = n.find('./uri')\n            if url_n is None:\n                continue\n\n            entries.append({\n                'id': os.path.splitext(url_n.text.rpartition('/')[2])[0],\n                'title': '%s - %s' % (title, n.tag),\n                'url': compat_urlparse.urljoin(url, url_n.text),\n                'duration': float_or_none(n.find('./duration').text),\n            })\n\n        return {\n            '_type': 'playlist',\n            'entries': entries,\n            'title': title,\n        }",
        "begin_line": 434,
        "end_line": 469,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002207505518763797,
            "pseudo_dstar_susp": 0.0017574692442882249,
            "pseudo_tarantula_susp": 0.0014814814814814814,
            "pseudo_op2_susp": 0.0017574692442882249,
            "pseudo_barinel_susp": 0.0014792899408284023
        }
    },
    {
        "name": "youtube_dl.extractor.generic.GenericIE._real_extract#471",
        "src_path": "youtube_dl/extractor/generic.py",
        "class_name": "youtube_dl.extractor.generic.GenericIE",
        "signature": "youtube_dl.extractor.generic.GenericIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        if url.startswith('//'):\n            return {\n                '_type': 'url',\n                'url': self.http_scheme() + url,\n            }\n\n        parsed_url = compat_urlparse.urlparse(url)\n        if not parsed_url.scheme:\n            default_search = self._downloader.params.get('default_search')\n            if default_search is None:\n                default_search = 'fixup_error'\n\n            if default_search in ('auto', 'auto_warning', 'fixup_error'):\n                if '/' in url:\n                    self._downloader.report_warning('The url doesn\\'t specify the protocol, trying with http')\n                    return self.url_result('http://' + url)\n                elif default_search != 'fixup_error':\n                    if default_search == 'auto_warning':\n                        if re.match(r'^(?:url|URL)$', url):\n                            raise ExtractorError(\n                                'Invalid URL:  %r . Call youtube-dl like this:  youtube-dl -v \"https://www.youtube.com/watch?v=BaW_jenozKc\"  ' % url,\n                                expected=True)\n                        else:\n                            self._downloader.report_warning(\n                                'Falling back to youtube search for  %s . Set --default-search \"auto\" to suppress this warning.' % url)\n                    return self.url_result('ytsearch:' + url)\n\n            if default_search in ('error', 'fixup_error'):\n                raise ExtractorError(\n                    ('%r is not a valid URL. '\n                     'Set --default-search \"ytsearch\" (or run  youtube-dl \"ytsearch:%s\" ) to search YouTube'\n                    ) % (url, url), expected=True)\n            else:\n                if ':' not in default_search:\n                    default_search += ':'\n                return self.url_result(default_search + url)\n\n        url, smuggled_data = unsmuggle_url(url)\n        force_videoid = None\n        is_intentional = smuggled_data and smuggled_data.get('to_generic')\n        if smuggled_data and 'force_videoid' in smuggled_data:\n            force_videoid = smuggled_data['force_videoid']\n            video_id = force_videoid\n        else:\n            video_id = os.path.splitext(url.rstrip('/').split('/')[-1])[0]\n\n        self.to_screen('%s: Requesting header' % video_id)\n\n        head_req = HEADRequest(url)\n        head_response = self._request_webpage(\n            head_req, video_id,\n            note=False, errnote='Could not send HEAD request to %s' % url,\n            fatal=False)\n\n        if head_response is not False:\n            # Check for redirect\n            new_url = head_response.geturl()\n            if url != new_url:\n                self.report_following_redirect(new_url)\n                if force_videoid:\n                    new_url = smuggle_url(\n                        new_url, {'force_videoid': force_videoid})\n                return self.url_result(new_url)\n\n        full_response = None\n        if head_response is False:\n            full_response = self._request_webpage(url, video_id)\n            head_response = full_response\n\n        # Check for direct link to a video\n        content_type = head_response.headers.get('Content-Type', '')\n        m = re.match(r'^(?P<type>audio|video|application(?=/ogg$))/(?P<format_id>.+)$', content_type)\n        if m:\n            upload_date = unified_strdate(\n                head_response.headers.get('Last-Modified'))\n            return {\n                'id': video_id,\n                'title': os.path.splitext(url_basename(url))[0],\n                'formats': [{\n                    'format_id': m.group('format_id'),\n                    'url': url,\n                    'vcodec': 'none' if m.group('type') == 'audio' else None\n                }],\n                'upload_date': upload_date,\n            }\n\n        if not self._downloader.params.get('test', False) and not is_intentional:\n            self._downloader.report_warning('Falling back on generic information extractor.')\n\n        if full_response:\n            webpage = self._webpage_read_content(full_response, url, video_id)\n        else:\n            webpage = self._download_webpage(url, video_id)\n        self.report_extraction(video_id)\n\n        # Is it an RSS feed?\n        try:\n            doc = parse_xml(webpage)\n            if doc.tag == 'rss':\n                return self._extract_rss(url, video_id, doc)\n        except compat_xml_parse_error:\n            pass\n\n        # Is it a Camtasia project?\n        camtasia_res = self._extract_camtasia(url, video_id, webpage)\n        if camtasia_res is not None:\n            return camtasia_res\n\n        # Sometimes embedded video player is hidden behind percent encoding\n        # (e.g. https://github.com/rg3/youtube-dl/issues/2448)\n        # Unescaping the whole page allows to handle those cases in a generic way\n        webpage = compat_urllib_parse.unquote(webpage)\n\n        # it's tempting to parse this further, but you would\n        # have to take into account all the variations like\n        #   Video Title - Site Name\n        #   Site Name | Video Title\n        #   Video Title - Tagline | Site Name\n        # and so on and so forth; it's just not practical\n        video_title = self._html_search_regex(\n            r'(?s)<title>(.*?)</title>', webpage, 'video title',\n            default='video')\n\n        # Try to detect age limit automatically\n        age_limit = self._rta_search(webpage)\n        # And then there are the jokers who advertise that they use RTA,\n        # but actually don't.\n        AGE_LIMIT_MARKERS = [\n            r'Proudly Labeled <a href=\"http://www.rtalabel.org/\" title=\"Restricted to Adults\">RTA</a>',\n        ]\n        if any(re.search(marker, webpage) for marker in AGE_LIMIT_MARKERS):\n            age_limit = 18\n\n        # video uploader is domain name\n        video_uploader = self._search_regex(\n            r'^(?:https?://)?([^/]*)/.*', url, 'video uploader')\n\n        # Helper method\n        def _playlist_from_matches(matches, getter, ie=None):\n            urlrs = orderedSet(\n                self.url_result(self._proto_relative_url(getter(m)), ie)\n                for m in matches)\n            return self.playlist_result(\n                urlrs, playlist_id=video_id, playlist_title=video_title)\n\n        # Look for BrightCove:\n        bc_urls = BrightcoveIE._extract_brightcove_urls(webpage)\n        if bc_urls:\n            self.to_screen('Brightcove video detected.')\n            entries = [{\n                '_type': 'url',\n                'url': smuggle_url(bc_url, {'Referer': url}),\n                'ie_key': 'Brightcove'\n            } for bc_url in bc_urls]\n\n            return {\n                '_type': 'playlist',\n                'title': video_title,\n                'id': video_id,\n                'entries': entries,\n            }\n\n        # Look for embedded (iframe) Vimeo player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//player\\.vimeo\\.com/video/.+?)\\1', webpage)\n        if mobj:\n            player_url = unescapeHTML(mobj.group('url'))\n            surl = smuggle_url(player_url, {'Referer': url})\n            return self.url_result(surl)\n\n        # Look for embedded (swf embed) Vimeo player\n        mobj = re.search(\n            r'<embed[^>]+?src=\"((?:https?:)?//(?:www\\.)?vimeo\\.com/moogaloop\\.swf.+?)\"', webpage)\n        if mobj:\n            return self.url_result(mobj.group(1))\n\n        # Look for embedded YouTube player\n        matches = re.findall(r'''(?x)\n            (?:\n                <iframe[^>]+?src=|\n                data-video-url=|\n                <embed[^>]+?src=|\n                embedSWF\\(?:\\s*|\n                new\\s+SWFObject\\(\n            )\n            ([\"\\'])\n                (?P<url>(?:https?:)?//(?:www\\.)?youtube(?:-nocookie)?\\.com/\n                (?:embed|v|p)/.+?)\n            \\1''', webpage)\n        if matches:\n            return _playlist_from_matches(\n                matches, lambda m: unescapeHTML(m[1]))\n\n        # Look for embedded Dailymotion player\n        matches = re.findall(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?dailymotion\\.com/embed/video/.+?)\\1', webpage)\n        if matches:\n            return _playlist_from_matches(\n                matches, lambda m: unescapeHTML(m[1]))\n\n        # Look for embedded Dailymotion playlist player (#3822)\n        m = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:www\\.)?dailymotion\\.[a-z]{2,3}/widget/jukebox\\?.+?)\\1', webpage)\n        if m:\n            playlists = re.findall(\n                r'list\\[\\]=/playlist/([^/]+)/', unescapeHTML(m.group('url')))\n            if playlists:\n                return _playlist_from_matches(\n                    playlists, lambda p: '//dailymotion.com/playlist/%s' % p)\n\n        # Look for embedded Wistia player\n        match = re.search(\n            r'<(?:meta[^>]+?content|iframe[^>]+?src)=([\"\\'])(?P<url>(?:https?:)?//(?:fast\\.)?wistia\\.net/embed/iframe/.+?)\\1', webpage)\n        if match:\n            embed_url = self._proto_relative_url(\n                unescapeHTML(match.group('url')))\n            return {\n                '_type': 'url_transparent',\n                'url': embed_url,\n                'ie_key': 'Wistia',\n                'uploader': video_uploader,\n                'title': video_title,\n                'id': video_id,\n            }\n            \n        match = re.search(r'(?:id=[\"\\']wistia_|data-wistia-?id=[\"\\']|Wistia\\.embed\\([\"\\'])(?P<id>[^\"\\']+)', webpage)\n        if match:\n            return {\n                '_type': 'url_transparent',\n                'url': 'http://fast.wistia.net/embed/iframe/{0:}'.format(match.group('id')),\n                'ie_key': 'Wistia',\n                'uploader': video_uploader,\n                'title': video_title,\n                'id': match.group('id')\n            }\n\n        # Look for embedded blip.tv player\n        mobj = re.search(r'<meta\\s[^>]*https?://api\\.blip\\.tv/\\w+/redirect/\\w+/(\\d+)', webpage)\n        if mobj:\n            return self.url_result('http://blip.tv/a/a-'+mobj.group(1), 'BlipTV')\n        mobj = re.search(r'<(?:iframe|embed|object)\\s[^>]*(https?://(?:\\w+\\.)?blip\\.tv/(?:play/|api\\.swf#)[a-zA-Z0-9_]+)', webpage)\n        if mobj:\n            return self.url_result(mobj.group(1), 'BlipTV')\n\n        # Look for embedded condenast player\n        matches = re.findall(\n            r'<iframe\\s+(?:[a-zA-Z-]+=\"[^\"]+\"\\s+)*?src=\"(https?://player\\.cnevids\\.com/embed/[^\"]+\")',\n            webpage)\n        if matches:\n            return {\n                '_type': 'playlist',\n                'entries': [{\n                    '_type': 'url',\n                    'ie_key': 'CondeNast',\n                    'url': ma,\n                } for ma in matches],\n                'title': video_title,\n                'id': video_id,\n            }\n\n        # Look for Bandcamp pages with custom domain\n        mobj = re.search(r'<meta property=\"og:url\"[^>]*?content=\"(.*?bandcamp\\.com.*?)\"', webpage)\n        if mobj is not None:\n            burl = unescapeHTML(mobj.group(1))\n            # Don't set the extractor because it can be a track url or an album\n            return self.url_result(burl)\n\n        # Look for embedded Vevo player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>(?:https?:)?//(?:cache\\.)?vevo\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for Ooyala videos\n        mobj = (re.search(r'player.ooyala.com/[^\"?]+\\?[^\"]*?(?:embedCode|ec)=(?P<ec>[^\"&]+)', webpage) or\n             re.search(r'OO.Player.create\\([\\'\"].*?[\\'\"],\\s*[\\'\"](?P<ec>.{32})[\\'\"]', webpage))\n        if mobj is not None:\n            return OoyalaIE._build_url_result(mobj.group('ec'))\n\n        # Look for Aparat videos\n        mobj = re.search(r'<iframe .*?src=\"(http://www\\.aparat\\.com/video/[^\"]+)\"', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group(1), 'Aparat')\n\n        # Look for MPORA videos\n        mobj = re.search(r'<iframe .*?src=\"(http://mpora\\.(?:com|de)/videos/[^\"]+)\"', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group(1), 'Mpora')\n\n        # Look for embedded NovaMov-based player\n        mobj = re.search(\n            r'''(?x)<(?:pagespeed_)?iframe[^>]+?src=([\"\\'])\n                    (?P<url>http://(?:(?:embed|www)\\.)?\n                        (?:novamov\\.com|\n                           nowvideo\\.(?:ch|sx|eu|at|ag|co)|\n                           videoweed\\.(?:es|com)|\n                           movshare\\.(?:net|sx|ag)|\n                           divxstage\\.(?:eu|net|ch|co|at|ag))\n                        /embed\\.php.+?)\\1''', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n\n        # Look for embedded Facebook player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https://www\\.facebook\\.com/video/embed.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Facebook')\n\n        # Look for embedded VK player\n        mobj = re.search(r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://vk\\.com/video_ext\\.php.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'VK')\n\n        # Look for embedded ivi player\n        mobj = re.search(r'<embed[^>]+?src=([\"\\'])(?P<url>https?://(?:www\\.)?ivi\\.ru/video/player.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Ivi')\n\n        # Look for embedded Huffington Post player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://embed\\.live\\.huffingtonpost\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'HuffPost')\n\n        # Look for embed.ly\n        mobj = re.search(r'class=[\"\\']embedly-card[\"\\'][^>]href=[\"\\'](?P<url>[^\"\\']+)', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'))\n        mobj = re.search(r'class=[\"\\']embedly-embed[\"\\'][^>]src=[\"\\'][^\"\\']*url=(?P<url>[^&]+)', webpage)\n        if mobj is not None:\n            return self.url_result(compat_urllib_parse.unquote(mobj.group('url')))\n\n        # Look for funnyordie embed\n        matches = re.findall(r'<iframe[^>]+?src=\"(https?://(?:www\\.)?funnyordie\\.com/embed/[^\"]+)\"', webpage)\n        if matches:\n            return _playlist_from_matches(\n                matches, getter=unescapeHTML, ie='FunnyOrDie')\n\n        # Look for embedded RUTV player\n        rutv_url = RUTVIE._extract_url(webpage)\n        if rutv_url:\n            return self.url_result(rutv_url, 'RUTV')\n\n        # Look for embedded TED player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>http://embed\\.ted\\.com/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'TED')\n\n        # Look for embedded Ustream videos\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>http://www\\.ustream\\.tv/embed/.+?)\\1', webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Ustream')\n\n        # Look for embedded arte.tv player\n        mobj = re.search(\n            r'<script [^>]*?src=\"(?P<url>http://www\\.arte\\.tv/playerv2/embed[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'ArteTVEmbed')\n\n        # Look for embedded smotri.com player\n        smotri_url = SmotriIE._extract_url(webpage)\n        if smotri_url:\n            return self.url_result(smotri_url, 'Smotri')\n\n        # Look for embeded soundcloud player\n        mobj = re.search(\n            r'<iframe src=\"(?P<url>https?://(?:w\\.)?soundcloud\\.com/player[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            url = unescapeHTML(mobj.group('url'))\n            return self.url_result(url)\n\n        # Look for embedded vulture.com player\n        mobj = re.search(\n            r'<iframe src=\"(?P<url>https?://video\\.vulture\\.com/[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            url = unescapeHTML(mobj.group('url'))\n            return self.url_result(url, ie='Vulture')\n\n        # Look for embedded mtvservices player\n        mobj = re.search(\n            r'<iframe src=\"(?P<url>https?://media\\.mtvnservices\\.com/embed/[^\"]+)\"',\n            webpage)\n        if mobj is not None:\n            url = unescapeHTML(mobj.group('url'))\n            return self.url_result(url, ie='MTVServicesEmbedded')\n\n        # Look for embedded yahoo player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://(?:screen|movies)\\.yahoo\\.com/.+?\\.html\\?format=embed)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'Yahoo')\n\n        # Look for embedded sbs.com.au player\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://(?:www\\.)sbs\\.com\\.au/ondemand/video/single/.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'SBS')\n\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>https?://m\\.mlb\\.com/shared/video/embed/embed\\.html\\?.+?)\\1',\n            webpage)\n        if mobj is not None:\n            return self.url_result(mobj.group('url'), 'MLB')\n\n        mobj = re.search(\n            r'<iframe[^>]+?src=([\"\\'])(?P<url>%s)\\1' % CondeNastIE.EMBED_URL,\n            webpage)\n        if mobj is not None:\n            return self.url_result(self._proto_relative_url(mobj.group('url'), scheme='http:'), 'CondeNast')\n\n        def check_video(vurl):\n            vpath = compat_urlparse.urlparse(vurl).path\n            vext = determine_ext(vpath)\n            return '.' in vpath and vext not in ('swf', 'png', 'jpg', 'srt', 'sbv', 'sub', 'vtt', 'ttml')\n\n        def filter_video(urls):\n            return list(filter(check_video, urls))\n\n        # Start with something easy: JW Player in SWFObject\n        found = filter_video(re.findall(r'flashvars: [\\'\"](?:.*&)?file=(http[^\\'\"&]*)', webpage))\n        if not found:\n            # Look for gorilla-vid style embedding\n            found = filter_video(re.findall(r'''(?sx)\n                (?:\n                    jw_plugins|\n                    JWPlayerOptions|\n                    jwplayer\\s*\\(\\s*[\"'][^'\"]+[\"']\\s*\\)\\s*\\.setup\n                )\n                .*?file\\s*:\\s*[\"\\'](.*?)[\"\\']''', webpage))\n        if not found:\n            # Broaden the search a little bit\n            found = filter_video(re.findall(r'[^A-Za-z0-9]?(?:file|source)=(http[^\\'\"&]*)', webpage))\n        if not found:\n            # Broaden the findall a little bit: JWPlayer JS loader\n            found = filter_video(re.findall(\n                r'[^A-Za-z0-9]?file[\"\\']?:\\s*[\"\\'](http(?![^\\'\"]+\\.[0-9]+[\\'\"])[^\\'\"]+)[\"\\']', webpage))\n        if not found:\n            # Flow player\n            found = filter_video(re.findall(r'''(?xs)\n                flowplayer\\(\"[^\"]+\",\\s*\n                    \\{[^}]+?\\}\\s*,\n                    \\s*{[^}]+? [\"']?clip[\"']?\\s*:\\s*\\{\\s*\n                        [\"']?url[\"']?\\s*:\\s*[\"']([^\"']+)[\"']\n            ''', webpage))\n        if not found:\n            # Try to find twitter cards info\n            found = filter_video(re.findall(\n                r'<meta (?:property|name)=\"twitter:player:stream\" (?:content|value)=\"(.+?)\"', webpage))\n        if not found:\n            # We look for Open Graph info:\n            # We have to match any number spaces between elements, some sites try to align them (eg.: statigr.am)\n            m_video_type = re.findall(r'<meta.*?property=\"og:video:type\".*?content=\"video/(.*?)\"', webpage)\n            # We only look in og:video if the MIME type is a video, don't try if it's a Flash player:\n            if m_video_type is not None:\n                found = filter_video(re.findall(r'<meta.*?property=\"og:video\".*?content=\"(.*?)\"', webpage))\n        if not found:\n            # HTML5 video\n            found = re.findall(r'(?s)<video[^<]*(?:>.*?<source[^>]*)?\\s+src=\"([^\"]+)\"', webpage)\n        if not found:\n            found = re.search(\n                r'(?i)<meta\\s+(?=(?:[a-z-]+=\"[^\"]+\"\\s+)*http-equiv=\"refresh\")'\n                r'(?:[a-z-]+=\"[^\"]+\"\\s+)*?content=\"[0-9]{,2};url=\\'?([^\\'\"]+)',\n                webpage)\n            if found:\n                new_url = found.group(1)\n                self.report_following_redirect(new_url)\n                return {\n                    '_type': 'url',\n                    'url': new_url,\n                }\n        if not found:\n            raise ExtractorError('Unsupported URL: %s' % url)\n\n        entries = []\n        for video_url in found:\n            video_url = compat_urlparse.urljoin(url, video_url)\n            video_id = compat_urllib_parse.unquote(os.path.basename(video_url))\n\n            # Sometimes, jwplayer extraction will result in a YouTube URL\n            if YoutubeIE.suitable(video_url):\n                entries.append(self.url_result(video_url, 'Youtube'))\n                continue\n\n            # here's a fun little line of code for you:\n            video_id = os.path.splitext(video_id)[0]\n\n            entries.append({\n                'id': video_id,\n                'url': video_url,\n                'uploader': video_uploader,\n                'title': video_title,\n                'age_limit': age_limit,\n            })\n\n        if len(entries) == 1:\n            return entries[0]\n        else:\n            for num, e in enumerate(entries, start=1):\n                e['title'] = '%s (%d)' % (e['title'], num)\n            return {\n                '_type': 'playlist',\n                'entries': entries,\n            }",
        "begin_line": 471,
        "end_line": 981,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002688172043010753,
            "pseudo_dstar_susp": 0.002564102564102564,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.002564102564102564,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.downloader.http.HttpFD.real_download#17",
        "src_path": "youtube_dl/downloader/http.py",
        "class_name": "youtube_dl.downloader.http.HttpFD",
        "signature": "youtube_dl.downloader.http.HttpFD.real_download(self, filename, info_dict)",
        "snippet": "    def real_download(self, filename, info_dict):\n        url = info_dict['url']\n        tmpfilename = self.temp_name(filename)\n        stream = None\n\n        # Do not include the Accept-Encoding header\n        headers = {'Youtubedl-no-compression': 'True'}\n        if 'user_agent' in info_dict:\n            headers['Youtubedl-user-agent'] = info_dict['user_agent']\n        if 'http_referer' in info_dict:\n            headers['Referer'] = info_dict['http_referer']\n        add_headers = info_dict.get('http_headers')\n        if add_headers:\n            headers.update(add_headers)\n        data = info_dict.get('http_post_data')\n        http_method = info_dict.get('http_method')\n        basic_request = compat_urllib_request.Request(url, data, headers)\n        request = compat_urllib_request.Request(url, data, headers)\n        if http_method is not None:\n            basic_request.get_method = lambda: http_method\n            request.get_method = lambda: http_method\n\n        is_test = self.params.get('test', False)\n\n        if is_test:\n            request.add_header('Range', 'bytes=0-%s' % str(self._TEST_FILE_SIZE - 1))\n\n        # Establish possible resume length\n        if os.path.isfile(encodeFilename(tmpfilename)):\n            resume_len = os.path.getsize(encodeFilename(tmpfilename))\n        else:\n            resume_len = 0\n\n        open_mode = 'wb'\n        if resume_len != 0:\n            if self.params.get('continuedl', False):\n                self.report_resuming_byte(resume_len)\n                request.add_header('Range', 'bytes=%d-' % resume_len)\n                open_mode = 'ab'\n            else:\n                resume_len = 0\n\n        count = 0\n        retries = self.params.get('retries', 0)\n        while count <= retries:\n            # Establish connection\n            try:\n                data = self.ydl.urlopen(request)\n                break\n            except (compat_urllib_error.HTTPError, ) as err:\n                if (err.code < 500 or err.code >= 600) and err.code != 416:\n                    # Unexpected HTTP error\n                    raise\n                elif err.code == 416:\n                    # Unable to resume (requested range not satisfiable)\n                    try:\n                        # Open the connection again without the range header\n                        data = self.ydl.urlopen(basic_request)\n                        content_length = data.info()['Content-Length']\n                    except (compat_urllib_error.HTTPError, ) as err:\n                        if err.code < 500 or err.code >= 600:\n                            raise\n                    else:\n                        # Examine the reported length\n                        if (content_length is not None and\n                                (resume_len - 100 < int(content_length) < resume_len + 100)):\n                            # The file had already been fully downloaded.\n                            # Explanation to the above condition: in issue #175 it was revealed that\n                            # YouTube sometimes adds or removes a few bytes from the end of the file,\n                            # changing the file size slightly and causing problems for some users. So\n                            # I decided to implement a suggested change and consider the file\n                            # completely downloaded if the file size differs less than 100 bytes from\n                            # the one in the hard drive.\n                            self.report_file_already_downloaded(filename)\n                            self.try_rename(tmpfilename, filename)\n                            self._hook_progress({\n                                'filename': filename,\n                                'status': 'finished',\n                            })\n                            return True\n                        else:\n                            # The length does not match, we start the download over\n                            self.report_unable_to_resume()\n                            resume_len = 0\n                            open_mode = 'wb'\n                            break\n            # Retry\n            count += 1\n            if count <= retries:\n                self.report_retry(count, retries)\n\n        if count > retries:\n            self.report_error(u'giving up after %s retries' % retries)\n            return False\n\n        data_len = data.info().get('Content-length', None)\n\n        # Range HTTP header may be ignored/unsupported by a webserver\n        # (e.g. extractor/scivee.py, extractor/bambuser.py).\n        # However, for a test we still would like to download just a piece of a file.\n        # To achieve this we limit data_len to _TEST_FILE_SIZE and manually control\n        # block size when downloading a file.\n        if is_test and (data_len is None or int(data_len) > self._TEST_FILE_SIZE):\n            data_len = self._TEST_FILE_SIZE\n\n        if data_len is not None:\n            data_len = int(data_len) + resume_len\n            min_data_len = self.params.get(\"min_filesize\", None)\n            max_data_len = self.params.get(\"max_filesize\", None)\n            if min_data_len is not None and data_len < min_data_len:\n                self.to_screen(u'\\r[download] File is smaller than min-filesize (%s bytes < %s bytes). Aborting.' % (data_len, min_data_len))\n                return False\n            if max_data_len is not None and data_len > max_data_len:\n                self.to_screen(u'\\r[download] File is larger than max-filesize (%s bytes > %s bytes). Aborting.' % (data_len, max_data_len))\n                return False\n\n        data_len_str = format_bytes(data_len)\n        byte_counter = 0 + resume_len\n        block_size = self.params.get('buffersize', 1024)\n        start = time.time()\n        while True:\n            # Download and write\n            before = time.time()\n            data_block = data.read(block_size if not is_test else min(block_size, data_len - byte_counter))\n            after = time.time()\n            if len(data_block) == 0:\n                break\n            byte_counter += len(data_block)\n\n            # Open file just in time\n            if stream is None:\n                try:\n                    (stream, tmpfilename) = sanitize_open(tmpfilename, open_mode)\n                    assert stream is not None\n                    filename = self.undo_temp_name(tmpfilename)\n                    self.report_destination(filename)\n                except (OSError, IOError) as err:\n                    self.report_error(u'unable to open for writing: %s' % str(err))\n                    return False\n            try:\n                stream.write(data_block)\n            except (IOError, OSError) as err:\n                self.to_stderr(u\"\\n\")\n                self.report_error(u'unable to write data: %s' % str(err))\n                return False\n            if not self.params.get('noresizebuffer', False):\n                block_size = self.best_block_size(after - before, len(data_block))\n\n            # Progress message\n            speed = self.calc_speed(start, time.time(), byte_counter - resume_len)\n            if data_len is None:\n                eta = percent = None\n            else:\n                percent = self.calc_percent(byte_counter, data_len)\n                eta = self.calc_eta(start, time.time(), data_len - resume_len, byte_counter - resume_len)\n            self.report_progress(percent, data_len_str, speed, eta)\n\n            self._hook_progress({\n                'downloaded_bytes': byte_counter,\n                'total_bytes': data_len,\n                'tmpfilename': tmpfilename,\n                'filename': filename,\n                'status': 'downloading',\n                'eta': eta,\n                'speed': speed,\n            })\n\n            if is_test and byte_counter == data_len:\n                break\n\n            # Apply rate limit\n            self.slow_down(start, byte_counter - resume_len)\n\n        if stream is None:\n            self.to_stderr(u\"\\n\")\n            self.report_error(u'Did not get any data blocks')\n            return False\n        if tmpfilename != u'-':\n            stream.close()\n        self.report_finish(data_len_str, (time.time() - start))\n        if data_len is not None and byte_counter != data_len:\n            raise ContentTooShortError(byte_counter, int(data_len))\n        self.try_rename(tmpfilename, filename)\n\n        # Update file modification time\n        if self.params.get('updatetime', True):\n            info_dict['filetime'] = self.try_utime(filename, data.info().get('last-modified', None))\n\n        self._hook_progress({\n            'downloaded_bytes': byte_counter,\n            'total_bytes': byte_counter,\n            'filename': filename,\n            'status': 'finished',\n        })\n\n        return True",
        "begin_line": 17,
        "end_line": 212,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003289473684210526,
            "pseudo_dstar_susp": 0.003289473684210526,
            "pseudo_tarantula_susp": 0.001199040767386091,
            "pseudo_op2_susp": 0.003289473684210526,
            "pseudo_barinel_susp": 0.001199040767386091
        }
    },
    {
        "name": "youtube_dl.extractor.aol.AolIE._real_extract#42",
        "src_path": "youtube_dl/extractor/aol.py",
        "class_name": "youtube_dl.extractor.aol.AolIE",
        "signature": "youtube_dl.extractor.aol.AolIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n\n        playlist_id = mobj.group('playlist_id')\n        if playlist_id and not self._downloader.params.get('noplaylist'):\n            self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n\n            webpage = self._download_webpage(url, playlist_id)\n            title = self._html_search_regex(\n                r'<h1 class=\"video-title[^\"]*\">(.+?)</h1>', webpage, 'title')\n            playlist_html = self._search_regex(\n                r\"(?s)<ul\\s+class='video-related[^']*'>(.*?)</ul>\", webpage,\n                'playlist HTML')\n            entries = [{\n                '_type': 'url',\n                'url': 'aol-video:%s' % m.group('id'),\n                'ie_key': 'Aol',\n            } for m in re.finditer(\n                r\"<a\\s+href='.*videoid=(?P<id>[0-9]+)'\\s+class='video-thumb'>\",\n                playlist_html)]\n\n            return {\n                '_type': 'playlist',\n                'id': playlist_id,\n                'display_id': mobj.group('playlist_display_id'),\n                'title': title,\n                'entries': entries,\n            }\n\n        return FiveMinIE._build_result(video_id)",
        "begin_line": 42,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0036101083032490976,
            "pseudo_dstar_susp": 0.0035971223021582736,
            "pseudo_tarantula_susp": 0.002506265664160401,
            "pseudo_op2_susp": 0.0035971223021582736,
            "pseudo_barinel_susp": 0.002506265664160401
        }
    },
    {
        "name": "youtube_dl.extractor.bandcamp.BandcampAlbumIE._real_extract#139",
        "src_path": "youtube_dl/extractor/bandcamp.py",
        "class_name": "youtube_dl.extractor.bandcamp.BandcampAlbumIE",
        "signature": "youtube_dl.extractor.bandcamp.BandcampAlbumIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        playlist_id = mobj.group('subdomain')\n        title = mobj.group('title')\n        display_id = title or playlist_id\n        webpage = self._download_webpage(url, display_id)\n        tracks_paths = re.findall(r'<a href=\"(.*?)\" itemprop=\"url\">', webpage)\n        if not tracks_paths:\n            raise ExtractorError('The page doesn\\'t contain any tracks')\n        entries = [\n            self.url_result(compat_urlparse.urljoin(url, t_path), ie=BandcampIE.ie_key())\n            for t_path in tracks_paths]\n        title = self._search_regex(r'album_title : \"(.*?)\"', webpage, 'title')\n        return {\n            '_type': 'playlist',\n            'id': playlist_id,\n            'display_id': display_id,\n            'title': title,\n            'entries': entries,\n        }",
        "begin_line": 139,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0016181229773462784,
            "pseudo_dstar_susp": 0.0014619883040935672,
            "pseudo_tarantula_susp": 0.0012224938875305623,
            "pseudo_op2_susp": 0.0014619883040935672,
            "pseudo_barinel_susp": 0.0012224938875305623
        }
    },
    {
        "name": "youtube_dl.extractor.comedycentral.ComedyCentralShowsIE._real_extract#120",
        "src_path": "youtube_dl/extractor/comedycentral.py",
        "class_name": "youtube_dl.extractor.comedycentral.ComedyCentralShowsIE",
        "signature": "youtube_dl.extractor.comedycentral.ComedyCentralShowsIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url, re.VERBOSE)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n\n        if mobj.group('shortname'):\n            if mobj.group('shortname') in ('tds', 'thedailyshow'):\n                url = 'http://thedailyshow.cc.com/full-episodes/'\n            else:\n                url = 'http://thecolbertreport.cc.com/full-episodes/'\n            mobj = re.match(self._VALID_URL, url, re.VERBOSE)\n            assert mobj is not None\n\n        if mobj.group('clip'):\n            if mobj.group('videotitle'):\n                epTitle = mobj.group('videotitle')\n            elif mobj.group('showname') == 'thedailyshow':\n                epTitle = mobj.group('tdstitle')\n            else:\n                epTitle = mobj.group('cntitle')\n            dlNewest = False\n        elif mobj.group('interview'):\n            epTitle = mobj.group('interview_title')\n            dlNewest = False\n        else:\n            dlNewest = not mobj.group('episode')\n            if dlNewest:\n                epTitle = mobj.group('showname')\n            else:\n                epTitle = mobj.group('episode')\n        show_name = mobj.group('showname')\n\n        webpage, htmlHandle = self._download_webpage_handle(url, epTitle)\n        if dlNewest:\n            url = htmlHandle.geturl()\n            mobj = re.match(self._VALID_URL, url, re.VERBOSE)\n            if mobj is None:\n                raise ExtractorError('Invalid redirected URL: ' + url)\n            if mobj.group('episode') == '':\n                raise ExtractorError('Redirected URL is still not specific: ' + url)\n            epTitle = (mobj.group('episode') or mobj.group('videotitle')).rpartition('/')[-1]\n\n        mMovieParams = re.findall('(?:<param name=\"movie\" value=\"|var url = \")(http://media.mtvnservices.com/([^\"]*(?:episode|video).*?:.*?))\"', webpage)\n        if len(mMovieParams) == 0:\n            # The Colbert Report embeds the information in a without\n            # a URL prefix; so extract the alternate reference\n            # and then add the URL prefix manually.\n\n            altMovieParams = re.findall('data-mgid=\"([^\"]*(?:episode|video|playlist).*?:.*?)\"', webpage)\n            if len(altMovieParams) == 0:\n                raise ExtractorError('unable to find Flash URL in webpage ' + url)\n            else:\n                mMovieParams = [(\"http://media.mtvnservices.com/\" + altMovieParams[0], altMovieParams[0])]\n\n        uri = mMovieParams[0][1]\n        # Correct cc.com in uri\n        uri = re.sub(r'(episode:[^.]+)(\\.cc)?\\.com', r'\\1.cc.com', uri)\n\n        index_url = 'http://%s.cc.com/feeds/mrss?%s' % (show_name, compat_urllib_parse.urlencode({'uri': uri}))\n        idoc = self._download_xml(\n            index_url, epTitle,\n            'Downloading show index', 'Unable to download episode index')\n\n        title = idoc.find('./channel/title').text\n        description = idoc.find('./channel/description').text\n\n        entries = []\n        item_els = idoc.findall('.//item')\n        for part_num, itemEl in enumerate(item_els):\n            upload_date = unified_strdate(itemEl.findall('./pubDate')[0].text)\n            thumbnail = itemEl.find('.//{http://search.yahoo.com/mrss/}thumbnail').attrib.get('url')\n\n            content = itemEl.find('.//{http://search.yahoo.com/mrss/}content')\n            duration = float_or_none(content.attrib.get('duration'))\n            mediagen_url = content.attrib['url']\n            guid = itemEl.find('./guid').text.rpartition(':')[-1]\n\n            cdoc = self._download_xml(\n                mediagen_url, epTitle,\n                'Downloading configuration for segment %d / %d' % (part_num + 1, len(item_els)))\n\n            turls = []\n            for rendition in cdoc.findall('.//rendition'):\n                finfo = (rendition.attrib['bitrate'], rendition.findall('./src')[0].text)\n                turls.append(finfo)\n\n            formats = []\n            for format, rtmp_video_url in turls:\n                w, h = self._video_dimensions.get(format, (None, None))\n                formats.append({\n                    'format_id': 'vhttp-%s' % format,\n                    'url': self._transform_rtmp_url(rtmp_video_url),\n                    'ext': self._video_extensions.get(format, 'mp4'),\n                    'height': h,\n                    'width': w,\n\n                    'format_note': 'HTTP 400 at the moment (patches welcome!)',\n                    'preference': -100,\n                })\n                formats.append({\n                    'format_id': 'rtmp-%s' % format,\n                    'url': rtmp_video_url.replace('viacomccstrm', 'viacommtvstrm'),\n                    'ext': self._video_extensions.get(format, 'mp4'),\n                    'height': h,\n                    'width': w,\n                })\n                self._sort_formats(formats)\n\n            virtual_id = show_name + ' ' + epTitle + ' part ' + compat_str(part_num + 1)\n            entries.append({\n                'id': guid,\n                'title': virtual_id,\n                'formats': formats,\n                'uploader': show_name,\n                'upload_date': upload_date,\n                'duration': duration,\n                'thumbnail': thumbnail,\n                'description': description,\n            })\n\n        return {\n            '_type': 'playlist',\n            'entries': entries,\n            'title': show_name + ' ' + title,\n            'description': description,\n        }",
        "begin_line": 120,
        "end_line": 245,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019801980198019802,
            "pseudo_dstar_susp": 0.0017123287671232876,
            "pseudo_tarantula_susp": 0.001221001221001221,
            "pseudo_op2_susp": 0.0017123287671232876,
            "pseudo_barinel_susp": 0.001221001221001221
        }
    },
    {
        "name": "youtube_dl.extractor.__init__.gen_extractors#515",
        "src_path": "youtube_dl/extractor/__init__.py",
        "class_name": "youtube_dl.extractor.__init__",
        "signature": "youtube_dl.extractor.__init__.gen_extractors()",
        "snippet": "def gen_extractors():\n    \"\"\" Return a list of an instance of every supported extractor.\n    The order does matter; the first extractor matched is the one handling the URL.\n    \"\"\"\n    return [klass() for klass in _ALL_CLASSES]",
        "begin_line": 515,
        "end_line": 519,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001095290251916758,
            "pseudo_dstar_susp": 0.0010976948408342481,
            "pseudo_tarantula_susp": 0.001092896174863388,
            "pseudo_op2_susp": 0.0010976948408342481,
            "pseudo_barinel_susp": 0.001092896174863388
        }
    },
    {
        "name": "youtube_dl.extractor.__init__.get_info_extractor#522",
        "src_path": "youtube_dl/extractor/__init__.py",
        "class_name": "youtube_dl.extractor.__init__",
        "signature": "youtube_dl.extractor.__init__.get_info_extractor(ie_name)",
        "snippet": "def get_info_extractor(ie_name):\n    \"\"\"Returns the info extractor class with the given ie_name\"\"\"\n    return globals()[ie_name+'IE']",
        "begin_line": 522,
        "end_line": 524,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.get_version#22",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg",
        "signature": "youtube_dl.postprocessor.ffmpeg.get_version(executable)",
        "snippet": "def get_version(executable):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    try:\n        out, err = subprocess.Popen(\n            [executable, '-version'],\n            stdout=subprocess.PIPE, stderr=subprocess.STDOUT).communicate()\n    except OSError:\n        return False\n    firstline = out.partition(b'\\n')[0].decode('ascii', 'ignore')\n    m = re.search(r'version\\s+([0-9._-a-zA-Z]+)', firstline)\n    if not m:\n        return u'present'\n    else:\n        return m.group(1)",
        "begin_line": 22,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.3333333333333333,
            "pseudo_dstar_susp": 0.3333333333333333,
            "pseudo_tarantula_susp": 0.002242152466367713,
            "pseudo_op2_susp": 0.3333333333333333,
            "pseudo_barinel_susp": 0.002242152466367713
        }
    },
    {
        "name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_versions#62",
        "src_path": "youtube_dl/postprocessor/ffmpeg.py",
        "class_name": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor",
        "signature": "youtube_dl.postprocessor.ffmpeg.FFmpegPostProcessor.get_versions()",
        "snippet": "    def get_versions():\n        programs = ['avprobe', 'avconv', 'ffmpeg', 'ffprobe']\n        return dict((program, get_version(program)) for program in programs)",
        "begin_line": 62,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.16666666666666666,
            "pseudo_dstar_susp": 0.16666666666666666,
            "pseudo_tarantula_susp": 0.001466275659824047,
            "pseudo_op2_susp": 0.16666666666666666,
            "pseudo_barinel_susp": 0.001466275659824047
        }
    },
    {
        "name": "youtube_dl.extractor.pornotube.PornotubeIE._real_extract#27",
        "src_path": "youtube_dl/extractor/pornotube.py",
        "class_name": "youtube_dl.extractor.pornotube.PornotubeIE",
        "signature": "youtube_dl.extractor.pornotube.PornotubeIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n\n        video_id = mobj.group('videoid')\n        video_title = mobj.group('title')\n\n        # Get webpage content\n        webpage = self._download_webpage(url, video_id)\n\n        # Get the video URL\n        VIDEO_URL_RE = r'url: \"(?P<url>http://video[0-9].pornotube.com/.+\\.flv)\",'\n        video_url = self._search_regex(VIDEO_URL_RE, webpage, 'video url')\n        video_url = compat_urllib_parse.unquote(video_url)\n\n        #Get the uploaded date\n        VIDEO_UPLOADED_RE = r'<div class=\"video_added_by\">Added (?P<date>[0-9\\/]+) by'\n        upload_date = self._html_search_regex(VIDEO_UPLOADED_RE, webpage, 'upload date', fatal=False)\n        if upload_date:\n            upload_date = unified_strdate(upload_date)\n        age_limit = self._rta_search(webpage)\n\n        return {\n            'id': video_id,\n            'url': video_url,\n            'upload_date': upload_date,\n            'title': video_title,\n            'ext': 'flv',\n            'format': 'flv',\n            'age_limit': age_limit,\n        }",
        "begin_line": 27,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.preferredencoding#293",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.preferredencoding()",
        "snippet": "def preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        u'TEST'.encode(pref)\n    except:\n        pref = 'UTF-8'\n\n    return pref",
        "begin_line": 293,
        "end_line": 305,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007518796992481203,
            "pseudo_dstar_susp": 0.007142857142857143,
            "pseudo_tarantula_susp": 0.0018181818181818182,
            "pseudo_op2_susp": 0.007142857142857143,
            "pseudo_barinel_susp": 0.0018181818181818182
        }
    },
    {
        "name": "youtube_dl.utils.write_json_file#316",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.write_json_file(obj, fn)",
        "snippet": "def write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically \"\"\"\n\n    args = {\n        'suffix': '.tmp',\n        'prefix': os.path.basename(fn) + '.',\n        'dir': os.path.dirname(fn),\n        'delete': False,\n    }\n\n    # In Python 2.x, json.dump expects a bytestream.\n    # In Python 3.x, it writes to a character stream\n    if sys.version_info < (3, 0):\n        args['mode'] = 'wb'\n    else:\n        args.update({\n            'mode': 'w',\n            'encoding': 'utf-8',\n        })\n\n    tf = tempfile.NamedTemporaryFile(**args)\n\n    try:\n        with tf:\n            json.dump(obj, tf)\n        os.rename(tf.name, fn)\n    except:\n        try:\n            os.remove(tf.name)\n        except OSError:\n            pass\n        raise",
        "begin_line": 316,
        "end_line": 347,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001639344262295082,
            "pseudo_dstar_susp": 0.001893939393939394,
            "pseudo_tarantula_susp": 0.0009354536950420954,
            "pseudo_op2_susp": 0.001893939393939394,
            "pseudo_barinel_susp": 0.0009354536950420954
        }
    },
    {
        "name": "youtube_dl.utils.find_xpath_attr#351",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.find_xpath_attr(node, xpath, key, val)",
        "snippet": "    def find_xpath_attr(node, xpath, key, val):\n        \"\"\" Find the xpath xpath[@key=val] \"\"\"\n        assert re.match(r'^[a-zA-Z-]+$', key)\n        assert re.match(r'^[a-zA-Z0-9@\\s:._-]*$', val)\n        expr = xpath + u\"[@%s='%s']\" % (key, val)\n        return node.find(expr)",
        "begin_line": 351,
        "end_line": 356,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.xpath_with_ns#371",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.xpath_with_ns(path, ns_map)",
        "snippet": "def xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)",
        "begin_line": 371,
        "end_line": 380,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007782101167315176,
            "pseudo_dstar_susp": 0.0007782101167315176,
            "pseudo_tarantula_susp": 0.0008216926869350862,
            "pseudo_op2_susp": 0.0007782101167315176,
            "pseudo_barinel_susp": 0.0008216926869350862
        }
    },
    {
        "name": "youtube_dl.utils.BaseHTMLParser.loads#403",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.BaseHTMLParser",
        "signature": "youtube_dl.utils.BaseHTMLParser.loads(self, html)",
        "snippet": "    def loads(self, html):\n        self.html = html\n        self.feed(html)\n        self.close()",
        "begin_line": 403,
        "end_line": 406,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.0008928571428571428,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008928571428571428,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.AttrParser.__init__#410",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.AttrParser",
        "signature": "youtube_dl.utils.AttrParser.__init__(self, attribute, value)",
        "snippet": "    def __init__(self, attribute, value):\n        self.attribute = attribute\n        self.value = value\n        self.result = None\n        self.started = False\n        self.depth = {}\n        self.watch_startpos = False\n        self.error_count = 0\n        BaseHTMLParser.__init__(self)",
        "begin_line": 410,
        "end_line": 418,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.AttrParser.handle_starttag#427",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.AttrParser",
        "signature": "youtube_dl.utils.AttrParser.handle_starttag(self, tag, attrs)",
        "snippet": "    def handle_starttag(self, tag, attrs):\n        attrs = dict(attrs)\n        if self.started:\n            self.find_startpos(None)\n        if self.attribute in attrs and attrs[self.attribute] == self.value:\n            self.result = [tag]\n            self.started = True\n            self.watch_startpos = True\n        if self.started:\n            if not tag in self.depth: self.depth[tag] = 0\n            self.depth[tag] += 1",
        "begin_line": 427,
        "end_line": 437,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.AttrParser.handle_endtag#439",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.AttrParser",
        "signature": "youtube_dl.utils.AttrParser.handle_endtag(self, tag)",
        "snippet": "    def handle_endtag(self, tag):\n        if self.started:\n            if tag in self.depth: self.depth[tag] -= 1\n            if self.depth[self.result[0]] == 0:\n                self.started = False\n                self.result.append(self.getpos())",
        "begin_line": 439,
        "end_line": 444,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.AttrParser.find_startpos#446",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.AttrParser",
        "signature": "youtube_dl.utils.AttrParser.find_startpos(self, x)",
        "snippet": "    def find_startpos(self, x):\n        \"\"\"Needed to put the start position of the result (self.result[1])\n        after the opening tag with the requested id\"\"\"\n        if self.watch_startpos:\n            self.watch_startpos = False\n            self.result.append(self.getpos())",
        "begin_line": 446,
        "end_line": 451,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.AttrParser.get_result#455",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.AttrParser",
        "signature": "youtube_dl.utils.AttrParser.get_result(self)",
        "snippet": "    def get_result(self):\n        if self.result is None:\n            return None\n        if len(self.result) != 3:\n            return None\n        lines = self.html.split('\\n')\n        lines = lines[self.result[1][0]-1:self.result[2][0]]\n        lines[0] = lines[0][self.result[1][1]:]\n        if len(lines) == 1:\n            lines[-1] = lines[-1][:self.result[2][1]-self.result[1][1]]\n        lines[-1] = lines[-1][:self.result[2][1]]\n        return '\\n'.join(lines).strip()",
        "begin_line": 455,
        "end_line": 466,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.get_element_by_attribute#478",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_element_by_attribute(attribute, value, html)",
        "snippet": "def get_element_by_attribute(attribute, value, html):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n    parser = AttrParser(attribute, value)\n    try:\n        parser.loads(html)\n    except compat_html_parser.HTMLParseError:\n        pass\n    return parser.get_result()",
        "begin_line": 478,
        "end_line": 485,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009285051067780873,
            "pseudo_dstar_susp": 0.0009000900090009,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009000900090009,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.MetaParser.__init__#492",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.MetaParser",
        "signature": "youtube_dl.utils.MetaParser.__init__(self, name)",
        "snippet": "    def __init__(self, name):\n        BaseHTMLParser.__init__(self)\n        self.name = name\n        self.content = None\n        self.result = None",
        "begin_line": 492,
        "end_line": 496,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.MetaParser.handle_starttag#498",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.MetaParser",
        "signature": "youtube_dl.utils.MetaParser.handle_starttag(self, tag, attrs)",
        "snippet": "    def handle_starttag(self, tag, attrs):\n        if tag != 'meta':\n            return\n        attrs = dict(attrs)\n        if attrs.get('name') == self.name:\n            self.result = attrs.get('content')",
        "begin_line": 498,
        "end_line": 503,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.MetaParser.get_result#505",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.MetaParser",
        "signature": "youtube_dl.utils.MetaParser.get_result(self)",
        "snippet": "    def get_result(self):\n        return self.result",
        "begin_line": 505,
        "end_line": 506,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.get_meta_content#508",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_meta_content(name, html)",
        "snippet": "def get_meta_content(name, html):\n    \"\"\"\n    Return the content attribute from the meta tag with the given name attribute.\n    \"\"\"\n    parser = MetaParser(name)\n    try:\n        parser.loads(html)\n    except compat_html_parser.HTMLParseError:\n        pass\n    return parser.get_result()",
        "begin_line": 508,
        "end_line": 517,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.clean_html#520",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.clean_html(html)",
        "snippet": "def clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n    # Newline vs <br />\n    html = html.replace('\\n', ' ')\n    html = re.sub(r'\\s*<\\s*br\\s*/?\\s*>\\s*', '\\n', html)\n    html = re.sub(r'<\\s*/\\s*p\\s*>\\s*<\\s*p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()",
        "begin_line": 520,
        "end_line": 530,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00273224043715847,
            "pseudo_dstar_susp": 0.002717391304347826,
            "pseudo_tarantula_susp": 0.0012755102040816326,
            "pseudo_op2_susp": 0.002717391304347826,
            "pseudo_barinel_susp": 0.0012755102040816326
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_open#533",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_open(filename, open_mode)",
        "snippet": "def sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    try:\n        if filename == u'-':\n            if sys.platform == 'win32':\n                import msvcrt\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n            return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n        stream = open(encodeFilename(filename), open_mode)\n        return (stream, filename)\n    except (IOError, OSError) as err:\n        if err.errno in (errno.EACCES,):\n            raise\n\n        # In case of error, try to remove win32 forbidden chars\n        alt_filename = os.path.join(\n                        re.sub(u'[/<>:\"\\\\|\\\\\\\\?\\\\*]', u'#', path_part)\n                        for path_part in os.path.split(filename)\n                       )\n        if alt_filename == filename:\n            raise\n        else:\n            # An exception here should be caught in the caller\n            stream = open(encodeFilename(filename), open_mode)\n            return (stream, alt_filename)",
        "begin_line": 533,
        "end_line": 565,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00117096018735363,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.0008216926869350862,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.0008216926869350862
        }
    },
    {
        "name": "youtube_dl.utils.timeconvert#568",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.timeconvert(timestr)",
        "snippet": "def timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp",
        "begin_line": 568,
        "end_line": 574,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001183431952662722,
            "pseudo_dstar_susp": 0.0012658227848101266,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0012658227848101266,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.sanitize_filename#576",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.sanitize_filename(s, restricted=False, is_id=False)",
        "snippet": "def sanitize_filename(s, restricted=False, is_id=False):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    If restricted is set, use a stricter subset of allowed characters.\n    Set is_id if this is not an arbitrary string, but an ID that should be kept if possible\n    \"\"\"\n    def replace_insane(char):\n        if char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '_-' if restricted else ' -'\n        elif char in '\\\\/|*<>':\n            return '_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n            return '_'\n        if restricted and ord(char) > 127:\n            return '_'\n        return char\n\n    result = u''.join(map(replace_insane, s))\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if not result:\n            result = '_'\n    return result",
        "begin_line": 576,
        "end_line": 606,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004166666666666667,
            "pseudo_dstar_susp": 0.004166666666666667,
            "pseudo_tarantula_susp": 0.0016501650165016502,
            "pseudo_op2_susp": 0.004166666666666667,
            "pseudo_barinel_susp": 0.0016501650165016502
        }
    },
    {
        "name": "youtube_dl.utils.replace_insane#581",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.replace_insane(char)",
        "snippet": "    def replace_insane(char):\n        if char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '_-' if restricted else ' -'\n        elif char in '\\\\/|*<>':\n            return '_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n            return '_'\n        if restricted and ord(char) > 127:\n            return '_'\n        return char",
        "begin_line": 581,
        "end_line": 594,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004081632653061225,
            "pseudo_dstar_susp": 0.004081632653061225,
            "pseudo_tarantula_susp": 0.0016313213703099511,
            "pseudo_op2_susp": 0.004081632653061225,
            "pseudo_barinel_susp": 0.0016313213703099511
        }
    },
    {
        "name": "youtube_dl.utils.orderedSet#608",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.orderedSet(iterable)",
        "snippet": "def orderedSet(iterable):\n    \"\"\" Remove all duplicates from the input iterable \"\"\"\n    res = []\n    for el in iterable:\n        if el not in res:\n            res.append(el)\n    return res",
        "begin_line": 608,
        "end_line": 614,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0022371364653243847,
            "pseudo_dstar_susp": 0.001692047377326565,
            "pseudo_tarantula_susp": 0.002967359050445104,
            "pseudo_op2_susp": 0.001692047377326565,
            "pseudo_barinel_susp": 0.002967359050445104
        }
    },
    {
        "name": "youtube_dl.utils._htmlentity_transform#617",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils._htmlentity_transform(entity)",
        "snippet": "def _htmlentity_transform(entity):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    # Known non-numeric HTML entity\n    if entity in compat_html_entities.name2codepoint:\n        return compat_chr(compat_html_entities.name2codepoint[entity])\n\n    mobj = re.match(r'#(x?[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith(u'x'):\n            base = 16\n            numstr = u'0%s' % numstr\n        else:\n            base = 10\n        return compat_chr(int(numstr, base))\n\n    # Unknown entity in name, return its literal representation\n    return (u'&%s;' % entity)",
        "begin_line": 617,
        "end_line": 634,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0010141987829614604,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0010141987829614604,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.unescapeHTML#637",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unescapeHTML(s)",
        "snippet": "def unescapeHTML(s):\n    if s is None:\n        return None\n    assert type(s) == compat_str\n\n    return re.sub(\n        r'&([^;]+);', lambda m: _htmlentity_transform(m.group(1)), s)",
        "begin_line": 637,
        "end_line": 643,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.encodeFilename#646",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.encodeFilename(s, for_subprocess=False)",
        "snippet": "def encodeFilename(s, for_subprocess=False):\n    \"\"\"\n    @param s The name of the file\n    \"\"\"\n\n    assert type(s) == compat_str\n\n    # Python 3 has a Unicode API\n    if sys.version_info >= (3, 0):\n        return s\n\n    if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n        # Pass u'' directly to use Unicode APIs on Windows 2000 and up\n        # (Detecting Windows NT 4 is tricky because 'major >= 4' would\n        # match Windows 9x series as well. Besides, NT 4 is obsolete.)\n        if not for_subprocess:\n            return s\n        else:\n            # For subprocess calls, encode with locale encoding\n            # Refer to http://stackoverflow.com/a/9951851/35070\n            encoding = preferredencoding()\n    else:\n        encoding = sys.getfilesystemencoding()\n    if encoding is None:\n        encoding = 'utf-8'\n    return s.encode(encoding, 'ignore')",
        "begin_line": 646,
        "end_line": 671,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001639344262295082,
            "pseudo_dstar_susp": 0.001893939393939394,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.001893939393939394,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.make_HTTPS_handler#701",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.make_HTTPS_handler(opts_no_check_certificate, **kwargs)",
        "snippet": "def make_HTTPS_handler(opts_no_check_certificate, **kwargs):\n    if sys.version_info < (3, 2):\n        import httplib\n\n        class HTTPSConnectionV3(httplib.HTTPSConnection):\n            def __init__(self, *args, **kwargs):\n                httplib.HTTPSConnection.__init__(self, *args, **kwargs)\n\n            def connect(self):\n                sock = socket.create_connection((self.host, self.port), self.timeout)\n                if getattr(self, '_tunnel_host', False):\n                    self.sock = sock\n                    self._tunnel()\n                try:\n                    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file, ssl_version=ssl.PROTOCOL_TLSv1)\n                except ssl.SSLError:\n                    self.sock = ssl.wrap_socket(sock, self.key_file, self.cert_file, ssl_version=ssl.PROTOCOL_SSLv23)\n\n        class HTTPSHandlerV3(compat_urllib_request.HTTPSHandler):\n            def https_open(self, req):\n                return self.do_open(HTTPSConnectionV3, req)\n        return HTTPSHandlerV3(**kwargs)\n    elif hasattr(ssl, 'create_default_context'):  # Python >= 3.4\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.options &= ~ssl.OP_NO_SSLv3  # Allow older, not-as-secure SSLv3\n        if opts_no_check_certificate:\n            context.verify_mode = ssl.CERT_NONE\n        return compat_urllib_request.HTTPSHandler(context=context, **kwargs)\n    else:  # Python < 3.4\n        context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n        context.verify_mode = (ssl.CERT_NONE\n                               if opts_no_check_certificate\n                               else ssl.CERT_REQUIRED)\n        context.set_default_verify_paths()\n        try:\n            context.load_default_certs()\n        except AttributeError:\n            pass  # Python < 3.4\n        return compat_urllib_request.HTTPSHandler(context=context, **kwargs)",
        "begin_line": 701,
        "end_line": 739,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.010101010101010102,
            "pseudo_dstar_susp": 0.0125,
            "pseudo_tarantula_susp": 0.001697792869269949,
            "pseudo_op2_susp": 0.0125,
            "pseudo_barinel_susp": 0.001697792869269949
        }
    },
    {
        "name": "youtube_dl.utils.ExtractorError.__init__#743",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ExtractorError",
        "signature": "youtube_dl.utils.ExtractorError.__init__(self, msg, tb=None, expected=False, cause=None, video_id=None)",
        "snippet": "    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in youtube-dl.\n        \"\"\"\n\n        if sys.exc_info()[0] in (compat_urllib_error.URLError, socket.timeout, UnavailableVideoError):\n            expected = True\n        if video_id is not None:\n            msg = video_id + ': ' + msg\n        if cause:\n            msg += u' (caused by %r)' % cause\n        if not expected:\n            msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'\n        super(ExtractorError, self).__init__(msg)\n\n        self.traceback = tb\n        self.exc_info = sys.exc_info()  # preserve original exception\n        self.cause = cause\n        self.video_id = video_id",
        "begin_line": 743,
        "end_line": 761,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00625,
            "pseudo_dstar_susp": 0.005076142131979695,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.005076142131979695,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.ExtractorError.format_traceback#763",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.ExtractorError",
        "signature": "youtube_dl.utils.ExtractorError.format_traceback(self)",
        "snippet": "    def format_traceback(self):\n        if self.traceback is None:\n            return None\n        return u''.join(traceback.format_tb(self.traceback))",
        "begin_line": 763,
        "end_line": 766,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004629629629629629,
            "pseudo_dstar_susp": 0.004608294930875576,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.004608294930875576,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.DownloadError.__init__#781",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DownloadError",
        "signature": "youtube_dl.utils.DownloadError.__init__(self, msg, exc_info=None)",
        "snippet": "    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super(DownloadError, self).__init__(msg)\n        self.exc_info = exc_info",
        "begin_line": 781,
        "end_line": 784,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.004807692307692308,
            "pseudo_dstar_susp": 0.004807692307692308,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.004807692307692308,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.addinfourl_wrapper#860",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.addinfourl_wrapper(stream, headers, url, code)",
        "snippet": "    def addinfourl_wrapper(stream, headers, url, code):\n        if hasattr(compat_urllib_request.addinfourl, 'getcode'):\n            return compat_urllib_request.addinfourl(stream, headers, url, code)\n        ret = compat_urllib_request.addinfourl(stream, headers, url)\n        ret.code = code\n        return ret",
        "begin_line": 860,
        "end_line": 865,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0038461538461538464,
            "pseudo_dstar_susp": 0.0038461538461538464,
            "pseudo_tarantula_susp": 0.002840909090909091,
            "pseudo_op2_susp": 0.0038461538461538464,
            "pseudo_barinel_susp": 0.002840909090909091
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.http_request#867",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.http_request(self, req)",
        "snippet": "    def http_request(self, req):\n        for h, v in std_headers.items():\n            if h not in req.headers:\n                req.add_header(h, v)\n        if 'Youtubedl-no-compression' in req.headers:\n            if 'Accept-encoding' in req.headers:\n                del req.headers['Accept-encoding']\n            del req.headers['Youtubedl-no-compression']\n        if 'Youtubedl-user-agent' in req.headers:\n            if 'User-agent' in req.headers:\n                del req.headers['User-agent']\n            req.headers['User-agent'] = req.headers['Youtubedl-user-agent']\n            del req.headers['Youtubedl-user-agent']\n\n        if sys.version_info < (2, 7) and '#' in req.get_full_url():\n            # Python 2.6 is brain-dead when it comes to fragments\n            req._Request__original = req._Request__original.partition('#')[0]\n            req._Request__r_type = req._Request__r_type.partition('#')[0]\n\n        return req",
        "begin_line": 867,
        "end_line": 886,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0196078431372549,
            "pseudo_dstar_susp": 0.00980392156862745,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.00980392156862745,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.YoutubeDLHandler.http_response#888",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.YoutubeDLHandler",
        "signature": "youtube_dl.utils.YoutubeDLHandler.http_response(self, req, resp)",
        "snippet": "    def http_response(self, req, resp):\n        old_resp = resp\n        # gzip\n        if resp.headers.get('Content-encoding', '') == 'gzip':\n            content = resp.read()\n            gz = gzip.GzipFile(fileobj=io.BytesIO(content), mode='rb')\n            try:\n                uncompressed = io.BytesIO(gz.read())\n            except IOError as original_ioerror:\n                # There may be junk add the end of the file\n                # See http://stackoverflow.com/q/4928560/35070 for details\n                for i in range(1, 1024):\n                    try:\n                        gz = gzip.GzipFile(fileobj=io.BytesIO(content[:-i]), mode='rb')\n                        uncompressed = io.BytesIO(gz.read())\n                    except IOError:\n                        continue\n                    break\n                else:\n                    raise original_ioerror\n            resp = self.addinfourl_wrapper(uncompressed, old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n        # deflate\n        if resp.headers.get('Content-encoding', '') == 'deflate':\n            gz = io.BytesIO(self.deflate(resp.read()))\n            resp = self.addinfourl_wrapper(gz, old_resp.headers, old_resp.url, old_resp.code)\n            resp.msg = old_resp.msg\n        return resp",
        "begin_line": 888,
        "end_line": 915,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.037037037037037035,
            "pseudo_dstar_susp": 0.006134969325153374,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.006134969325153374,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.parse_iso8601#921",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_iso8601(date_str, delimiter='T')",
        "snippet": "def parse_iso8601(date_str, delimiter='T'):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    m = re.search(\n        r'Z$| ?(?P<sign>\\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',\n        date_str)\n    if not m:\n        timezone = datetime.timedelta()\n    else:\n        date_str = date_str[:-len(m.group(0))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    date_format =  '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n    dt = datetime.datetime.strptime(date_str, date_format) - timezone\n    return calendar.timegm(dt.timetuple())",
        "begin_line": 921,
        "end_line": 943,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.0010141987829614604,
            "pseudo_dstar_susp": 0.0011261261261261261,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011261261261261261,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.unified_strdate#946",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unified_strdate(date_str)",
        "snippet": "def unified_strdate(date_str):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n\n    upload_date = None\n    #Replace commas\n    date_str = date_str.replace(',', ' ')\n    # %z (UTC offset) is only supported in python>=3.2\n    date_str = re.sub(r' ?(\\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)\n    format_expressions = [\n        '%d %B %Y',\n        '%d %b %Y',\n        '%B %d %Y',\n        '%b %d %Y',\n        '%b %dst %Y %I:%M%p',\n        '%b %dnd %Y %I:%M%p',\n        '%b %dth %Y %I:%M%p',\n        '%Y-%m-%d',\n        '%Y/%m/%d',\n        '%d.%m.%Y',\n        '%d/%m/%Y',\n        '%d/%m/%y',\n        '%Y/%m/%d %H:%M:%S',\n        '%d/%m/%Y %H:%M:%S',\n        '%Y-%m-%d %H:%M:%S',\n        '%Y-%m-%d %H:%M:%S.%f',\n        '%d.%m.%Y %H:%M',\n        '%d.%m.%Y %H.%M',\n        '%Y-%m-%dT%H:%M:%SZ',\n        '%Y-%m-%dT%H:%M:%S.%fZ',\n        '%Y-%m-%dT%H:%M:%S.%f0Z',\n        '%Y-%m-%dT%H:%M:%S',\n        '%Y-%m-%dT%H:%M:%S.%f',\n        '%Y-%m-%dT%H:%M',\n    ]\n    for expression in format_expressions:\n        try:\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n        except ValueError:\n            pass\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    return upload_date",
        "begin_line": 946,
        "end_line": 992,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012594458438287153,
            "pseudo_dstar_susp": 0.0012300123001230013,
            "pseudo_tarantula_susp": 0.001092896174863388,
            "pseudo_op2_susp": 0.0012300123001230013,
            "pseudo_barinel_susp": 0.001092896174863388
        }
    },
    {
        "name": "youtube_dl.utils.determine_ext#994",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.determine_ext(url, default_ext=u'unknown_video')",
        "snippet": "def determine_ext(url, default_ext=u'unknown_video'):\n    if url is None:\n        return default_ext\n    guess = url.partition(u'?')[0].rpartition(u'.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    else:\n        return default_ext",
        "begin_line": 994,
        "end_line": 1001,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0014005602240896359,
            "pseudo_dstar_susp": 0.0017094017094017094,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0017094017094017094,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.date_from_str#1006",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.date_from_str(date_str)",
        "snippet": "def date_from_str(date_str):\n    \"\"\"\n    Return a datetime object from a string in the format YYYYMMDD or\n    (now|today)[+-][0-9](day|week|month|year)(s)?\"\"\"\n    today = datetime.date.today()\n    if date_str == 'now'or date_str == 'today':\n        return today\n    match = re.match('(now|today)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>day|week|month|year)(s)?', date_str)\n    if match is not None:\n        sign = match.group('sign')\n        time = int(match.group('time'))\n        if sign == '-':\n            time = -time\n        unit = match.group('unit')\n        #A bad aproximation?\n        if unit == 'month':\n            unit = 'day'\n            time *= 30\n        elif unit == 'year':\n            unit = 'day'\n            time *= 365\n        unit += 's'\n        delta = datetime.timedelta(**{unit: time})\n        return today + delta\n    return datetime.datetime.strptime(date_str, \"%Y%m%d\").date()",
        "begin_line": 1006,
        "end_line": 1030,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010822510822510823,
            "pseudo_dstar_susp": 0.0012048192771084338,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0012048192771084338,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.DateRange.__init__#1043",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DateRange",
        "signature": "youtube_dl.utils.DateRange.__init__(self, start=None, end=None)",
        "snippet": "    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)",
        "begin_line": 1043,
        "end_line": 1054,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010822510822510823,
            "pseudo_dstar_susp": 0.0012048192771084338,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0012048192771084338,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.DateRange.__contains__#1059",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.DateRange",
        "signature": "youtube_dl.utils.DateRange.__contains__(self, date)",
        "snippet": "    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end",
        "begin_line": 1059,
        "end_line": 1063,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010822510822510823,
            "pseudo_dstar_susp": 0.0012048192771084338,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0012048192771084338,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.platform_name#1068",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.platform_name()",
        "snippet": "def platform_name():\n    \"\"\" Returns the platform name as a compat_str \"\"\"\n    res = platform.platform()\n    if isinstance(res, bytes):\n        res = res.decode(preferredencoding())\n\n    assert isinstance(res, compat_str)\n    return res",
        "begin_line": 1068,
        "end_line": 1075,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.007518796992481203,
            "pseudo_dstar_susp": 0.007142857142857143,
            "pseudo_tarantula_susp": 0.0018181818181818182,
            "pseudo_op2_susp": 0.007142857142857143,
            "pseudo_barinel_susp": 0.0018181818181818182
        }
    },
    {
        "name": "youtube_dl.utils.write_string#1150",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.write_string(s, out=None, encoding=None)",
        "snippet": "def write_string(s, out=None, encoding=None):\n    if out is None:\n        out = sys.stderr\n    assert type(s) == compat_str\n\n    if sys.platform == 'win32' and encoding is None and hasattr(out, 'fileno'):\n        if _windows_write_string(s, out):\n            return\n\n    if ('b' in getattr(out, 'mode', '') or\n            sys.version_info[0] < 3):  # Python 2 lies about mode of sys.stderr\n        byt = s.encode(encoding or preferredencoding(), 'ignore')\n        out.write(byt)\n    elif hasattr(out, 'buffer'):\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n        byt = s.encode(enc, 'ignore')\n        out.buffer.write(byt)\n    else:\n        out.write(s)\n    out.flush()",
        "begin_line": 1150,
        "end_line": 1169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.013157894736842105,
            "pseudo_dstar_susp": 0.015873015873015872,
            "pseudo_tarantula_susp": 0.001996007984031936,
            "pseudo_op2_susp": 0.015873015873015872,
            "pseudo_barinel_susp": 0.001996007984031936
        }
    },
    {
        "name": "youtube_dl.utils.get_filesystem_encoding#1286",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.get_filesystem_encoding()",
        "snippet": "def get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'",
        "begin_line": 1286,
        "end_line": 1288,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009285051067780873,
            "pseudo_dstar_susp": 0.0009000900090009,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009000900090009,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.shell_quote#1291",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.shell_quote(args)",
        "snippet": "def shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(pipes.quote(a))\n    return u' '.join(quoted_args)",
        "begin_line": 1291,
        "end_line": 1299,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.takewhile_inclusive#1302",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.takewhile_inclusive(pred, seq)",
        "snippet": "def takewhile_inclusive(pred, seq):\n    \"\"\" Like itertools.takewhile, but include the latest evaluated element\n        (the first element so that Not pred(e)) \"\"\"\n    for e in seq:\n        yield e\n        if not pred(e):\n            return",
        "begin_line": 1302,
        "end_line": 1308,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.smuggle_url#1311",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.smuggle_url(url, data)",
        "snippet": "def smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    sdata = compat_urllib_parse.urlencode(\n        {u'__youtubedl_smuggle': json.dumps(data)})\n    return url + u'#' + sdata",
        "begin_line": 1311,
        "end_line": 1316,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.unsmuggle_url#1319",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.unsmuggle_url(smug_url, default=None)",
        "snippet": "def unsmuggle_url(smug_url, default=None):\n    if not '#__youtubedl_smuggle' in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition(u'#')\n    jsond = compat_parse_qs(sdata)[u'__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data",
        "begin_line": 1319,
        "end_line": 1325,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003105590062111801,
            "pseudo_dstar_susp": 0.002890173410404624,
            "pseudo_tarantula_susp": 0.0024390243902439024,
            "pseudo_op2_susp": 0.002890173410404624,
            "pseudo_barinel_susp": 0.0024390243902439024
        }
    },
    {
        "name": "youtube_dl.utils.format_bytes#1328",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.format_bytes(bytes)",
        "snippet": "def format_bytes(bytes):\n    if bytes is None:\n        return u'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = [u'B', u'KiB', u'MiB', u'GiB', u'TiB', u'PiB', u'EiB', u'ZiB', u'YiB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return u'%.2f%s' % (converted, suffix)",
        "begin_line": 1328,
        "end_line": 1339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.00117096018735363,
            "pseudo_dstar_susp": 0.0012515644555694619,
            "pseudo_tarantula_susp": 0.0008216926869350862,
            "pseudo_op2_susp": 0.0012515644555694619,
            "pseudo_barinel_susp": 0.0008216926869350862
        }
    },
    {
        "name": "youtube_dl.utils.fix_xml_ampersands#1370",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fix_xml_ampersands(xml_str)",
        "snippet": "def fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        u'&amp;',\n        xml_str)",
        "begin_line": 1370,
        "end_line": 1375,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.remove_end#1399",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.remove_end(s, end)",
        "snippet": "def remove_end(s, end):\n    if s.endswith(end):\n        return s[:-len(end)]\n    return s",
        "begin_line": 1399,
        "end_line": 1402,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.url_basename#1405",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.url_basename(url)",
        "snippet": "def url_basename(url):\n    path = compat_urlparse.urlparse(url).path\n    return path.strip(u'/').split(u'/')[-1]",
        "begin_line": 1405,
        "end_line": 1407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003067484662576687,
            "pseudo_dstar_susp": 0.003125,
            "pseudo_tarantula_susp": 0.0011750881316098707,
            "pseudo_op2_susp": 0.003125,
            "pseudo_barinel_susp": 0.0011750881316098707
        }
    },
    {
        "name": "youtube_dl.utils.HEADRequest.get_method#1411",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.HEADRequest",
        "signature": "youtube_dl.utils.HEADRequest.get_method(self)",
        "snippet": "    def get_method(self):\n        return \"HEAD\"",
        "begin_line": 1411,
        "end_line": 1412,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002688172043010753,
            "pseudo_dstar_susp": 0.002564102564102564,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.002564102564102564,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.int_or_none#1415",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.int_or_none(v, scale=1, default=None, get_attr=None, invscale=1)",
        "snippet": "def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr:\n        if v is not None:\n            v = getattr(v, get_attr, None)\n    if v == '':\n        v = None\n    return default if v is None else (int(v) * invscale // scale)",
        "begin_line": 1415,
        "end_line": 1421,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012224938875305623,
            "pseudo_dstar_susp": 0.001226993865030675,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001226993865030675,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.str_or_none#1424",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.str_or_none(v, default=None)",
        "snippet": "def str_or_none(v, default=None):\n    return default if v is None else compat_str(v)",
        "begin_line": 1424,
        "end_line": 1425,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.str_to_int#1428",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.str_to_int(int_str)",
        "snippet": "def str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if int_str is None:\n        return None\n    int_str = re.sub(r'[,\\.\\+]', u'', int_str)\n    return int(int_str)",
        "begin_line": 1428,
        "end_line": 1433,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.float_or_none#1436",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.float_or_none(v, scale=1, invscale=1, default=None)",
        "snippet": "def float_or_none(v, scale=1, invscale=1, default=None):\n    return default if v is None else (float(v) * invscale / scale)",
        "begin_line": 1436,
        "end_line": 1437,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011198208286674132,
            "pseudo_dstar_susp": 0.0011025358324145535,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011025358324145535,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.parse_duration#1440",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_duration(s)",
        "snippet": "def parse_duration(s):\n    if s is None:\n        return None\n\n    s = s.strip()\n\n    m = re.match(\n        r'(?i)(?:(?:(?P<hours>[0-9]+)\\s*(?:[:h]|hours?)\\s*)?(?P<mins>[0-9]+)\\s*(?:[:m]|mins?|minutes?)\\s*)?(?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*(?:s|secs?|seconds?)?$', s)\n    if not m:\n        return None\n    res = int(m.group('secs'))\n    if m.group('mins'):\n        res += int(m.group('mins')) * 60\n        if m.group('hours'):\n            res += int(m.group('hours')) * 60 * 60\n    if m.group('ms'):\n        res += float(m.group('ms'))\n    return res",
        "begin_line": 1440,
        "end_line": 1457,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001072961373390558,
            "pseudo_dstar_susp": 0.0011876484560570072,
            "pseudo_tarantula_susp": 0.0011534025374855825,
            "pseudo_op2_susp": 0.0011876484560570072,
            "pseudo_barinel_susp": 0.0011534025374855825
        }
    },
    {
        "name": "youtube_dl.utils.check_executable#1465",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.check_executable(exe, args=[])",
        "snippet": "def check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        subprocess.Popen([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n    except OSError:\n        return False\n    return exe",
        "begin_line": 1465,
        "end_line": 1472,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.OnDemandPagedList.__init__#1482",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.OnDemandPagedList",
        "signature": "youtube_dl.utils.OnDemandPagedList.__init__(self, pagefunc, pagesize)",
        "snippet": "    def __init__(self, pagefunc, pagesize):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize",
        "begin_line": 1482,
        "end_line": 1484,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001006036217303823,
            "pseudo_dstar_susp": 0.0010224948875255625,
            "pseudo_tarantula_susp": 0.0011534025374855825,
            "pseudo_op2_susp": 0.0010224948875255625,
            "pseudo_barinel_susp": 0.0011534025374855825
        }
    },
    {
        "name": "youtube_dl.utils.OnDemandPagedList.getslice#1486",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.OnDemandPagedList",
        "signature": "youtube_dl.utils.OnDemandPagedList.getslice(self, start=0, end=None)",
        "snippet": "    def getslice(self, start=0, end=None):\n        res = []\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            page_results = list(self._pagefunc(pagenum))\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            res.extend(page_results)\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n        return res",
        "begin_line": 1486,
        "end_line": 1521,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001006036217303823,
            "pseudo_dstar_susp": 0.0010224948875255625,
            "pseudo_tarantula_susp": 0.0011534025374855825,
            "pseudo_op2_susp": 0.0010224948875255625,
            "pseudo_barinel_susp": 0.0011534025374855825
        }
    },
    {
        "name": "youtube_dl.utils.InAdvancePagedList.__init__#1525",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.InAdvancePagedList",
        "signature": "youtube_dl.utils.InAdvancePagedList.__init__(self, pagefunc, pagecount, pagesize)",
        "snippet": "    def __init__(self, pagefunc, pagecount, pagesize):\n        self._pagefunc = pagefunc\n        self._pagecount = pagecount\n        self._pagesize = pagesize",
        "begin_line": 1525,
        "end_line": 1528,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.InAdvancePagedList.getslice#1530",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.InAdvancePagedList",
        "signature": "youtube_dl.utils.InAdvancePagedList.getslice(self, start=0, end=None)",
        "snippet": "    def getslice(self, start=0, end=None):\n        res = []\n        start_page = start // self._pagesize\n        end_page = (\n            self._pagecount if end is None else (end // self._pagesize + 1))\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page = list(self._pagefunc(pagenum))\n            if skip_elems:\n                page = page[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page) < only_more:\n                    only_more -= len(page)\n                else:\n                    page = page[:only_more]\n                    res.extend(page)\n                    break\n            res.extend(page)\n        return res",
        "begin_line": 1530,
        "end_line": 1550,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.uppercase_escape#1553",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.uppercase_escape(s)",
        "snippet": "def uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)",
        "begin_line": 1553,
        "end_line": 1558,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.escape_rfc3986#1561",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.escape_rfc3986(s)",
        "snippet": "def escape_rfc3986(s):\n    \"\"\"Escape non-ASCII characters as suggested by RFC 3986\"\"\"\n    if sys.version_info < (3, 0) and isinstance(s, unicode):\n        s = s.encode('utf-8')\n    return compat_urllib_parse.quote(s, \"%/;:@&=+$,!~*'()?#[]\")",
        "begin_line": 1561,
        "end_line": 1565,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015625,
            "pseudo_dstar_susp": 0.008695652173913044,
            "pseudo_tarantula_susp": 0.0022624434389140274,
            "pseudo_op2_susp": 0.008695652173913044,
            "pseudo_barinel_susp": 0.0022624434389140274
        }
    },
    {
        "name": "youtube_dl.utils.escape_url#1568",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.escape_url(url)",
        "snippet": "def escape_url(url):\n    \"\"\"Escape URL as suggested by RFC 3986\"\"\"\n    url_parsed = compat_urllib_parse_urlparse(url)\n    return url_parsed._replace(\n        path=escape_rfc3986(url_parsed.path),\n        params=escape_rfc3986(url_parsed.params),\n        query=escape_rfc3986(url_parsed.query),\n        fragment=escape_rfc3986(url_parsed.fragment)\n    ).geturl()",
        "begin_line": 1568,
        "end_line": 1576,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.016666666666666666,
            "pseudo_dstar_susp": 0.009009009009009009,
            "pseudo_tarantula_susp": 0.00228310502283105,
            "pseudo_op2_susp": 0.009009009009009009,
            "pseudo_barinel_susp": 0.00228310502283105
        }
    },
    {
        "name": "youtube_dl.utils.read_batch_urls#1596",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.read_batch_urls(batch_fd)",
        "snippet": "def read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, compat_str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = u'\\xef\\xbb\\xbf'\n        if url.startswith(BOM_UTF8):\n            url = url[len(BOM_UTF8):]\n        url = url.strip()\n        if url.startswith(('#', ';', ']')):\n            return False\n        return url\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]",
        "begin_line": 1596,
        "end_line": 1609,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.fixup#1597",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fixup(url)",
        "snippet": "    def fixup(url):\n        if not isinstance(url, compat_str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = u'\\xef\\xbb\\xbf'\n        if url.startswith(BOM_UTF8):\n            url = url[len(BOM_UTF8):]\n        url = url.strip()\n        if url.startswith(('#', ';', ']')):\n            return False\n        return url",
        "begin_line": 1597,
        "end_line": 1606,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.utils.urlencode_postdata#1612",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.urlencode_postdata(*args, **kargs)",
        "snippet": "def urlencode_postdata(*args, **kargs):\n    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')",
        "begin_line": 1612,
        "end_line": 1613,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.parse_xml#1622",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.parse_xml(s)",
        "snippet": "def parse_xml(s):\n    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):\n        def doctype(self, name, pubid, system):\n            pass  # Ignore doctypes\n\n    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())\n    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}\n    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)\n    # Fix up XML parser in Python 2.x\n    if sys.version_info < (3, 0):\n        for n in etree_iter(tree):\n            if n.text is not None:\n                if not isinstance(n.text, compat_str):\n                    n.text = n.text.decode('utf-8')\n    return tree",
        "begin_line": 1622,
        "end_line": 1636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002207505518763797,
            "pseudo_dstar_susp": 0.0017574692442882249,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0017574692442882249,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.TreeBuilder.parse_xml#1622",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TreeBuilder",
        "signature": "youtube_dl.utils.TreeBuilder.parse_xml(s)",
        "snippet": "def parse_xml(s):\n    class TreeBuilder(xml.etree.ElementTree.TreeBuilder):\n        def doctype(self, name, pubid, system):\n            pass  # Ignore doctypes\n\n    parser = xml.etree.ElementTree.XMLParser(target=TreeBuilder())\n    kwargs = {'parser': parser} if sys.version_info >= (2, 7) else {}\n    tree = xml.etree.ElementTree.XML(s.encode('utf-8'), **kwargs)\n    # Fix up XML parser in Python 2.x\n    if sys.version_info < (3, 0):\n        for n in etree_iter(tree):\n            if n.text is not None:\n                if not isinstance(n.text, compat_str):\n                    n.text = n.text.decode('utf-8')\n    return tree",
        "begin_line": 1622,
        "end_line": 1636,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.TreeBuilder.doctype#1624",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils.TreeBuilder",
        "signature": "youtube_dl.utils.TreeBuilder.doctype(self, name, pubid, system)",
        "snippet": "        def doctype(self, name, pubid, system):\n            pass  # Ignore doctypes",
        "begin_line": 1624,
        "end_line": 1625,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0019723865877712033,
            "pseudo_dstar_susp": 0.001584786053882726,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001584786053882726,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.strip_jsonp#1664",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.strip_jsonp(code)",
        "snippet": "def strip_jsonp(code):\n    return re.sub(r'(?s)^[a-zA-Z0-9_]+\\s*\\(\\s*(.*)\\);?\\s*?\\s*$', r'\\1', code)",
        "begin_line": 1664,
        "end_line": 1665,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.js_to_json#1668",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.js_to_json(code)",
        "snippet": "def js_to_json(code):\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        if v.startswith('\"'):\n            return v\n        if v.startswith(\"'\"):\n            v = v[1:-1]\n            v = re.sub(r\"\\\\\\\\|\\\\'|\\\"\", lambda m: {\n                '\\\\\\\\': '\\\\\\\\',\n                \"\\\\'\": \"'\",\n                '\"': '\\\\\"',\n            }[m.group(0)], v)\n        return '\"%s\"' % v\n\n    res = re.sub(r'''(?x)\n        \"(?:[^\"\\\\]*(?:\\\\\\\\|\\\\\")?)*\"|\n        '(?:[^'\\\\]*(?:\\\\\\\\|\\\\')?)*'|\n        [a-zA-Z_][a-zA-Z_0-9]*\n        ''', fix_kv, code)\n    res = re.sub(r',(\\s*\\])', lambda m: m.group(1), res)\n    return res",
        "begin_line": 1668,
        "end_line": 1690,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.fix_kv#1669",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.fix_kv(m)",
        "snippet": "    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        if v.startswith('\"'):\n            return v\n        if v.startswith(\"'\"):\n            v = v[1:-1]\n            v = re.sub(r\"\\\\\\\\|\\\\'|\\\"\", lambda m: {\n                '\\\\\\\\': '\\\\\\\\',\n                \"\\\\'\": \"'\",\n                '\"': '\\\\\"',\n            }[m.group(0)], v)\n        return '\"%s\"' % v",
        "begin_line": 1669,
        "end_line": 1682,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.utils.qualities#1693",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.qualities(quality_ids)",
        "snippet": "def qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q",
        "begin_line": 1693,
        "end_line": 1700,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011467889908256881,
            "pseudo_dstar_susp": 0.0011337868480725624,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011337868480725624,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.q#1695",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.q(qid)",
        "snippet": "    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1",
        "begin_line": 1695,
        "end_line": 1699,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.000998003992015968,
            "pseudo_dstar_susp": 0.0010162601626016261,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010162601626016261,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.utils.limit_length#1718",
        "src_path": "youtube_dl/utils.py",
        "class_name": "youtube_dl.utils",
        "signature": "youtube_dl.utils.limit_length(s, length)",
        "snippet": "def limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s",
        "begin_line": 1718,
        "end_line": 1725,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache.__init__#18",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.__init__(self, ydl)",
        "snippet": "    def __init__(self, ydl):\n        self._ydl = ydl",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.014084507042253521,
            "pseudo_dstar_susp": 0.03225806451612903,
            "pseudo_tarantula_susp": 0.0013297872340425532,
            "pseudo_op2_susp": 0.03225806451612903,
            "pseudo_barinel_susp": 0.0013297872340425532
        }
    },
    {
        "name": "youtube_dl.cache.Cache._get_root_dir#21",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache._get_root_dir(self)",
        "snippet": "    def _get_root_dir(self):\n        res = self._ydl.params.get('cachedir')\n        if res is None:\n            cache_root = os.environ.get('XDG_CACHE_HOME', '~/.cache')\n            res = os.path.join(cache_root, 'youtube-dl')\n        return compat_expanduser(res)",
        "begin_line": 21,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache._get_cache_fn#28",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache._get_cache_fn(self, section, key, dtype)",
        "snippet": "    def _get_cache_fn(self, section, key, dtype):\n        assert re.match(r'^[a-zA-Z0-9_.-]+$', section), \\\n            'invalid section %r' % section\n        assert re.match(r'^[a-zA-Z0-9_.-]+$', key), 'invalid key %r' % key\n        return os.path.join(\n            self._get_root_dir(), section, '%s.%s' % (key, dtype))",
        "begin_line": 28,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache.enabled#36",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.enabled(self)",
        "snippet": "    def enabled(self):\n        return self._ydl.params.get('cachedir') is not False",
        "begin_line": 36,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache.store#39",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.store(self, section, key, data, dtype='json')",
        "snippet": "    def store(self, section, key, data, dtype='json'):\n        assert dtype in ('json',)\n\n        if not self.enabled:\n            return\n\n        fn = self._get_cache_fn(section, key, dtype)\n        try:\n            try:\n                os.makedirs(os.path.dirname(fn))\n            except OSError as ose:\n                if ose.errno != errno.EEXIST:\n                    raise\n            write_json_file(data, fn)\n        except Exception:\n            tb = traceback.format_exc()\n            self._ydl.report_warning(\n                'Writing cache to %r failed: %s' % (fn, tb))",
        "begin_line": 39,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache.load#58",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.load(self, section, key, dtype='json', default=None)",
        "snippet": "    def load(self, section, key, dtype='json', default=None):\n        assert dtype in ('json',)\n\n        if not self.enabled:\n            return default\n\n        cache_fn = self._get_cache_fn(section, key, dtype)\n        try:\n            try:\n                with io.open(cache_fn, 'r', encoding='utf-8') as cachef:\n                    return json.load(cachef)\n            except ValueError:\n                try:\n                    file_size = os.path.getsize(cache_fn)\n                except (OSError, IOError) as oe:\n                    file_size = str(oe)\n                self._ydl.report_warning(\n                    'Cache retrieval from %s failed (%s)' % (cache_fn, file_size))\n        except IOError:\n            pass  # No cache available\n\n        return default",
        "begin_line": 58,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.cache.Cache.remove#81",
        "src_path": "youtube_dl/cache.py",
        "class_name": "youtube_dl.cache.Cache",
        "signature": "youtube_dl.cache.Cache.remove(self)",
        "snippet": "    def remove(self):\n        if not self.enabled:\n            self._ydl.to_screen('Cache is disabled (Did you combine --no-cache-dir and --rm-cache-dir?)')\n            return\n\n        cachedir = self._get_root_dir()\n        if not any((term in cachedir) for term in ('cache', 'tmp')):\n            raise Exception('Not removing directory %s - this does not look like a cache dir' % cachedir)\n\n        self._ydl.to_screen(\n            'Removing cache dir %s .' % cachedir, skip_eol=True)\n        if os.path.exists(cachedir):\n            self._ydl.to_screen('.', skip_eol=True)\n            shutil.rmtree(cachedir)\n        self._ydl.to_screen('.')",
        "begin_line": 81,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.extractor.ard.ARDIE._real_extract#156",
        "src_path": "youtube_dl/extractor/ard.py",
        "class_name": "youtube_dl.extractor.ard.ARDIE",
        "signature": "youtube_dl.extractor.ard.ARDIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        display_id = mobj.group('display_id')\n\n        player_url = mobj.group('mainurl') + '~playerXml.xml'\n        doc = self._download_xml(player_url, display_id)\n        video_node = doc.find('./video')\n        upload_date = unified_strdate(xpath_text(\n            video_node, './broadcastDate'))\n        thumbnail = xpath_text(video_node, './/teaserImage//variant/url')\n\n        formats = []\n        for a in video_node.findall('.//asset'):\n            f = {\n                'format_id': a.attrib['type'],\n                'width': int_or_none(a.find('./frameWidth').text),\n                'height': int_or_none(a.find('./frameHeight').text),\n                'vbr': int_or_none(a.find('./bitrateVideo').text),\n                'abr': int_or_none(a.find('./bitrateAudio').text),\n                'vcodec': a.find('./codecVideo').text,\n                'tbr': int_or_none(a.find('./totalBitrate').text),\n            }\n            if a.find('./serverPrefix').text:\n                f['url'] = a.find('./serverPrefix').text\n                f['playpath'] = a.find('./fileName').text\n            else:\n                f['url'] = a.find('./fileName').text\n            formats.append(f)\n        self._sort_formats(formats)\n\n        return {\n            'id': mobj.group('id'),\n            'formats': formats,\n            'display_id': display_id,\n            'title': video_node.find('./title').text,\n            'duration': parse_duration(video_node.find('./duration').text),\n            'upload_date': upload_date,\n            'thumbnail': thumbnail,\n        }",
        "begin_line": 156,
        "end_line": 194,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001976284584980237,
            "pseudo_dstar_susp": 0.0022935779816513763,
            "pseudo_tarantula_susp": 0.0014064697609001407,
            "pseudo_op2_susp": 0.0022935779816513763,
            "pseudo_barinel_susp": 0.0014044943820224719
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudIE._real_extract#207",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url, flags=re.VERBOSE)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n\n        track_id = mobj.group('track_id')\n        token = None\n        if track_id is not None:\n            info_json_url = 'http://api.soundcloud.com/tracks/' + track_id + '.json?client_id=' + self._CLIENT_ID\n            full_title = track_id\n            token = mobj.group('secret_token')\n            if token:\n                info_json_url += \"&secret_token=\" + token\n        elif mobj.group('player'):\n            query = compat_urlparse.parse_qs(compat_urlparse.urlparse(url).query)\n            return self.url_result(query['url'][0])\n        else:\n            # extract uploader (which is in the url)\n            uploader = mobj.group('uploader')\n            # extract simple title (uploader + slug of song title)\n            slug_title =  mobj.group('title')\n            token = mobj.group('token')\n            full_title = resolve_title = '%s/%s' % (uploader, slug_title)\n            if token:\n                resolve_title += '/%s' % token\n    \n            self.report_resolve(full_title)\n    \n            url = 'http://soundcloud.com/%s' % resolve_title\n            info_json_url = self._resolv_url(url)\n        info = self._download_json(info_json_url, full_title, 'Downloading info JSON')\n\n        return self._extract_info_dict(info, full_title, secret_token=token)",
        "begin_line": 207,
        "end_line": 239,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012195121951219512,
            "pseudo_dstar_susp": 0.0011350737797956867,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011350737797956867,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudSetIE._real_extract#253",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudSetIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudSetIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n\n        # extract uploader (which is in the url)\n        uploader = mobj.group('uploader')\n        # extract simple title (uploader + slug of song title)\n        slug_title = mobj.group('slug_title')\n        full_title = '%s/sets/%s' % (uploader, slug_title)\n        url = 'http://soundcloud.com/%s/sets/%s' % (uploader, slug_title)\n\n        token = mobj.group('token')\n        if token:\n            full_title += '/' + token\n            url += '/' + token\n\n        self.report_resolve(full_title)\n\n        resolv_url = self._resolv_url(url)\n        info = self._download_json(resolv_url, full_title)\n\n        if 'errors' in info:\n            for err in info['errors']:\n                self._downloader.report_error('unable to download video webpage: %s' % compat_str(err['error_message']))\n            return\n\n        return {\n            '_type': 'playlist',\n            'entries': [self._extract_info_dict(track, secret_token=token) for track in info['tracks']],\n            'id': info['id'],\n            'title': info['title'],\n        }",
        "begin_line": 253,
        "end_line": 283,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudUserIE._real_extract#305",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudUserIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudUserIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        uploader = mobj.group('user')\n        resource = mobj.group('rsrc')\n        if resource is None:\n            resource = 'tracks'\n        elif resource == 'likes':\n            resource = 'favorites'\n\n        url = 'http://soundcloud.com/%s/' % uploader\n        resolv_url = self._resolv_url(url)\n        user = self._download_json(\n            resolv_url, uploader, 'Downloading user info')\n        base_url = 'http://api.soundcloud.com/users/%s/%s.json?' % (uploader, resource)\n\n        entries = []\n        for i in itertools.count():\n            data = compat_urllib_parse.urlencode({\n                'offset': i * 50,\n                'limit': 50,\n                'client_id': self._CLIENT_ID,\n            })\n            new_entries = self._download_json(\n                base_url + data, uploader, 'Downloading track page %s' % (i + 1))\n            if len(new_entries) == 0:\n                self.to_screen('%s: End page received' % uploader)\n                break\n            entries.extend(self._extract_info_dict(e, quiet=True) for e in new_entries)\n\n        return {\n            '_type': 'playlist',\n            'id': compat_str(user['id']),\n            'title': user['username'],\n            'entries': entries,\n        }",
        "begin_line": 305,
        "end_line": 339,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011534025374855825,
            "pseudo_dstar_susp": 0.0010905125408942203,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010905125408942203,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE._real_extract#355",
        "src_path": "youtube_dl/extractor/soundcloud.py",
        "class_name": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE",
        "signature": "youtube_dl.extractor.soundcloud.SoundcloudPlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        playlist_id = mobj.group('id')\n        base_url = '%s//api.soundcloud.com/playlists/%s.json?' % (self.http_scheme(), playlist_id)\n\n        data_dict = {\n            'client_id': self._CLIENT_ID,\n        }\n        token = mobj.group('token')\n\n        if token:\n            data_dict['secret_token'] = token\n\n        data = compat_urllib_parse.urlencode(data_dict)\n        data = self._download_json(\n            base_url + data, playlist_id, 'Downloading playlist')\n\n        entries = [\n            self._extract_info_dict(t, quiet=True, secret_token=token)\n                for t in data['tracks']]\n\n        return {\n            '_type': 'playlist',\n            'id': playlist_id,\n            'title': data.get('title'),\n            'description': data.get('description'),\n            'entries': entries,\n        }",
        "begin_line": 355,
        "end_line": 382,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE._real_extract#307",
        "src_path": "youtube_dl/extractor/crunchyroll.py",
        "class_name": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE",
        "signature": "youtube_dl.extractor.crunchyroll.CrunchyrollShowPlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        show_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, show_id)\n        title = self._html_search_regex(\n            r'(?s)<h1[^>]*>\\s*<span itemprop=\"name\">(.*?)</span>',\n            webpage, 'title')\n        episode_paths = re.findall(\n            r'(?s)<li id=\"showview_videos_media_[0-9]+\"[^>]+>.*?<a href=\"([^\"]+)\"',\n            webpage)\n        entries = [\n            self.url_result('http://www.crunchyroll.com' + ep, 'Crunchyroll')\n            for ep in episode_paths\n        ]\n        entries.reverse()\n\n        return {\n            '_type': 'playlist',\n            'id': show_id,\n            'title': title,\n            'entries': entries,\n        }",
        "begin_line": 307,
        "end_line": 328,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._login#52",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._login(self)",
        "snippet": "    def _login(self):\n        \"\"\"\n        Attempt to log in to YouTube.\n        True is returned if successful or skipped.\n        False is returned if login failed.\n\n        If _LOGIN_REQUIRED is set and no authentication was provided, an error is raised.\n        \"\"\"\n        (username, password) = self._get_login_info()\n        # No authentication to be performed\n        if username is None:\n            if self._LOGIN_REQUIRED:\n                raise ExtractorError('No login info available, needed for using %s.' % self.IE_NAME, expected=True)\n            return True\n\n        login_page = self._download_webpage(\n            self._LOGIN_URL, None,\n            note='Downloading login page',\n            errnote='unable to fetch login page', fatal=False)\n        if login_page is False:\n            return\n\n        galx = self._search_regex(r'(?s)<input.+?name=\"GALX\".+?value=\"(.+?)\"',\n                                  login_page, 'Login GALX parameter')\n\n        # Log in\n        login_form_strs = {\n                'continue': 'https://www.youtube.com/signin?action_handle_signin=true&feature=sign_in_button&hl=en_US&nomobiletemp=1',\n                'Email': username,\n                'GALX': galx,\n                'Passwd': password,\n\n                'PersistentCookie': 'yes',\n                '_utf8': '\u9731',\n                'bgresponse': 'js_disabled',\n                'checkConnection': '',\n                'checkedDomains': 'youtube',\n                'dnConn': '',\n                'pstMsg': '0',\n                'rmShown': '1',\n                'secTok': '',\n                'signIn': 'Sign in',\n                'timeStmp': '',\n                'service': 'youtube',\n                'uilel': '3',\n                'hl': 'en_US',\n        }\n\n        # Convert to UTF-8 *before* urlencode because Python 2.x's urlencode\n        # chokes on unicode\n        login_form = dict((k.encode('utf-8'), v.encode('utf-8')) for k,v in login_form_strs.items())\n        login_data = compat_urllib_parse.urlencode(login_form).encode('ascii')\n\n        req = compat_urllib_request.Request(self._LOGIN_URL, login_data)\n        login_results = self._download_webpage(\n            req, None,\n            note='Logging in', errnote='unable to log in', fatal=False)\n        if login_results is False:\n            return False\n\n        if re.search(r'id=\"errormsg_0_Passwd\"', login_results) is not None:\n            raise ExtractorError('Please use your account password and a two-factor code instead of an application-specific password.', expected=True)\n\n        # Two-Factor\n        # TODO add SMS and phone call support - these require making a request and then prompting the user\n\n        if re.search(r'(?i)<form[^>]* id=\"gaia_secondfactorform\"', login_results) is not None:\n            tfa_code = self._get_tfa_info()\n\n            if tfa_code is None:\n                self._downloader.report_warning('Two-factor authentication required. Provide it with --twofactor <code>')\n                self._downloader.report_warning('(Note that only TOTP (Google Authenticator App) codes work at this time.)')\n                return False\n\n            # Unlike the first login form, secTok and timeStmp are both required for the TFA form\n\n            match = re.search(r'id=\"secTok\"\\n\\s+value=\\'(.+)\\'/>', login_results, re.M | re.U)\n            if match is None:\n                self._downloader.report_warning('Failed to get secTok - did the page structure change?')\n            secTok = match.group(1)\n            match = re.search(r'id=\"timeStmp\"\\n\\s+value=\\'(.+)\\'/>', login_results, re.M | re.U)\n            if match is None:\n                self._downloader.report_warning('Failed to get timeStmp - did the page structure change?')\n            timeStmp = match.group(1)\n\n            tfa_form_strs = {\n                'continue': 'https://www.youtube.com/signin?action_handle_signin=true&feature=sign_in_button&hl=en_US&nomobiletemp=1',\n                'smsToken': '',\n                'smsUserPin': tfa_code,\n                'smsVerifyPin': 'Verify',\n\n                'PersistentCookie': 'yes',\n                'checkConnection': '',\n                'checkedDomains': 'youtube',\n                'pstMsg': '1',\n                'secTok': secTok,\n                'timeStmp': timeStmp,\n                'service': 'youtube',\n                'hl': 'en_US',\n            }\n            tfa_form = dict((k.encode('utf-8'), v.encode('utf-8')) for k,v in tfa_form_strs.items())\n            tfa_data = compat_urllib_parse.urlencode(tfa_form).encode('ascii')\n\n            tfa_req = compat_urllib_request.Request(self._TWOFACTOR_URL, tfa_data)\n            tfa_results = self._download_webpage(\n                tfa_req, None,\n                note='Submitting TFA code', errnote='unable to submit tfa', fatal=False)\n\n            if tfa_results is False:\n                return False\n\n            if re.search(r'(?i)<form[^>]* id=\"gaia_secondfactorform\"', tfa_results) is not None:\n                self._downloader.report_warning('Two-factor code expired. Please try again, or use a one-use backup code instead.')\n                return False\n            if re.search(r'(?i)<form[^>]* id=\"gaia_loginform\"', tfa_results) is not None:\n                self._downloader.report_warning('unable to log in - did the page structure change?')\n                return False\n            if re.search(r'smsauth-interstitial-reviewsettings', tfa_results) is not None:\n                self._downloader.report_warning('Your Google account has a security notice. Please log in on your web browser, resolve the notice, and try again.')\n                return False\n\n        if re.search(r'(?i)<form[^>]* id=\"gaia_loginform\"', login_results) is not None:\n            self._downloader.report_warning('unable to log in: bad username or password')\n            return False\n        return True",
        "begin_line": 52,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.003215434083601286,
            "pseudo_dstar_susp": 0.003105590062111801,
            "pseudo_tarantula_susp": 0.003205128205128205,
            "pseudo_op2_susp": 0.003105590062111801,
            "pseudo_barinel_susp": 0.003205128205128205
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._confirm_age#178",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._confirm_age(self)",
        "snippet": "    def _confirm_age(self):\n        age_form = {\n            'next_url': '/',\n            'action_confirm': 'Confirm',\n        }\n        req = compat_urllib_request.Request(self._AGE_URL,\n            compat_urllib_parse.urlencode(age_form).encode('ascii'))\n\n        self._download_webpage(\n            req, None,\n            note='Confirming age', errnote='Unable to confirm age',\n            fatal=False)",
        "begin_line": 178,
        "end_line": 189,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002840909090909091,
            "pseudo_dstar_susp": 0.0026041666666666665,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0026041666666666665,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._real_initialize#191",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeBaseInfoExtractor._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        if self._downloader is None:\n            return\n        if self._get_login_info()[0] is not None:\n            if not self._set_language():\n                return\n        if not self._login():\n            return\n        self._confirm_age()",
        "begin_line": 191,
        "end_line": 199,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002824858757062147,
            "pseudo_dstar_susp": 0.0025974025974025974,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0025974025974025974,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.__init__#401",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(YoutubeIE, self).__init__(*args, **kwargs)\n        self._player_cache = {}",
        "begin_line": 401,
        "end_line": 403,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.006211180124223602,
            "pseudo_dstar_susp": 0.010526315789473684,
            "pseudo_tarantula_susp": 0.0014388489208633094,
            "pseudo_op2_susp": 0.010526315789473684,
            "pseudo_barinel_susp": 0.0014388489208633094
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.report_video_info_webpage_download#405",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.report_video_info_webpage_download(self, video_id)",
        "snippet": "    def report_video_info_webpage_download(self, video_id):\n        \"\"\"Report attempt to download video info webpage.\"\"\"\n        self.to_screen('%s: Downloading video info webpage' % video_id)",
        "begin_line": 405,
        "end_line": 407,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024509803921568627,
            "pseudo_dstar_susp": 0.0018115942028985507,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0018115942028985507,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._parse_sig_js#506",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._parse_sig_js(self, jscode)",
        "snippet": "    def _parse_sig_js(self, jscode):\n        funcname = self._search_regex(\n            r'signature=([$a-zA-Z]+)', jscode,\n             'Initial JS player signature function name')\n\n        jsi = JSInterpreter(jscode)\n        initial_function = jsi.extract_function(funcname)\n        return lambda s: initial_function([s])",
        "begin_line": 506,
        "end_line": 513,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0007112375533428165,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE.extract_id#622",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE.extract_id(cls, url)",
        "snippet": "    def extract_id(cls, url):\n        mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n        video_id = mobj.group(2)\n        return video_id",
        "begin_line": 622,
        "end_line": 627,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013812154696132596,
            "pseudo_dstar_susp": 0.0012738853503184713,
            "pseudo_tarantula_susp": 0.0024937655860349127,
            "pseudo_op2_susp": 0.0012738853503184713,
            "pseudo_barinel_susp": 0.0024813895781637717
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeIE._real_extract#647",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        proto = (\n            'http' if self._downloader.params.get('prefer_insecure', False)\n            else 'https')\n\n        # Extract original video URL from URL with redirection, like age verification, using next_url parameter\n        mobj = re.search(self._NEXT_URL_RE, url)\n        if mobj:\n            url = proto + '://www.youtube.com/' + compat_urllib_parse.unquote(mobj.group(1)).lstrip('/')\n        video_id = self.extract_id(url)\n\n        # Get video webpage\n        url = proto + '://www.youtube.com/watch?v=%s&gl=US&hl=en&has_verified=1' % video_id\n        pref_cookies = [\n            c for c in self._downloader.cookiejar\n            if c.domain == '.youtube.com' and c.name == 'PREF']\n        for pc in pref_cookies:\n            if 'hl=' in pc.value:\n                pc.value = re.sub(r'hl=[^&]+', 'hl=en', pc.value)\n            else:\n                if pc.value:\n                    pc.value += '&'\n                pc.value += 'hl=en'\n        video_webpage = self._download_webpage(url, video_id)\n\n        # Attempt to extract SWF player URL\n        mobj = re.search(r'swfConfig.*?\"(https?:\\\\/\\\\/.*?watch.*?-.*?\\.swf)\"', video_webpage)\n        if mobj is not None:\n            player_url = re.sub(r'\\\\(.)', r'\\1', mobj.group(1))\n        else:\n            player_url = None\n\n        # Get video info\n        self.report_video_info_webpage_download(video_id)\n        if re.search(r'player-age-gate-content\">', video_webpage) is not None:\n            self.report_age_confirmation()\n            age_gate = True\n            # We simulate the access to the video from www.youtube.com/v/{video_id}\n            # this can be viewed without login into Youtube\n            data = compat_urllib_parse.urlencode({\n                'video_id': video_id,\n                'eurl': 'https://youtube.googleapis.com/v/' + video_id,\n                'sts': self._search_regex(\n                    r'\"sts\"\\s*:\\s*(\\d+)', video_webpage, 'sts'),\n            })\n            video_info_url = proto + '://www.youtube.com/get_video_info?' + data\n            video_info_webpage = self._download_webpage(video_info_url, video_id,\n                                    note=False,\n                                    errnote='unable to download video info webpage')\n            video_info = compat_parse_qs(video_info_webpage)\n        else:\n            age_gate = False\n            for el_type in ['&el=embedded', '&el=detailpage', '&el=vevo', '']:\n                video_info_url = (proto + '://www.youtube.com/get_video_info?&video_id=%s%s&ps=default&eurl=&gl=US&hl=en'\n                        % (video_id, el_type))\n                video_info_webpage = self._download_webpage(video_info_url, video_id,\n                                        note=False,\n                                        errnote='unable to download video info webpage')\n                video_info = compat_parse_qs(video_info_webpage)\n                if 'token' in video_info:\n                    break\n        if 'token' not in video_info:\n            if 'reason' in video_info:\n                raise ExtractorError(\n                    'YouTube said: %s' % video_info['reason'][0],\n                    expected=True, video_id=video_id)\n            else:\n                raise ExtractorError(\n                    '\"token\" parameter not in video info for unknown reason',\n                    video_id=video_id)\n\n        if 'view_count' in video_info:\n            view_count = int(video_info['view_count'][0])\n        else:\n            view_count = None\n\n        # Check for \"rental\" videos\n        if 'ypc_video_rental_bar_text' in video_info and 'author' not in video_info:\n            raise ExtractorError('\"rental\" videos not supported')\n\n        # Start extracting information\n        self.report_information_extraction(video_id)\n\n        # uploader\n        if 'author' not in video_info:\n            raise ExtractorError('Unable to extract uploader name')\n        video_uploader = compat_urllib_parse.unquote_plus(video_info['author'][0])\n\n        # uploader_id\n        video_uploader_id = None\n        mobj = re.search(r'<link itemprop=\"url\" href=\"http://www.youtube.com/(?:user|channel)/([^\"]+)\">', video_webpage)\n        if mobj is not None:\n            video_uploader_id = mobj.group(1)\n        else:\n            self._downloader.report_warning('unable to extract uploader nickname')\n\n        # title\n        if 'title' in video_info:\n            video_title = video_info['title'][0]\n        else:\n            self._downloader.report_warning('Unable to extract video title')\n            video_title = '_'\n\n        # thumbnail image\n        # We try first to get a high quality image:\n        m_thumb = re.search(r'<span itemprop=\"thumbnail\".*?href=\"(.*?)\">',\n                            video_webpage, re.DOTALL)\n        if m_thumb is not None:\n            video_thumbnail = m_thumb.group(1)\n        elif 'thumbnail_url' not in video_info:\n            self._downloader.report_warning('unable to extract video thumbnail')\n            video_thumbnail = None\n        else:   # don't panic if we can't find it\n            video_thumbnail = compat_urllib_parse.unquote_plus(video_info['thumbnail_url'][0])\n\n        # upload date\n        upload_date = None\n        mobj = re.search(r'(?s)id=\"eow-date.*?>(.*?)</span>', video_webpage)\n        if mobj is None:\n            mobj = re.search(\n                r'(?s)id=\"watch-uploader-info\".*?>.*?(?:Published|Uploaded|Streamed live) on (.*?)</strong>',\n                video_webpage)\n        if mobj is not None:\n            upload_date = ' '.join(re.sub(r'[/,-]', r' ', mobj.group(1)).split())\n            upload_date = unified_strdate(upload_date)\n\n        m_cat_container = self._search_regex(\n            r'(?s)<h4[^>]*>\\s*Category\\s*</h4>\\s*<ul[^>]*>(.*?)</ul>',\n            video_webpage, 'categories', fatal=False)\n        if m_cat_container:\n            category = self._html_search_regex(\n                r'(?s)<a[^<]+>(.*?)</a>', m_cat_container, 'category',\n                default=None)\n            video_categories = None if category is None else [category]\n        else:\n            video_categories = None\n\n        # description\n        video_description = get_element_by_id(\"eow-description\", video_webpage)\n        if video_description:\n            video_description = re.sub(r'''(?x)\n                <a\\s+\n                    (?:[a-zA-Z-]+=\"[^\"]+\"\\s+)*?\n                    title=\"([^\"]+)\"\\s+\n                    (?:[a-zA-Z-]+=\"[^\"]+\"\\s+)*?\n                    class=\"yt-uix-redirect-link\"\\s*>\n                [^<]+\n                </a>\n            ''', r'\\1', video_description)\n            video_description = clean_html(video_description)\n        else:\n            fd_mobj = re.search(r'<meta name=\"description\" content=\"([^\"]+)\"', video_webpage)\n            if fd_mobj:\n                video_description = unescapeHTML(fd_mobj.group(1))\n            else:\n                video_description = ''\n\n        def _extract_count(count_name):\n            count = self._search_regex(\n                r'id=\"watch-%s\"[^>]*>.*?([\\d,]+)\\s*</span>' % re.escape(count_name),\n                video_webpage, count_name, default=None)\n            if count is not None:\n                return int(count.replace(',', ''))\n            return None\n        like_count = _extract_count('like')\n        dislike_count = _extract_count('dislike')\n\n        # subtitles\n        video_subtitles = self.extract_subtitles(video_id, video_webpage)\n\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, video_webpage)\n            return\n\n        if 'length_seconds' not in video_info:\n            self._downloader.report_warning('unable to extract video duration')\n            video_duration = None\n        else:\n            video_duration = int(compat_urllib_parse.unquote_plus(video_info['length_seconds'][0]))\n\n        # annotations\n        video_annotations = None\n        if self._downloader.params.get('writeannotations', False):\n                video_annotations = self._extract_annotations(video_id)\n\n        # Decide which formats to download\n        try:\n            mobj = re.search(r';ytplayer\\.config\\s*=\\s*({.*?});', video_webpage)\n            if not mobj:\n                raise ValueError('Could not find vevo ID')\n            json_code = uppercase_escape(mobj.group(1))\n            ytplayer_config = json.loads(json_code)\n            args = ytplayer_config['args']\n            # Easy way to know if the 's' value is in url_encoded_fmt_stream_map\n            # this signatures are encrypted\n            if 'url_encoded_fmt_stream_map' not in args:\n                raise ValueError('No stream_map present')  # caught below\n            re_signature = re.compile(r'[&,]s=')\n            m_s = re_signature.search(args['url_encoded_fmt_stream_map'])\n            if m_s is not None:\n                self.to_screen('%s: Encrypted signatures detected.' % video_id)\n                video_info['url_encoded_fmt_stream_map'] = [args['url_encoded_fmt_stream_map']]\n            m_s = re_signature.search(args.get('adaptive_fmts', ''))\n            if m_s is not None:\n                if 'adaptive_fmts' in video_info:\n                    video_info['adaptive_fmts'][0] += ',' + args['adaptive_fmts']\n                else:\n                    video_info['adaptive_fmts'] = [args['adaptive_fmts']]\n        except ValueError:\n            pass\n\n        def _map_to_format_list(urlmap):\n            formats = []\n            for itag, video_real_url in urlmap.items():\n                dct = {\n                    'format_id': itag,\n                    'url': video_real_url,\n                    'player_url': player_url,\n                }\n                if itag in self._formats:\n                    dct.update(self._formats[itag])\n                formats.append(dct)\n            return formats\n\n        if 'conn' in video_info and video_info['conn'][0].startswith('rtmp'):\n            self.report_rtmp_download()\n            formats = [{\n                'format_id': '_rtmp',\n                'protocol': 'rtmp',\n                'url': video_info['conn'][0],\n                'player_url': player_url,\n            }]\n        elif len(video_info.get('url_encoded_fmt_stream_map', [])) >= 1 or len(video_info.get('adaptive_fmts', [])) >= 1:\n            encoded_url_map = video_info.get('url_encoded_fmt_stream_map', [''])[0] + ',' + video_info.get('adaptive_fmts',[''])[0]\n            if 'rtmpe%3Dyes' in encoded_url_map:\n                raise ExtractorError('rtmpe downloads are not supported, see https://github.com/rg3/youtube-dl/issues/343 for more information.', expected=True)\n            url_map = {}\n            for url_data_str in encoded_url_map.split(','):\n                url_data = compat_parse_qs(url_data_str)\n                if 'itag' not in url_data or 'url' not in url_data:\n                    continue\n                format_id = url_data['itag'][0]\n                url = url_data['url'][0]\n\n                if 'sig' in url_data:\n                    url += '&signature=' + url_data['sig'][0]\n                elif 's' in url_data:\n                    encrypted_sig = url_data['s'][0]\n\n                    if not age_gate:\n                        jsplayer_url_json = self._search_regex(\n                            r'\"assets\":.+?\"js\":\\s*(\"[^\"]+\")',\n                            video_webpage, 'JS player URL')\n                        player_url = json.loads(jsplayer_url_json)\n                    if player_url is None:\n                        player_url_json = self._search_regex(\n                            r'ytplayer\\.config.*?\"url\"\\s*:\\s*(\"[^\"]+\")',\n                            video_webpage, 'age gate player URL')\n                        player_url = json.loads(player_url_json)\n\n                    if self._downloader.params.get('verbose'):\n                        if player_url is None:\n                            player_version = 'unknown'\n                            player_desc = 'unknown'\n                        else:\n                            if player_url.endswith('swf'):\n                                player_version = self._search_regex(\n                                    r'-(.+?)(?:/watch_as3)?\\.swf$', player_url,\n                                    'flash player', fatal=False)\n                                player_desc = 'flash player %s' % player_version\n                            else:\n                                player_version = self._search_regex(\n                                    r'html5player-([^/]+?)(?:/html5player)?\\.js',\n                                    player_url,\n                                    'html5 player', fatal=False)\n                                player_desc = 'html5 player %s' % player_version\n\n                        parts_sizes = self._signature_cache_id(encrypted_sig)\n                        self.to_screen('{%s} signature length %s, %s' %\n                            (format_id, parts_sizes, player_desc))\n\n                    signature = self._decrypt_signature(\n                        encrypted_sig, video_id, player_url, age_gate)\n                    url += '&signature=' + signature\n                if 'ratebypass' not in url:\n                    url += '&ratebypass=yes'\n                url_map[format_id] = url\n            formats = _map_to_format_list(url_map)\n        elif video_info.get('hlsvp'):\n            manifest_url = video_info['hlsvp'][0]\n            url_map = self._extract_from_m3u8(manifest_url, video_id)\n            formats = _map_to_format_list(url_map)\n        else:\n            raise ExtractorError('no conn, hlsvp or url_encoded_fmt_stream_map information found in video info')\n\n        # Look for the DASH manifest\n        if self._downloader.params.get('youtube_include_dash_manifest', True):\n            try:\n                # The DASH manifest used needs to be the one from the original video_webpage.\n                # The one found in get_video_info seems to be using different signatures.\n                # However, in the case of an age restriction there won't be any embedded dashmpd in the video_webpage.\n                # Luckily, it seems, this case uses some kind of default signature (len == 86), so the\n                # combination of get_video_info and the _static_decrypt_signature() decryption fallback will work here.\n                if age_gate:\n                    dash_manifest_url = video_info.get('dashmpd')[0]\n                else:\n                    dash_manifest_url = ytplayer_config['args']['dashmpd']\n                def decrypt_sig(mobj):\n                    s = mobj.group(1)\n                    dec_s = self._decrypt_signature(s, video_id, player_url, age_gate)\n                    return '/signature/%s' % dec_s\n                dash_manifest_url = re.sub(r'/s/([\\w\\.]+)', decrypt_sig, dash_manifest_url)\n                dash_doc = self._download_xml(\n                    dash_manifest_url, video_id,\n                    note='Downloading DASH manifest',\n                    errnote='Could not download DASH manifest')\n                for r in dash_doc.findall('.//{urn:mpeg:DASH:schema:MPD:2011}Representation'):\n                    url_el = r.find('{urn:mpeg:DASH:schema:MPD:2011}BaseURL')\n                    if url_el is None:\n                        continue\n                    format_id = r.attrib['id']\n                    video_url = url_el.text\n                    filesize = int_or_none(url_el.attrib.get('{http://youtube.com/yt/2012/10/10}contentLength'))\n                    f = {\n                        'format_id': format_id,\n                        'url': video_url,\n                        'width': int_or_none(r.attrib.get('width')),\n                        'tbr': int_or_none(r.attrib.get('bandwidth'), 1000),\n                        'asr': int_or_none(r.attrib.get('audioSamplingRate')),\n                        'filesize': filesize,\n                    }\n                    try:\n                        existing_format = next(\n                            fo for fo in formats\n                            if fo['format_id'] == format_id)\n                    except StopIteration:\n                        f.update(self._formats.get(format_id, {}))\n                        formats.append(f)\n                    else:\n                        existing_format.update(f)\n\n            except (ExtractorError, KeyError) as e:\n                self.report_warning('Skipping DASH manifest: %s' % e, video_id)\n\n        self._sort_formats(formats)\n\n        return {\n            'id':           video_id,\n            'uploader':     video_uploader,\n            'uploader_id':  video_uploader_id,\n            'upload_date':  upload_date,\n            'title':        video_title,\n            'thumbnail':    video_thumbnail,\n            'description':  video_description,\n            'categories':   video_categories,\n            'subtitles':    video_subtitles,\n            'duration':     video_duration,\n            'age_limit':    18 if age_gate else 0,\n            'annotations':  video_annotations,\n            'webpage_url': proto + '://www.youtube.com/watch?v=%s' % video_id,\n            'view_count':   view_count,\n            'like_count': like_count,\n            'dislike_count': dislike_count,\n            'formats':      formats,\n        }",
        "begin_line": 647,
        "end_line": 1011,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0024509803921568627,
            "pseudo_dstar_susp": 0.0018115942028985507,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0018115942028985507,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_initialize#1092",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 1092,
        "end_line": 1093,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020242914979757085,
            "pseudo_dstar_susp": 0.001564945226917058,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001564945226917058,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_mix#1100",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._extract_mix(self, playlist_id)",
        "snippet": "    def _extract_mix(self, playlist_id):\n        # The mixes are generated from a a single video\n        # the id of the playlist is just 'RD' + video_id\n        url = 'https://youtube.com/watch?v=%s&list=%s' % (playlist_id[-11:], playlist_id)\n        webpage = self._download_webpage(\n            url, playlist_id, 'Downloading Youtube mix')\n        search_title = lambda class_name: get_element_by_attribute('class', class_name, webpage)\n        title_span = (\n            search_title('playlist-title') or\n            search_title('title long-title') or\n            search_title('title'))\n        title = clean_html(title_span)\n        ids = orderedSet(re.findall(\n            r'''(?xs)data-video-username=\".*?\".*?\n                       href=\"/watch\\?v=([0-9A-Za-z_-]{11})&amp;[^\"]*?list=%s''' % re.escape(playlist_id),\n            webpage))\n        url_results = self._ids_to_results(ids)\n\n        return self.playlist_result(url_results, playlist_id, title)",
        "begin_line": 1100,
        "end_line": 1118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_extract#1120",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubePlaylistIE",
        "signature": "youtube_dl.extractor.youtube.YoutubePlaylistIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        # Extract playlist id\n        mobj = re.match(self._VALID_URL, url)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n        playlist_id = mobj.group(1) or mobj.group(2)\n\n        # Check if it's a video-specific URL\n        query_dict = compat_urlparse.parse_qs(compat_urlparse.urlparse(url).query)\n        if 'v' in query_dict:\n            video_id = query_dict['v'][0]\n            if self._downloader.params.get('noplaylist'):\n                self.to_screen('Downloading just video %s because of --no-playlist' % video_id)\n                return self.url_result(video_id, 'Youtube', video_id=video_id)\n            else:\n                self.to_screen('Downloading playlist %s - add --no-playlist to just download video %s' % (playlist_id, video_id))\n\n        if playlist_id.startswith('RD'):\n            # Mixes require a custom extraction process\n            return self._extract_mix(playlist_id)\n        if playlist_id.startswith('TL'):\n            raise ExtractorError('For downloading YouTube.com top lists, use '\n                'the \"yttoplist\" keyword, for example \"youtube-dl \\'yttoplist:music:Top Tracks\\'\"', expected=True)\n\n        url = self._TEMPLATE_URL % playlist_id\n        page = self._download_webpage(url, playlist_id)\n        more_widget_html = content_html = page\n\n        # Check if the playlist exists or is private\n        if re.search(r'<div class=\"yt-alert-message\">[^<]*?(The|This) playlist (does not exist|is private)[^<]*?</div>', page) is not None:\n            raise ExtractorError(\n                'The playlist doesn\\'t exist or is private, use --username or '\n                '--netrc to access it.',\n                expected=True)\n\n        # Extract the video ids from the playlist pages\n        ids = []\n\n        for page_num in itertools.count(1):\n            matches = re.finditer(self._VIDEO_RE, content_html)\n            # We remove the duplicates and the link with index 0\n            # (it's not the first video of the playlist)\n            new_ids = orderedSet(m.group('id') for m in matches if m.group('index') != '0')\n            ids.extend(new_ids)\n\n            mobj = re.search(r'data-uix-load-more-href=\"/?(?P<more>[^\"]+)\"', more_widget_html)\n            if not mobj:\n                break\n\n            more = self._download_json(\n                'https://youtube.com/%s' % mobj.group('more'), playlist_id,\n                'Downloading page #%s' % page_num,\n                transform_source=uppercase_escape)\n            content_html = more['content_html']\n            more_widget_html = more['load_more_widget_html']\n\n        playlist_title = self._html_search_regex(\n            r'(?s)<h1 class=\"pl-header-title[^\"]*\">\\s*(.*?)\\s*</h1>',\n            page, 'title')\n\n        url_results = self._ids_to_results(ids)\n        return self.playlist_result(url_results, playlist_id, playlist_title)",
        "begin_line": 1120,
        "end_line": 1181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020876826722338203,
            "pseudo_dstar_susp": 0.0015772870662460567,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0015772870662460567,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeChannelIE._real_extract#1246",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeChannelIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeChannelIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        # Extract channel id\n        mobj = re.match(self._VALID_URL, url)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n\n        # Download channel page\n        channel_id = mobj.group(1)\n        video_ids = []\n        url = 'https://www.youtube.com/channel/%s/videos' % channel_id\n        channel_page = self._download_webpage(url, channel_id)\n        autogenerated = re.search(r'''(?x)\n                class=\"[^\"]*?(?:\n                    channel-header-autogenerated-label|\n                    yt-channel-title-autogenerated\n                )[^\"]*\"''', channel_page) is not None\n\n        if autogenerated:\n            # The videos are contained in a single page\n            # the ajax pages can't be used, they are empty\n            video_ids = self.extract_videos_from_page(channel_page)\n        else:\n            # Download all channel pages using the json-based channel_ajax query\n            for pagenum in itertools.count(1):\n                url = self._MORE_PAGES_URL % (pagenum, channel_id)\n                page = self._download_json(\n                    url, channel_id, note='Downloading page #%s' % pagenum,\n                    transform_source=uppercase_escape)\n\n                ids_in_page = self.extract_videos_from_page(page['content_html'])\n                video_ids.extend(ids_in_page)\n    \n                if self._MORE_PAGES_INDICATOR not in page['load_more_widget_html']:\n                    break\n\n        self._downloader.to_screen('[youtube] Channel %s: Found %i videos' % (channel_id, len(video_ids)))\n\n        url_entries = [self.url_result(video_id, 'Youtube', video_id=video_id)\n                       for video_id in video_ids]\n        return self.playlist_result(url_entries, channel_id)",
        "begin_line": 1246,
        "end_line": 1285,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeUserIE.suitable#1308",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeUserIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeUserIE.suitable(cls, url)",
        "snippet": "    def suitable(cls, url):\n        # Don't return True if the url can be extracted with other youtube\n        # extractor, the regex would is too permissive and it would match.\n        other_ies = iter(klass for (name, klass) in globals().items() if name.endswith('IE') and klass is not cls)\n        if any(ie.suitable(url) for ie in other_ies): return False\n        else: return super(YoutubeUserIE, cls).suitable(url)",
        "begin_line": 1308,
        "end_line": 1313,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002652519893899204,
            "pseudo_dstar_susp": 0.0025906735751295338,
            "pseudo_tarantula_susp": 0.0012345679012345679,
            "pseudo_op2_susp": 0.0025906735751295338,
            "pseudo_barinel_susp": 0.0012345679012345679
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeUserIE._real_extract#1315",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeUserIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeUserIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        # Extract username\n        mobj = re.match(self._VALID_URL, url)\n        if mobj is None:\n            raise ExtractorError('Invalid URL: %s' % url)\n\n        username = mobj.group(1)\n\n        # Download video ids using YouTube Data API. Result size per\n        # query is limited (currently to 50 videos) so we need to query\n        # page by page until there are no video ids - it means we got\n        # all of them.\n\n        def download_page(pagenum):\n            start_index = pagenum * self._GDATA_PAGE_SIZE + 1\n\n            gdata_url = self._GDATA_URL % (username, self._GDATA_PAGE_SIZE, start_index)\n            page = self._download_webpage(\n                gdata_url, username,\n                'Downloading video ids from %d to %d' % (\n                    start_index, start_index + self._GDATA_PAGE_SIZE))\n\n            try:\n                response = json.loads(page)\n            except ValueError as err:\n                raise ExtractorError('Invalid JSON in API response: ' + compat_str(err))\n            if 'entry' not in response['feed']:\n                return\n\n            # Extract video identifiers\n            entries = response['feed']['entry']\n            for entry in entries:\n                title = entry['title']['$t']\n                video_id = entry['id']['$t'].split('/')[-1]\n                yield {\n                    '_type': 'url',\n                    'url': video_id,\n                    'ie_key': 'Youtube',\n                    'id': video_id,\n                    'title': title,\n                }\n        url_results = OnDemandPagedList(download_page, self._GDATA_PAGE_SIZE)\n\n        return self.playlist_result(url_results, playlist_title=username)",
        "begin_line": 1315,
        "end_line": 1358,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010615711252653928,
            "pseudo_dstar_susp": 0.0010351966873706005,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010351966873706005,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeUserIE.download_page#1328",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeUserIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeUserIE.download_page(pagenum)",
        "snippet": "        def download_page(pagenum):\n            start_index = pagenum * self._GDATA_PAGE_SIZE + 1\n\n            gdata_url = self._GDATA_URL % (username, self._GDATA_PAGE_SIZE, start_index)\n            page = self._download_webpage(\n                gdata_url, username,\n                'Downloading video ids from %d to %d' % (\n                    start_index, start_index + self._GDATA_PAGE_SIZE))\n\n            try:\n                response = json.loads(page)\n            except ValueError as err:\n                raise ExtractorError('Invalid JSON in API response: ' + compat_str(err))\n            if 'entry' not in response['feed']:\n                return\n\n            # Extract video identifiers\n            entries = response['feed']['entry']\n            for entry in entries:\n                title = entry['title']['$t']\n                video_id = entry['id']['$t'].split('/')[-1]\n                yield {\n                    '_type': 'url',\n                    'url': video_id,\n                    'ie_key': 'Youtube',\n                    'id': video_id,\n                    'title': title,\n                }",
        "begin_line": 1328,
        "end_line": 1355,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010615711252653928,
            "pseudo_dstar_susp": 0.0010351966873706005,
            "pseudo_tarantula_susp": 0.00130718954248366,
            "pseudo_op2_susp": 0.0010351966873706005,
            "pseudo_barinel_susp": 0.00130718954248366
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeSearchURLIE._real_extract#1423",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeSearchURLIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeSearchURLIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        query = compat_urllib_parse.unquote_plus(mobj.group('query'))\n\n        webpage = self._download_webpage(url, query)\n        result_code = self._search_regex(\n            r'(?s)<ol class=\"item-section\"(.*?)</ol>', webpage, 'result HTML')\n\n        part_codes = re.findall(\n            r'(?s)<h3 class=\"yt-lockup-title\">(.*?)</h3>', result_code)\n        entries = []\n        for part_code in part_codes:\n            part_title = self._html_search_regex(\n                [r'(?s)title=\"([^\"]+)\"', r'>([^<]+)</a>'], part_code, 'item title', fatal=False)\n            part_url_snippet = self._html_search_regex(\n                r'(?s)href=\"([^\"]+)\"', part_code, 'item URL')\n            part_url = compat_urlparse.urljoin(\n                'https://www.youtube.com/', part_url_snippet)\n            entries.append({\n                '_type': 'url',\n                'url': part_url,\n                'title': part_title,\n            })\n\n        return {\n            '_type': 'playlist',\n            'entries': entries,\n            'title': query,\n        }",
        "begin_line": 1423,
        "end_line": 1451,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeShowIE._real_extract#1467",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeShowIE",
        "signature": "youtube_dl.extractor.youtube.YoutubeShowIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        playlist_id = mobj.group('id')\n        webpage = self._download_webpage(\n            url, playlist_id, 'Downloading show webpage')\n        # There's one playlist for each season of the show\n        m_seasons = list(re.finditer(r'href=\"(/playlist\\?list=.*?)\"', webpage))\n        self.to_screen('%s: Found %s seasons' % (playlist_id, len(m_seasons)))\n        entries = [\n            self.url_result(\n                'https://www.youtube.com' + season.group(1), 'YoutubePlaylist')\n            for season in m_seasons\n        ]\n        title = self._og_search_title(webpage, fatal=False)\n\n        return {\n            '_type': 'playlist',\n            'id': playlist_id,\n            'title': title,\n            'entries': entries,\n        }",
        "begin_line": 1467,
        "end_line": 1487,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.000851063829787234,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000851063829787234,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor.IE_NAME#1508",
        "src_path": "youtube_dl/extractor/youtube.py",
        "class_name": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor",
        "signature": "youtube_dl.extractor.youtube.YoutubeFeedsInfoExtractor.IE_NAME(self)",
        "snippet": "    def IE_NAME(self):\n        return 'youtube:%s' % self._FEED_NAME",
        "begin_line": 1508,
        "end_line": 1509,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0006561679790026247,
            "pseudo_dstar_susp": 0.0006561679790026247,
            "pseudo_tarantula_susp": 0.0006561679790026247,
            "pseudo_op2_susp": 0.0006464124111182935,
            "pseudo_barinel_susp": 0.0006561679790026247
        }
    },
    {
        "name": "youtube_dl.extractor.dailymotion.DailymotionIE._real_extract#84",
        "src_path": "youtube_dl/extractor/dailymotion.py",
        "class_name": "youtube_dl.extractor.dailymotion.DailymotionIE",
        "signature": "youtube_dl.extractor.dailymotion.DailymotionIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n        url = 'http://www.dailymotion.com/video/%s' % video_id\n\n        # Retrieve video webpage to extract further information\n        request = self._build_request(url)\n        webpage = self._download_webpage(request, video_id)\n\n        # Extract URL, uploader and title from webpage\n        self.report_extraction(video_id)\n\n        # It may just embed a vevo video:\n        m_vevo = re.search(\n            r'<link rel=\"video_src\" href=\"[^\"]*?vevo.com[^\"]*?videoId=(?P<id>[\\w]*)',\n            webpage)\n        if m_vevo is not None:\n            vevo_id = m_vevo.group('id')\n            self.to_screen('Vevo video detected: %s' % vevo_id)\n            return self.url_result('vevo:%s' % vevo_id, ie='Vevo')\n\n        age_limit = self._rta_search(webpage)\n\n        video_upload_date = None\n        mobj = re.search(r'<div class=\"[^\"]*uploaded_cont[^\"]*\" title=\"[^\"]*\">([0-9]{2})-([0-9]{2})-([0-9]{4})</div>', webpage)\n        if mobj is not None:\n            video_upload_date = mobj.group(3) + mobj.group(2) + mobj.group(1)\n\n        embed_url = 'http://www.dailymotion.com/embed/video/%s' % video_id\n        embed_page = self._download_webpage(embed_url, video_id,\n                                            'Downloading embed page')\n        info = self._search_regex(r'var info = ({.*?}),$', embed_page,\n            'video info', flags=re.MULTILINE)\n        info = json.loads(info)\n        if info.get('error') is not None:\n            msg = 'Couldn\\'t get video, Dailymotion says: %s' % info['error']['title']\n            raise ExtractorError(msg, expected=True)\n\n        formats = []\n        for (key, format_id) in self._FORMATS:\n            video_url = info.get(key)\n            if video_url is not None:\n                m_size = re.search(r'H264-(\\d+)x(\\d+)', video_url)\n                if m_size is not None:\n                    width, height = map(int_or_none, (m_size.group(1), m_size.group(2)))\n                else:\n                    width, height = None, None\n                formats.append({\n                    'url': video_url,\n                    'ext': 'mp4',\n                    'format_id': format_id,\n                    'width': width,\n                    'height': height,\n                })\n        if not formats:\n            raise ExtractorError('Unable to extract video URL')\n\n        # subtitles\n        video_subtitles = self.extract_subtitles(video_id, webpage)\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, webpage)\n            return\n\n        view_count = str_to_int(self._search_regex(\n            r'video_views_count[^>]+>\\s+([\\d\\.,]+)',\n            webpage, 'view count', fatal=False))\n\n        title = self._og_search_title(webpage, default=None)\n        if title is None:\n            title = self._html_search_regex(\n                r'(?s)<span\\s+id=\"video_title\"[^>]*>(.*?)</span>', webpage,\n                'title')\n\n        return {\n            'id': video_id,\n            'formats': formats,\n            'uploader': info['owner.screenname'],\n            'upload_date': video_upload_date,\n            'title': title,\n            'subtitles': video_subtitles,\n            'thumbnail': info['thumbnail_url'],\n            'age_limit': age_limit,\n            'view_count': view_count,\n        }",
        "begin_line": 84,
        "end_line": 166,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.001607717041800643,
            "pseudo_dstar_susp": 0.0013531799729364006,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0013531799729364006,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.dailymotion.DailymotionUserIE._real_extract#237",
        "src_path": "youtube_dl/extractor/dailymotion.py",
        "class_name": "youtube_dl.extractor.dailymotion.DailymotionUserIE",
        "signature": "youtube_dl.extractor.dailymotion.DailymotionUserIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        user = mobj.group('user')\n        webpage = self._download_webpage(url, user)\n        full_user = unescapeHTML(self._html_search_regex(\n            r'<a class=\"nav-image\" title=\"([^\"]+)\" href=\"/%s\">' % re.escape(user),\n            webpage, 'user'))\n\n        return {\n            '_type': 'playlist',\n            'id': user,\n            'title': full_user,\n            'entries': self._extract_entries(user),\n        }",
        "begin_line": 237,
        "end_line": 250,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._login#29",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor",
        "signature": "youtube_dl.extractor.vimeo.VimeoBaseInfoExtractor._login(self)",
        "snippet": "    def _login(self):\n        (username, password) = self._get_login_info()\n        if username is None:\n            if self._LOGIN_REQUIRED:\n                raise ExtractorError('No login info available, needed for using %s.' % self.IE_NAME, expected=True)\n            return\n        self.report_login()\n        login_url = 'https://vimeo.com/log_in'\n        webpage = self._download_webpage(login_url, None, False)\n        token = self._search_regex(r'xsrft: \\'(.*?)\\'', webpage, 'login token')\n        data = urlencode_postdata({\n            'email': username,\n            'password': password,\n            'action': 'login',\n            'service': 'vimeo',\n            'token': token,\n        })\n        login_request = compat_urllib_request.Request(login_url, data)\n        login_request.add_header('Content-Type', 'application/x-www-form-urlencoded')\n        login_request.add_header('Cookie', 'xsrft=%s' % token)\n        self._download_webpage(login_request, None, False, 'Wrong login info')",
        "begin_line": 29,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012594458438287153,
            "pseudo_dstar_susp": 0.0011737089201877935,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011737089201877935,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE._real_initialize#206",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE._real_initialize(self)",
        "snippet": "    def _real_initialize(self):\n        self._login()",
        "begin_line": 206,
        "end_line": 207,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0012594458438287153,
            "pseudo_dstar_susp": 0.0011737089201877935,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0011737089201877935,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoIE._real_extract#209",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        url, data = unsmuggle_url(url)\n        headers = std_headers\n        if data is not None:\n            headers = headers.copy()\n            headers.update(data)\n        if 'Referer' not in headers:\n            headers['Referer'] = url\n\n        # Extract ID from URL\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        orig_url = url\n        if mobj.group('pro') or mobj.group('player'):\n            url = 'http://player.vimeo.com/video/' + video_id\n\n        # Retrieve video webpage to extract further information\n        request = compat_urllib_request.Request(url, None, headers)\n        try:\n            webpage = self._download_webpage(request, video_id)\n        except ExtractorError as ee:\n            if isinstance(ee.cause, compat_HTTPError) and ee.cause.code == 403:\n                errmsg = ee.cause.read()\n                if b'Because of its privacy settings, this video cannot be played here' in errmsg:\n                    raise ExtractorError(\n                        'Cannot download embed-only video without embedding '\n                        'URL. Please call youtube-dl with the URL of the page '\n                        'that embeds this video.',\n                        expected=True)\n            raise\n\n        # Now we begin extracting as much information as we can from what we\n        # retrieved. First we extract the information common to all extractors,\n        # and latter we extract those that are Vimeo specific.\n        self.report_extraction(video_id)\n\n        # Extract the config JSON\n        try:\n            try:\n                config_url = self._html_search_regex(\n                    r' data-config-url=\"(.+?)\"', webpage, 'config URL')\n                config_json = self._download_webpage(config_url, video_id)\n                config = json.loads(config_json)\n            except RegexNotFoundError:\n                # For pro videos or player.vimeo.com urls\n                # We try to find out to which variable is assigned the config dic\n                m_variable_name = re.search('(\\w)\\.video\\.id', webpage)\n                if m_variable_name is not None:\n                    config_re = r'%s=({[^}].+?});' % re.escape(m_variable_name.group(1))\n                else:\n                    config_re = [r' = {config:({.+?}),assets:', r'(?:[abc])=({.+?});']\n                config = self._search_regex(config_re, webpage, 'info section',\n                    flags=re.DOTALL)\n                config = json.loads(config)\n        except Exception as e:\n            if re.search('The creator of this video has not given you permission to embed it on this domain.', webpage):\n                raise ExtractorError('The author has restricted the access to this video, try with the \"--referer\" option')\n\n            if re.search('<form[^>]+?id=\"pw_form\"', webpage) is not None:\n                self._verify_video_password(url, video_id, webpage)\n                return self._real_extract(url)\n            else:\n                raise ExtractorError('Unable to extract info section',\n                                     cause=e)\n        else:\n            if config.get('view') == 4:\n                config = self._verify_player_video_password(url, video_id)\n\n        # Extract title\n        video_title = config[\"video\"][\"title\"]\n\n        # Extract uploader and uploader_id\n        video_uploader = config[\"video\"][\"owner\"][\"name\"]\n        video_uploader_id = config[\"video\"][\"owner\"][\"url\"].split('/')[-1] if config[\"video\"][\"owner\"][\"url\"] else None\n\n        # Extract video thumbnail\n        video_thumbnail = config[\"video\"].get(\"thumbnail\")\n        if video_thumbnail is None:\n            video_thumbs = config[\"video\"].get(\"thumbs\")\n            if video_thumbs and isinstance(video_thumbs, dict):\n                _, video_thumbnail = sorted((int(width if width.isdigit() else 0), t_url) for (width, t_url) in video_thumbs.items())[-1]\n\n        # Extract video description\n\n        video_description = self._html_search_regex(\n            r'(?s)<div\\s+class=\"[^\"]*description[^\"]*\"[^>]*>(.*?)</div>',\n            webpage, 'description', default=None)\n        if not video_description:\n            video_description = self._html_search_meta(\n                'description', webpage, default=None)\n        if not video_description and mobj.group('pro'):\n            orig_webpage = self._download_webpage(\n                orig_url, video_id,\n                note='Downloading webpage for description',\n                fatal=False)\n            if orig_webpage:\n                video_description = self._html_search_meta(\n                    'description', orig_webpage, default=None)\n        if not video_description and not mobj.group('player'):\n            self._downloader.report_warning('Cannot find video description')\n\n        # Extract video duration\n        video_duration = int_or_none(config[\"video\"].get(\"duration\"))\n\n        # Extract upload date\n        video_upload_date = None\n        mobj = re.search(r'<meta itemprop=\"dateCreated\" content=\"(\\d{4})-(\\d{2})-(\\d{2})T', webpage)\n        if mobj is not None:\n            video_upload_date = mobj.group(1) + mobj.group(2) + mobj.group(3)\n\n        try:\n            view_count = int(self._search_regex(r'UserPlays:(\\d+)', webpage, 'view count'))\n            like_count = int(self._search_regex(r'UserLikes:(\\d+)', webpage, 'like count'))\n            comment_count = int(self._search_regex(r'UserComments:(\\d+)', webpage, 'comment count'))\n        except RegexNotFoundError:\n            # This info is only available in vimeo.com/{id} urls\n            view_count = None\n            like_count = None\n            comment_count = None\n\n        # Vimeo specific: extract request signature and timestamp\n        sig = config['request']['signature']\n        timestamp = config['request']['timestamp']\n\n        # Vimeo specific: extract video codec and quality information\n        # First consider quality, then codecs, then take everything\n        codecs = [('vp6', 'flv'), ('vp8', 'flv'), ('h264', 'mp4')]\n        files = {'hd': [], 'sd': [], 'other': []}\n        config_files = config[\"video\"].get(\"files\") or config[\"request\"].get(\"files\")\n        for codec_name, codec_extension in codecs:\n            for quality in config_files.get(codec_name, []):\n                format_id = '-'.join((codec_name, quality)).lower()\n                key = quality if quality in files else 'other'\n                video_url = None\n                if isinstance(config_files[codec_name], dict):\n                    file_info = config_files[codec_name][quality]\n                    video_url = file_info.get('url')\n                else:\n                    file_info = {}\n                if video_url is None:\n                    video_url = \"http://player.vimeo.com/play_redirect?clip_id=%s&sig=%s&time=%s&quality=%s&codecs=%s&type=moogaloop_local&embed_location=\" \\\n                        % (video_id, sig, timestamp, quality, codec_name.upper())\n\n                files[key].append({\n                    'ext': codec_extension,\n                    'url': video_url,\n                    'format_id': format_id,\n                    'width': file_info.get('width'),\n                    'height': file_info.get('height'),\n                })\n        formats = []\n        for key in ('other', 'sd', 'hd'):\n            formats += files[key]\n        if len(formats) == 0:\n            raise ExtractorError('No known codec found')\n\n        subtitles = {}\n        text_tracks = config['request'].get('text_tracks')\n        if text_tracks:\n            for tt in text_tracks:\n                subtitles[tt['lang']] = 'http://vimeo.com' + tt['url']\n\n        video_subtitles = self.extract_subtitles(video_id, subtitles)\n        if self._downloader.params.get('listsubtitles', False):\n            self._list_available_subtitles(video_id, subtitles)\n            return\n\n        return {\n            'id': video_id,\n            'uploader': video_uploader,\n            'uploader_id': video_uploader_id,\n            'upload_date': video_upload_date,\n            'title': video_title,\n            'thumbnail': video_thumbnail,\n            'description': video_description,\n            'duration': video_duration,\n            'formats': formats,\n            'webpage_url': url,\n            'view_count': view_count,\n            'like_count': like_count,\n            'comment_count': comment_count,\n            'subtitles': video_subtitles,\n        }",
        "begin_line": 209,
        "end_line": 391,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0020876826722338203,
            "pseudo_dstar_susp": 0.0015772870662460567,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0015772870662460567,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._page_url#407",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._page_url(self, base_url, pagenum)",
        "snippet": "    def _page_url(self, base_url, pagenum):\n        return '%s/videos/page:%d/' % (base_url, pagenum)",
        "begin_line": 407,
        "end_line": 408,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008583690987124463,
            "pseudo_dstar_susp": 0.0008928571428571428,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008928571428571428,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_videos#413",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._extract_videos(self, list_id, base_url)",
        "snippet": "    def _extract_videos(self, list_id, base_url):\n        video_ids = []\n        for pagenum in itertools.count(1):\n            webpage = self._download_webpage(\n                self._page_url(base_url, pagenum), list_id,\n                'Downloading page %s' % pagenum)\n            video_ids.extend(re.findall(r'id=\"clip_(\\d+?)\"', webpage))\n            if re.search(self._MORE_PAGES_INDICATOR, webpage, re.DOTALL) is None:\n                break\n\n        entries = [self.url_result('http://vimeo.com/%s' % video_id, 'Vimeo')\n                   for video_id in video_ids]\n        return {'_type': 'playlist',\n                'id': list_id,\n                'title': self._extract_list_title(webpage),\n                'entries': entries,\n                }",
        "begin_line": 413,
        "end_line": 429,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011441647597254005,
            "pseudo_dstar_susp": 0.0011074197120708748,
            "pseudo_tarantula_susp": 0.0012285012285012285,
            "pseudo_op2_susp": 0.0011074197120708748,
            "pseudo_barinel_susp": 0.0012285012285012285
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoChannelIE._real_extract#431",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoChannelIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoChannelIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        channel_id = mobj.group('id')\n        return self._extract_videos(channel_id, 'http://vimeo.com/channels/%s' % channel_id)",
        "begin_line": 431,
        "end_line": 434,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoAlbumIE._page_url#467",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoAlbumIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoAlbumIE._page_url(self, base_url, pagenum)",
        "snippet": "    def _page_url(self, base_url, pagenum):\n        return '%s/page:%d/' % (base_url, pagenum)",
        "begin_line": 467,
        "end_line": 468,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0011198208286674132,
            "pseudo_dstar_susp": 0.0010559662090813093,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0010559662090813093,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoGroupsIE._real_extract#490",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoGroupsIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoGroupsIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        name = mobj.group('name')\n        return self._extract_videos(name, 'http://vimeo.com/groups/%s' % name)",
        "begin_line": 490,
        "end_line": 493,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.vimeo.VimeoLikesIE._real_extract#568",
        "src_path": "youtube_dl/extractor/vimeo.py",
        "class_name": "youtube_dl.extractor.vimeo.VimeoLikesIE",
        "signature": "youtube_dl.extractor.vimeo.VimeoLikesIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        user_id = self._match_id(url)\n        webpage = self._download_webpage(url, user_id)\n        page_count = self._int(\n            self._search_regex(\n                r'''(?x)<li><a\\s+href=\"[^\"]+\"\\s+data-page=\"([0-9]+)\">\n                    .*?</a></li>\\s*<li\\s+class=\"pagination_next\">\n                ''', webpage, 'page count'),\n            'page count', fatal=True)\n        PAGE_SIZE = 12\n        title = self._html_search_regex(\n            r'(?s)<h1>(.+?)</h1>', webpage, 'title', fatal=False)\n        description = self._html_search_meta('description', webpage)\n\n        def _get_page(idx):\n            page_url = '%s//vimeo.com/user%s/likes/page:%d/sort:date' % (\n                self.http_scheme(), user_id, idx + 1)\n            webpage = self._download_webpage(\n                page_url, user_id,\n                note='Downloading page %d/%d' % (idx + 1, page_count))\n            video_list = self._search_regex(\n                r'(?s)<ol class=\"js-browse_list[^\"]+\"[^>]*>(.*?)</ol>',\n                webpage, 'video content')\n            paths = re.findall(\n                r'<li[^>]*>\\s*<a\\s+href=\"([^\"]+)\"', video_list)\n            for path in paths:\n                yield {\n                    '_type': 'url',\n                    'url': compat_urlparse.urljoin(page_url, path),\n                }\n\n        pl = InAdvancePagedList(_get_page, page_count, PAGE_SIZE)\n\n        return {\n            '_type': 'playlist',\n            'id': 'user%s_likes' % user_id,\n            'title': title,\n            'description': description,\n            'entries': pl,\n        }",
        "begin_line": 568,
        "end_line": 607,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0008012820512820513,
            "pseudo_dstar_susp": 0.0008012820512820513,
            "pseudo_tarantula_susp": 0.0010152284263959391,
            "pseudo_op2_susp": 0.0008012820512820513,
            "pseudo_barinel_susp": 0.0010141987829614604
        }
    },
    {
        "name": "youtube_dl.extractor.rtve._decrypt_url#15",
        "src_path": "youtube_dl/extractor/rtve.py",
        "class_name": "youtube_dl.extractor.rtve",
        "signature": "youtube_dl.extractor.rtve._decrypt_url(png)",
        "snippet": "def _decrypt_url(png):\n    encrypted_data = base64.b64decode(png)\n    text_index = encrypted_data.find(b'tEXt')\n    text_chunk = encrypted_data[text_index - 4:]\n    length = struct_unpack('!I', text_chunk[:4])[0]\n    # Use bytearray to get integers when iterating in both python 2.x and 3.x\n    data = bytearray(text_chunk[8:8 + length])\n    data = [chr(b) for b in data if b != 0]\n    hash_index = data.index('#')\n    alphabet_data = data[:hash_index]\n    url_data = data[hash_index + 1:]\n\n    alphabet = []\n    e = 0\n    d = 0\n    for l in alphabet_data:\n        if d == 0:\n            alphabet.append(l)\n            d = e = (e + 1) % 4\n        else:\n            d -= 1\n    url = ''\n    f = 0\n    e = 3\n    b = 1\n    for letter in url_data:\n        if f == 0:\n            l = int(letter) * 10\n            f = 1\n        else:\n            if e == 0:\n                l += int(letter)\n                url += alphabet[l]\n                e = (b + 3) % 4\n                f = 0\n                b += 1\n            else:\n                e -= 1\n\n    return url",
        "begin_line": 15,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0010330578512396695,
            "pseudo_dstar_susp": 0.000970873786407767,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.000970873786407767,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.globo.GloboIE._real_extract#337",
        "src_path": "youtube_dl/extractor/globo.py",
        "class_name": "youtube_dl.extractor.globo.GloboIE",
        "signature": "youtube_dl.extractor.globo.GloboIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        video_id = self._match_id(url)\n\n        webpage = self._download_webpage(url, video_id)\n        video_id = self._search_regex(self._VIDEOID_REGEXES, webpage, 'video id')\n\n        video = self._download_json(\n            self._API_URL_TEMPLATE % video_id, video_id)['videos'][0]\n\n        title = video['title']\n        duration = float_or_none(video['duration'], 1000)\n        like_count = video['likes']\n        uploader = video['channel']\n        uploader_id = video['channel_id']\n\n        formats = []\n\n        for resource in video['resources']:\n            resource_id = resource.get('_id')\n            if not resource_id:\n                continue\n\n            security = self._download_json(\n                self._SECURITY_URL_TEMPLATE % (video_id, resource_id),\n                video_id, 'Downloading security hash for %s' % resource_id)\n\n            security_hash = security.get('hash')\n            if not security_hash:\n                message = security.get('message')\n                if message:\n                    raise ExtractorError(\n                        '%s returned error: %s' % (self.IE_NAME, message), expected=True)\n                continue\n\n            hash_code = security_hash[:2]\n            received_time = int(security_hash[2:12])\n            received_random = security_hash[12:22]\n            received_md5 = security_hash[22:]\n\n            sign_time = received_time + self._RESIGN_EXPIRATION\n            padding = '%010d' % random.randint(1, 10000000000)\n\n            signed_md5 = self.MD5.b64_md5(received_md5 + compat_str(sign_time) + padding)\n            signed_hash = hash_code + compat_str(received_time) + received_random + compat_str(sign_time) + padding + signed_md5\n\n            formats.append({\n                'url': '%s?h=%s&k=%s' % (resource['url'], signed_hash, 'flash'),\n                'format_id': resource_id,\n                'height': resource['height']\n            })\n\n        self._sort_formats(formats)\n\n        return {\n            'id': video_id,\n            'title': title,\n            'duration': duration,\n            'uploader': uploader,\n            'uploader_id': uploader_id,\n            'like_count': like_count,\n            'formats': formats\n        }",
        "begin_line": 337,
        "end_line": 398,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.extractor.brightcove.BrightcoveIE._get_video_info#210",
        "src_path": "youtube_dl/extractor/brightcove.py",
        "class_name": "youtube_dl.extractor.brightcove.BrightcoveIE",
        "signature": "youtube_dl.extractor.brightcove.BrightcoveIE._get_video_info(self, video_id, query_str, query, referer=None)",
        "snippet": "    def _get_video_info(self, video_id, query_str, query, referer=None):\n        request_url = self._FEDERATED_URL_TEMPLATE % query_str\n        req = compat_urllib_request.Request(request_url)\n        linkBase = query.get('linkBaseURL')\n        if linkBase is not None:\n            referer = linkBase[0]\n        if referer is not None:\n            req.add_header('Referer', referer)\n        webpage = self._download_webpage(req, video_id)\n\n        error_msg = self._html_search_regex(\n            r\"<h1>We're sorry.</h1>\\s*<p>(.*?)</p>\", webpage,\n            'error message', default=None)\n        if error_msg is not None:\n            raise ExtractorError(\n                'brightcove said: %s' % error_msg, expected=True)\n\n        self.report_extraction(video_id)\n        info = self._search_regex(r'var experienceJSON = ({.*});', webpage, 'json')\n        info = json.loads(info)['data']\n        video_info = info['programmedContent']['videoPlayer']['mediaDTO']\n        video_info['_youtubedl_adServerURL'] = info.get('adServerURL')\n\n        return self._extract_video_info(video_info)",
        "begin_line": 210,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.002325581395348837,
            "pseudo_dstar_susp": 0.001694915254237288,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.001694915254237288,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    },
    {
        "name": "youtube_dl.downloader.__init__.get_suitable_downloader#16",
        "src_path": "youtube_dl/downloader/__init__.py",
        "class_name": "youtube_dl.downloader.__init__",
        "signature": "youtube_dl.downloader.__init__.get_suitable_downloader(info_dict)",
        "snippet": "def get_suitable_downloader(info_dict):\n    \"\"\"Get the downloader class that can handle the info dict.\"\"\"\n    url = info_dict['url']\n    protocol = info_dict.get('protocol')\n\n    if url.startswith('rtmp'):\n        return RtmpFD\n    if protocol == 'm3u8_native':\n        return NativeHlsFD\n    if (protocol == 'm3u8') or (protocol is None and determine_ext(url) == 'm3u8'):\n        return HlsFD\n    if url.startswith('mms') or url.startswith('rtsp'):\n        return MplayerFD\n    if determine_ext(url) == 'f4m':\n        return F4mFD\n    else:\n        return HttpFD",
        "begin_line": 16,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0013908205841446453,
            "pseudo_dstar_susp": 0.001736111111111111,
            "pseudo_tarantula_susp": 0.000931098696461825,
            "pseudo_op2_susp": 0.001736111111111111,
            "pseudo_barinel_susp": 0.000931098696461825
        }
    },
    {
        "name": "youtube_dl.extractor.br.BRIE._real_extract#68",
        "src_path": "youtube_dl/extractor/br.py",
        "class_name": "youtube_dl.extractor.br.BRIE",
        "signature": "youtube_dl.extractor.br.BRIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        display_id = self._match_id(url)\n        page = self._download_webpage(url, display_id)\n        xml_url = self._search_regex(\n            r\"return BRavFramework\\.register\\(BRavFramework\\('avPlayer_(?:[a-f0-9-]{36})'\\)\\.setup\\({dataURL:'(/(?:[a-z0-9\\-]+/)+[a-z0-9/~_.-]+)'}\\)\\);\", page, 'XMLURL')\n        xml = self._download_xml(self._BASE_URL + xml_url, None)\n\n        medias = []\n\n        for xml_media in xml.findall('video') + xml.findall('audio'):\n            media = {\n                'id': xml_media.get('externalId'),\n                'title': xml_media.find('title').text,\n                'duration': parse_duration(xml_media.find('duration').text),\n                'formats': self._extract_formats(xml_media.find('assets')),\n                'thumbnails': self._extract_thumbnails(xml_media.find('teaserImage/variants')),\n                'description': ' '.join(xml_media.find('shareTitle').text.splitlines()),\n                'webpage_url': xml_media.find('permalink').text\n            }\n            if xml_media.find('author').text:\n                media['uploader'] = xml_media.find('author').text\n            if xml_media.find('broadcastDate').text:\n                media['upload_date'] = ''.join(reversed(xml_media.find('broadcastDate').text.split('.')))\n            medias.append(media)\n\n        if len(medias) > 1:\n            self._downloader.report_warning(\n                'found multiple medias; please '\n                'report this with the video URL to http://yt-dl.org/bug')\n        if not medias:\n            raise ExtractorError('No media entries found')\n        return medias[0]",
        "begin_line": 68,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0021413276231263384,
            "pseudo_dstar_susp": 0.0020242914979757085,
            "pseudo_tarantula_susp": 0.0012004801920768306,
            "pseudo_op2_susp": 0.0020242914979757085,
            "pseudo_barinel_susp": 0.0012004801920768306
        }
    },
    {
        "name": "youtube_dl.extractor.br.BRIE.text_or_none#103",
        "src_path": "youtube_dl/extractor/br.py",
        "class_name": "youtube_dl.extractor.br.BRIE",
        "signature": "youtube_dl.extractor.br.BRIE.text_or_none(asset, tag)",
        "snippet": "        def text_or_none(asset, tag):\n            elem = asset.find(tag)\n            return None if elem is None else elem.text",
        "begin_line": 103,
        "end_line": 105,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0026666666666666666,
            "pseudo_dstar_susp": 0.0027548209366391185,
            "pseudo_tarantula_susp": 0.0012106537530266344,
            "pseudo_op2_susp": 0.0027548209366391185,
            "pseudo_barinel_susp": 0.0012106537530266344
        }
    },
    {
        "name": "youtube_dl.extractor.smotri.SmotriCommunityIE._real_extract#279",
        "src_path": "youtube_dl/extractor/smotri.py",
        "class_name": "youtube_dl.extractor.smotri.SmotriCommunityIE",
        "signature": "youtube_dl.extractor.smotri.SmotriCommunityIE._real_extract(self, url)",
        "snippet": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        community_id = mobj.group('communityid')\n\n        url = 'http://smotri.com/export/rss/video/by/community/-/%s/video.xml' % community_id\n        rss = self._download_xml(url, community_id, 'Downloading community RSS')\n\n        entries = [self.url_result(video_url.text, 'Smotri')\n                   for video_url in rss.findall('./channel/item/link')]\n\n        description_text = rss.find('./channel/description').text\n        community_title = self._html_search_regex(\n            '^\u0412\u0438\u0434\u0435\u043e \u0441\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0430 \"([^\"]+)\"$', description_text, 'community title')\n\n        return self.playlist_result(entries, community_id, community_title)",
        "begin_line": 279,
        "end_line": 293,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0009615384615384616,
            "pseudo_dstar_susp": 0.0009276437847866419,
            "pseudo_tarantula_susp": 0.006493506493506494,
            "pseudo_op2_susp": 0.0009276437847866419,
            "pseudo_barinel_susp": 0.006493506493506494
        }
    }
]