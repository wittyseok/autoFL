[
    {
        "name": "test.helpers.with_config._make_dict#85",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.with_config",
        "signature": "test.helpers.with_config._make_dict(self, old_dict)",
        "snippet": "    def _make_dict(self, old_dict):\n        if self.replace_sections:\n            old_dict.update(self.config)\n            return old_dict\n\n        def get_section(sec):\n            old_sec = old_dict.get(sec, {})\n            new_sec = self.config.get(sec, {})\n            old_sec.update(new_sec)\n            return old_sec\n\n        all_sections = itertools.chain(old_dict.keys(), self.config.keys())\n        return {sec: get_section(sec) for sec in all_sections}",
        "begin_line": 85,
        "end_line": 97,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.with_config.get_section#90",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.with_config",
        "signature": "test.helpers.with_config.get_section(sec)",
        "snippet": "        def get_section(sec):\n            old_sec = old_dict.get(sec, {})\n            new_sec = self.config.get(sec, {})\n            old_sec.update(new_sec)\n            return old_sec",
        "begin_line": 90,
        "end_line": 94,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.with_config.__call__#99",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.with_config",
        "signature": "test.helpers.with_config.__call__(self, fun)",
        "snippet": "    def __call__(self, fun):\n        @functools.wraps(fun)\n        def wrapper(*args, **kwargs):\n            import luigi.configuration\n            orig_conf = luigi.configuration.LuigiConfigParser.instance()\n            new_conf = luigi.configuration.LuigiConfigParser()\n            luigi.configuration.LuigiConfigParser._instance = new_conf\n            orig_dict = {k: dict(orig_conf.items(k)) for k in orig_conf.sections()}\n            new_dict = self._make_dict(orig_dict)\n            for (section, settings) in six.iteritems(new_dict):\n                new_conf.add_section(section)\n                for (name, value) in six.iteritems(settings):\n                    new_conf.set(section, name, value)\n            try:\n                return fun(*args, **kwargs)\n            finally:\n                luigi.configuration.LuigiConfigParser._instance = orig_conf\n        return wrapper",
        "begin_line": 99,
        "end_line": 116,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.with_config.wrapper#101",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.with_config",
        "signature": "test.helpers.with_config.wrapper(*args, **kwargs)",
        "snippet": "        def wrapper(*args, **kwargs):\n            import luigi.configuration\n            orig_conf = luigi.configuration.LuigiConfigParser.instance()\n            new_conf = luigi.configuration.LuigiConfigParser()\n            luigi.configuration.LuigiConfigParser._instance = new_conf\n            orig_dict = {k: dict(orig_conf.items(k)) for k in orig_conf.sections()}\n            new_dict = self._make_dict(orig_dict)\n            for (section, settings) in six.iteritems(new_dict):\n                new_conf.add_section(section)\n                for (name, value) in six.iteritems(settings):\n                    new_conf.set(section, name, value)\n            try:\n                return fun(*args, **kwargs)\n            finally:\n                luigi.configuration.LuigiConfigParser._instance = orig_conf",
        "begin_line": 101,
        "end_line": 115,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.LuigiTestCase.setUp#123",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.LuigiTestCase",
        "signature": "test.helpers.LuigiTestCase.setUp(self)",
        "snippet": "    def setUp(self):\n        super(LuigiTestCase, self).setUp()\n        self._stashed_reg = luigi.task_register.Register._get_reg()",
        "begin_line": 123,
        "end_line": 125,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.LuigiTestCase.tearDown#127",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.LuigiTestCase",
        "signature": "test.helpers.LuigiTestCase.tearDown(self)",
        "snippet": "    def tearDown(self):\n        luigi.task_register.Register._set_reg(self._stashed_reg)\n        super(LuigiTestCase, self).tearDown()",
        "begin_line": 127,
        "end_line": 129,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.LuigiTestCase.run_locally#131",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.LuigiTestCase",
        "signature": "test.helpers.LuigiTestCase.run_locally(self, args)",
        "snippet": "    def run_locally(self, args):\n        \"\"\" Helper for running tests testing more of the stack, the command\n        line parsing and task from name intstantiation parts in particular. \"\"\"\n        run_exit_status = luigi.run(['--local-scheduler', '--no-lock'] + args)\n        return run_exit_status",
        "begin_line": 131,
        "end_line": 135,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.helpers.LuigiTestCase.run_locally_split#137",
        "src_path": "test/helpers.py",
        "class_name": "test.helpers.LuigiTestCase",
        "signature": "test.helpers.LuigiTestCase.run_locally_split(self, space_seperated_args)",
        "snippet": "    def run_locally_split(self, space_seperated_args):\n        \"\"\" Helper for running tests testing more of the stack, the command\n        line parsing and task from name intstantiation parts in particular. \"\"\"\n        return self.run_locally(space_seperated_args.split(' '))",
        "begin_line": 137,
        "end_line": 140,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.poll_generator#31",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test",
        "signature": "test.contrib.spark_test.poll_generator()",
        "snippet": "def poll_generator():\n    yield None\n    yield 1",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.setup_run_process#36",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test",
        "signature": "test.contrib.spark_test.setup_run_process(proc)",
        "snippet": "def setup_run_process(proc):\n    poll_gen = poll_generator()\n    proc.return_value.poll = lambda: next(poll_gen)\n    proc.return_value.returncode = 0\n    proc.return_value.stdout = BytesIO()\n    proc.return_value.stderr = BytesIO()",
        "begin_line": 36,
        "end_line": 41,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkSubmitTask.app_options#67",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkSubmitTask",
        "signature": "test.contrib.spark_test.TestSparkSubmitTask.app_options(self)",
        "snippet": "    def app_options(self):\n        return [\"arg1\", \"arg2\"]",
        "begin_line": 67,
        "end_line": 68,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkSubmitTask.output#70",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkSubmitTask",
        "signature": "test.contrib.spark_test.TestSparkSubmitTask.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget('output')",
        "begin_line": 70,
        "end_line": 71,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestDefaultSparkSubmitTask.output#77",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestDefaultSparkSubmitTask",
        "signature": "test.contrib.spark_test.TestDefaultSparkSubmitTask.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget('output')",
        "begin_line": 77,
        "end_line": 78,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestPySparkTask.input#83",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestPySparkTask",
        "signature": "test.contrib.spark_test.TestPySparkTask.input(self)",
        "snippet": "    def input(self):\n        return MockTarget('input')",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestPySparkTask.output#86",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestPySparkTask",
        "signature": "test.contrib.spark_test.TestPySparkTask.output(self)",
        "snippet": "    def output(self):\n        return MockTarget('output')",
        "begin_line": 86,
        "end_line": 87,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestPySparkTask.main#89",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestPySparkTask",
        "signature": "test.contrib.spark_test.TestPySparkTask.main(self, sc, *args)",
        "snippet": "    def main(self, sc, *args):\n        sc.textFile(self.input().path).saveAsTextFile(self.output().path)",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.HdfsJob.output#95",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.HdfsJob",
        "signature": "test.contrib.spark_test.HdfsJob.output(self)",
        "snippet": "    def output(self):\n        return luigi.contrib.hdfs.HdfsTarget('test')",
        "begin_line": 95,
        "end_line": 96,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkJob.requires_hadoop#105",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkJob",
        "signature": "test.contrib.spark_test.TestSparkJob.requires_hadoop(self)",
        "snippet": "    def requires_hadoop(self):\n        return HdfsJob()",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkJob.jar#108",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkJob",
        "signature": "test.contrib.spark_test.TestSparkJob.jar(self)",
        "snippet": "    def jar(self):\n        return 'jar'",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkJob.job_class#111",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkJob",
        "signature": "test.contrib.spark_test.TestSparkJob.job_class(self)",
        "snippet": "    def job_class(self):\n        return 'job_class'",
        "begin_line": 111,
        "end_line": 112,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSparkJob.output#114",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSparkJob",
        "signature": "test.contrib.spark_test.TestSparkJob.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget('output')",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSpark1xJob.jar#120",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSpark1xJob",
        "signature": "test.contrib.spark_test.TestSpark1xJob.jar(self)",
        "snippet": "    def jar(self):\n        return 'jar'",
        "begin_line": 120,
        "end_line": 121,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSpark1xJob.job_class#123",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSpark1xJob",
        "signature": "test.contrib.spark_test.TestSpark1xJob.job_class(self)",
        "snippet": "    def job_class(self):\n        return 'job_class'",
        "begin_line": 123,
        "end_line": 124,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestSpark1xJob.output#126",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestSpark1xJob",
        "signature": "test.contrib.spark_test.TestSpark1xJob.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget('output')",
        "begin_line": 126,
        "end_line": 127,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestPySpark1xJob.program#132",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestPySpark1xJob",
        "signature": "test.contrib.spark_test.TestPySpark1xJob.program(self)",
        "snippet": "    def program(self):\n        return 'python_file'",
        "begin_line": 132,
        "end_line": 133,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.TestPySpark1xJob.output#135",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.TestPySpark1xJob",
        "signature": "test.contrib.spark_test.TestPySpark1xJob.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget('output')",
        "begin_line": 135,
        "end_line": 136,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.test_run#144",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.test_run(self, proc)",
        "snippet": "    def test_run(self, proc):\n        setup_run_process(proc)\n        job = TestSparkSubmitTask()\n        job.run()\n\n        self.assertEqual(proc.call_args[0][0],\n                         ['ss-stub', '--master', 'yarn-client', '--deploy-mode', 'client', '--name', 'AppName',\n                          '--class', 'org.test.MyClass', '--jars', 'jars/my.jar', '--py-files', 'file1.py,file2.py',\n                          '--files', 'file1,file2', '--archives', 'archive1,archive2', '--conf', 'Prop=Value',\n                          '--properties-file', 'conf/spark-defaults.conf', '--driver-memory', '4G', '--driver-java-options', '-Xopt',\n                          '--driver-library-path', 'library/path', '--driver-class-path', 'class/path', '--executor-memory', '8G',\n                          '--driver-cores', '8', '--supervise', '--total-executor-cores', '150', '--executor-cores', '10',\n                          '--queue', 'queue', '--num-executors', '2', 'file', 'arg1', 'arg2'])",
        "begin_line": 144,
        "end_line": 156,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.test_defaults#161",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.test_defaults(self, proc)",
        "snippet": "    def test_defaults(self, proc):\n        proc.return_value.returncode = 0\n        job = TestDefaultSparkSubmitTask()\n        job.run()\n        self.assertEqual(proc.call_args[0][0],\n                         ['ss-stub', '--master', 'spark://host:7077', '--jars', 'jar1.jar,jar2.jar',\n                          '--py-files', 'file1.py,file2.py', '--files', 'file1,file2', '--archives', 'archive1',\n                          '--conf', 'prop1=val1', 'test.py'])",
        "begin_line": 161,
        "end_line": 168,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.test_handle_failed_job#172",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.test_handle_failed_job(self, proc, file)",
        "snippet": "    def test_handle_failed_job(self, proc, file):\n        proc.return_value.returncode = 1\n        file.return_value = BytesIO(b'stderr')\n        try:\n            job = TestSparkSubmitTask()\n            job.run()\n        except SparkJobError as e:\n            self.assertEqual(e.err, 'stderr')\n            self.assertTrue('STDERR: stderr' in six.text_type(e))\n        else:\n            self.fail(\"Should have thrown SparkJobError\")",
        "begin_line": 172,
        "end_line": 182,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.test_app_must_be_set#185",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.test_app_must_be_set(self, proc)",
        "snippet": "    def test_app_must_be_set(self, proc):\n        with self.assertRaises(NotImplementedError):\n            job = SparkSubmitTask()\n            job.run()",
        "begin_line": 185,
        "end_line": 188,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.test_app_interruption#191",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.test_app_interruption(self, proc)",
        "snippet": "    def test_app_interruption(self, proc):\n\n        def interrupt():\n            raise KeyboardInterrupt()\n\n        proc.return_value.wait = interrupt\n        try:\n            job = TestSparkSubmitTask()\n            job.run()\n        except KeyboardInterrupt:\n            pass\n        proc.return_value.kill.assert_called()",
        "begin_line": 191,
        "end_line": 202,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkSubmitTaskTest.interrupt#193",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkSubmitTaskTest",
        "signature": "test.contrib.spark_test.SparkSubmitTaskTest.interrupt()",
        "snippet": "        def interrupt():\n            raise KeyboardInterrupt()",
        "begin_line": 193,
        "end_line": 194,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.PySparkTaskTest.test_run#210",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.PySparkTaskTest",
        "signature": "test.contrib.spark_test.PySparkTaskTest.test_run(self, proc)",
        "snippet": "    def test_run(self, proc):\n        setup_run_process(proc)\n        job = TestPySparkTask()\n        job.run()\n        proc_arg_list = proc.call_args[0][0]\n        self.assertEqual(proc_arg_list[0:7], ['ss-stub', '--master', 'spark://host:7077', '--deploy-mode', 'client', '--name', 'TestPySparkTask'])\n        self.assertTrue(os.path.exists(proc_arg_list[7]))\n        self.assertTrue(proc_arg_list[8].endswith('TestPySparkTask.pickle'))",
        "begin_line": 210,
        "end_line": 217,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.PySparkTaskTest.test_pyspark_runner#222",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.PySparkTaskTest",
        "signature": "test.contrib.spark_test.PySparkTaskTest.test_pyspark_runner(self, spark_context)",
        "snippet": "    def test_pyspark_runner(self, spark_context):\n        sc = spark_context.return_value.__enter__.return_value\n\n        def mock_spark_submit(task):\n            from luigi.contrib.pyspark_runner import PySparkRunner\n            PySparkRunner(*task.app_command()[1:]).run()\n            # Check py-package exists\n            self.assertTrue(os.path.exists(sc.addPyFile.call_args[0][0]))\n\n        with patch.object(SparkSubmitTask, 'run', mock_spark_submit):\n            job = TestPySparkTask()\n            job.run()\n\n        sc.textFile.assert_called_with('input')\n        sc.textFile.return_value.saveAsTextFile.assert_called_with('output')",
        "begin_line": 222,
        "end_line": 236,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.PySparkTaskTest.mock_spark_submit#225",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.PySparkTaskTest",
        "signature": "test.contrib.spark_test.PySparkTaskTest.mock_spark_submit(task)",
        "snippet": "        def mock_spark_submit(task):\n            from luigi.contrib.pyspark_runner import PySparkRunner\n            PySparkRunner(*task.app_command()[1:]).run()\n            # Check py-package exists\n            self.assertTrue(os.path.exists(sc.addPyFile.call_args[0][0]))",
        "begin_line": 225,
        "end_line": 229,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkJobTest.test_run#248",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkJobTest",
        "signature": "test.contrib.spark_test.SparkJobTest.test_run(self, target, proc)",
        "snippet": "    def test_run(self, target, proc):\n        setup_run_process(proc)\n        job = TestSparkJob()\n        job.run()\n        self.assertEqual(proc.call_args[0][0], [self.sc, 'org.apache.spark.deploy.yarn.Client', '--jar', job.jar(), '--class', job.job_class(),\n                                                '--num-workers', '2', '--master-memory', '1g', '--worker-memory', '1g'])",
        "begin_line": 248,
        "end_line": 253,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.SparkJobTest.test_handle_failed_job#258",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.SparkJobTest",
        "signature": "test.contrib.spark_test.SparkJobTest.test_handle_failed_job(self, proc, file)",
        "snippet": "    def test_handle_failed_job(self, proc, file):\n        proc.return_value.returncode = 1\n        file.return_value = BytesIO(b'stderr')\n        try:\n            job = TestSparkJob()\n            job.run()\n        except SparkJobError as e:\n            self.assertEqual(e.err, 'stderr')\n            self.assertTrue('STDERR: stderr' in six.text_type(e))\n        else:\n            self.fail(\"Should have thrown SparkJobError\")",
        "begin_line": 258,
        "end_line": 268,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.Spark1xTest.test_run#276",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.Spark1xTest",
        "signature": "test.contrib.spark_test.Spark1xTest.test_run(self, proc)",
        "snippet": "    def test_run(self, proc):\n        setup_run_process(proc)\n        job = TestSpark1xJob()\n        job.run()\n        self.assertEqual(proc.call_args[0][0], [self.ss, '--master', 'yarn-client', '--class', job.job_class(), job.jar()])",
        "begin_line": 276,
        "end_line": 280,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.Spark1xTest.test_handle_failed_job#285",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.Spark1xTest",
        "signature": "test.contrib.spark_test.Spark1xTest.test_handle_failed_job(self, proc, file)",
        "snippet": "    def test_handle_failed_job(self, proc, file):\n        proc.return_value.returncode = 1\n        file.return_value = BytesIO(b'stderr')\n        try:\n            job = TestSpark1xJob()\n            job.run()\n        except SparkJobError as e:\n            self.assertEqual(e.err, 'stderr')\n            self.assertTrue('STDERR: stderr' in six.text_type(e))\n        else:\n            self.fail(\"Should have thrown SparkJobError\")",
        "begin_line": 285,
        "end_line": 295,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.PySpark1xTest.test_run#303",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.PySpark1xTest",
        "signature": "test.contrib.spark_test.PySpark1xTest.test_run(self, proc)",
        "snippet": "    def test_run(self, proc):\n        setup_run_process(proc)\n        job = TestPySpark1xJob()\n        job.run()\n        self.assertEqual(proc.call_args[0][0], [self.ss, '--master', 'yarn-client', job.program()])",
        "begin_line": 303,
        "end_line": 307,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.contrib.spark_test.PySpark1xTest.test_handle_failed_job#312",
        "src_path": "test/contrib/spark_test.py",
        "class_name": "test.contrib.spark_test.PySpark1xTest",
        "signature": "test.contrib.spark_test.PySpark1xTest.test_handle_failed_job(self, proc, file)",
        "snippet": "    def test_handle_failed_job(self, proc, file):\n        proc.return_value.returncode = 1\n        file.return_value = BytesIO(b'stderr')\n        try:\n            job = TestPySpark1xJob()\n            job.run()\n        except SparkJobError as e:\n            self.assertEqual(e.err, 'stderr')\n            self.assertTrue('STDERR: stderr' in six.text_type(e))\n        else:\n            self.fail(\"Should have thrown SparkJobError\")",
        "begin_line": 312,
        "end_line": 322,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.other_module.OtherModuleTask.output#24",
        "src_path": "test/other_module.py",
        "class_name": "test.other_module.OtherModuleTask",
        "signature": "test.other_module.OtherModuleTask.output(self)",
        "snippet": "    def output(self):\n        return luigi.LocalTarget(self.p)",
        "begin_line": 24,
        "end_line": 25,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.other_module.OtherModuleTask.run#27",
        "src_path": "test/other_module.py",
        "class_name": "test.other_module.OtherModuleTask",
        "signature": "test.other_module.OtherModuleTask.run(self)",
        "snippet": "    def run(self):\n        with self.output().open('w') as f:\n            f.write('Done!')",
        "begin_line": 27,
        "end_line": 29,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.minicluster.MiniClusterTestCase.tearDownClass#47",
        "src_path": "test/minicluster.py",
        "class_name": "test.minicluster.MiniClusterTestCase",
        "signature": "test.minicluster.MiniClusterTestCase.tearDownClass(cls)",
        "snippet": "    def tearDownClass(cls):\n        if cls.cluster:\n            cls.cluster.terminate()",
        "begin_line": 47,
        "end_line": 49,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "test.minicluster.MiniClusterTestCase.setUp#51",
        "src_path": "test/minicluster.py",
        "class_name": "test.minicluster.MiniClusterTestCase",
        "signature": "test.minicluster.MiniClusterTestCase.setUp(self)",
        "snippet": "    def setUp(self):\n        self.fs = luigi.contrib.hdfs.client\n        cfg_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"testconfig\")\n        hadoop_bin = os.path.join(os.environ['HADOOP_HOME'], 'bin/hadoop')\n        cmd = \"{} --config {}\".format(hadoop_bin, cfg_path)\n        self.stashed_hdfs_client = luigi.configuration.get_config().get('hadoop', 'command', None)\n        luigi.configuration.get_config().set('hadoop', 'command', cmd)",
        "begin_line": 51,
        "end_line": 57,
        "comment": "",
        "is_bug": false
    }
]