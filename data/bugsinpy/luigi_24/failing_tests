coverage run -m pytest test/contrib/spark_test.py::SparkSubmitTaskTest::test_run
============================= test session starts ==============================
platform linux -- Python 3.8.3, pytest-5.4.3, py-1.8.1, pluggy-0.13.1
benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/user/BugsInPy/temp/projects/luigi
plugins: benchmark-3.2.3, requests-mock-1.8.0, sanic-1.6.1, sugar-0.9.3, cov-2.9.0
collected 1 item

test/contrib/spark_test.py F                                             [100%]

=================================== FAILURES ===================================
_________________________ SparkSubmitTaskTest.test_run _________________________

self = <contrib.spark_test.SparkSubmitTaskTest testMethod=test_run>
proc = <MagicMock name='Popen' id='139941103455056'>

    @with_config({'spark': {'spark-submit': ss, 'master': "yarn-client", 'hadoop-conf-dir': 'path'}})
    @patch('luigi.contrib.spark.subprocess.Popen')
    def test_run(self, proc):
        setup_run_process(proc)
        job = TestSparkSubmitTask()
        job.run()
    
>       self.assertEqual(proc.call_args[0][0],
                         ['ss-stub', '--master', 'yarn-client', '--deploy-mode', 'client', '--name', 'AppName',
                          '--class', 'org.test.MyClass', '--jars', 'jars/my.jar', '--py-files', 'file1.py,file2.py',
                          '--files', 'file1,file2', '--archives', 'archive1,archive2', '--conf', 'Prop=Value',
                          '--properties-file', 'conf/spark-defaults.conf', '--driver-memory', '4G', '--driver-java-options', '-Xopt',
                          '--driver-library-path', 'library/path', '--driver-class-path', 'class/path', '--executor-memory', '8G',
                          '--driver-cores', '8', '--supervise', '--total-executor-cores', '150', '--executor-cores', '10',
                          '--queue', 'queue', '--num-executors', '2', 'file', 'arg1', 'arg2'])
E       AssertionError: Lists differ: ['ss-[240 chars]f', '"Prop=Value"', '--properties-file', 'conf[346 chars]rg2'] != ['ss-[240 chars]f', 'Prop=Value', '--properties-file', 'conf/s[344 chars]rg2']
E       
E       First differing element 18:
E       '"Prop=Value"'
E       'Prop=Value'
E       
E       Diff is 812 characters long. Set self.maxDiff to None to see it.

test/contrib/spark_test.py:149: AssertionError
=============================== warnings summary ===============================
luigi/parameter.py:533
  /home/user/BugsInPy/temp/projects/luigi/luigi/parameter.py:533: DeprecationWarning: invalid escape sequence \d
    return "(?P<%s>\d+)%s" % (key, key[0].upper())

luigi/parameter.py:545
  /home/user/BugsInPy/temp/projects/luigi/luigi/parameter.py:545: DeprecationWarning: invalid escape sequence \d
    regex = "".join(["((?P<%s>\d+) ?%s(%s)?(%s)? ?)?" % (k, k[0], k[1:-1], k[-1]) for k in keys])

luigi/contrib/hdfs/snakebite_client.py:131
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/hdfs/snakebite_client.py:131: DeprecationWarning: invalid escape sequence \*
    """

luigi/contrib/hdfs/snakebite_client.py:145
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/hdfs/snakebite_client.py:145: DeprecationWarning: invalid escape sequence \*
    """

luigi/contrib/hdfs/snakebite_client.py:228
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/hdfs/snakebite_client.py:228: DeprecationWarning: invalid escape sequence \*
    """

luigi/contrib/spark.py:479
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/spark.py:479: DeprecationWarning: invalid escape sequence \w
    app_id_s = re.compile('application identifier: (\w+)').search(s)

luigi/contrib/spark.py:483
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/spark.py:483: DeprecationWarning: invalid escape sequence \w
    app_status_s = re.compile('yarnAppState: (\w+)').search(s)

luigi/contrib/spark.py:489
  /home/user/BugsInPy/temp/projects/luigi/luigi/contrib/spark.py:489: DeprecationWarning: invalid escape sequence \w
    final_state_s = re.compile('distributedFinalState: (\w+)').search(s)

-- Docs: https://docs.pytest.org/en/latest/warnings.html
=========================== short test summary info ============================
FAILED test/contrib/spark_test.py::SparkSubmitTaskTest::test_run - AssertionE...
======================== 1 failed, 8 warnings in 0.21s =========================

coverage run -m pytest test/contrib/spark_test.py::SparkSubmitTaskTest::test_defaults
============================= test session starts ==============================
platform linux -- Python 3.8.3, pytest-5.4.3, py-1.8.1, pluggy-0.13.1
benchmark: 3.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/user/BugsInPy/temp/projects/luigi
plugins: benchmark-3.2.3, requests-mock-1.8.0, sanic-1.6.1, sugar-0.9.3, cov-2.9.0
collected 1 item

test/contrib/spark_test.py F                                             [100%]

=================================== FAILURES ===================================
______________________ SparkSubmitTaskTest.test_defaults _______________________

self = <contrib.spark_test.SparkSubmitTaskTest testMethod=test_defaults>
proc = <MagicMock name='Popen' id='139956945250096'>

    @with_config({'spark': {'spark-submit': ss, 'master': 'spark://host:7077', 'conf': 'prop1=val1', 'jars': 'jar1.jar,jar2.jar',
                            'files': 'file1,file2', 'py-files': 'file1.py,file2.py', 'archives': 'archive1'}})
    @patch('luigi.contrib.spark.subprocess.Popen')
    def test_defaults(self, proc):
        proc.return_value.returncode = 0
        job = TestDefaultSparkSubmitTask()
        job.run()
>       self.assertEqual(proc.call_args[0][0],
                         ['ss-stub', '--master', 'spark://host:7077', '--jars', 'jar1.jar,jar2.jar',
                          '--py-files', 'file1.py,file2.py', '--files', 'file1,file2', '--archives', 'archive1',
                          '--conf', 'prop1=val1', 'test.py'])
E       AssertionError: Lists differ: ['ss-[131 chars] '--archives', 'archive1', '--conf', '"prop1=val1"', 'test.py'] != ['ss-[131 chars] '--archives', 'archive1', '--conf', 'prop1=val1', 'test.py']
E       
E       First differing element 12:
E       '"prop1=val1"'
E       'prop1=val1'
E       
E         ['ss-stub',
E          '--master',
E          'spark://host:7077',
E          '--jars',
E          'jar1.jar,jar2.jar',
E          '--py-files',
E          'file1.py,file2.py',
E          '--files',
E          'file1,file2',
E          '--archives',
E          'archive1',
E          '--conf',
E       -  '"prop1=val1"',
E       ?   -          -
E       
E       +  'prop1=val1',
E          'test.py']

test/contrib/spark_test.py:165: AssertionError
=========================== short test summary info ============================
FAILED test/contrib/spark_test.py::SparkSubmitTaskTest::test_defaults - Asser...
============================== 1 failed in 0.18s ===============================
