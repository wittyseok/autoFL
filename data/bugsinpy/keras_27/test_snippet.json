[
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed#16",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed()",
        "snippet": "def test_TimeDistributed():\n    # first, test with Dense layer\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)),\n              epochs=1,\n              batch_size=10)\n\n    # test config\n    model.get_config()\n\n    # test when specifying a batch_input_shape\n    test_input = np.random.random((1, 3, 4))\n    test_output = model.predict(test_input)\n    weights = model.layers[0].get_weights()\n\n    reference = Sequential()\n    reference.add(wrappers.TimeDistributed(layers.Dense(2),\n                                           batch_input_shape=(1, 3, 4)))\n    reference.add(layers.Activation('relu'))\n    reference.compile(optimizer='rmsprop', loss='mse')\n    reference.layers[0].set_weights(weights)\n\n    reference_output = reference.predict(test_input)\n    assert_allclose(test_output, reference_output, atol=1e-05)\n\n    # test with Embedding\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n                                       batch_input_shape=(10, 3, 4),\n                                       dtype='int32'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.fit(np.random.randint(5, size=(10, 3, 4), dtype='int32'),\n              np.random.random((10, 3, 4, 6)), epochs=1, batch_size=10)\n\n    # compare to not using batch_input_shape\n    test_input = np.random.randint(5, size=(10, 3, 4), dtype='int32')\n    test_output = model.predict(test_input)\n    weights = model.layers[0].get_weights()\n\n    reference = Sequential()\n    reference.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n                                           input_shape=(3, 4), dtype='int32'))\n    reference.compile(optimizer='rmsprop', loss='mse')\n    reference.layers[0].set_weights(weights)\n\n    reference_output = reference.predict(test_input)\n    assert_allclose(test_output, reference_output, atol=1e-05)\n\n    # test with Conv2D\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Conv2D(5, (2, 2),\n                                                     padding='same'),\n                                       input_shape=(2, 4, 4, 3)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(np.random.random((1, 2, 4, 4, 3)),\n                         np.random.random((1, 2, 4, 4, 5)))\n\n    model = model_from_json(model.to_json())\n    model.summary()\n\n    # test stacked layers\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n    model.add(wrappers.TimeDistributed(layers.Dense(3)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n\n    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)),\n              epochs=1, batch_size=10)\n\n    # test wrapping Sequential model\n    model = Sequential()\n    model.add(layers.Dense(3, input_dim=2))\n    outer_model = Sequential()\n    outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n    outer_model.compile(optimizer='rmsprop', loss='mse')\n    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n                    epochs=1, batch_size=10)\n\n    # test with functional API\n    x = Input(shape=(3, 2))\n    y = wrappers.TimeDistributed(model)(x)\n    outer_model = Model(x, y)\n    outer_model.compile(optimizer='rmsprop', loss='mse')\n    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n                    epochs=1, batch_size=10)\n\n    # test with BatchNormalization\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.BatchNormalization(center=True, scale=True),\n        name='bn', input_shape=(10, 2)))\n    model.compile(optimizer='rmsprop', loss='mse')\n    # Assert that mean and variance are 0 and 1.\n    td = model.layers[0]\n    assert np.array_equal(td.get_weights()[2], np.array([0, 0]))\n    assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n    # Train\n    model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n                         np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n    # Assert that mean and variance changed.\n    assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n    assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n    # Verify input_map has one mapping from inputs to reshaped inputs.\n    uid = object_list_uid(model.inputs)\n    assert len(td._input_map.keys()) == 1\n    assert uid in td._input_map\n    assert K.int_shape(td._input_map[uid]) == (None, 2)",
        "begin_line": 16,
        "end_line": 127,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed_learning_phase#133",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed_learning_phase()",
        "snippet": "def test_TimeDistributed_learning_phase():\n    # test layers that need learning_phase to be set\n    np.random.seed(1234)\n    x = Input(shape=(3, 2))\n    y = wrappers.TimeDistributed(layers.Dropout(.999))(x, training=True)\n    model = Model(x, y)\n    y = model.predict(np.random.random((10, 3, 2)))\n    assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)",
        "begin_line": 133,
        "end_line": 140,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_TimeDistributed_trainable#144",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_TimeDistributed_trainable()",
        "snippet": "def test_TimeDistributed_trainable():\n    # test layers that need learning_phase to be set\n    x = Input(shape=(3, 2))\n    layer = wrappers.TimeDistributed(layers.BatchNormalization())\n    _ = layer(x)\n    assert len(layer.updates) == 2\n    assert len(layer.trainable_weights) == 2\n    layer.trainable = False\n    assert len(layer.updates) == 0\n    assert len(layer.trainable_weights) == 0\n    layer.trainable = True\n    assert len(layer.updates) == 2\n    assert len(layer.trainable_weights) == 2",
        "begin_line": 144,
        "end_line": 156,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_regularizers#160",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_regularizers()",
        "snippet": "def test_regularizers():\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.Dense(2, kernel_regularizer='l1'), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    assert len(model.layers[0].layer.losses) == 1\n    assert len(model.layers[0].losses) == 1\n    assert len(model.layers[0].get_losses_for(None)) == 1\n    assert len(model.losses) == 1\n\n    model = Sequential()\n    model.add(wrappers.TimeDistributed(\n        layers.Dense(2, activity_regularizer='l1'), input_shape=(3, 4)))\n    model.add(layers.Activation('relu'))\n    model.compile(optimizer='rmsprop', loss='mse')\n    assert len(model.losses) == 1",
        "begin_line": 160,
        "end_line": 176,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional#180",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional()",
        "snippet": "def test_Bidirectional():\n    rnn = layers.SimpleRNN\n    samples = 2\n    dim = 2\n    timesteps = 2\n    output_dim = 2\n    dropout_rate = 0.2\n    for mode in ['sum', 'concat']:\n        x = np.random.random((samples, timesteps, dim))\n        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n        y = np.random.random((samples, target_dim))\n\n        # test with Sequential model\n        model = Sequential()\n        model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                             recurrent_dropout=dropout_rate),\n                                         merge_mode=mode,\n                                         input_shape=(timesteps, dim)))\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # test config\n        model.get_config()\n        model = model_from_json(model.to_json())\n        model.summary()\n\n        # test stacked bidirectional layers\n        model = Sequential()\n        model.add(wrappers.Bidirectional(rnn(output_dim,\n                                             return_sequences=True),\n                                         merge_mode=mode,\n                                         input_shape=(timesteps, dim)))\n        model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # test with functional API\n        inputs = Input((timesteps, dim))\n        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                             recurrent_dropout=dropout_rate),\n                                         merge_mode=mode)(inputs)\n        model = Model(inputs, outputs)\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)\n\n        # Bidirectional and stateful\n        inputs = Input(batch_shape=(1, timesteps, dim))\n        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n                                         merge_mode=mode)(inputs)\n        model = Model(inputs, outputs)\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)",
        "begin_line": 180,
        "end_line": 231,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_unkown_timespamps#237",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_unkown_timespamps()",
        "snippet": "def test_Bidirectional_unkown_timespamps():\n    # test with functional API with unknown length\n    rnn = layers.SimpleRNN\n    samples = 2\n    dim = 2\n    timesteps = 2\n    output_dim = 2\n    dropout_rate = 0.2\n    for mode in ['sum', 'concat']:\n        x = np.random.random((samples, timesteps, dim))\n        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n        y = np.random.random((samples, target_dim))\n\n        inputs = Input((None, dim))\n        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n                                             recurrent_dropout=dropout_rate),\n                                         merge_mode=mode)(inputs)\n        model = Model(inputs, outputs)\n        model.compile(loss='mse', optimizer='sgd')\n        model.fit(x, y, epochs=1, batch_size=1)",
        "begin_line": 237,
        "end_line": 256,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_merged_value#261",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_merged_value(merge_mode)",
        "snippet": "def test_Bidirectional_merged_value(merge_mode):\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n    X = [np.random.rand(samples, timesteps, dim)]\n\n    if merge_mode == 'sum':\n        merge_func = lambda y, y_rev: y + y_rev\n    elif merge_mode == 'mul':\n        merge_func = lambda y, y_rev: y * y_rev\n    elif merge_mode == 'ave':\n        merge_func = lambda y, y_rev: (y + y_rev) / 2\n    elif merge_mode == 'concat':\n        merge_func = lambda y, y_rev: np.concatenate((y, y_rev), axis=-1)\n    else:\n        merge_func = lambda y, y_rev: [y, y_rev]\n\n    # basic case\n    inputs = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_sequences=True), merge_mode=merge_mode)\n    f_merged = K.function([inputs], to_list(layer(inputs)))\n    f_forward = K.function([inputs], [layer.forward_layer.call(inputs)])\n    f_backward = K.function([inputs], [K.reverse(layer.backward_layer.call(inputs), 1)])\n\n    y_merged = f_merged(X)\n    y_expected = to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n    assert len(y_merged) == len(y_expected)\n    for x1, x2 in zip(y_merged, y_expected):\n        assert_allclose(x1, x2, atol=1e-5)\n\n    # test return_state\n    inputs = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_state=True), merge_mode=merge_mode)\n    f_merged = K.function([inputs], layer(inputs))\n    f_forward = K.function([inputs], layer.forward_layer.call(inputs))\n    f_backward = K.function([inputs], layer.backward_layer.call(inputs))\n    n_states = len(layer.layer.states)\n\n    y_merged = f_merged(X)\n    y_forward = f_forward(X)\n    y_backward = f_backward(X)\n    y_expected = to_list(merge_func(y_forward[0], y_backward[0]))\n    assert len(y_merged) == len(y_expected) + n_states * 2\n    for x1, x2 in zip(y_merged, y_expected):\n        assert_allclose(x1, x2, atol=1e-5)\n\n    # test if the state of a BiRNN is the concatenation of the underlying RNNs\n    y_merged = y_merged[-n_states * 2:]\n    y_forward = y_forward[-n_states:]\n    y_backward = y_backward[-n_states:]\n    for state_birnn, state_inner in zip(y_merged, y_forward + y_backward):\n        assert_allclose(state_birnn, state_inner, atol=1e-5)",
        "begin_line": 261,
        "end_line": 314,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_dropout#320",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_dropout(merge_mode)",
        "snippet": "def test_Bidirectional_dropout(merge_mode):\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n    X = [np.random.rand(samples, timesteps, dim)]\n\n    inputs = Input((timesteps, dim))\n    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, recurrent_dropout=0.2),\n                                     merge_mode=merge_mode)\n    outputs = to_list(wrapped(inputs, training=True))\n    assert all(not getattr(x, '_uses_learning_phase') for x in outputs)\n\n    inputs = Input((timesteps, dim))\n    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, return_state=True),\n                                     merge_mode=merge_mode)\n    outputs = to_list(wrapped(inputs))\n    assert all(x._uses_learning_phase for x in outputs)\n\n    model = Model(inputs, outputs)\n    assert model.uses_learning_phase\n    y1 = to_list(model.predict(X))\n    y2 = to_list(model.predict(X))\n    for x1, x2 in zip(y1, y2):\n        assert_allclose(x1, x2, atol=1e-5)",
        "begin_line": 320,
        "end_line": 345,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_state_reuse#349",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_state_reuse()",
        "snippet": "def test_Bidirectional_state_reuse():\n    rnn = layers.LSTM\n    samples = 2\n    dim = 5\n    timesteps = 3\n    units = 3\n\n    input1 = Input((timesteps, dim))\n    layer = wrappers.Bidirectional(rnn(units, return_state=True, return_sequences=True))\n    state = layer(input1)[1:]\n\n    # test passing invalid initial_state: passing a tensor\n    input2 = Input((timesteps, dim))\n    with pytest.raises(ValueError):\n        output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state[0])\n\n    # test valid usage: passing a list\n    output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state)\n    model = Model([input1, input2], output)\n    assert len(model.layers) == 4\n    assert isinstance(model.layers[-1].input, list)\n    inputs = [np.random.rand(samples, timesteps, dim),\n              np.random.rand(samples, timesteps, dim)]\n    outputs = model.predict(inputs)",
        "begin_line": 349,
        "end_line": 372,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_with_constants#376",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_with_constants()",
        "snippet": "def test_Bidirectional_with_constants():\n    class RNNCellWithConstants(Layer):\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True\n\n        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]\n\n        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    # Test basic case.\n    x = Input((5, 5))\n    c = Input((3,))\n    cell = RNNCellWithConstants(32)\n    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional(RNN(cell))\n    y = layer(x, constants=c)\n    model = Model([x, c], y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(\n        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n        np.zeros((6, 64))\n    )\n\n    # Test basic case serialization.\n    x_np = np.random.random((6, 5, 5))\n    c_np = np.random.random((6, 3))\n    y_np = model.predict([x_np, c_np])\n    weights = model.get_weights()\n    config = layer.get_config()\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer(x, constants=c)\n    model = Model([x, c], y)\n    model.set_weights(weights)\n    y_np_2 = model.predict([x_np, c_np])\n    assert_allclose(y_np, y_np_2, atol=1e-4)\n\n    # test flat list inputs\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer([x, c])\n    model = Model([x, c], y)\n    model.set_weights(weights)\n    y_np_3 = model.predict([x_np, c_np])\n    assert_allclose(y_np, y_np_3, atol=1e-4)",
        "begin_line": 376,
        "end_line": 453,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.test_Bidirectional_with_constants#376",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.test_Bidirectional_with_constants()",
        "snippet": "def test_Bidirectional_with_constants():\n    class RNNCellWithConstants(Layer):\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True\n\n        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]\n\n        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    # Test basic case.\n    x = Input((5, 5))\n    c = Input((3,))\n    cell = RNNCellWithConstants(32)\n    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional(RNN(cell))\n    y = layer(x, constants=c)\n    model = Model([x, c], y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(\n        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n        np.zeros((6, 64))\n    )\n\n    # Test basic case serialization.\n    x_np = np.random.random((6, 5, 5))\n    c_np = np.random.random((6, 3))\n    y_np = model.predict([x_np, c_np])\n    weights = model.get_weights()\n    config = layer.get_config()\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer(x, constants=c)\n    model = Model([x, c], y)\n    model.set_weights(weights)\n    y_np_2 = model.predict([x_np, c_np])\n    assert_allclose(y_np, y_np_2, atol=1e-4)\n\n    # test flat list inputs\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer([x, c])\n    model = Model([x, c], y)\n    model.set_weights(weights)\n    y_np_3 = model.predict([x_np, c_np])\n    assert_allclose(y_np, y_np_3, atol=1e-4)",
        "begin_line": 376,
        "end_line": 453,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.__init__#378",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.__init__(self, units, **kwargs)",
        "snippet": "        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)",
        "begin_line": 378,
        "end_line": 381,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.build#383",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.build(self, input_shape)",
        "snippet": "        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True",
        "begin_line": 383,
        "end_line": 401,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.call#403",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.call(self, inputs, states, constants)",
        "snippet": "        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]",
        "begin_line": 403,
        "end_line": 410,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.get_config#412",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.get_config(self)",
        "snippet": "        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 412,
        "end_line": 415,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_with_constants_layer_passing_initial_state#457",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_with_constants_layer_passing_initial_state()",
        "snippet": "def test_Bidirectional_with_constants_layer_passing_initial_state():\n    class RNNCellWithConstants(Layer):\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True\n\n        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]\n\n        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    # Test basic case.\n    x = Input((5, 5))\n    c = Input((3,))\n    s_for = Input((32,))\n    s_bac = Input((32,))\n    cell = RNNCellWithConstants(32)\n    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional(RNN(cell))\n    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n    model = Model([x, s_for, s_bac, c], y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(\n        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 32)), np.zeros((6, 3))],\n        np.zeros((6, 64))\n    )\n\n    # Test basic case serialization.\n    x_np = np.random.random((6, 5, 5))\n    s_fw_np = np.random.random((6, 32))\n    s_bk_np = np.random.random((6, 32))\n    c_np = np.random.random((6, 3))\n    y_np = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    weights = model.get_weights()\n    config = layer.get_config()\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n    model = Model([x, s_for, s_bac, c], y)\n    model.set_weights(weights)\n    y_np_2 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    assert_allclose(y_np, y_np_2, atol=1e-4)\n\n    # verify that state is used\n    y_np_2_different_s = model.predict([x_np, s_fw_np + 10., s_bk_np + 10., c_np])\n    with pytest.raises(AssertionError):\n        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n\n    # test flat list inputs\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer([x, s_for, s_bac, c])\n    model = Model([x, s_for, s_bac, c], y)\n    model.set_weights(weights)\n    y_np_3 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    assert_allclose(y_np, y_np_3, atol=1e-4)",
        "begin_line": 457,
        "end_line": 543,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.test_Bidirectional_with_constants_layer_passing_initial_state#457",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.test_Bidirectional_with_constants_layer_passing_initial_state()",
        "snippet": "def test_Bidirectional_with_constants_layer_passing_initial_state():\n    class RNNCellWithConstants(Layer):\n        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)\n\n        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True\n\n        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]\n\n        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))\n\n    # Test basic case.\n    x = Input((5, 5))\n    c = Input((3,))\n    s_for = Input((32,))\n    s_bac = Input((32,))\n    cell = RNNCellWithConstants(32)\n    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional(RNN(cell))\n    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n    model = Model([x, s_for, s_bac, c], y)\n    model.compile(optimizer='rmsprop', loss='mse')\n    model.train_on_batch(\n        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 32)), np.zeros((6, 3))],\n        np.zeros((6, 64))\n    )\n\n    # Test basic case serialization.\n    x_np = np.random.random((6, 5, 5))\n    s_fw_np = np.random.random((6, 32))\n    s_bk_np = np.random.random((6, 32))\n    c_np = np.random.random((6, 3))\n    y_np = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    weights = model.get_weights()\n    config = layer.get_config()\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n    model = Model([x, s_for, s_bac, c], y)\n    model.set_weights(weights)\n    y_np_2 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    assert_allclose(y_np, y_np_2, atol=1e-4)\n\n    # verify that state is used\n    y_np_2_different_s = model.predict([x_np, s_fw_np + 10., s_bk_np + 10., c_np])\n    with pytest.raises(AssertionError):\n        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n\n    # test flat list inputs\n    with CustomObjectScope(custom_objects):\n        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n    y = layer([x, s_for, s_bac, c])\n    model = Model([x, s_for, s_bac, c], y)\n    model.set_weights(weights)\n    y_np_3 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n    assert_allclose(y_np, y_np_3, atol=1e-4)",
        "begin_line": 457,
        "end_line": 543,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.__init__#459",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.__init__(self, units, **kwargs)",
        "snippet": "        def __init__(self, units, **kwargs):\n            self.units = units\n            self.state_size = units\n            super(RNNCellWithConstants, self).__init__(**kwargs)",
        "begin_line": 459,
        "end_line": 462,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.build#464",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.build(self, input_shape)",
        "snippet": "        def build(self, input_shape):\n            if not isinstance(input_shape, list):\n                raise TypeError('expects constants shape')\n            [input_shape, constant_shape] = input_shape\n            # will (and should) raise if more than one constant passed\n\n            self.input_kernel = self.add_weight(\n                shape=(input_shape[-1], self.units),\n                initializer='uniform',\n                name='kernel')\n            self.recurrent_kernel = self.add_weight(\n                shape=(self.units, self.units),\n                initializer='uniform',\n                name='recurrent_kernel')\n            self.constant_kernel = self.add_weight(\n                shape=(constant_shape[-1], self.units),\n                initializer='uniform',\n                name='constant_kernel')\n            self.built = True",
        "begin_line": 464,
        "end_line": 482,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.call#484",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.call(self, inputs, states, constants)",
        "snippet": "        def call(self, inputs, states, constants):\n            [prev_output] = states\n            [constant] = constants\n            h_input = K.dot(inputs, self.input_kernel)\n            h_state = K.dot(prev_output, self.recurrent_kernel)\n            h_const = K.dot(constant, self.constant_kernel)\n            output = h_input + h_state + h_const\n            return output, [output]",
        "begin_line": 484,
        "end_line": 491,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.RNNCellWithConstants.get_config#493",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test.RNNCellWithConstants",
        "signature": "tests.keras.layers.wrappers_test.RNNCellWithConstants.get_config(self)",
        "snippet": "        def get_config(self):\n            config = {'units': self.units}\n            base_config = super(RNNCellWithConstants, self).get_config()\n            return dict(list(base_config.items()) + list(config.items()))",
        "begin_line": 493,
        "end_line": 496,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_trainable#547",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_trainable()",
        "snippet": "def test_Bidirectional_trainable():\n    # test layers that need learning_phase to be set\n    x = Input(shape=(3, 2))\n    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n    _ = layer(x)\n    assert len(layer.trainable_weights) == 6\n    layer.trainable = False\n    assert len(layer.trainable_weights) == 0\n    layer.trainable = True\n    assert len(layer.trainable_weights) == 6",
        "begin_line": 547,
        "end_line": 556,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_updates#560",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_updates()",
        "snippet": "def test_Bidirectional_updates():\n    x = Input(shape=(3, 2))\n    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n    assert len(layer.updates) == 0\n    assert len(layer.get_updates_for(None)) == 0\n    assert len(layer.get_updates_for(x)) == 0\n    layer.forward_layer.add_update(0, inputs=x)\n    layer.forward_layer.add_update(1, inputs=None)\n    layer.backward_layer.add_update(0, inputs=x)\n    layer.backward_layer.add_update(1, inputs=None)\n    assert len(layer.updates) == 4\n    assert len(layer.get_updates_for(None)) == 2\n    assert len(layer.get_updates_for(x)) == 2",
        "begin_line": 560,
        "end_line": 572,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.layers.wrappers_test.test_Bidirectional_losses#576",
        "src_path": "tests/keras/layers/wrappers_test.py",
        "class_name": "tests.keras.layers.wrappers_test",
        "signature": "tests.keras.layers.wrappers_test.test_Bidirectional_losses()",
        "snippet": "def test_Bidirectional_losses():\n    x = Input(shape=(3, 2))\n    layer = wrappers.Bidirectional(\n        layers.SimpleRNN(3, kernel_regularizer='l1', bias_regularizer='l1'))\n    _ = layer(x)\n    assert len(layer.losses) == 4\n    assert len(layer.get_losses_for(None)) == 4\n    assert len(layer.get_losses_for(x)) == 0\n    layer.forward_layer.add_loss(0, inputs=x)\n    layer.forward_layer.add_loss(1, inputs=None)\n    layer.backward_layer.add_loss(0, inputs=x)\n    layer.backward_layer.add_loss(1, inputs=None)\n    assert len(layer.losses) == 8\n    assert len(layer.get_losses_for(None)) == 6\n    assert len(layer.get_losses_for(x)) == 2",
        "begin_line": 576,
        "end_line": 590,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.wrapper#11",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.wrapper(*args)",
        "snippet": "    def wrapper(*args):\n        x = args[0]\n        w = args[1]\n        if x.ndim == 3:\n            w = np.flipud(w)\n            w = np.transpose(w, (1, 2, 0))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            w = np.fliplr(np.flipud(w))\n            w = np.transpose(w, (2, 3, 0, 1))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            w = np.flip(np.fliplr(np.flipud(w)), axis=2)\n            w = np.transpose(w, (3, 4, 0, 1, 2))\n            if args[3] == 'channels_last':\n                x = np.transpose(x, (0, 4, 1, 2, 3))\n\n        y = func(x, w, args[2], args[3])\n\n        if args[3] == 'channels_last':\n            if y.ndim == 3:\n                y = np.transpose(y, (0, 2, 1))\n            elif y.ndim == 4:\n                y = np.transpose(y, (0, 2, 3, 1))\n            else:\n                y = np.transpose(y, (0, 2, 3, 4, 1))\n\n        return y",
        "begin_line": 11,
        "end_line": 40,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.conv#46",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.conv(x, w, padding, data_format)",
        "snippet": "def conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[1]):\n            __y = []\n            for k in range(w.shape[0]):\n                __y.append(signal.convolve(x[i, k], w[k, j], mode=padding))\n            _y.append(np.sum(np.stack(__y, axis=-1), axis=-1))\n        y.append(_y)\n    y = np.array(y)\n    return y",
        "begin_line": 46,
        "end_line": 57,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.depthwise_conv#61",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.depthwise_conv(x, w, padding, data_format)",
        "snippet": "def depthwise_conv(x, w, padding, data_format):\n    y = []\n    for i in range(x.shape[0]):\n        _y = []\n        for j in range(w.shape[0]):\n            __y = []\n            for k in range(w.shape[1]):\n                __y.append(signal.convolve(x[i, j], w[j, k], mode=padding))\n            _y.append(np.stack(__y, axis=0))\n        y.append(np.concatenate(_y, axis=0))\n    y = np.array(y)\n    return y",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.separable_conv#75",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.separable_conv(x, w1, w2, padding, data_format)",
        "snippet": "def separable_conv(x, w1, w2, padding, data_format):\n    x2 = depthwise_conv(x, w1, padding, data_format)\n    return conv(x2, w2, padding, data_format)",
        "begin_line": 75,
        "end_line": 77,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.pool#80",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.pool(x, pool_size, strides, padding, data_format, pool_mode)",
        "snippet": "def pool(x, pool_size, strides, padding, data_format, pool_mode):\n    if data_format == 'channels_last':\n        if x.ndim == 3:\n            x = np.transpose(x, (0, 2, 1))\n        elif x.ndim == 4:\n            x = np.transpose(x, (0, 3, 1, 2))\n        else:\n            x = np.transpose(x, (0, 4, 1, 2, 3))\n\n    if padding == 'same':\n        pad = [(0, 0), (0, 0)] + [(s // 2, s // 2) for s in pool_size]\n        x = np.pad(x, pad, 'constant', constant_values=-np.inf)\n\n    # indexing trick\n    x = np.pad(x, [(0, 0), (0, 0)] + [(0, 1) for _ in pool_size],\n               'constant', constant_values=0)\n\n    if x.ndim == 3:\n        y = [x[:, :, k:k1:strides[0]]\n             for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0))]\n    elif x.ndim == 4:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                y.append(x[:, :, k:k1:strides[0], l:l1:strides[1]])\n    else:\n        y = []\n        for (k, k1) in zip(range(pool_size[0]), range(-pool_size[0], 0)):\n            for (l, l1) in zip(range(pool_size[1]), range(-pool_size[1], 0)):\n                for (m, m1) in zip(range(pool_size[2]), range(-pool_size[2], 0)):\n                    y.append(x[:, :, k:k1:strides[0], l:l1:strides[1], m:m1:strides[2]])\n    y = np.stack(y, axis=-1)\n    if pool_mode == 'avg':\n        y = np.mean(np.ma.masked_invalid(y), axis=-1).data\n    elif pool_mode == 'max':\n        y = np.max(y, axis=-1)\n\n    if data_format == 'channels_last':\n        if y.ndim == 3:\n            y = np.transpose(y, (0, 2, 1))\n        elif y.ndim == 4:\n            y = np.transpose(y, (0, 2, 3, 1))\n        else:\n            y = np.transpose(y, (0, 2, 3, 4, 1))\n\n    return y",
        "begin_line": 80,
        "end_line": 125,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.backend.reference_operations.rnn#128",
        "src_path": "tests/keras/backend/reference_operations.py",
        "class_name": "tests.keras.backend.reference_operations",
        "signature": "tests.keras.backend.reference_operations.rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None)",
        "snippet": "def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=None):\n    w_i, w_h, w_o = w\n    h = []\n    o = []\n\n    if go_backwards:\n        t_list = range(x.shape[1] - 1, -1, -1)\n    else:\n        t_list = range(x.shape[1])\n\n    if mask is not None:\n        from keras import backend as K\n        np_mask = K.eval(mask)\n    else:\n        np_mask = None\n\n    for (i, t) in enumerate(t_list):\n        h_t = np.dot(x[:, t], w_i)\n\n        if w_h is not None:\n            prev = h[i - 1] if i > 0 else init\n            h_t1 = np.dot(prev, w_h)\n            if np_mask is not None:\n                h_t1[np_mask[:, t] == 0] = prev[np_mask[:, t] == 0]\n        else:\n            h_t1 = 0\n\n        o_t = h_t + h_t1\n        if w_o is not None:\n            o_t = np.dot(o_t, w_o)\n        o.append(o_t)\n\n        if np_mask is not None:\n            h_t = h_t * np_mask[:, t].reshape(-1, 1)\n        h.append(h_t + h_t1)\n\n    return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)",
        "begin_line": 128,
        "end_line": 164,
        "comment": "",
        "is_bug": false
    }
]