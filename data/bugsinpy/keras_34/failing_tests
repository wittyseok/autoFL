coverage run -m pytest tests/test_multiprocessing.py::test_multiprocessing_training
============================= test session starts ==============================
platform linux -- Python 3.7.3, pytest-5.4.3, py-1.8.1, pluggy-0.13.1 -- /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/bin/python
cachedir: .pytest_cache
rootdir: /home/user/BugsInPy/temp/projects/keras, inifile: pytest.ini
plugins: forked-1.1.3, xdist-1.32.0
gw0 I / gw1 I

[gw0] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras

[gw1] linux Python 3.7.3 cwd: /home/user/BugsInPy/temp/projects/keras

[gw0] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]

[gw1] Python 3.7.3 (default, Mar 27 2019, 22:11:17)  -- [GCC 7.3.0]
gw0 [1] / gw1 [1]

scheduling tests via LoadScheduling

tests/test_multiprocessing.py::test_multiprocessing_training 
[gw0] [100%] FAILED tests/test_multiprocessing.py::test_multiprocessing_training 

=================================== FAILURES ===================================
________________________ test_multiprocessing_training _________________________
[gw0] linux -- Python 3.7.3 /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/bin/python

    @keras_test
    def test_multiprocessing_training():
        arr_data = np.random.randint(0, 256, (50, 2))
        arr_labels = np.random.randint(0, 2, 50)
        arr_weights = np.random.random(50)
    
        def custom_generator(use_weights=False):
            batch_size = 10
            n_samples = 50
    
            while True:
                batch_index = np.random.randint(0, n_samples - batch_size)
                start = batch_index
                end = start + batch_size
                X = arr_data[start: end]
                y = arr_labels[start: end]
                if use_weights:
                    w = arr_weights[start: end]
                    yield X, y, w
                else:
                    yield X, y
    
        # Build a NN
        model = Sequential()
        model.add(Dense(1, input_shape=(2, )))
        model.compile(loss='mse', optimizer='adadelta')
    
        # - Produce data on 4 worker processes, consume on main process:
        #   - Each worker process runs OWN copy of generator
        #   - BUT on Windows, `multiprocessing` won't marshall generators across
        #     process boundaries -> make sure `fit_generator()` raises ValueError
        #     exception and does not attempt to run the generator.
        if os.name is 'nt':
            with pytest.raises(ValueError):
                model.fit_generator(custom_generator(),
                                    steps_per_epoch=STEPS_PER_EPOCH,
                                    epochs=1,
                                    verbose=1,
                                    validation_steps=None,
                                    max_queue_size=10,
                                    workers=WORKERS,
                                    use_multiprocessing=True)
        else:
            model.fit_generator(custom_generator(),
                                steps_per_epoch=STEPS_PER_EPOCH,
                                epochs=1,
                                verbose=1,
                                validation_steps=None,
                                max_queue_size=10,
                                workers=WORKERS,
                                use_multiprocessing=True)
    
        # - Produce data on 4 worker threads, consume on main thread:
        #   - All worker threads share the SAME generator
        model.fit_generator(custom_generator(),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            epochs=1,
                            verbose=1,
                            validation_steps=None,
                            max_queue_size=10,
                            workers=WORKERS,
                            use_multiprocessing=False)
    
        # - Produce data on 1 worker process, consume on main process:
        #   - Worker process runs generator
        #   - BUT on Windows, `multiprocessing` won't marshall generators across
        #     process boundaries -> make sure `fit_generator()` raises ValueError
        #     exception and does not attempt to run the generator.
        if os.name is 'nt':
            with pytest.raises(ValueError):
                model.fit_generator(custom_generator(True),
                                    steps_per_epoch=STEPS_PER_EPOCH,
                                    validation_data=(arr_data[:10],
                                                     arr_labels[:10],
                                                     arr_weights[:10]),
                                    validation_steps=1,
                                    max_queue_size=10,
                                    workers=1,
                                    use_multiprocessing=True)
        else:
            model.fit_generator(custom_generator(True),
                                steps_per_epoch=STEPS_PER_EPOCH,
                                validation_data=(arr_data[:10],
                                                 arr_labels[:10],
                                                 arr_weights[:10]),
                                validation_steps=1,
                                max_queue_size=10,
                                workers=1,
                                use_multiprocessing=True)
    
        # - Produce data on 1 worker thread, consume on main thread:
        #   - Worker thread is the only thread running the generator
        model.fit_generator(custom_generator(True),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            validation_data=(arr_data[:10],
                                             arr_labels[:10],
                                             arr_weights[:10]),
                            validation_steps=1,
                            max_queue_size=10,
                            workers=1,
                            use_multiprocessing=False)
    
        # - Produce data on 1 worker process, consume on main process:
        #   - Worker process runs generator
        #   - BUT on Windows, `multiprocessing` won't marshall generators across
        #     process boundaries -> make sure `fit_generator()` raises ValueError
        #     exception and does not attempt to run the generator.
        if os.name is 'nt':
            with pytest.raises(ValueError):
                model.fit_generator(custom_generator(True),
                                    steps_per_epoch=STEPS_PER_EPOCH,
                                    validation_data=custom_generator(True),
                                    validation_steps=1,
                                    max_queue_size=10,
                                    workers=1,
                                    use_multiprocessing=True)
        else:
            model.fit_generator(custom_generator(True),
                                steps_per_epoch=STEPS_PER_EPOCH,
                                validation_data=custom_generator(True),
                                validation_steps=1,
                                max_queue_size=10,
                                workers=1,
                                use_multiprocessing=True)
    
        # - Produce data on 1 worker thread AT A TIME, consume on main thread:
        #   - Worker threads for training and validation run generator SEQUENTIALLY
        model.fit_generator(custom_generator(True),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            validation_data=custom_generator(True),
                            validation_steps=1,
                            max_queue_size=10,
                            workers=1,
                            use_multiprocessing=False)
    
        # - Produce and consume data without a queue on main thread
        #   - Make sure the value of `use_multiprocessing` is ignored
        model.fit_generator(custom_generator(True),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            validation_data=custom_generator(True),
                            validation_steps=1,
                            max_queue_size=10,
                            workers=0,
                            use_multiprocessing=True)
        model.fit_generator(custom_generator(True),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            validation_data=custom_generator(True),
                            validation_steps=1,
                            max_queue_size=10,
                            workers=0,
                            use_multiprocessing=False)
    
        # - For Sequence
        model.fit_generator(DummySequence(),
                            steps_per_epoch=STEPS_PER_EPOCH,
                            validation_data=custom_generator(True),
                            validation_steps=1,
                            max_queue_size=10,
                            workers=0,
>                           use_multiprocessing=True)

tests/test_multiprocessing.py:194: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
keras/models.py:1253: in fit_generator
    initial_epoch=initial_epoch)
keras/legacy/interfaces.py:91: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.engine.training.Model object at 0x7f7d40fc2588>
generator = <test_multiprocessing.DummySequence object at 0x7f7d40fc2978>
steps_per_epoch = 100, epochs = 1, verbose = 1
callbacks = <keras.callbacks.CallbackList object at 0x7f7d4064ea20>
validation_data = <generator object test_multiprocessing_training.<locals>.custom_generator at 0x7f7d40fbed68>
validation_steps = 1, class_weight = None, max_queue_size = 10, workers = 0
use_multiprocessing = True, shuffle = True, initial_epoch = 0

    @interfaces.legacy_generator_methods_support
    def fit_generator(self,
                      generator,
                      steps_per_epoch=None,
                      epochs=1,
                      verbose=1,
                      callbacks=None,
                      validation_data=None,
                      validation_steps=None,
                      class_weight=None,
                      max_queue_size=10,
                      workers=1,
                      use_multiprocessing=False,
                      shuffle=True,
                      initial_epoch=0):
        """Trains the model on data yielded batch-by-batch by a Python generator.
    
        The generator is run in parallel to the model, for efficiency.
        For instance, this allows you to do real-time data augmentation
        on images on CPU in parallel to training your model on GPU.
    
        The use of `keras.utils.Sequence` guarantees the ordering
        and guarantees the single use of every input per epoch when
        using `use_multiprocessing=True`.
    
        # Arguments
            generator: A generator or an instance of `Sequence`
                (`keras.utils.Sequence`) object in order to avoid
                duplicate data when using multiprocessing.
                The output of the generator must be either
                - a tuple `(inputs, targets)`
                - a tuple `(inputs, targets, sample_weights)`.
                This tuple (a single output of the generator) makes a single
                batch. Therefore, all arrays in this tuple must have the same
                length (equal to the size of this batch). Different batches
                may have different sizes. For example, the last batch of the
                epoch is commonly smaller than the others, if the size of the
                dataset is not divisible by the batch size.
                The generator is expected to loop over its data
                indefinitely. An epoch finishes when `steps_per_epoch`
                batches have been seen by the model.
            steps_per_epoch: Integer.
                Total number of steps (batches of samples)
                to yield from `generator` before declaring one epoch
                finished and starting the next epoch. It should typically
                be equal to the number of samples of your dataset
                divided by the batch size.
                Optional for `Sequence`: if unspecified, will use
                the `len(generator)` as a number of steps.
            epochs: Integer. Number of epochs to train the model.
                An epoch is an iteration over the entire data provided,
                as defined by `steps_per_epoch`.
                Note that in conjunction with `initial_epoch`,
                `epochs` is to be understood as "final epoch".
                The model is not trained for a number of iterations
                given by `epochs`, but merely until the epoch
                of index `epochs` is reached.
            verbose: Integer. 0, 1, or 2. Verbosity mode.
                0 = silent, 1 = progress bar, 2 = one line per epoch.
            callbacks: List of `keras.callbacks.Callback` instances.
                List of callbacks to apply during training.
                See [callbacks](/callbacks).
            validation_data: This can be either
                - a generator for the validation data
                - tuple `(x_val, y_val)`
                - tuple `(x_val, y_val, val_sample_weights)`
                on which to evaluate
                the loss and any model metrics at the end of each epoch.
                The model will not be trained on this data.
            validation_steps: Only relevant if `validation_data`
                is a generator. Total number of steps (batches of samples)
                to yield from `validation_data` generator before stopping.
                Optional for `Sequence`: if unspecified, will use
                the `len(validation_data)` as a number of steps.
            class_weight: Optional dictionary mapping class indices (integers)
                to a weight (float) value, used for weighting the loss function
                (during training only).
                This can be useful to tell the model to
                "pay more attention" to samples from
                an under-represented class.
            max_queue_size: Integer. Maximum size for the generator queue.
                If unspecified, `max_queue_size` will default to 10.
            workers: Integer. Maximum number of processes to spin up
                when using process based threading.
                If unspecified, `workers` will default to 1. If 0, will
                execute the generator on the main thread.
            use_multiprocessing: Boolean. If True, use process based threading.
                If unspecified, `use_multiprocessing` will default to False.
                Note that because
                this implementation relies on multiprocessing,
                you should not pass
                non picklable arguments to the generator
                as they can't be passed
                easily to children processes.
            shuffle: Boolean. Whether to shuffle the training data
                in batch-sized chunks before each epoch.
                Only used with instances of `Sequence` (`keras.utils.Sequence`).
            initial_epoch: Integer.
                Epoch at which to start training
                (useful for resuming a previous training run).
    
        # Returns
            A `History` object. Its `History.history` attribute is
            a record of training loss values and metrics values
            at successive epochs, as well as validation loss values
            and validation metrics values (if applicable).
    
        # Example
    
        ```python
            def generate_arrays_from_file(path):
                while 1:
                    with open(path) as f:
                        for line in f:
                            # create numpy arrays of input data
                            # and labels, from each line in the file
                            x1, x2, y = process_line(line)
                            yield ({'input_1': x1, 'input_2': x2}, {'output': y})
    
            model.fit_generator(generate_arrays_from_file('/my_file.txt'),
                                steps_per_epoch=10000, epochs=10)
        ```
    
        # Raises
            ValueError: In case the generator yields
                data in an invalid format.
        """
        wait_time = 0.01  # in seconds
        epoch = initial_epoch
    
        do_validation = bool(validation_data)
        self._make_train_function()
        if do_validation:
            self._make_test_function()
    
        is_sequence = isinstance(generator, Sequence)
        if not is_sequence and use_multiprocessing and workers > 1:
            warnings.warn(
                UserWarning('Using a generator with `use_multiprocessing=True`'
                            ' and multiple workers may duplicate your data.'
                            ' Please consider using the`keras.utils.Sequence'
                            ' class.'))
        if steps_per_epoch is None:
            if is_sequence:
                steps_per_epoch = len(generator)
            else:
                raise ValueError('`steps_per_epoch=None` is only valid for a'
                                 ' generator based on the `keras.utils.Sequence`'
                                 ' class. Please specify `steps_per_epoch` or use'
                                 ' the `keras.utils.Sequence` class.')
    
        # python 2 has 'next', 3 has '__next__'
        # avoid any explicit version checks
        val_gen = (hasattr(validation_data, 'next') or
                   hasattr(validation_data, '__next__') or
                   isinstance(validation_data, Sequence))
        if (val_gen and not isinstance(validation_data, Sequence) and
                not validation_steps):
            raise ValueError('`validation_steps=None` is only valid for a'
                             ' generator based on the `keras.utils.Sequence`'
                             ' class. Please specify `validation_steps` or use'
                             ' the `keras.utils.Sequence` class.')
    
        # Prepare display labels.
        out_labels = self.metrics_names
        callback_metrics = out_labels + ['val_' + n for n in out_labels]
    
        # prepare callbacks
        self.history = cbks.History()
        _callbacks = [cbks.BaseLogger(
            stateful_metrics=self.stateful_metric_names)]
        if verbose:
            _callbacks.append(
                cbks.ProgbarLogger(
                    count_mode='steps',
                    stateful_metrics=self.stateful_metric_names))
        _callbacks += (callbacks or []) + [self.history]
        callbacks = cbks.CallbackList(_callbacks)
    
        # it's possible to callback a different model than self:
        if hasattr(self, 'callback_model') and self.callback_model:
            callback_model = self.callback_model
        else:
            callback_model = self
        callbacks.set_model(callback_model)
        callbacks.set_params({
            'epochs': epochs,
            'steps': steps_per_epoch,
            'verbose': verbose,
            'do_validation': do_validation,
            'metrics': callback_metrics,
        })
        callbacks.on_train_begin()
    
        enqueuer = None
        val_enqueuer = None
    
        try:
            if do_validation:
                if val_gen:
                    if workers > 0:
                        if isinstance(validation_data, Sequence):
                            val_enqueuer = OrderedEnqueuer(
                                validation_data,
                                use_multiprocessing=use_multiprocessing)
                            if validation_steps is None:
                                validation_steps = len(validation_data)
                        else:
                            val_enqueuer = GeneratorEnqueuer(
                                validation_data,
                                use_multiprocessing=use_multiprocessing,
                                wait_time=wait_time)
                        val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                        validation_generator = val_enqueuer.get()
                    else:
                        validation_generator = validation_data
                else:
                    if len(validation_data) == 2:
                        val_x, val_y = validation_data
                        val_sample_weight = None
                    elif len(validation_data) == 3:
                        val_x, val_y, val_sample_weight = validation_data
                    else:
                        raise ValueError('`validation_data` should be a tuple '
                                         '`(val_x, val_y, val_sample_weight)` '
                                         'or `(val_x, val_y)`. Found: ' +
                                         str(validation_data))
                    val_x, val_y, val_sample_weights = self._standardize_user_data(
                        val_x, val_y, val_sample_weight)
                    val_data = val_x + val_y + val_sample_weights
                    if self.uses_learning_phase and not isinstance(K.learning_phase(), int):
                        val_data += [0.]
                    for cbk in callbacks:
                        cbk.validation_data = val_data
    
            if workers > 0:
                if is_sequence:
                    enqueuer = OrderedEnqueuer(generator,
                                               use_multiprocessing=use_multiprocessing,
                                               shuffle=shuffle)
                else:
                    enqueuer = GeneratorEnqueuer(generator,
                                                 use_multiprocessing=use_multiprocessing,
                                                 wait_time=wait_time)
                enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                output_generator = enqueuer.get()
            else:
                output_generator = generator
    
            callback_model.stop_training = False
            # Construct epoch logs.
            epoch_logs = {}
            while epoch < epochs:
                callbacks.on_epoch_begin(epoch)
                steps_done = 0
                batch_index = 0
                while steps_done < steps_per_epoch:
>                   generator_output = next(output_generator)
E                   TypeError: 'DummySequence' object is not an iterator

keras/engine/training.py:2207: TypeError
----------------------------- Captured stdout call -----------------------------
Epoch 1/1

  1/100 [..............................] - ETA: 8s - loss: 27650.4375
 62/100 [=================>............] - ETA: 0s - loss: 25218.5187
100/100 [==============================] - 0s 2ms/step - loss: 23044.2147
Epoch 1/1

  1/100 [..............................] - ETA: 0s - loss: 24263.0176
 22/100 [=====>........................] - ETA: 0s - loss: 17569.0145
 39/100 [==========>...................] - ETA: 0s - loss: 16798.2776
 56/100 [===============>..............] - ETA: 0s - loss: 16359.4252
 73/100 [====================>.........] - ETA: 0s - loss: 15798.0869
 89/100 [=========================>....] - ETA: 0s - loss: 15435.1302
100/100 [==============================] - 0s 3ms/step - loss: 15160.7794
Epoch 1/1

  1/100 [..............................] - ETA: 2s - loss: 8484.2109
 42/100 [===========>..................] - ETA: 0s - loss: 5914.1953
 69/100 [===================>..........] - ETA: 0s - loss: 5666.7333
 98/100 [============================>.] - ETA: 0s - loss: 5446.8838
100/100 [==============================] - 0s 2ms/step - loss: 5418.2114 - val_loss: 6917.5913
Epoch 1/1

  1/100 [..............................] - ETA: 0s - loss: 6917.5913
 33/100 [========>.....................] - ETA: 0s - loss: 4873.9453
 64/100 [==================>...........] - ETA: 0s - loss: 4487.2180
 95/100 [===========================>..] - ETA: 0s - loss: 4265.4640
100/100 [==============================] - 0s 2ms/step - loss: 4206.7954 - val_loss: 5277.9556
Epoch 1/1

  1/100 [..............................] - ETA: 1s - loss: 2197.5913
 37/100 [==========>...................] - ETA: 0s - loss: 3357.9609
 73/100 [====================>.........] - ETA: 0s - loss: 3267.7915
100/100 [==============================] - 0s 2ms/step - loss: 3148.2450 - val_loss: 1845.9846
Epoch 1/1

  1/100 [..............................] - ETA: 0s - loss: 4701.1899
 50/100 [==============>...............] - ETA: 0s - loss: 2468.6257
 99/100 [============================>.] - ETA: 0s - loss: 2420.1147
100/100 [==============================] - 0s 1ms/step - loss: 2413.3806 - val_loss: 1967.6846
Epoch 1/1

  1/100 [..............................] - ETA: 0s - loss: 1928.8369
 50/100 [==============>...............] - ETA: 0s - loss: 2062.6747
 99/100 [============================>.] - ETA: 0s - loss: 2003.2890
100/100 [==============================] - 0s 1ms/step - loss: 2001.7748 - val_loss: 1646.7125
Epoch 1/1

  1/100 [..............................] - ETA: 0s - loss: 1661.9675
 50/100 [==============>...............] - ETA: 0s - loss: 1826.5075
 99/100 [============================>.] - ETA: 0s - loss: 1740.9621
100/100 [==============================] - 0s 1ms/step - loss: 1738.7733 - val_loss: 1700.1621
Epoch 1/1
----------------------------- Captured stderr call -----------------------------
WARNING:tensorflow:From /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2023-09-01 19:16:42.733206: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2023-09-01 19:16:42.752975: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399905000 Hz
2023-09-01 19:16:42.753696: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56319cddd360 executing computations on platform Host. Devices:
2023-09-01 19:16:42.753713: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
------------------------------ Captured log call -------------------------------
WARNING  tensorflow:deprecation.py:323 From /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING  tensorflow:deprecation.py:323 From /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
=============================== warnings summary ===============================
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    _np_qint8 = np.dtype([("qint8", np.int8, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    _np_quint8 = np.dtype([("quint8", np.uint8, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    _np_qint16 = np.dtype([("qint16", np.int16, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    _np_quint16 = np.dtype([("quint16", np.uint16, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    _np_qint32 = np.dtype([("qint32", np.int32, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
    np_resource = np.dtype([("resource", np.ubyte, 1)])

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:823
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:823
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:823: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:824
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:824
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/util/nest.py:824: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    _pywrap_tensorflow.RegisterType("Sequence", _collections.Sequence)

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:312
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:312
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:312: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    class _ListWrapper(List, collections.MutableSequence,

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:546
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:546
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/data_structures.py:546: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    class _DictWrapper(Mapping, collections.MutableMapping):

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    class _ObjectIdentitySet(collections.MutableSet):

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _nlv = LooseVersion(_np_version)

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _np_version_under1p14 = _nlv < LooseVersion("1.14")

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _np_version_under1p15 = _nlv < LooseVersion("1.15")

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:13: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _np_version_under1p16 = _nlv < LooseVersion("1.16")

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:14: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _np_version_under1p17 = _nlv < LooseVersion("1.17")

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:15: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    _np_version_under1p18 = _nlv < LooseVersion("1.18")

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/setuptools/_distutils/version.py:346
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/setuptools/_distutils/version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/function.py:114
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/function.py:114
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/function.py:114
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/function.py:114
  /opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/pandas/compat/numpy/function.py:114: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(_np_version) >= LooseVersion("1.17.0"):

keras/callbacks.py:18
keras/callbacks.py:18
  /home/user/BugsInPy/temp/projects/keras/keras/callbacks.py:18: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    from collections import Iterable

keras/engine/training.py:2088
  /home/user/BugsInPy/temp/projects/keras/keras/engine/training.py:2088: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.
    UserWarning('Using a generator with `use_multiprocessing=True`'

-- Docs: https://docs.pytest.org/en/latest/warnings.html
========================== slowest 10 test durations ===========================
1.59s call     tests/test_multiprocessing.py::test_multiprocessing_training

(0.00 durations hidden.  Use -vv to show these durations.)
=========================== short test summary info ============================
FAILED tests/test_multiprocessing.py::test_multiprocessing_training - TypeErr...
======================== 1 failed, 45 warnings in 3.66s ========================
/opt/conda/envs/50ed758a536fff6a0703bf56934e9ce8/lib/python3.7/site-packages/coverage/control.py:793: CoverageWarning: No data was collected. (no-data-collected)
  self._warn("No data was collected.", slug="no-data-collected")
