[
    {
        "name": "scrapy.spiderloader.SpiderLoader.__init__#17",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.spider_modules = settings.getlist('SPIDER_MODULES')\n        self._spiders = {}\n        for name in self.spider_modules:\n            for module in walk_modules(name):\n                self._load_spiders(module)",
        "begin_line": 17,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader._load_spiders#24",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader._load_spiders(self, module)",
        "snippet": "    def _load_spiders(self, module):\n        for spcls in iter_spider_classes(module):\n            self._spiders[spcls.name] = spcls",
        "begin_line": 24,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.from_settings#29",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings)",
        "begin_line": 29,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.load#32",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.load(self, spider_name)",
        "snippet": "    def load(self, spider_name):\n        \"\"\"\n        Return the Spider class for the given spider name. If the spider\n        name is not found, raise a KeyError.\n        \"\"\"\n        try:\n            return self._spiders[spider_name]\n        except KeyError:\n            raise KeyError(\"Spider not found: {}\".format(spider_name))",
        "begin_line": 32,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.find_by_request#42",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.find_by_request(self, request)",
        "snippet": "    def find_by_request(self, request):\n        \"\"\"\n        Return the list of spider names that can handle the given request.\n        \"\"\"\n        return [name for name, cls in self._spiders.items()\n                if cls.handles_request(request)]",
        "begin_line": 42,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiderloader.SpiderLoader.list#49",
        "src_path": "scrapy/spiderloader.py",
        "class_name": "scrapy.spiderloader.SpiderLoader",
        "signature": "scrapy.spiderloader.SpiderLoader.list(self)",
        "snippet": "    def list(self):\n        \"\"\"\n        Return a list with the names of all spiders available in the project.\n        \"\"\"\n        return list(self._spiders.keys())",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.default.UrlContract.adjust_request_args#16",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.UrlContract",
        "signature": "scrapy.contracts.default.UrlContract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        args['url'] = self.args[0]\n        return args",
        "begin_line": 16,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.__init__#42",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        super(ReturnsContract, self).__init__(*args, **kwargs)\n\n        assert len(self.args) in [1, 2, 3]\n        self.obj_name = self.args[0] or None\n        self.obj_type = self.objects[self.obj_name]\n\n        try:\n            self.min_bound = int(self.args[1])\n        except IndexError:\n            self.min_bound = 1\n\n        try:\n            self.max_bound = int(self.args[2])\n        except IndexError:\n            self.max_bound = float('inf')",
        "begin_line": 42,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.default.ReturnsContract.post_process#59",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ReturnsContract",
        "signature": "scrapy.contracts.default.ReturnsContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        occurrences = 0\n        for x in output:\n            if isinstance(x, self.obj_type):\n                occurrences += 1\n\n        assertion = (self.min_bound <= occurrences <= self.max_bound)\n\n        if not assertion:\n            if self.min_bound == self.max_bound:\n                expected = self.min_bound\n            else:\n                expected = '%s..%s' % (self.min_bound, self.max_bound)\n\n            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n                (occurrences, self.obj_name, expected))",
        "begin_line": 59,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.default.ScrapesContract.post_process#84",
        "src_path": "scrapy/contracts/default.py",
        "class_name": "scrapy.contracts.default.ScrapesContract",
        "signature": "scrapy.contracts.default.ScrapesContract.post_process(self, output)",
        "snippet": "    def post_process(self, output):\n        for x in output:\n            if isinstance(x, (BaseItem, dict)):\n                for arg in self.args:\n                    if not arg in x:\n                        raise ContractFail(\"'%s' field is missing\" % arg)",
        "begin_line": 84,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.ItemMeta.__new__#27",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.ItemMeta",
        "signature": "scrapy.item.ItemMeta.__new__(mcs, class_name, bases, attrs)",
        "snippet": "    def __new__(mcs, class_name, bases, attrs):\n        new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))\n        _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)\n\n        fields = getattr(_class, 'fields', {})\n        new_attrs = {}\n        for n in dir(_class):\n            v = getattr(_class, n)\n            if isinstance(v, Field):\n                fields[n] = v\n            elif n in attrs:\n                new_attrs[n] = attrs[n]\n\n        new_attrs['fields'] = fields\n        new_attrs['_class'] = _class\n        return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)",
        "begin_line": 27,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046948356807511736,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__init__#49",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._values = {}\n        if args or kwargs:  # avoid creating dict for most common case\n            for k, v in six.iteritems(dict(*args, **kwargs)):\n                self[k] = v",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046948356807511736,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__getitem__#55",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return self._values[key]",
        "begin_line": 55,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004380201489268506,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__setitem__#58",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        if key in self.fields:\n            self._values[key] = value\n        else:\n            raise KeyError(\"%s does not support field: %s\" %\n                (self.__class__.__name__, key))",
        "begin_line": 58,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__getattr__#68",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__getattr__(self, name)",
        "snippet": "    def __getattr__(self, name):\n        if name in self.fields:\n            raise AttributeError(\"Use item[%r] to get field value\" % name)\n        raise AttributeError(name)",
        "begin_line": 68,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__setattr__#73",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__setattr__(self, name, value)",
        "snippet": "    def __setattr__(self, name, value):\n        if not name.startswith('_'):\n            raise AttributeError(\"Use item[%r] = %r to set field value\" %\n                (name, value))\n        super(DictItem, self).__setattr__(name, value)",
        "begin_line": 73,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__len__#79",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__len__(self)",
        "snippet": "    def __len__(self):\n        return len(self._values)",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__iter__#82",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__iter__(self)",
        "snippet": "    def __iter__(self):\n        return iter(self._values)",
        "begin_line": 82,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.keys#87",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.keys(self)",
        "snippet": "    def keys(self):\n        return self._values.keys()",
        "begin_line": 87,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.__repr__#90",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return pformat(dict(self))",
        "begin_line": 90,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.item.DictItem.copy#93",
        "src_path": "scrapy/item.py",
        "class_name": "scrapy.item.DictItem",
        "signature": "scrapy.item.DictItem.copy(self)",
        "snippet": "    def copy(self):\n        return self.__class__(self)",
        "begin_line": 93,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.SpiderInfo.__init__#21",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.SpiderInfo",
        "signature": "scrapy.pipelines.media.SpiderInfo.__init__(self, spider)",
        "snippet": "        def __init__(self, spider):\n            self.spider = spider\n            self.downloading = set()\n            self.downloaded = {}\n            self.waiting = defaultdict(list)",
        "begin_line": 21,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.__init__#27",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.__init__(self, download_func=None)",
        "snippet": "    def __init__(self, download_func=None):\n        self.download_func = download_func",
        "begin_line": 27,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.open_spider#39",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        self.spiderinfo = self.SpiderInfo(spider)",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.process_item#42",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.process_item(self, item, spider)",
        "snippet": "    def process_item(self, item, spider):\n        info = self.spiderinfo\n        requests = arg_to_iter(self.get_media_requests(item, info))\n        dlist = [self._process_request(r, info) for r in requests]\n        dfd = DeferredList(dlist, consumeErrors=1)\n        return dfd.addCallback(self.item_completed, item, info)",
        "begin_line": 42,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._process_request#49",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._process_request(self, request, info)",
        "snippet": "    def _process_request(self, request, info):\n        fp = request_fingerprint(request)\n        cb = request.callback or (lambda _: _)\n        eb = request.errback\n        request.callback = None\n        request.errback = None\n\n        # Return cached result if request was already seen\n        if fp in info.downloaded:\n            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)\n\n        # Otherwise, wait for result\n        wad = Deferred().addCallbacks(cb, eb)\n        info.waiting[fp].append(wad)\n\n        # Check if request is downloading right now to avoid doing it twice\n        if fp in info.downloading:\n            return wad\n\n        # Download request checking media_to_download hook output first\n        info.downloading.add(fp)\n        dfd = mustbe_deferred(self.media_to_download, request, info)\n        dfd.addCallback(self._check_media_to_download, request, info)\n        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n        dfd.addErrback(lambda f: logger.error(\n            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})\n        )\n        return dfd.addBoth(lambda _: wad)  # it must return wad at last",
        "begin_line": 49,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._check_media_to_download#78",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._check_media_to_download(self, result, request, info)",
        "snippet": "    def _check_media_to_download(self, result, request, info):\n        if result is not None:\n            return result\n        if self.download_func:\n            # this ugly code was left only to support tests. TODO: remove\n            dfd = mustbe_deferred(self.download_func, request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        else:\n            request.meta['handle_httpstatus_all'] = True\n            dfd = self.crawler.engine.download(request, info.spider)\n            dfd.addCallbacks(\n                callback=self.media_downloaded, callbackArgs=(request, info),\n                errback=self.media_failed, errbackArgs=(request, info))\n        return dfd",
        "begin_line": 78,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters#95",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline._cache_result_and_execute_waiters(self, result, fp, info)",
        "snippet": "    def _cache_result_and_execute_waiters(self, result, fp, info):\n        if isinstance(result, Failure):\n            # minimize cached information for failure\n            result.cleanFailure()\n            result.frames = []\n            result.stack = None\n        info.downloading.remove(fp)\n        info.downloaded[fp] = result  # cache result\n        for wad in info.waiting.pop(fp):\n            defer_result(result).chainDeferred(wad)",
        "begin_line": 95,
        "end_line": 104,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_to_download#107",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_to_download(self, request, info)",
        "snippet": "    def media_to_download(self, request, info):\n        \"\"\"Check request before starting download\"\"\"\n        pass",
        "begin_line": 107,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.get_media_requests#111",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.get_media_requests(self, item, info)",
        "snippet": "    def get_media_requests(self, item, info):\n        \"\"\"Returns the media requests to download\"\"\"\n        pass",
        "begin_line": 111,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_downloaded#115",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_downloaded(self, response, request, info)",
        "snippet": "    def media_downloaded(self, response, request, info):\n        \"\"\"Handler for success downloads\"\"\"\n        return response",
        "begin_line": 115,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005558643690939411,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.media_failed#119",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.media_failed(self, failure, request, info)",
        "snippet": "    def media_failed(self, failure, request, info):\n        \"\"\"Handler for failed downloads\"\"\"\n        return failure",
        "begin_line": 119,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.pipelines.media.MediaPipeline.item_completed#123",
        "src_path": "scrapy/pipelines/media.py",
        "class_name": "scrapy.pipelines.media.MediaPipeline",
        "signature": "scrapy.pipelines.media.MediaPipeline.item_completed(self, results, item, info)",
        "snippet": "    def item_completed(self, results, item, info):\n        \"\"\"Called per item when all media requests has been processed\"\"\"\n        if self.LOG_FAILED_RESULTS:\n            for ok, value in results:\n                if not ok:\n                    logger.error(\n                        '%(class)s found errors processing %(item)s',\n                        {'class': self.__class__.__name__, 'item': item},\n                        exc_info=failure_to_exc_info(value),\n                        extra={'spider': info.spider}\n                    )\n        return item",
        "begin_line": 123,
        "end_line": 134,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__init__#19",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__init__(self, url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding='utf-8', priority=0, dont_filter=False, errback=None)",
        "snippet": "    def __init__(self, url, callback=None, method='GET', headers=None, body=None,\n                 cookies=None, meta=None, encoding='utf-8', priority=0,\n                 dont_filter=False, errback=None):\n\n        self._encoding = encoding  # this one has to be set first\n        self.method = str(method).upper()\n        self._set_url(url)\n        self._set_body(body)\n        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n        self.priority = priority\n\n        assert callback or not errback, \"Cannot use errback without a callback\"\n        self.callback = callback\n        self.errback = errback\n\n        self.cookies = cookies or {}\n        self.headers = Headers(headers or {}, encoding=encoding)\n        self.dont_filter = dont_filter\n\n        self._meta = dict(meta) if meta else None",
        "begin_line": 19,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.037037037037037035,
            "pseudo_dstar_susp": 0.037037037037037035,
            "pseudo_tarantula_susp": 0.037037037037037035,
            "pseudo_op2_susp": 0.037037037037037035,
            "pseudo_barinel_susp": 0.037037037037037035
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.meta#41",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.meta(self)",
        "snippet": "    def meta(self):\n        if self._meta is None:\n            self._meta = {}\n        return self._meta",
        "begin_line": 41,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004175365344467641,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_url#46",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 46,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.05555555555555555,
            "pseudo_dstar_susp": 0.05555555555555555,
            "pseudo_tarantula_susp": 0.05555555555555555,
            "pseudo_op2_susp": 0.05555555555555555,
            "pseudo_barinel_susp": 0.05555555555555555
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_url#49",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if not isinstance(url, six.string_types):\n            raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n\n        url = to_native_str(url, self.encoding)\n        self._url = escape_ajax(safe_url_string(url))\n\n        if ':' not in self._url:\n            raise ValueError('Missing scheme in request url: %s' % self._url)",
        "begin_line": 49,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.025,
            "pseudo_dstar_susp": 0.025,
            "pseudo_tarantula_susp": 0.025,
            "pseudo_op2_susp": 0.025,
            "pseudo_barinel_susp": 0.025
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._get_body#61",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00043233895373973193,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request._set_body#64",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        else:\n            self._body = to_bytes(body, self.encoding)",
        "begin_line": 64,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.05,
            "pseudo_dstar_susp": 0.05,
            "pseudo_tarantula_susp": 0.05,
            "pseudo_op2_susp": 0.05,
            "pseudo_barinel_susp": 0.05
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.encoding#73",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._encoding",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.025,
            "pseudo_dstar_susp": 0.025,
            "pseudo_tarantula_susp": 0.025,
            "pseudo_op2_susp": 0.025,
            "pseudo_barinel_susp": 0.025
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.__str__#76",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %s>\" % (self.method, self.url)",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.copy#81",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Request\"\"\"\n        return self.replace()",
        "begin_line": 81,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.__init__.Request.replace#85",
        "src_path": "scrapy/http/request/__init__.py",
        "class_name": "scrapy.http.request.__init__.Request",
        "signature": "scrapy.http.request.__init__.Request.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Request with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta',\n                  'encoding', 'priority', 'dont_filter', 'callback', 'errback']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 85,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.rpc.XmlRpcRequest.__init__#18",
        "src_path": "scrapy/http/request/rpc.py",
        "class_name": "scrapy.http.request.rpc.XmlRpcRequest",
        "signature": "scrapy.http.request.rpc.XmlRpcRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        encoding = kwargs.get('encoding', None)\n        if 'body' not in kwargs and 'params' in kwargs:\n            kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n            kwargs['body'] = xmlrpclib.dumps(**kw)\n\n        # spec defines that requests must use POST method\n        kwargs.setdefault('method', 'POST')\n\n        # xmlrpc query multiples times over the same url\n        kwargs.setdefault('dont_filter', True)\n\n        # restore encoding\n        if encoding is not None:\n            kwargs['encoding'] = encoding\n\n        super(XmlRpcRequest, self).__init__(*args, **kwargs)\n        self.headers.setdefault('Content-Type', 'text/xml')",
        "begin_line": 18,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__#22",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n            raise NotConfigured\n\n        self.crawler = crawler\n        self._useragent = crawler.settings.get('USER_AGENT')\n        self._parsers = {}",
        "begin_line": 22,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request#34",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_obey_robotstxt'):\n            return\n        rp = self.robot_parser(request, spider)\n        if rp and not rp.can_fetch(self._useragent, request.url):\n            logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                         {'request': request}, extra={'spider': spider})\n            raise IgnoreRequest",
        "begin_line": 34,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser#43",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware.robot_parser(self, request, spider)",
        "snippet": "    def robot_parser(self, request, spider):\n        url = urlparse_cached(request)\n        netloc = url.netloc\n        if netloc not in self._parsers:\n            self._parsers[netloc] = None\n            robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n            robotsreq = Request(\n                robotsurl,\n                priority=self.DOWNLOAD_PRIORITY,\n                meta={'dont_obey_robotstxt': True}\n            )\n            dfd = self.crawler.engine.download(robotsreq, spider)\n            dfd.addCallback(self._parse_robots)\n            dfd.addErrback(self._logerror, robotsreq, spider)\n        return self._parsers[netloc]",
        "begin_line": 43,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots#66",
        "src_path": "scrapy/downloadermiddlewares/robotstxt.py",
        "class_name": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware",
        "signature": "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware._parse_robots(self, response)",
        "snippet": "    def _parse_robots(self, response):\n        rp = robotparser.RobotFileParser(response.url)\n        body = ''\n        if hasattr(response, 'body_as_unicode'):\n            body = response.body_as_unicode()\n        else: # last effort try\n            try:\n                body = response.body.decode('utf-8')\n            except UnicodeDecodeError:\n                # If we found garbage, disregard it:,\n                # but keep the lookup cached (in self._parsers)\n                # Running rp.parse() will set rp state from\n                # 'disallow all' to 'allow any'.\n                pass\n        rp.parse(body.splitlines())\n        self._parsers[urlparse_cached(response).netloc] = rp",
        "begin_line": 66,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.spider.iterate_spider_output#12",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iterate_spider_output(result)",
        "snippet": "def iterate_spider_output(result):\n    return arg_to_iter(result)",
        "begin_line": 12,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.spider.iter_spider_classes#16",
        "src_path": "scrapy/utils/spider.py",
        "class_name": "scrapy.utils.spider",
        "signature": "scrapy.utils.spider.iter_spider_classes(module)",
        "snippet": "def iter_spider_classes(module):\n    \"\"\"Return an iterator over all spider classes defined in the given module\n    that can be instantiated (ie. which have name)\n    \"\"\"\n    # this needs to be imported here until get rid of the spider manager\n    # singleton in scrapy.spider.spiders\n    from scrapy.spiders import Spider\n\n    for obj in six.itervalues(vars(module)):\n        if inspect.isclass(obj) and \\\n           issubclass(obj, Spider) and \\\n           obj.__module__ == module.__name__ and \\\n           getattr(obj, 'name', None):\n            yield obj",
        "begin_line": 16,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest.execute#14",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest.execute(self, args, check_code=True, settings=None)",
        "snippet": "    def execute(self, args, check_code=True, settings=None):\n        env = os.environ.copy()\n        if settings is not None:\n            env['SCRAPY_SETTINGS_MODULE'] = settings\n        cmd = self.prefix + [self.command] + list(args)\n        pp = TestProcessProtocol()\n        pp.deferred.addBoth(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred",
        "begin_line": 14,
        "end_line": 22,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.ProcessTest._process_finished#24",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.ProcessTest",
        "signature": "scrapy.utils.testproc.ProcessTest._process_finished(self, pp, cmd, check_code)",
        "snippet": "    def _process_finished(self, pp, cmd, check_code):\n        if pp.exitcode and check_code:\n            msg = \"process %s exit with code %d\" % (cmd, pp.exitcode)\n            msg += \"\\n>>> stdout <<<\\n%s\" % pp.out\n            msg += \"\\n\"\n            msg += \"\\n>>> stderr <<<\\n%s\" % pp.err\n            raise RuntimeError(msg)\n        return pp.exitcode, pp.out, pp.err",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.__init__#36",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.__init__(self)",
        "snippet": "    def __init__(self):\n        self.deferred = defer.Deferred()\n        self.out = b''\n        self.err = b''\n        self.exitcode = None",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.outReceived#42",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.outReceived(self, data)",
        "snippet": "    def outReceived(self, data):\n        self.out += data",
        "begin_line": 42,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.errReceived#45",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.errReceived(self, data)",
        "snippet": "    def errReceived(self, data):\n        self.err += data",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.testproc.TestProcessProtocol.processEnded#48",
        "src_path": "scrapy/utils/testproc.py",
        "class_name": "scrapy.utils.testproc.TestProcessProtocol",
        "signature": "scrapy.utils.testproc.TestProcessProtocol.processEnded(self, status)",
        "snippet": "    def processEnded(self, status):\n        self.exitcode = status.value.exitCode\n        self.deferred.callback(self)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.common.wrap_loader_context#6",
        "src_path": "scrapy/loader/common.py",
        "class_name": "scrapy.loader.common",
        "signature": "scrapy.loader.common.wrap_loader_context(function, context)",
        "snippet": "def wrap_loader_context(function, context):\n    \"\"\"Wrap functions that receive loader_context to contain the context\n    \"pre-loaded\" and expose a interface that receives only one argument\n    \"\"\"\n    if 'loader_context' in get_func_args(function):\n        return partial(function, loader_context=context)\n    else:\n        return function",
        "begin_line": 6,
        "end_line": 13,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__#17",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.__init__(self, debug=False)",
        "snippet": "    def __init__(self, debug=False):\n        self.jars = defaultdict(CookieJar)\n        self.debug = debug",
        "begin_line": 17,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler#22",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('COOKIES_ENABLED'):\n            raise NotConfigured\n        return cls(crawler.settings.getbool('COOKIES_DEBUG'))",
        "begin_line": 22,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request#27",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_request(self, request, spider)",
        "snippet": "    def process_request(self, request, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return\n\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        cookies = self._get_request_cookies(jar, request)\n        for cookie in cookies:\n            jar.set_cookie_if_ok(cookie, request)\n\n        # set Cookie header\n        request.headers.pop('Cookie', None)\n        jar.add_cookie_header(request)\n        self._debug_cookie(request, spider)",
        "begin_line": 27,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response#42",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if request.meta.get('dont_merge_cookies', False):\n            return response\n\n        # extract cookies from Set-Cookie and drop invalid/expired cookies\n        cookiejarkey = request.meta.get(\"cookiejar\")\n        jar = self.jars[cookiejarkey]\n        jar.extract_cookies(response, request)\n        self._debug_set_cookie(response, spider)\n\n        return response",
        "begin_line": 42,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie#54",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_cookie(self, request, spider)",
        "snippet": "    def _debug_cookie(self, request, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in request.headers.getlist('Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Sending cookies to: {}\\n{}\".format(request, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 54,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie#63",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._debug_set_cookie(self, response, spider)",
        "snippet": "    def _debug_set_cookie(self, response, spider):\n        if self.debug:\n            cl = [to_native_str(c, errors='replace')\n                  for c in response.headers.getlist('Set-Cookie')]\n            if cl:\n                cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n                msg = \"Received cookies from: {}\\n{}\".format(response, cookies)\n                logger.debug(msg, extra={'spider': spider})",
        "begin_line": 63,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie#72",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._format_cookie(self, cookie)",
        "snippet": "    def _format_cookie(self, cookie):\n        # build cookie string\n        cookie_str = '%s=%s' % (cookie['name'], cookie['value'])\n\n        if cookie.get('path', None):\n            cookie_str += '; Path=%s' % cookie['path']\n        if cookie.get('domain', None):\n            cookie_str += '; Domain=%s' % cookie['domain']\n\n        return cookie_str",
        "begin_line": 72,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies#83",
        "src_path": "scrapy/downloadermiddlewares/cookies.py",
        "class_name": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware",
        "signature": "scrapy.downloadermiddlewares.cookies.CookiesMiddleware._get_request_cookies(self, jar, request)",
        "snippet": "    def _get_request_cookies(self, jar, request):\n        if isinstance(request.cookies, dict):\n            cookie_list = [{'name': k, 'value': v} for k, v in \\\n                    six.iteritems(request.cookies)]\n        else:\n            cookie_list = request.cookies\n\n        cookies = [self._format_cookie(x) for x in cookie_list]\n        headers = {'Set-Cookie': cookies}\n        response = Response(request.url, headers=headers)\n\n        return jar.make_cookies(response, request)",
        "begin_line": 83,
        "end_line": 94,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.__init__#22",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        self._encoding = kwargs.pop('encoding', None)\n        self._cached_benc = None\n        self._cached_ubody = None\n        self._cached_selector = None\n        super(TextResponse, self).__init__(*args, **kwargs)",
        "begin_line": 22,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00040241448692152917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_url#29",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, six.text_type):\n            if six.PY2 and self.encoding is None:\n                raise TypeError(\"Cannot convert unicode url - %s \"\n                                \"has no encoding\" % type(self).__name__)\n            self._url = to_native_str(url, self.encoding)\n        else:\n            super(TextResponse, self)._set_url(url)",
        "begin_line": 29,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00040241448692152917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._set_body#38",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        self._body = b''  # used by encoding detection\n        if isinstance(body, six.text_type):\n            if self._encoding is None:\n                raise TypeError('Cannot convert unicode body - %s has no encoding' %\n                    type(self).__name__)\n            self._body = body.encode(self._encoding)\n        else:\n            super(TextResponse, self)._set_body(body)",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.replace#48",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        kwargs.setdefault('encoding', self.encoding)\n        return Response.replace(self, *args, **kwargs)",
        "begin_line": 48,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.encoding#53",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.encoding(self)",
        "snippet": "    def encoding(self):\n        return self._declared_encoding() or self._body_inferred_encoding()",
        "begin_line": 53,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00040096230954290296,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._declared_encoding#56",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._declared_encoding(self)",
        "snippet": "    def _declared_encoding(self):\n        return self._encoding or self._headers_encoding() \\\n            or self._body_declared_encoding()",
        "begin_line": 56,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004098360655737705,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.body_as_unicode#60",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.body_as_unicode(self)",
        "snippet": "    def body_as_unicode(self):\n        \"\"\"Return body as unicode\"\"\"\n        # check for self.encoding before _cached_ubody just in\n        # _body_inferred_encoding is called\n        benc = self.encoding\n        if self._cached_ubody is None:\n            charset = 'charset=%s' % benc\n            self._cached_ubody = html_to_unicode(charset, self.body)[1]\n        return self._cached_ubody",
        "begin_line": 60,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004297378599054577,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.urljoin#70",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(get_base_url(self), url)",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._headers_encoding#76",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._headers_encoding(self)",
        "snippet": "    def _headers_encoding(self):\n        content_type = self.headers.get(b'Content-Type', b'')\n        return http_content_type_encoding(to_native_str(content_type))",
        "begin_line": 76,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004098360655737705,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_inferred_encoding#80",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_inferred_encoding(self)",
        "snippet": "    def _body_inferred_encoding(self):\n        if self._cached_benc is None:\n            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n            benc, ubody = html_to_unicode(content_type, self.body,\n                    auto_detect_fun=self._auto_detect_fun,\n                    default_encoding=self._DEFAULT_ENCODING)\n            self._cached_benc = benc\n            self._cached_ubody = ubody\n        return self._cached_benc",
        "begin_line": 80,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004139072847682119,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._auto_detect_fun#90",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._auto_detect_fun(self, text)",
        "snippet": "    def _auto_detect_fun(self, text):\n        for enc in (self._DEFAULT_ENCODING, 'utf-8', 'cp1252'):\n            try:\n                text.decode(enc)\n            except UnicodeError:\n                continue\n            return resolve_encoding(enc)",
        "begin_line": 90,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse._body_declared_encoding#99",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse._body_declared_encoding(self)",
        "snippet": "    def _body_declared_encoding(self):\n        return html_body_declared_encoding(self.body)",
        "begin_line": 99,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004098360655737705,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.selector#103",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.selector(self)",
        "snippet": "    def selector(self):\n        from scrapy.selector import Selector\n        if self._cached_selector is None:\n            self._cached_selector = Selector(self)\n        return self._cached_selector",
        "begin_line": 103,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.xpath#109",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.xpath(self, query)",
        "snippet": "    def xpath(self, query):\n        return self.selector.xpath(query)",
        "begin_line": 109,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.text.TextResponse.css#112",
        "src_path": "scrapy/http/response/text.py",
        "class_name": "scrapy.http.response.text.TextResponse",
        "signature": "scrapy.http.response.text.TextResponse.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.selector.css(query)",
        "begin_line": 112,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.serialize.ScrapyJSONEncoder.default#16",
        "src_path": "scrapy/utils/serialize.py",
        "class_name": "scrapy.utils.serialize.ScrapyJSONEncoder",
        "signature": "scrapy.utils.serialize.ScrapyJSONEncoder.default(self, o)",
        "snippet": "    def default(self, o):\n        if isinstance(o, datetime.datetime):\n            return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n        elif isinstance(o, datetime.date):\n            return o.strftime(self.DATE_FORMAT)\n        elif isinstance(o, datetime.time):\n            return o.strftime(self.TIME_FORMAT)\n        elif isinstance(o, decimal.Decimal):\n            return str(o)\n        elif isinstance(o, defer.Deferred):\n            return str(o)\n        elif isinstance(o, BaseItem):\n            return dict(o)\n        elif isinstance(o, Request):\n            return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n        elif isinstance(o, Response):\n            return \"<%s %s %s>\" % (type(o).__name__, o.status, o.url)\n        else:\n            return super(ScrapyJSONEncoder, self).default(o)",
        "begin_line": 16,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.__init__#8",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.__init__(self, sender=dispatcher.Anonymous)",
        "snippet": "    def __init__(self, sender=dispatcher.Anonymous):\n        self.sender = sender",
        "begin_line": 8,
        "end_line": 9,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.signalmanager.SignalManager.connect#11",
        "src_path": "scrapy/signalmanager.py",
        "class_name": "scrapy.signalmanager.SignalManager",
        "signature": "scrapy.signalmanager.SignalManager.connect(self, receiver, signal, **kwargs)",
        "snippet": "    def connect(self, receiver, signal, **kwargs):\n        \"\"\"\n        Connect a receiver function to a signal.\n\n        The signal can be any object, although Scrapy comes with some\n        predefined signals that are documented in the :ref:`topics-signals`\n        section.\n\n        :param receiver: the function to be connected\n        :type receiver: callable\n\n        :param signal: the signal to connect to\n        :type signal: object\n        \"\"\"\n        kwargs.setdefault('sender', self.sender)\n        return dispatcher.connect(receiver, signal, **kwargs)",
        "begin_line": 11,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.__init__#16",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n\n        self.close_on = {\n            'timeout': crawler.settings.getfloat('CLOSESPIDER_TIMEOUT'),\n            'itemcount': crawler.settings.getint('CLOSESPIDER_ITEMCOUNT'),\n            'pagecount': crawler.settings.getint('CLOSESPIDER_PAGECOUNT'),\n            'errorcount': crawler.settings.getint('CLOSESPIDER_ERRORCOUNT'),\n            }\n\n        self.counter = defaultdict(int)\n\n        if self.close_on.get('errorcount'):\n            crawler.signals.connect(self.error_count, signal=signals.spider_error)\n        if self.close_on.get('pagecount'):\n            crawler.signals.connect(self.page_count, signal=signals.response_received)\n        if self.close_on.get('timeout'):\n            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n        if self.close_on.get('itemcount'):\n            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)",
        "begin_line": 16,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.from_crawler#39",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 39,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.error_count#42",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.error_count(self, failure, response, spider)",
        "snippet": "    def error_count(self, failure, response, spider):\n        self.counter['errorcount'] += 1\n        if self.counter['errorcount'] == self.close_on['errorcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_errorcount')",
        "begin_line": 42,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.page_count#47",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.page_count(self, response, request, spider)",
        "snippet": "    def page_count(self, response, request, spider):\n        self.counter['pagecount'] += 1\n        if self.counter['pagecount'] == self.close_on['pagecount']:\n            self.crawler.engine.close_spider(spider, 'closespider_pagecount')",
        "begin_line": 47,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_opened#52",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.task = reactor.callLater(self.close_on['timeout'], \\\n            self.crawler.engine.close_spider, spider, \\\n            reason='closespider_timeout')",
        "begin_line": 52,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.item_scraped#57",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        self.counter['itemcount'] += 1\n        if self.counter['itemcount'] == self.close_on['itemcount']:\n            self.crawler.engine.close_spider(spider, 'closespider_itemcount')",
        "begin_line": 57,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.closespider.CloseSpider.spider_closed#62",
        "src_path": "scrapy/extensions/closespider.py",
        "class_name": "scrapy.extensions.closespider.CloseSpider",
        "signature": "scrapy.extensions.closespider.CloseSpider.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        task = getattr(self, 'task', False)\n        if task and task.active():\n            task.cancel()",
        "begin_line": 62,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender.__init__#29",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.__init__(self, smtphost='localhost', mailfrom='scrapy@localhost', smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False)",
        "snippet": "    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n        self.smtphost = smtphost\n        self.smtpport = smtpport\n        self.smtpuser = smtpuser\n        self.smtppass = smtppass\n        self.smtptls = smtptls\n        self.smtpssl = smtpssl\n        self.mailfrom = mailfrom\n        self.debug = debug",
        "begin_line": 29,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender.from_settings#41",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.from_settings(cls, settings)",
        "snippet": "    def from_settings(cls, settings):\n        return cls(settings['MAIL_HOST'], settings['MAIL_FROM'], settings['MAIL_USER'],\n            settings['MAIL_PASS'], settings.getint('MAIL_PORT'),\n            settings.getbool('MAIL_TLS'), settings.getbool('MAIL_SSL'))",
        "begin_line": 41,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender.send#46",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender.send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None)",
        "snippet": "    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None):\n        if attachs:\n            msg = MIMEMultipart()\n        else:\n            msg = MIMENonMultipart(*mimetype.split('/', 1))\n        msg['From'] = self.mailfrom\n        msg['To'] = COMMASPACE.join(to)\n        msg['Date'] = formatdate(localtime=True)\n        msg['Subject'] = subject\n        rcpts = to[:]\n        if cc:\n            rcpts.extend(cc)\n            msg['Cc'] = COMMASPACE.join(cc)\n\n        if attachs:\n            msg.attach(MIMEText(body))\n            for attach_name, mimetype, f in attachs:\n                part = MIMEBase(*mimetype.split('/'))\n                part.set_payload(f.read())\n                Encoders.encode_base64(part)\n                part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' \\\n                    % attach_name)\n                msg.attach(part)\n        else:\n            msg.set_payload(body)\n\n        if _callback:\n            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n\n        if self.debug:\n            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                         'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                         {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                          'mailattachs': len(attachs)})\n            return\n\n        dfd = self._sendmail(rcpts, msg.as_string())\n        dfd.addCallbacks(self._sent_ok, self._sent_failed,\n            callbackArgs=[to, cc, subject, len(attachs)],\n            errbackArgs=[to, cc, subject, len(attachs)])\n        reactor.addSystemEventTrigger('before', 'shutdown', lambda: dfd)\n        return dfd",
        "begin_line": 46,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender._sent_ok#89",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender._sent_ok(self, result, to, cc, subject, nattachs)",
        "snippet": "    def _sent_ok(self, result, to, cc, subject, nattachs):\n        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '\n                    'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n                    {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                     'mailattachs': nattachs})",
        "begin_line": 89,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender._sent_failed#95",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender._sent_failed(self, failure, to, cc, subject, nattachs)",
        "snippet": "    def _sent_failed(self, failure, to, cc, subject, nattachs):\n        errstr = str(failure.value)\n        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '\n                     'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d'\n                     '- %(mailerr)s',\n                     {'mailto': to, 'mailcc': cc, 'mailsubject': subject,\n                      'mailattachs': nattachs, 'mailerr': errstr})",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.mail.MailSender._sendmail#103",
        "src_path": "scrapy/mail.py",
        "class_name": "scrapy.mail.MailSender",
        "signature": "scrapy.mail.MailSender._sendmail(self, to_addrs, msg)",
        "snippet": "    def _sendmail(self, to_addrs, msg):\n        # Import twisted.mail here because it is not available in python3\n        from twisted.mail.smtp import ESMTPSenderFactory\n        msg = StringIO(msg)\n        d = defer.Deferred()\n        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n            to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n            requireTransportSecurity=self.smtptls)\n        factory.noisy = False\n\n        if self.smtpssl:\n            reactor.connectSSL(self.smtphost, self.smtpport, factory, ssl.ClientContextFactory())\n        else:\n            reactor.connectTCP(self.smtphost, self.smtpport, factory)\n\n        return d",
        "begin_line": 103,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log#20",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n    Failures instead of exceptions.\n    \"\"\"\n    dont_log = named.pop('dont_log', _IgnoredException)\n    spider = named.get('spider', None)\n    responses = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        try:\n            response = robustApply(receiver, signal=signal, sender=sender,\n                *arguments, **named)\n            if isinstance(response, Deferred):\n                logger.error(\"Cannot return deferreds from signal handler: %(receiver)s\",\n                             {'receiver': receiver}, extra={'spider': spider})\n        except dont_log:\n            result = Failure()\n        except Exception:\n            result = Failure()\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': receiver},\n                         exc_info=True, extra={'spider': spider})\n        else:\n            result = response\n        responses.append((receiver, result))\n    return responses",
        "begin_line": 20,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.signal.send_catch_log_deferred#47",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named)",
        "snippet": "def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n    Returns a deferred that gets fired once all signal handlers deferreds were\n    fired.\n    \"\"\"\n    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure\n\n    dont_log = named.pop('dont_log', None)\n    spider = named.get('spider', None)\n    dfds = []\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n                *arguments, **named)\n        d.addErrback(logerror, receiver)\n        d.addBoth(lambda result: (receiver, result))\n        dfds.append(d)\n    d = DeferredList(dfds)\n    d.addCallback(lambda out: [x[1] for x in out])\n    return d",
        "begin_line": 47,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.signal.logerror#52",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.logerror(failure, recv)",
        "snippet": "    def logerror(failure, recv):\n        if dont_log is None or not isinstance(failure.value, dont_log):\n            logger.error(\"Error caught on signal handler: %(receiver)s\",\n                         {'receiver': recv},\n                         exc_info=failure_to_exc_info(failure),\n                         extra={'spider': spider})\n        return failure",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.signal.disconnect_all#74",
        "src_path": "scrapy/utils/signal.py",
        "class_name": "scrapy.utils.signal",
        "signature": "scrapy.utils.signal.disconnect_all(signal=Any, sender=Any)",
        "snippet": "def disconnect_all(signal=Any, sender=Any):\n    \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n    tests\n    \"\"\"\n    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n        disconnect(receiver, signal=signal, sender=sender)",
        "begin_line": 74,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.trackref.object_ref.__new__#28",
        "src_path": "scrapy/utils/trackref.py",
        "class_name": "scrapy.utils.trackref.object_ref",
        "signature": "scrapy.utils.trackref.object_ref.__new__(cls, *args, **kwargs)",
        "snippet": "    def __new__(cls, *args, **kwargs):\n        obj = object.__new__(cls)\n        live_refs[cls][obj] = time()\n        return obj",
        "begin_line": 28,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.01639344262295082,
            "pseudo_dstar_susp": 0.01639344262295082,
            "pseudo_tarantula_susp": 0.01639344262295082,
            "pseudo_op2_susp": 0.01639344262295082,
            "pseudo_barinel_susp": 0.01639344262295082
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.__init__#31",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.__init__(self, value, priority)",
        "snippet": "    def __init__(self, value, priority):\n        self.value = value\n        self.priority = priority",
        "begin_line": 31,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004253509145044662,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.SettingsAttribute.set#35",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.SettingsAttribute",
        "signature": "scrapy.settings.__init__.SettingsAttribute.set(self, value, priority)",
        "snippet": "    def set(self, value, priority):\n        \"\"\"Sets value if priority is higher or equal than current priority.\"\"\"\n        if priority >= self.priority:\n            self.value = value\n            self.priority = priority",
        "begin_line": 35,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004422821760283061,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__init__#50",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__init__(self, values=None, priority='project')",
        "snippet": "    def __init__(self, values=None, priority='project'):\n        self.frozen = False\n        self.attributes = {}\n        self.setmodule(default_settings, priority='default')\n        if values is not None:\n            self.setdict(values, priority)",
        "begin_line": 50,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.__getitem__#57",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        value = None\n        if opt_name in self.attributes:\n            value = self.attributes[opt_name].value\n        return value",
        "begin_line": 57,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000444247001332741,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.get#63",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.get(self, name, default=None)",
        "snippet": "    def get(self, name, default=None):\n        return self[name] if self[name] is not None else default",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000444247001332741,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getbool#66",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getbool(self, name, default=False)",
        "snippet": "    def getbool(self, name, default=False):\n        \"\"\"\n        True is: 1, '1', True\n        False is: 0, '0', False, None\n        \"\"\"\n        return bool(int(self.get(name, default)))",
        "begin_line": 66,
        "end_line": 71,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getint#73",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getint(self, name, default=0)",
        "snippet": "    def getint(self, name, default=0):\n        return int(self.get(name, default))",
        "begin_line": 73,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getfloat#76",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getfloat(self, name, default=0.0)",
        "snippet": "    def getfloat(self, name, default=0.0):\n        return float(self.get(name, default))",
        "begin_line": 76,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getlist#79",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getlist(self, name, default=None)",
        "snippet": "    def getlist(self, name, default=None):\n        value = self.get(name, default or [])\n        if isinstance(value, six.string_types):\n            value = value.split(',')\n        return list(value)",
        "begin_line": 79,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.getdict#85",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.getdict(self, name, default=None)",
        "snippet": "    def getdict(self, name, default=None):\n        value = self.get(name, default or {})\n        if isinstance(value, six.string_types):\n            value = json.loads(value)\n        return dict(value)",
        "begin_line": 85,
        "end_line": 89,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.set#91",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.set(self, name, value, priority='project')",
        "snippet": "    def set(self, name, value, priority='project'):\n        self._assert_mutability()\n        if isinstance(priority, six.string_types):\n            priority = SETTINGS_PRIORITIES[priority]\n        if name not in self.attributes:\n            self.attributes[name] = SettingsAttribute(value, priority)\n        else:\n            self.attributes[name].set(value, priority)",
        "begin_line": 91,
        "end_line": 98,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000444247001332741,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.setdict#100",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.setdict(self, values, priority='project')",
        "snippet": "    def setdict(self, values, priority='project'):\n        self._assert_mutability()\n        for name, value in six.iteritems(values):\n            self.set(name, value, priority)",
        "begin_line": 100,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00047732696897374703,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.setmodule#105",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.setmodule(self, module, priority='project')",
        "snippet": "    def setmodule(self, module, priority='project'):\n        self._assert_mutability()\n        if isinstance(module, six.string_types):\n            module = import_module(module)\n        for key in dir(module):\n            if key.isupper():\n                self.set(key, getattr(module, key), priority)",
        "begin_line": 105,
        "end_line": 111,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings._assert_mutability#113",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings._assert_mutability(self)",
        "snippet": "    def _assert_mutability(self):\n        if self.frozen:\n            raise TypeError(\"Trying to modify an immutable Settings object\")",
        "begin_line": 113,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.copy#117",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.copy(self)",
        "snippet": "    def copy(self):\n        return copy.deepcopy(self)",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.freeze#120",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.freeze(self)",
        "snippet": "    def freeze(self):\n        self.frozen = True",
        "begin_line": 120,
        "end_line": 121,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.frozencopy#123",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.frozencopy(self)",
        "snippet": "    def frozencopy(self):\n        copy = self.copy()\n        copy.freeze()\n        return copy",
        "begin_line": 123,
        "end_line": 126,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.overrides#129",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.overrides(self)",
        "snippet": "    def overrides(self):\n        warnings.warn(\"`Settings.overrides` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='cmdline')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._overrides\n        except AttributeError:\n            self._overrides = o = _DictProxy(self, 'cmdline')\n        return o",
        "begin_line": 129,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.Settings.defaults#141",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.Settings",
        "signature": "scrapy.settings.__init__.Settings.defaults(self)",
        "snippet": "    def defaults(self):\n        warnings.warn(\"`Settings.defaults` attribute is deprecated and won't \"\n                      \"be supported in Scrapy 0.26, use \"\n                      \"`Settings.set(name, value, priority='default')` instead\",\n                      category=ScrapyDeprecationWarning, stacklevel=2)\n        try:\n            o = self._defaults\n        except AttributeError:\n            self._defaults = o = _DictProxy(self, 'default')\n        return o",
        "begin_line": 141,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__init__#155",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__init__(self, settings, priority)",
        "snippet": "    def __init__(self, settings, priority):\n        self.o = {}\n        self.settings = settings\n        self.priority = priority",
        "begin_line": 155,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__getitem__#163",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__getitem__(self, k)",
        "snippet": "    def __getitem__(self, k):\n        return self.o[k]",
        "begin_line": 163,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__._DictProxy.__setitem__#166",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__._DictProxy",
        "signature": "scrapy.settings.__init__._DictProxy.__setitem__(self, k, v)",
        "snippet": "    def __setitem__(self, k, v):\n        self.settings.set(k, v, priority=self.priority)\n        self.o[k] = v",
        "begin_line": 166,
        "end_line": 168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__init__#179",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__init__(self, settings_module=None, **kw)",
        "snippet": "    def __init__(self, settings_module=None, **kw):\n        Settings.__init__(self, **kw)\n        self.settings_module = settings_module",
        "begin_line": 179,
        "end_line": 181,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.settings.__init__.CrawlerSettings.__getitem__#183",
        "src_path": "scrapy/settings/__init__.py",
        "class_name": "scrapy.settings.__init__.CrawlerSettings",
        "signature": "scrapy.settings.__init__.CrawlerSettings.__getitem__(self, opt_name)",
        "snippet": "    def __getitem__(self, opt_name):\n        if opt_name in self.overrides:\n            return self.overrides[opt_name]\n        if self.settings_module and hasattr(self.settings_module, opt_name):\n            return getattr(self.settings_module, opt_name)\n        if opt_name in self.defaults:\n            return self.defaults[opt_name]\n        return Settings.__getitem__(self, opt_name)",
        "begin_line": 183,
        "end_line": 190,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.gz.gunzip#10",
        "src_path": "scrapy/utils/gz.py",
        "class_name": "scrapy.utils.gz",
        "signature": "scrapy.utils.gz.gunzip(data)",
        "snippet": "def gunzip(data):\n    \"\"\"Gunzip the given data and return as much data as possible.\n\n    This is resilient to CRC checksum errors.\n    \"\"\"\n    f = GzipFile(fileobj=BytesIO(data))\n    output = b''\n    chunk = b'.'\n    while chunk:\n        try:\n            chunk = f.read(8196)\n            output += chunk\n        except (IOError, EOFError, struct.error):\n            # complete only if there is some data, otherwise re-raise\n            # see issue 87 about catching struct.error\n            # some pages are quite small so output is '' and f.extrabuf\n            # contains the whole page content\n            if output or f.extrabuf:\n                output += f.extrabuf\n                break\n            else:\n                raise\n    return output",
        "begin_line": 10,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.__init__#16",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.__init__(self, *middlewares)",
        "snippet": "    def __init__(self, *middlewares):\n        self.middlewares = middlewares\n        self.methods = defaultdict(list)\n        for mw in middlewares:\n            self._add_middleware(mw)",
        "begin_line": 16,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_settings#27",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_settings(cls, settings, crawler=None)",
        "snippet": "    def from_settings(cls, settings, crawler=None):\n        mwlist = cls._get_mwlist_from_settings(settings)\n        middlewares = []\n        for clspath in mwlist:\n            try:\n                mwcls = load_object(clspath)\n                if crawler and hasattr(mwcls, 'from_crawler'):\n                    mw = mwcls.from_crawler(crawler)\n                elif hasattr(mwcls, 'from_settings'):\n                    mw = mwcls.from_settings(settings)\n                else:\n                    mw = mwcls()\n                middlewares.append(mw)\n            except NotConfigured as e:\n                if e.args:\n                    clsname = clspath.split('.')[-1]\n                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n                                   {'clsname': clsname, 'eargs': e.args[0]},\n                                   extra={'crawler': crawler})\n\n        enabled = [x.__class__.__name__ for x in middlewares]\n        logger.info(\"Enabled %(componentname)ss: %(enabledlist)s\",\n                    {'componentname': cls.component_name,\n                     'enabledlist': ', '.join(enabled)},\n                    extra={'crawler': crawler})\n        return cls(*middlewares)",
        "begin_line": 27,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager.from_crawler#55",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls.from_settings(crawler.settings, crawler)",
        "begin_line": 55,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.middleware.MiddlewareManager._add_middleware#58",
        "src_path": "scrapy/middleware.py",
        "class_name": "scrapy.middleware.MiddlewareManager",
        "signature": "scrapy.middleware.MiddlewareManager._add_middleware(self, mw)",
        "snippet": "    def _add_middleware(self, mw):\n        if hasattr(mw, 'open_spider'):\n            self.methods['open_spider'].append(mw.open_spider)\n        if hasattr(mw, 'close_spider'):\n            self.methods['close_spider'].insert(0, mw.close_spider)",
        "begin_line": 58,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__init__#14",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__init__(self, xmltext)",
        "snippet": "    def __init__(self, xmltext):\n        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)\n        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n        rt = self._root.tag\n        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.sitemap.Sitemap.__iter__#20",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap.Sitemap",
        "signature": "scrapy.utils.sitemap.Sitemap.__iter__(self)",
        "snippet": "    def __iter__(self):\n        for elem in self._root.getchildren():\n            d = {}\n            for el in elem.getchildren():\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n\n            if 'loc' in d:\n                yield d",
        "begin_line": 20,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.sitemap.sitemap_urls_from_robots#37",
        "src_path": "scrapy/utils/sitemap.py",
        "class_name": "scrapy.utils.sitemap",
        "signature": "scrapy.utils.sitemap.sitemap_urls_from_robots(robots_text)",
        "snippet": "def sitemap_urls_from_robots(robots_text):\n    \"\"\"Return an iterator over all sitemap urls contained in the given\n    robots.txt file\n    \"\"\"\n    for line in robots_text.splitlines():\n        if line.lstrip().startswith('Sitemap:'):\n            yield line.split(':', 1)[1].strip()",
        "begin_line": 37,
        "end_line": 43,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__init__#14",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.default_loader_context = default_loader_context",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.MapCompose.__call__#18",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.MapCompose",
        "signature": "scrapy.loader.processors.MapCompose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        values = arg_to_iter(value)\n        if loader_context:\n            context = MergeDict(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            next_values = []\n            for v in values:\n                next_values += arg_to_iter(func(v))\n            values = next_values\n        return values",
        "begin_line": 18,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__init__#35",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__init__(self, *functions, **default_loader_context)",
        "snippet": "    def __init__(self, *functions, **default_loader_context):\n        self.functions = functions\n        self.stop_on_none = default_loader_context.get('stop_on_none', True)\n        self.default_loader_context = default_loader_context",
        "begin_line": 35,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.Compose.__call__#40",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Compose",
        "signature": "scrapy.loader.processors.Compose.__call__(self, value, loader_context=None)",
        "snippet": "    def __call__(self, value, loader_context=None):\n        if loader_context:\n            context = MergeDict(loader_context, self.default_loader_context)\n        else:\n            context = self.default_loader_context\n        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n        for func in wrapped_funcs:\n            if value is None and self.stop_on_none:\n                break\n            value = func(value)\n        return value",
        "begin_line": 40,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.TakeFirst.__call__#55",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.TakeFirst",
        "signature": "scrapy.loader.processors.TakeFirst.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        for value in values:\n            if value is not None and value != '':\n                return value",
        "begin_line": 55,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.Identity.__call__#63",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Identity",
        "signature": "scrapy.loader.processors.Identity.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return values",
        "begin_line": 63,
        "end_line": 64,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00042354934349851756,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__init__#74",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__init__(self, json_path)",
        "snippet": "    def __init__(self, json_path):\n        self.json_path = json_path\n        import jmespath\n        self.compiled_path = jmespath.compile(self.json_path)",
        "begin_line": 74,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.SelectJmes.__call__#79",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.SelectJmes",
        "signature": "scrapy.loader.processors.SelectJmes.__call__(self, value)",
        "snippet": "    def __call__(self, value):\n        \"\"\"Query value for the jmespath query and return answer\n        :param value: a data structure (dict, list) to extract from\n        :return: Element extracted according to jmespath query\n        \"\"\"\n        return self.compiled_path.search(value)",
        "begin_line": 79,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__init__#89",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__init__(self, separator=u' ')",
        "snippet": "    def __init__(self, separator=u' '):\n        self.separator = separator",
        "begin_line": 89,
        "end_line": 90,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.processors.Join.__call__#92",
        "src_path": "scrapy/loader/processors.py",
        "class_name": "scrapy.loader.processors.Join",
        "signature": "scrapy.loader.processors.Join.__call__(self, values)",
        "snippet": "    def __call__(self, values):\n        return self.separator.join(values)",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.__init__#10",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.from_crawler#14",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n        crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n        crawler.signals.connect(o.response_received, signal=signals.response_received)\n        return o",
        "begin_line": 14,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_opened#23",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)",
        "begin_line": 23,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.spider_closed#26",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        self.stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n        self.stats.set_value('finish_reason', reason, spider=spider)",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.item_scraped#30",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.item_scraped(self, item, spider)",
        "snippet": "    def item_scraped(self, item, spider):\n        self.stats.inc_value('item_scraped_count', spider=spider)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.response_received#33",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.response_received(self, spider)",
        "snippet": "    def response_received(self, spider):\n        self.stats.inc_value('response_received_count', spider=spider)",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.corestats.CoreStats.item_dropped#36",
        "src_path": "scrapy/extensions/corestats.py",
        "class_name": "scrapy.extensions.corestats.CoreStats",
        "signature": "scrapy.extensions.corestats.CoreStats.item_dropped(self, item, spider, exception)",
        "snippet": "    def item_dropped(self, item, spider, exception):\n        reason = exception.__class__.__name__\n        self.stats.inc_value('item_dropped_count', spider=spider)\n        self.stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)",
        "begin_line": 36,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.__init__#14",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.__init__(self, contracts)",
        "snippet": "    def __init__(self, contracts):\n        for contract in contracts:\n            self.contracts[contract.name] = contract",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.extract_contracts#27",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.extract_contracts(self, method)",
        "snippet": "    def extract_contracts(self, method):\n        contracts = []\n        for line in method.__doc__.split('\\n'):\n            line = line.strip()\n\n            if line.startswith('@'):\n                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n                args = re.split(r'\\s+', args)\n\n                contracts.append(self.contracts[name](method, *args))\n\n        return contracts",
        "begin_line": 27,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.from_method#48",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.from_method(self, method, results)",
        "snippet": "    def from_method(self, method, results):\n        contracts = self.extract_contracts(method)\n        if contracts:\n            # calculate request args\n            args, kwargs = get_spec(Request.__init__)\n            kwargs['callback'] = method\n            for contract in contracts:\n                kwargs = contract.adjust_request_args(kwargs)\n\n            # create and prepare request\n            args.remove('self')\n            if set(args).issubset(set(kwargs)):\n                request = Request(**kwargs)\n\n                # execute pre and post hooks in order\n                for contract in reversed(contracts):\n                    request = contract.add_pre_hook(request, results)\n                for contract in contracts:\n                    request = contract.add_post_hook(request, results)\n\n                self._clean_req(request, method, results)\n                return request",
        "begin_line": 48,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager._clean_req#71",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager._clean_req(self, request, method, results)",
        "snippet": "    def _clean_req(self, request, method, results):\n        \"\"\" stop the request from returning objects and records any errors \"\"\"\n\n        cb = request.callback\n\n        @wraps(cb)\n        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())\n\n        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)\n\n        request.callback = cb_wrapper\n        request.errback = eb_wrapper",
        "begin_line": 71,
        "end_line": 91,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.cb_wrapper#77",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.cb_wrapper(response)",
        "snippet": "        def cb_wrapper(response):\n            try:\n                output = cb(response)\n                output = list(iterate_spider_output(output))\n            except:\n                case = _create_testcase(method, 'callback')\n                results.addError(case, sys.exc_info())",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractsManager.eb_wrapper#85",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractsManager",
        "signature": "scrapy.contracts.__init__.ContractsManager.eb_wrapper(failure)",
        "snippet": "        def eb_wrapper(failure):\n            case = _create_testcase(method, 'errback')\n            exc_info = failure.value, failure.type, failure.getTracebackObject()\n            results.addError(case, exc_info)",
        "begin_line": 85,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.__init__#97",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.__init__(self, method, *args)",
        "snippet": "    def __init__(self, method, *args):\n        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)\n        self.testcase_post = _create_testcase(method, '@%s post-hook' % self.name)\n        self.args = args",
        "begin_line": 97,
        "end_line": 100,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_pre_hook#102",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_pre_hook(self, request, results)",
        "snippet": "    def add_pre_hook(self, request, results):\n        if hasattr(self, 'pre_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                try:\n                    results.startTest(self.testcase_pre)\n                    self.pre_process(response)\n                    results.stopTest(self.testcase_pre)\n                except AssertionError:\n                    results.addFailure(self.testcase_pre, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_pre, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_pre)\n                finally:\n                    return list(iterate_spider_output(cb(response)))\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 102,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.add_post_hook#125",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.add_post_hook(self, request, results)",
        "snippet": "    def add_post_hook(self, request, results):\n        if hasattr(self, 'post_process'):\n            cb = request.callback\n\n            @wraps(cb)\n            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output\n\n            request.callback = wrapper\n\n        return request",
        "begin_line": 125,
        "end_line": 147,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.wrapper#130",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.wrapper(response)",
        "snippet": "            def wrapper(response):\n                output = list(iterate_spider_output(cb(response)))\n                try:\n                    results.startTest(self.testcase_post)\n                    self.post_process(output)\n                    results.stopTest(self.testcase_post)\n                except AssertionError:\n                    results.addFailure(self.testcase_post, sys.exc_info())\n                except Exception:\n                    results.addError(self.testcase_post, sys.exc_info())\n                else:\n                    results.addSuccess(self.testcase_post)\n                finally:\n                    return output",
        "begin_line": 130,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.Contract.adjust_request_args#149",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.Contract",
        "signature": "scrapy.contracts.__init__.Contract.adjust_request_args(self, args)",
        "snippet": "    def adjust_request_args(self, args):\n        return args",
        "begin_line": 149,
        "end_line": 150,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__",
        "signature": "scrapy.contracts.__init__._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase._create_testcase#153",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase._create_testcase(method, desc)",
        "snippet": "def _create_testcase(method, desc):\n    spider = method.__self__.name\n\n    class ContractTestCase(TestCase):\n        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)\n\n    name = '%s_%s' % (spider, method.__name__)\n    setattr(ContractTestCase, name, lambda x: x)\n    return ContractTestCase(name)",
        "begin_line": 153,
        "end_line": 162,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.contracts.__init__.ContractTestCase.__str__#157",
        "src_path": "scrapy/contracts/__init__.py",
        "class_name": "scrapy.contracts.__init__.ContractTestCase",
        "signature": "scrapy.contracts.__init__.ContractTestCase.__str__(_self)",
        "snippet": "        def __str__(_self):\n            return \"[%s] %s (%s)\" % (spider, method.__name__, desc)",
        "begin_line": 157,
        "end_line": 158,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.dupefilters.BaseDupeFilter.open#18",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.BaseDupeFilter",
        "signature": "scrapy.dupefilters.BaseDupeFilter.open(self)",
        "snippet": "    def open(self):  # can return deferred\n        pass",
        "begin_line": 18,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.__init__#31",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.__init__(self, path=None, debug=False)",
        "snippet": "    def __init__(self, path=None, debug=False):\n        self.file = None\n        self.fingerprints = set()\n        self.logdupes = True\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n        if path:\n            self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n            self.fingerprints.update(x.rstrip() for x in self.file)",
        "begin_line": 31,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_seen#46",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_seen(self, request)",
        "snippet": "    def request_seen(self, request):\n        fp = self.request_fingerprint(request)\n        if fp in self.fingerprints:\n            return True\n        self.fingerprints.add(fp)\n        if self.file:\n            self.file.write(fp + os.linesep)",
        "begin_line": 46,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint#54",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.request_fingerprint(self, request)",
        "snippet": "    def request_fingerprint(self, request):\n        return request_fingerprint(request)",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.dupefilters.RFPDupeFilter.close#57",
        "src_path": "scrapy/dupefilters.py",
        "class_name": "scrapy.dupefilters.RFPDupeFilter",
        "signature": "scrapy.dupefilters.RFPDupeFilter.close(self, reason)",
        "snippet": "    def close(self, reason):\n        if self.file:\n            self.file.close()",
        "begin_line": 57,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.engine.get_engine_status#6",
        "src_path": "scrapy/utils/engine.py",
        "class_name": "scrapy.utils.engine",
        "signature": "scrapy.utils.engine.get_engine_status(engine)",
        "snippet": "def get_engine_status(engine):\n    \"\"\"Return a report of the current engine status\"\"\"\n    tests = [\n        \"time()-engine.start_time\",\n        \"engine.has_capacity()\",\n        \"len(engine.downloader.active)\",\n        \"engine.scraper.is_idle()\",\n        \"engine.spider.name\",\n        \"engine.spider_is_idle(engine.spider)\",\n        \"engine.slot.closing\",\n        \"len(engine.slot.inprogress)\",\n        \"len(engine.slot.scheduler.dqs or [])\",\n        \"len(engine.slot.scheduler.mqs)\",\n        \"len(engine.scraper.slot.queue)\",\n        \"len(engine.scraper.slot.active)\",\n        \"engine.scraper.slot.active_size\",\n        \"engine.scraper.slot.itemproc_size\",\n        \"engine.scraper.slot.needs_backout()\",\n    ]\n\n    checks = []\n    for test in tests:\n        try:\n            checks += [(test, eval(test))]\n        except Exception as e:\n            checks += [(test, \"%s (exception)\" % type(e).__name__)]\n\n    return checks",
        "begin_line": 6,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.engine.format_engine_status#35",
        "src_path": "scrapy/utils/engine.py",
        "class_name": "scrapy.utils.engine",
        "signature": "scrapy.utils.engine.format_engine_status(engine=None)",
        "snippet": "def format_engine_status(engine=None):\n    checks = get_engine_status(engine)\n    s = \"Execution engine status\\n\\n\"\n    for test, result in checks:\n        s += \"%-47s : %s\\n\" % (test, result)\n    s += \"\\n\"\n\n    return s",
        "begin_line": 35,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.engine.print_engine_status#44",
        "src_path": "scrapy/utils/engine.py",
        "class_name": "scrapy.utils.engine",
        "signature": "scrapy.utils.engine.print_engine_status(engine)",
        "snippet": "def print_engine_status(engine):\n    print(format_engine_status(engine))",
        "begin_line": 44,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__#17",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.__init__(self, maxlength)",
        "snippet": "    def __init__(self, maxlength):\n        self.maxlength = maxlength",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output#27",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware.process_spider_output(self, response, result, spider)",
        "snippet": "    def process_spider_output(self, response, result, spider):\n        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True\n\n        return (r for r in result or () if _filter(r))",
        "begin_line": 27,
        "end_line": 37,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter#28",
        "src_path": "scrapy/spidermiddlewares/urllength.py",
        "class_name": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware",
        "signature": "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware._filter(request)",
        "snippet": "        def _filter(request):\n            if isinstance(request, Request) and len(request.url) > self.maxlength:\n                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                             {'maxlength': self.maxlength, 'url': request.url},\n                             extra={'spider': spider})\n                return False\n            else:\n                return True",
        "begin_line": 28,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__init__#9",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__init__(self, seq=None, encoding='utf-8')",
        "snippet": "    def __init__(self, seq=None, encoding='utf-8'):\n        self.encoding = encoding\n        super(Headers, self).__init__(seq)",
        "begin_line": 9,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.018867924528301886,
            "pseudo_dstar_susp": 0.018867924528301886,
            "pseudo_tarantula_susp": 0.018867924528301886,
            "pseudo_op2_susp": 0.018867924528301886,
            "pseudo_barinel_susp": 0.018867924528301886
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normkey#13",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Normalize key to bytes\"\"\"\n        return self._tobytes(key.title())",
        "begin_line": 13,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0003952569169960474,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.normvalue#17",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Normalize values to bytes\"\"\"\n        if value is None:\n            value = []\n        elif isinstance(value, (six.text_type, bytes)):\n            value = [value]\n        elif not hasattr(value, '__iter__'):\n            value = [value]\n\n        return [self._tobytes(x) for x in value]",
        "begin_line": 17,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers._tobytes#28",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers._tobytes(self, x)",
        "snippet": "    def _tobytes(self, x):\n        if isinstance(x, bytes):\n            return x\n        elif isinstance(x, six.text_type):\n            return x.encode(self.encoding)\n        elif isinstance(x, int):\n            return six.text_type(x).encode(self.encoding)\n        else:\n            raise TypeError('Unsupported value type: {}'.format(type(x)))",
        "begin_line": 28,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__getitem__#38",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        try:\n            return super(Headers, self).__getitem__(key)[-1]\n        except IndexError:\n            return None",
        "begin_line": 38,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.get#44",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        try:\n            return super(Headers, self).get(key, def_val)[-1]\n        except IndexError:\n            return None",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.getlist#50",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.getlist(self, key, def_val=None)",
        "snippet": "    def getlist(self, key, def_val=None):\n        try:\n            return super(Headers, self).__getitem__(key)\n        except KeyError:\n            if def_val is not None:\n                return self.normvalue(def_val)\n            return []",
        "begin_line": 50,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlist#58",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlist(self, key, list_)",
        "snippet": "    def setlist(self, key, list_):\n        self[key] = list_",
        "begin_line": 58,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.setlistdefault#61",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.setlistdefault(self, key, default_list=())",
        "snippet": "    def setlistdefault(self, key, default_list=()):\n        return self.setdefault(key, default_list)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.appendlist#64",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.appendlist(self, key, value)",
        "snippet": "    def appendlist(self, key, value):\n        lst = self.getlist(key)\n        lst.extend(self.normvalue(value))\n        self[key] = lst",
        "begin_line": 64,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.items#69",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.items(self)",
        "snippet": "    def items(self):\n        return list(self.iteritems())",
        "begin_line": 69,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004422821760283061,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.iteritems#72",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.iteritems(self)",
        "snippet": "    def iteritems(self):\n        return ((k, self.getlist(k)) for k in self.keys())",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004422821760283061,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.values#75",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.values(self)",
        "snippet": "    def values(self):\n        return [self[k] for k in self.keys()]",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.to_string#78",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.to_string(self)",
        "snippet": "    def to_string(self):\n        return headers_dict_to_raw(self)",
        "begin_line": 78,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.headers.Headers.__copy__#81",
        "src_path": "scrapy/http/headers.py",
        "class_name": "scrapy.http.headers.Headers",
        "signature": "scrapy.http.headers.Headers.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.conf.build_component_list#11",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.build_component_list(base, custom, convert=update_classpath)",
        "snippet": "def build_component_list(base, custom, convert=update_classpath):\n    \"\"\"Compose a component list based on a custom and base dict of components\n    (typically middlewares or extensions), unless custom is already a list, in\n    which case it's returned.\n    \"\"\"\n\n    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))\n\n    if isinstance(custom, (list, tuple)):\n        _check_components(custom)\n        return type(custom)(convert(c) for c in custom)\n\n    def _map_keys(compdict):\n        _check_components(compdict)\n        return {convert(k): v for k, v in six.iteritems(compdict)}\n\n    compdict = _map_keys(base)\n    compdict.update(_map_keys(custom))\n    items = (x for x in six.iteritems(compdict) if x[1] is not None)\n    return [x[0] for x in sorted(items, key=itemgetter(1))]",
        "begin_line": 11,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.conf._check_components#17",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._check_components(complist)",
        "snippet": "    def _check_components(complist):\n        if len({convert(c) for c in complist}) != len(complist):\n            raise ValueError('Some paths in {!r} convert to the same object, '\n                             'please update your settings'.format(complist))",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.conf._map_keys#26",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf._map_keys(compdict)",
        "snippet": "    def _map_keys(compdict):\n        _check_components(compdict)\n        return {convert(k): v for k, v in six.iteritems(compdict)}",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005558643690939411,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.conf.arglist_to_dict#36",
        "src_path": "scrapy/utils/conf.py",
        "class_name": "scrapy.utils.conf",
        "signature": "scrapy.utils.conf.arglist_to_dict(arglist)",
        "snippet": "def arglist_to_dict(arglist):\n    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n    dict\n    \"\"\"\n    return dict(x.split('=', 1) for x in arglist)",
        "begin_line": 36,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__init__#167",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__init__(self, seq=None)",
        "snippet": "    def __init__(self, seq=None):\n        super(CaselessDict, self).__init__()\n        if seq:\n            self.update(seq)",
        "begin_line": 167,
        "end_line": 170,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.017241379310344827,
            "pseudo_dstar_susp": 0.017241379310344827,
            "pseudo_tarantula_susp": 0.017241379310344827,
            "pseudo_op2_susp": 0.017241379310344827,
            "pseudo_barinel_susp": 0.017241379310344827
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__getitem__#172",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        return dict.__getitem__(self, self.normkey(key))",
        "begin_line": 172,
        "end_line": 173,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004060089321965083,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__setitem__#175",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__setitem__(self, key, value)",
        "snippet": "    def __setitem__(self, key, value):\n        dict.__setitem__(self, self.normkey(key), self.normvalue(value))",
        "begin_line": 175,
        "end_line": 176,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__delitem__#178",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__delitem__(self, key)",
        "snippet": "    def __delitem__(self, key):\n        dict.__delitem__(self, self.normkey(key))",
        "begin_line": 178,
        "end_line": 179,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__contains__#181",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__contains__(self, key)",
        "snippet": "    def __contains__(self, key):\n        return dict.__contains__(self, self.normkey(key))",
        "begin_line": 181,
        "end_line": 182,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046948356807511736,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.__copy__#185",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.__copy__(self)",
        "snippet": "    def __copy__(self):\n        return self.__class__(self)",
        "begin_line": 185,
        "end_line": 186,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normkey#189",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normkey(self, key)",
        "snippet": "    def normkey(self, key):\n        \"\"\"Method to normalize dictionary key access\"\"\"\n        return key.lower()",
        "begin_line": 189,
        "end_line": 191,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.normvalue#193",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.normvalue(self, value)",
        "snippet": "    def normvalue(self, value):\n        \"\"\"Method to normalize values prior to be setted\"\"\"\n        return value",
        "begin_line": 193,
        "end_line": 195,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.get#197",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.get(self, key, def_val=None)",
        "snippet": "    def get(self, key, def_val=None):\n        return dict.get(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 197,
        "end_line": 198,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004048582995951417,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.setdefault#200",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.setdefault(self, key, def_val=None)",
        "snippet": "    def setdefault(self, key, def_val=None):\n        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))",
        "begin_line": 200,
        "end_line": 201,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004422821760283061,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.update#203",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.update(self, seq)",
        "snippet": "    def update(self, seq):\n        seq = seq.items() if isinstance(seq, dict) else seq\n        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n        super(CaselessDict, self).update(iseq)",
        "begin_line": 203,
        "end_line": 206,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00039984006397441024,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.fromkeys#209",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.fromkeys(cls, keys, value=None)",
        "snippet": "    def fromkeys(cls, keys, value=None):\n        return cls((k, value) for k in keys)",
        "begin_line": 209,
        "end_line": 210,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.CaselessDict.pop#212",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.CaselessDict",
        "signature": "scrapy.utils.datatypes.CaselessDict.pop(self, key, *args)",
        "snippet": "    def pop(self, key, *args):\n        return dict.pop(self, self.normkey(key), *args)",
        "begin_line": 212,
        "end_line": 213,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.__init__#224",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.__init__(self, *dicts)",
        "snippet": "    def __init__(self, *dicts):\n        self.dicts = dicts",
        "begin_line": 224,
        "end_line": 225,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004329004329004329,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.__getitem__#227",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        for dict_ in self.dicts:\n            try:\n                return dict_[key]\n            except KeyError:\n                pass\n        raise KeyError",
        "begin_line": 227,
        "end_line": 233,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.datatypes.MergeDict.has_key#256",
        "src_path": "scrapy/utils/datatypes.py",
        "class_name": "scrapy.utils.datatypes.MergeDict",
        "signature": "scrapy.utils.datatypes.MergeDict.has_key(self, key)",
        "snippet": "    def has_key(self, key):\n        for dict_ in self.dicts:\n            if key in dict_:\n                return True\n        return False",
        "begin_line": 256,
        "end_line": 260,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__#28",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.__init__(self)",
        "snippet": "    def __init__(self):\n        self._formats = {\n            'tar': self._is_tar,\n            'zip': self._is_zip,\n            'gz': self._is_gzip,\n            'bz2': self._is_bzip2\n        }",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar#36",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_tar(self, response)",
        "snippet": "    def _is_tar(self, response):\n        archive = BytesIO(response.body)\n        try:\n            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n        except tarfile.ReadError:\n            return\n\n        body = tar_file.extractfile(tar_file.members[0]).read()\n        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 36,
        "end_line": 45,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip#47",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_zip(self, response)",
        "snippet": "    def _is_zip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            zip_file = zipfile.ZipFile(archive)\n        except zipfile.BadZipfile:\n            return\n\n        namelist = zip_file.namelist()\n        body = zip_file.read(namelist[0])\n        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 47,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip#59",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_gzip(self, response)",
        "snippet": "    def _is_gzip(self, response):\n        archive = BytesIO(response.body)\n        try:\n            body = gzip.GzipFile(fileobj=archive).read()\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 59,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2#69",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware._is_bzip2(self, response)",
        "snippet": "    def _is_bzip2(self, response):\n        try:\n            body = bz2.decompress(response.body)\n        except IOError:\n            return\n\n        respcls = responsetypes.from_args(body=body)\n        return response.replace(body=body, cls=respcls)",
        "begin_line": 69,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response#78",
        "src_path": "scrapy/downloadermiddlewares/decompression.py",
        "class_name": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware",
        "signature": "scrapy.downloadermiddlewares.decompression.DecompressionMiddleware.process_response(self, request, response, spider)",
        "snippet": "    def process_response(self, request, response, spider):\n        if not response.body:\n            return response\n\n        for fmt, func in six.iteritems(self._formats):\n            new_response = func(response)\n            if new_response:\n                logger.debug('Decompressed response with format: %(responsefmt)s',\n                             {'responsefmt': fmt}, extra={'spider': spider})\n                return new_response\n        return response",
        "begin_line": 78,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.__init__#17",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.__init__(self, stats)",
        "snippet": "    def __init__(self, stats):\n        self.stats = stats",
        "begin_line": 17,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler#21",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n            raise NotConfigured\n        o = cls(crawler.stats)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 21,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memdebug.MemoryDebugger.spider_closed#28",
        "src_path": "scrapy/extensions/memdebug.py",
        "class_name": "scrapy.extensions.memdebug.MemoryDebugger",
        "signature": "scrapy.extensions.memdebug.MemoryDebugger.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        gc.collect()\n        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)\n        for cls, wdict in six.iteritems(live_refs):\n            if not wdict:\n                continue\n            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)",
        "begin_line": 28,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class#15",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.create_deprecated_class(name, new_class, clsdict=None, warn_category=ScrapyDeprecationWarning, warn_once=True, old_class_path=None, new_class_path=None, subclass_warn_message='{cls} inherits from deprecated class {old}, please inherit from {new}.', instance_warn_message='{cls} is deprecated, instantiate {new} instead.')",
        "snippet": "def create_deprecated_class(name, new_class, clsdict=None,\n                            warn_category=ScrapyDeprecationWarning,\n                            warn_once=True,\n                            old_class_path=None,\n                            new_class_path=None,\n                            subclass_warn_message=\"{cls} inherits from \"\\\n                                    \"deprecated class {old}, please inherit \"\\\n                                    \"from {new}.\",\n                            instance_warn_message=\"{cls} is deprecated, \"\\\n                                    \"instantiate {new} instead.\"):\n    \"\"\"\n    Return a \"deprecated\" class that causes its subclasses to issue a warning.\n    Subclasses of ``new_class`` are considered subclasses of this class.\n    It also warns when the deprecated class is instantiated, but do not when\n    its subclasses are instantiated.\n\n    It can be used to rename a base class in a library. For example, if we\n    have\n\n        class OldName(SomeClass):\n            # ...\n\n    and we want to rename it to NewName, we can do the following::\n\n        class NewName(SomeClass):\n            # ...\n\n        OldName = create_deprecated_class('OldName', NewName)\n\n    Then, if user class inherits from OldName, warning is issued. Also, if\n    some code uses ``issubclass(sub, OldName)`` or ``isinstance(sub(), OldName)``\n    checks they'll still return True if sub is a subclass of NewName instead of\n    OldName.\n    \"\"\"\n\n    class DeprecatedClass(new_class.__class__):\n\n        deprecated_class = None\n        warned_on_subclass = False\n\n        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls\n\n        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n\n        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n        # for implementation details\n        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})\n\n        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)\n\n        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)\n\n    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})\n\n    try:\n        frm = inspect.stack()[1]\n        parent_module = inspect.getmodule(frm[0])\n        if parent_module is not None:\n            deprecated_cls.__module__ = parent_module.__name__\n    except Exception as e:\n        # Sometimes inspect.stack() fails (e.g. when the first import of\n        # deprecated class is in jinja2 template). __module__ attribute is not\n        # important enough to raise an exception as users may be unable\n        # to fix inspect.stack() errors.\n        warnings.warn(\"Error detecting parent module: %r\" % e)\n\n    return deprecated_cls",
        "begin_line": 15,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00047732696897374703,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__new__#55",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__new__(metacls, name, bases, clsdict_)",
        "snippet": "        def __new__(metacls, name, bases, clsdict_):\n            cls = super(DeprecatedClass, metacls).__new__(metacls, name, bases, clsdict_)\n            if metacls.deprecated_class is None:\n                metacls.deprecated_class = cls\n            return cls",
        "begin_line": 55,
        "end_line": 59,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00047732696897374703,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__init__#61",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__init__(cls, name, bases, clsdict_)",
        "snippet": "        def __init__(cls, name, bases, clsdict_):\n            meta = cls.__class__\n            old = meta.deprecated_class\n            if old in bases and not (warn_once and meta.warned_on_subclass):\n                meta.warned_on_subclass = True\n                msg = subclass_warn_message.format(cls=_clspath(cls),\n                                                   old=_clspath(old, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                if warn_once:\n                    msg += ' (warning only on first subclass, there may be others)'\n                warnings.warn(msg, warn_category, stacklevel=2)\n            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__#77",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__instancecheck__(cls, inst)",
        "snippet": "        def __instancecheck__(cls, inst):\n            return any(cls.__subclasscheck__(c)\n                       for c in {type(inst), inst.__class__})",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__#81",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__subclasscheck__(cls, sub)",
        "snippet": "        def __subclasscheck__(cls, sub):\n            if cls is not DeprecatedClass.deprecated_class:\n                # we should do the magic only if second `issubclass` argument\n                # is the deprecated class itself - subclasses of the\n                # deprecated class should not use custom `__subclasscheck__`\n                # method.\n                return super(DeprecatedClass, cls).__subclasscheck__(sub)\n\n            if not inspect.isclass(sub):\n                raise TypeError(\"issubclass() arg 1 must be a class\")\n\n            mro = getattr(sub, '__mro__', ())\n            return any(c in {cls, new_class} for c in mro)",
        "begin_line": 81,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.DeprecatedClass.__call__#95",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate.DeprecatedClass",
        "signature": "scrapy.utils.deprecate.DeprecatedClass.__call__(cls, *args, **kwargs)",
        "snippet": "        def __call__(cls, *args, **kwargs):\n            old = DeprecatedClass.deprecated_class\n            if cls is old:\n                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),\n                                                   new=_clspath(new_class, new_class_path))\n                warnings.warn(msg, warn_category, stacklevel=2)\n            return super(DeprecatedClass, cls).__call__(*args, **kwargs)",
        "begin_line": 95,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate._clspath#120",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate._clspath(cls, forced=None)",
        "snippet": "def _clspath(cls, forced=None):\n    if forced is not None:\n        return forced\n    return '{}.{}'.format(cls.__module__, cls.__name__)",
        "begin_line": 120,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.deprecate.update_classpath#149",
        "src_path": "scrapy/utils/deprecate.py",
        "class_name": "scrapy.utils.deprecate",
        "signature": "scrapy.utils.deprecate.update_classpath(path)",
        "snippet": "def update_classpath(path):\n    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n    for prefix, replacement in DEPRECATION_RULES:\n        if path.startswith(prefix):\n            new_path = path.replace(prefix, replacement, 1)\n            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n                          ScrapyDeprecationWarning)\n            return new_path\n    return path",
        "begin_line": 149,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_mimetype#42",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_mimetype(self, mimetype)",
        "snippet": "    def from_mimetype(self, mimetype):\n        \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n        if mimetype is None:\n            return Response\n        elif mimetype in self.classes:\n            return self.classes[mimetype]\n        else:\n            basetype = \"%s/*\" % mimetype.split('/')[0]\n            return self.classes.get(basetype, Response)",
        "begin_line": 42,
        "end_line": 50,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_type#52",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_type(self, content_type, content_encoding=None)",
        "snippet": "    def from_content_type(self, content_type, content_encoding=None):\n        \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n        header \"\"\"\n        if content_encoding:\n            return Response\n        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n        return self.from_mimetype(mimetype)",
        "begin_line": 52,
        "end_line": 58,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_content_disposition#60",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_content_disposition(self, content_disposition)",
        "snippet": "    def from_content_disposition(self, content_disposition):\n        try:\n            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]\n            filename = filename.strip('\"\\'')\n            return self.from_filename(filename)\n        except IndexError:\n            return Response",
        "begin_line": 60,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_headers#68",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_headers(self, headers)",
        "snippet": "    def from_headers(self, headers):\n        \"\"\"Return the most appropriate Response class by looking at the HTTP\n        headers\"\"\"\n        cls = Response\n        if b'Content-Type' in headers:\n            cls = self.from_content_type(\n                content_type=headers[b'Content-type'],\n                content_encoding=headers.get(b'Content-Encoding')\n            )\n        if cls is Response and b'Content-Disposition' in headers:\n            cls = self.from_content_disposition(headers[b'Content-Disposition'])\n        return cls",
        "begin_line": 68,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_filename#81",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_filename(self, filename)",
        "snippet": "    def from_filename(self, filename):\n        \"\"\"Return the most appropriate Response class from a file name\"\"\"\n        mimetype, encoding = self.mimetypes.guess_type(filename)\n        if mimetype and not encoding:\n            return self.from_mimetype(mimetype)\n        else:\n            return Response",
        "begin_line": 81,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_body#89",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_body(self, body)",
        "snippet": "    def from_body(self, body):\n        \"\"\"Try to guess the appropriate response based on the body content.\n        This method is a bit magic and could be improved in the future, but\n        it's not meant to be used except for special cases where response types\n        cannot be guess using more straightforward methods.\"\"\"\n        chunk = body[:5000]\n        chunk = to_bytes(chunk)\n        if isbinarytext(chunk):\n            return self.from_mimetype('application/octet-stream')\n        elif b\"<html>\" in chunk.lower():\n            return self.from_mimetype('text/html')\n        elif b\"<?xml\" in chunk.lower():\n            return self.from_mimetype('text/xml')\n        else:\n            return self.from_mimetype('text')",
        "begin_line": 89,
        "end_line": 103,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.responsetypes.ResponseTypes.from_args#105",
        "src_path": "scrapy/responsetypes.py",
        "class_name": "scrapy.responsetypes.ResponseTypes",
        "signature": "scrapy.responsetypes.ResponseTypes.from_args(self, headers=None, url=None, filename=None, body=None)",
        "snippet": "    def from_args(self, headers=None, url=None, filename=None, body=None):\n        \"\"\"Guess the most appropriate Response class based on\n        the given arguments.\"\"\"\n        cls = Response\n        if headers is not None:\n            cls = self.from_headers(headers)\n        if cls is Response and url is not None:\n            cls = self.from_filename(url)\n        if cls is Response and filename is not None:\n            cls = self.from_filename(filename)\n        if cls is Response and body is not None:\n            cls = self.from_body(body)\n        return cls",
        "begin_line": 105,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.defer_fail#10",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_fail(_failure)",
        "snippet": "def defer_fail(_failure):\n    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.errback, _failure)\n    return d",
        "begin_line": 10,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.defer_succeed#21",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_succeed(result)",
        "snippet": "def defer_succeed(result):\n    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n    next reactor loop\n\n    It delays by 100ms so reactor has a chance to go trough readers and writers\n    before attending pending delayed calls, so do not set delay to zero.\n    \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(0.1, d.callback, result)\n    return d",
        "begin_line": 21,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.defer_result#32",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.defer_result(result)",
        "snippet": "def defer_result(result):\n    if isinstance(result, defer.Deferred):\n        return result\n    elif isinstance(result, failure.Failure):\n        return defer_fail(result)\n    else:\n        return defer_succeed(result)",
        "begin_line": 32,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.mustbe_deferred#40",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.mustbe_deferred(f, *args, **kw)",
        "snippet": "def mustbe_deferred(f, *args, **kw):\n    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n    callback/errback to next reactor loop\n    \"\"\"\n    try:\n        result = f(*args, **kw)\n    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n    # processing of IgnoreRequest errors which are, by far, the most common\n    # exception in Scrapy - see #125\n    except IgnoreRequest as e:\n        return defer_fail(failure.Failure(e))\n    except:\n        return defer_fail(failure.Failure())\n    else:\n        return defer_result(result)",
        "begin_line": 40,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain#66",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain(callbacks, input, *a, **kw)",
        "snippet": "def process_chain(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n    d = defer.Deferred()\n    for x in callbacks:\n        d.addCallback(x, *a, **kw)\n    d.callback(input)\n    return d",
        "begin_line": 66,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.process_chain_both#74",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_chain_both(callbacks, errbacks, input, *a, **kw)",
        "snippet": "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n    d = defer.Deferred()\n    for cb, eb in zip(callbacks, errbacks):\n        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n            errbackArgs=a, errbackKeywords=kw)\n    if isinstance(input, failure.Failure):\n        d.errback(input)\n    else:\n        d.callback(input)\n    return d",
        "begin_line": 74,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.process_parallel#86",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.process_parallel(callbacks, input, *a, **kw)",
        "snippet": "def process_parallel(callbacks, input, *a, **kw):\n    \"\"\"Return a Deferred with the output of all successful calls to the given\n    callbacks\n    \"\"\"\n    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n    return d",
        "begin_line": 86,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.defer.iter_errback#95",
        "src_path": "scrapy/utils/defer.py",
        "class_name": "scrapy.utils.defer",
        "signature": "scrapy.utils.defer.iter_errback(iterable, errback, *a, **kw)",
        "snippet": "def iter_errback(iterable, errback, *a, **kw):\n    \"\"\"Wraps an iterable calling an errback if an error is caught while\n    iterating it.\n    \"\"\"\n    it = iter(iterable)\n    while True:\n        try:\n            yield next(it)\n        except StopIteration:\n            break\n        except:\n            errback(failure.Failure(), *a, **kw)",
        "begin_line": 95,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.reqser.request_to_dict#10",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_to_dict(request, spider=None)",
        "snippet": "def request_to_dict(request, spider=None):\n    \"\"\"Convert Request object to a dict.\n\n    If a spider is given, it will try to find out the name of the spider method\n    used in the callback and store that as the callback.\n    \"\"\"\n    cb = request.callback\n    if callable(cb):\n        cb = _find_method(spider, cb)\n    eb = request.errback\n    if callable(eb):\n        eb = _find_method(spider, eb)\n    d = {\n        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)\n        'callback': cb,\n        'errback': eb,\n        'method': request.method,\n        'headers': dict(request.headers),\n        'body': request.body,\n        'cookies': request.cookies,\n        'meta': request.meta,\n        '_encoding': request._encoding,\n        'priority': request.priority,\n        'dont_filter': request.dont_filter,\n    }\n    return d",
        "begin_line": 10,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.reqser.request_from_dict#38",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser.request_from_dict(d, spider=None)",
        "snippet": "def request_from_dict(d, spider=None):\n    \"\"\"Create Request object from a dict.\n\n    If a spider is given, it will try to resolve the callbacks looking at the\n    spider for methods with the same name.\n    \"\"\"\n    cb = d['callback']\n    if cb and spider:\n        cb = _get_method(spider, cb)\n    eb = d['errback']\n    if eb and spider:\n        eb = _get_method(spider, eb)\n    return Request(\n        url=to_native_str(d['url']),\n        callback=cb,\n        errback=eb,\n        method=d['method'],\n        headers=d['headers'],\n        body=d['body'],\n        cookies=d['cookies'],\n        meta=d['meta'],\n        encoding=d['_encoding'],\n        priority=d['priority'],\n        dont_filter=d['dont_filter'])",
        "begin_line": 38,
        "end_line": 61,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.reqser._find_method#64",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._find_method(obj, func)",
        "snippet": "def _find_method(obj, func):\n    if obj:\n        try:\n            func_self = six.get_method_self(func)\n        except AttributeError:  # func has no __self__\n            pass\n        else:\n            if func_self is obj:\n                return six.get_method_function(func).__name__\n    raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))",
        "begin_line": 64,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.reqser._get_method#76",
        "src_path": "scrapy/utils/reqser.py",
        "class_name": "scrapy.utils.reqser",
        "signature": "scrapy.utils.reqser._get_method(obj, name)",
        "snippet": "def _get_method(obj, name):\n    name = str(name)\n    try:\n        return getattr(obj, name)\n    except AttributeError:\n        raise ValueError(\"Method %r not found in: %s\" % (name, obj))",
        "begin_line": 76,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.misc.arg_to_iter#17",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.arg_to_iter(arg)",
        "snippet": "def arg_to_iter(arg):\n    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n    value, or an iterable.\n\n    Exception: if arg is a dict, [arg] will be returned\n    \"\"\"\n    if arg is None:\n        return []\n    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n        return arg\n    else:\n        return [arg]",
        "begin_line": 17,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.misc.load_object#31",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.load_object(path)",
        "snippet": "def load_object(path):\n    \"\"\"Load an object given its absolute object path, and return it.\n\n    object can be a class, function, variable o instance.\n    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n    \"\"\"\n\n    try:\n        dot = path.rindex('.')\n    except ValueError:\n        raise ValueError(\"Error loading object '%s': not a full path\" % path)\n\n    module, name = path[:dot], path[dot+1:]\n    mod = import_module(module)\n\n    try:\n        obj = getattr(mod, name)\n    except AttributeError:\n        raise NameError(\"Module '%s' doesn't define any object named '%s'\" % (module, name))\n\n    return obj",
        "begin_line": 31,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.misc.walk_modules#54",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.walk_modules(path)",
        "snippet": "def walk_modules(path):\n    \"\"\"Loads a module and all its submodules from a the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    For example: walk_modules('scrapy.utils')\n    \"\"\"\n\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, subpath, ispkg in iter_modules(mod.__path__):\n            fullpath = path + '.' + subpath\n            if ispkg:\n                mods += walk_modules(fullpath)\n            else:\n                submod = import_module(fullpath)\n                mods.append(submod)\n    return mods",
        "begin_line": 54,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.misc.extract_regex#76",
        "src_path": "scrapy/utils/misc.py",
        "class_name": "scrapy.utils.misc",
        "signature": "scrapy.utils.misc.extract_regex(regex, text, encoding='utf-8')",
        "snippet": "def extract_regex(regex, text, encoding='utf-8'):\n    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n\n    * if the regex contains a named group called \"extract\" that will be returned\n    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n    * if the regex doesn't contain any group the entire regex matching is returned\n    \"\"\"\n\n    if isinstance(regex, six.string_types):\n        regex = re.compile(regex, re.UNICODE)\n\n    try:\n        strings = [regex.search(text).group('extract')]   # named group\n    except:\n        strings = regex.findall(text)    # full regex or numbered groups\n    strings = flatten(strings)\n\n    if isinstance(text, six.text_type):\n        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]\n    else:\n        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])\n                for s in strings]",
        "begin_line": 76,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole.__init__#39",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n            raise NotConfigured\n        if not TWISTED_CONCH_AVAILABLE:\n            raise NotConfigured\n        self.crawler = crawler\n        self.noisy = False\n        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]\n        self.host = crawler.settings['TELNETCONSOLE_HOST']\n        self.crawler.signals.connect(self.start_listening, signals.engine_started)\n        self.crawler.signals.connect(self.stop_listening, signals.engine_stopped)",
        "begin_line": 39,
        "end_line": 49,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole.from_crawler#52",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 52,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole.start_listening#55",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole.start_listening(self)",
        "snippet": "    def start_listening(self):\n        self.port = listen_tcp(self.portrange, self.host, self)\n        h = self.port.getHost()\n        logger.debug(\"Telnet console listening on %(host)s:%(port)d\",\n                     {'host': h.host, 'port': h.port},\n                     extra={'crawler': self.crawler})",
        "begin_line": 55,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole.stop_listening#62",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole.stop_listening(self)",
        "snippet": "    def stop_listening(self):\n        self.port.stopListening()",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole.protocol#65",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole.protocol(self)",
        "snippet": "    def protocol(self):\n        telnet_vars = self._get_telnet_vars()\n        return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n            insults.ServerProtocol, manhole.Manhole, telnet_vars)",
        "begin_line": 65,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.telnet.TelnetConsole._get_telnet_vars#70",
        "src_path": "scrapy/telnet.py",
        "class_name": "scrapy.telnet.TelnetConsole",
        "signature": "scrapy.telnet.TelnetConsole._get_telnet_vars(self)",
        "snippet": "    def _get_telnet_vars(self):\n        # Note: if you add entries here also update topics/telnetconsole.rst\n        telnet_vars = {\n            'engine': self.crawler.engine,\n            'spider': self.crawler.engine.spider,\n            'slot': self.crawler.engine.slot,\n            'crawler': self.crawler,\n            'extensions': self.crawler.extensions,\n            'stats': self.crawler.stats,\n            'settings': self.crawler.settings,\n            'est': lambda: print_engine_status(self.crawler.engine),\n            'p': pprint.pprint,\n            'prefs': print_live_refs,\n            'hpy': hpy,\n            'help': \"This is Scrapy telnet console. For more info see: \" \\\n                \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n        }\n        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n        return telnet_vars",
        "begin_line": 70,
        "end_line": 88,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.link.Link.__init__#15",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__init__(self, url, text='', fragment='', nofollow=False)",
        "snippet": "    def __init__(self, url, text='', fragment='', nofollow=False):\n        if isinstance(url, six.text_type):\n            import warnings\n            warnings.warn(\"Do not instantiate Link objects with unicode urls. \"\n                \"Assuming utf-8 encoding (which could be wrong)\")\n            url = url.encode('utf-8')\n        self.url = url\n        self.text = text\n        self.fragment = fragment\n        self.nofollow = nofollow",
        "begin_line": 15,
        "end_line": 24,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.link.Link.__eq__#26",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__eq__(self, other)",
        "snippet": "    def __eq__(self, other):\n        return self.url == other.url and self.text == other.text and \\\n            self.fragment == other.fragment and self.nofollow == other.nofollow",
        "begin_line": 26,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.link.Link.__hash__#30",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__hash__(self)",
        "snippet": "    def __hash__(self):\n        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)",
        "begin_line": 30,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.link.Link.__repr__#33",
        "src_path": "scrapy/link.py",
        "class_name": "scrapy.link.Link",
        "signature": "scrapy.link.Link.__repr__(self)",
        "snippet": "    def __repr__(self):\n        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n            (self.url, self.text, self.fragment, self.nofollow)",
        "begin_line": 33,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.__init__#10",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.__init__(self, policy=None, check_expired_frequency=10000)",
        "snippet": "    def __init__(self, policy=None, check_expired_frequency=10000):\n        self.policy = policy or DefaultCookiePolicy()\n        self.jar = _CookieJar(self.policy)\n        self.jar._cookies_lock = _DummyLock()\n        self.check_expired_frequency = check_expired_frequency\n        self.processed = 0",
        "begin_line": 10,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.extract_cookies#17",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.extract_cookies(self, response, request)",
        "snippet": "    def extract_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.extract_cookies(wrsp, wreq)",
        "begin_line": 17,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.add_cookie_header#22",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.add_cookie_header(self, request)",
        "snippet": "    def add_cookie_header(self, request):\n        wreq = WrappedRequest(request)\n        self.policy._now = self.jar._now = int(time.time())\n\n        # the cookiejar implementation iterates through all domains\n        # instead we restrict to potential matches on the domain\n        req_host = urlparse_cached(request).hostname\n        if not req_host:\n            return\n\n        if not IPV4_RE.search(req_host):\n            hosts = potential_domain_matches(req_host)\n            if '.' not in req_host:\n                hosts += [req_host + \".local\"]\n        else:\n            hosts = [req_host]\n\n        cookies = []\n        for host in hosts:\n            if host in self.jar._cookies:\n                cookies += self.jar._cookies_for_domain(host, wreq)\n\n        attrs = self.jar._cookie_attrs(cookies)\n        if attrs:\n            if not wreq.has_header(\"Cookie\"):\n                wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n\n        self.processed += 1\n        if self.processed % self.check_expired_frequency == 0:\n            # This is still quite inefficient for large number of cookies\n            self.jar.clear_expired_cookies()",
        "begin_line": 22,
        "end_line": 52,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.make_cookies#73",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.make_cookies(self, response, request)",
        "snippet": "    def make_cookies(self, response, request):\n        wreq = WrappedRequest(request)\n        wrsp = WrappedResponse(response)\n        return self.jar.make_cookies(wrsp, wreq)",
        "begin_line": 73,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.CookieJar.set_cookie_if_ok#81",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.CookieJar",
        "signature": "scrapy.http.cookies.CookieJar.set_cookie_if_ok(self, cookie, request)",
        "snippet": "    def set_cookie_if_ok(self, cookie, request):\n        self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.potential_domain_matches#85",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies",
        "signature": "scrapy.http.cookies.potential_domain_matches(domain)",
        "snippet": "def potential_domain_matches(domain):\n    \"\"\"Potential domain matches for a cookie\n\n    >>> potential_domain_matches('www.example.com')\n    ['www.example.com', 'example.com', '.www.example.com', '.example.com']\n\n    \"\"\"\n    matches = [domain]\n    try:\n        start = domain.index('.') + 1\n        end = domain.rindex('.')\n        while start < end:\n            matches.append(domain[start:])\n            start = domain.index('.', start) + 1\n    except ValueError:\n        pass\n    return matches + ['.' + d for d in matches]",
        "begin_line": 85,
        "end_line": 101,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.acquire#105",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.acquire(self)",
        "snippet": "    def acquire(self):\n        pass",
        "begin_line": 105,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies._DummyLock.release#108",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies._DummyLock",
        "signature": "scrapy.http.cookies._DummyLock.release(self)",
        "snippet": "    def release(self):\n        pass",
        "begin_line": 108,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005324813631522897,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.__init__#118",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.__init__(self, request)",
        "snippet": "    def __init__(self, request):\n        self.request = request",
        "begin_line": 118,
        "end_line": 119,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004580852038479157,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_full_url#121",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_full_url(self)",
        "snippet": "    def get_full_url(self):\n        return self.request.url",
        "begin_line": 121,
        "end_line": 122,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_host#124",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_host(self)",
        "snippet": "    def get_host(self):\n        return urlparse_cached(self.request).netloc",
        "begin_line": 124,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_type#127",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_type(self)",
        "snippet": "    def get_type(self):\n        return urlparse_cached(self.request).scheme",
        "begin_line": 127,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.is_unverifiable#130",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.is_unverifiable(self)",
        "snippet": "    def is_unverifiable(self):\n        \"\"\"Unverifiable should indicate whether the request is unverifiable, as defined by RFC 2965.\n\n        It defaults to False. An unverifiable request is one whose URL the user did not have the\n        option to approve. For example, if the request is for an image in an\n        HTML document, and the user had no option to approve the automatic\n        fetching of the image, this should be true.\n        \"\"\"\n        return self.request.meta.get('is_unverifiable', False)",
        "begin_line": 130,
        "end_line": 138,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.unverifiable#142",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.unverifiable(self)",
        "snippet": "    def unverifiable(self):\n        return self.is_unverifiable()",
        "begin_line": 142,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_origin_req_host#145",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_origin_req_host(self)",
        "snippet": "    def get_origin_req_host(self):\n        return urlparse_cached(self.request).hostname",
        "begin_line": 145,
        "end_line": 146,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.has_header#148",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.has_header(self, name)",
        "snippet": "    def has_header(self, name):\n        return name in self.request.headers",
        "begin_line": 148,
        "end_line": 149,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.get_header#151",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.get_header(self, name, default=None)",
        "snippet": "    def get_header(self, name, default=None):\n        return to_native_str(self.request.headers.get(name, default),\n                             errors='replace')",
        "begin_line": 151,
        "end_line": 153,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.header_items#155",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.header_items(self)",
        "snippet": "    def header_items(self):\n        return [\n            (to_native_str(k, errors='replace'),\n             [to_native_str(x, errors='replace') for x in v])\n            for k, v in self.request.headers.items()\n        ]",
        "begin_line": 155,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedRequest.add_unredirected_header#162",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedRequest",
        "signature": "scrapy.http.cookies.WrappedRequest.add_unredirected_header(self, name, value)",
        "snippet": "    def add_unredirected_header(self, name, value):\n        self.request.headers.appendlist(name, value)",
        "begin_line": 162,
        "end_line": 163,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.__init__#168",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.__init__(self, response)",
        "snippet": "    def __init__(self, response):\n        self.response = response",
        "begin_line": 168,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.info#171",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.info(self)",
        "snippet": "    def info(self):\n        return self",
        "begin_line": 171,
        "end_line": 172,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.cookies.WrappedResponse.get_all#175",
        "src_path": "scrapy/http/cookies.py",
        "class_name": "scrapy.http.cookies.WrappedResponse",
        "signature": "scrapy.http.cookies.WrappedResponse.get_all(self, name, default=None)",
        "snippet": "    def get_all(self, name, default=None):\n        return [to_native_str(v, errors='replace')\n                for v in self.response.headers.getlist(name)]",
        "begin_line": 175,
        "end_line": 177,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__init__#15",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__init__(self, url, status=200, headers=None, body=b'', flags=None, request=None)",
        "snippet": "    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n        self.headers = Headers(headers or {})\n        self.status = int(status)\n        self._set_body(body)\n        self._set_url(url)\n        self.request = request\n        self.flags = [] if flags is None else list(flags)",
        "begin_line": 15,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0003965107057890563,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.meta#24",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.meta(self)",
        "snippet": "    def meta(self):\n        try:\n            return self.request.meta\n        except AttributeError:\n            raise AttributeError(\n                \"Response.meta not available, this response \"\n                \"is not tied to any request\"\n            )",
        "begin_line": 24,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_url#33",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_url(self)",
        "snippet": "    def _get_url(self):\n        return self._url",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00040666937779585197,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_url#36",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_url(self, url)",
        "snippet": "    def _set_url(self, url):\n        if isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n                type(url).__name__))",
        "begin_line": 36,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004297378599054577,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._get_body#45",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._get_body(self)",
        "snippet": "    def _get_body(self):\n        return self._body",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00039952057530962844,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response._set_body#48",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response._set_body(self, body)",
        "snippet": "    def _set_body(self, body):\n        if body is None:\n            self._body = b''\n        elif not isinstance(body, bytes):\n            raise TypeError(\n                \"Response body must be bytes. \"\n                \"If you want to pass unicode body use TextResponse \"\n                \"or HtmlResponse.\")\n        else:\n            self._body = body",
        "begin_line": 48,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00039824771007566706,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.__str__#61",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%d %s>\" % (self.status, self.url)",
        "begin_line": 61,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.copy#66",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.copy(self)",
        "snippet": "    def copy(self):\n        \"\"\"Return a copy of this Response\"\"\"\n        return self.replace()",
        "begin_line": 66,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005558643690939411,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.replace#70",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.replace(self, *args, **kwargs)",
        "snippet": "    def replace(self, *args, **kwargs):\n        \"\"\"Create a new Response with the same attributes except for those\n        given new values.\n        \"\"\"\n        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n            kwargs.setdefault(x, getattr(self, x))\n        cls = kwargs.pop('cls', self.__class__)\n        return cls(*args, **kwargs)",
        "begin_line": 70,
        "end_line": 77,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046707146193367583,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.response.__init__.Response.urljoin#79",
        "src_path": "scrapy/http/response/__init__.py",
        "class_name": "scrapy.http.response.__init__.Response",
        "signature": "scrapy.http.response.__init__.Response.urljoin(self, url)",
        "snippet": "    def urljoin(self, url):\n        \"\"\"Join this Response's url with a possible relative url to form an\n        absolute interpretation of the latter.\"\"\"\n        return urljoin(self.url, url)",
        "begin_line": 79,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.__init__#12",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self._dump = crawler.settings.getbool('STATS_DUMP')\n        self._stats = {}",
        "begin_line": 12,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_value#16",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return self._stats.get(key, default)",
        "begin_line": 16,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.get_stats#19",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.get_stats(self, spider=None)",
        "snippet": "    def get_stats(self, spider=None):\n        return self._stats",
        "begin_line": 19,
        "end_line": 20,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.set_value#22",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        self._stats[key] = value",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.set_stats#25",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.set_stats(self, stats, spider=None)",
        "snippet": "    def set_stats(self, stats, spider=None):\n        self._stats = stats",
        "begin_line": 25,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.inc_value#28",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        d = self._stats\n        d[key] = d.setdefault(key, start) + count",
        "begin_line": 28,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.max_value#32",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        self._stats[key] = max(self._stats.setdefault(key, value), value)",
        "begin_line": 32,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.min_value#35",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        self._stats[key] = min(self._stats.setdefault(key, value), value)",
        "begin_line": 35,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.clear_stats#38",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.clear_stats(self, spider=None)",
        "snippet": "    def clear_stats(self, spider=None):\n        self._stats.clear()",
        "begin_line": 38,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.open_spider#41",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.open_spider(self, spider)",
        "snippet": "    def open_spider(self, spider):\n        pass",
        "begin_line": 41,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector.close_spider#44",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector.close_spider(self, spider, reason)",
        "snippet": "    def close_spider(self, spider, reason):\n        if self._dump:\n            logger.info(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats),\n                        extra={'spider': spider})\n        self._persist_stats(self._stats, spider)",
        "begin_line": 44,
        "end_line": 48,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.StatsCollector._persist_stats#50",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.StatsCollector",
        "signature": "scrapy.statscollectors.StatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        pass",
        "begin_line": 50,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector.__init__#55",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        super(MemoryStatsCollector, self).__init__(crawler)\n        self.spider_stats = {}",
        "begin_line": 55,
        "end_line": 57,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.MemoryStatsCollector._persist_stats#59",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.MemoryStatsCollector",
        "signature": "scrapy.statscollectors.MemoryStatsCollector._persist_stats(self, stats, spider)",
        "snippet": "    def _persist_stats(self, stats, spider):\n        self.spider_stats[spider.name] = stats",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.get_value#65",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.get_value(self, key, default=None, spider=None)",
        "snippet": "    def get_value(self, key, default=None, spider=None):\n        return default",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.set_value#68",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.set_value(self, key, value, spider=None)",
        "snippet": "    def set_value(self, key, value, spider=None):\n        pass",
        "begin_line": 68,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.set_stats#71",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.set_stats(self, stats, spider=None)",
        "snippet": "    def set_stats(self, stats, spider=None):\n        pass",
        "begin_line": 71,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.inc_value#74",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.inc_value(self, key, count=1, start=0, spider=None)",
        "snippet": "    def inc_value(self, key, count=1, start=0, spider=None):\n        pass",
        "begin_line": 74,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.max_value#77",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.max_value(self, key, value, spider=None)",
        "snippet": "    def max_value(self, key, value, spider=None):\n        pass",
        "begin_line": 77,
        "end_line": 78,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.statscollectors.DummyStatsCollector.min_value#80",
        "src_path": "scrapy/statscollectors.py",
        "class_name": "scrapy.statscollectors.DummyStatsCollector",
        "signature": "scrapy.statscollectors.DummyStatsCollector.min_value(self, key, value, spider=None)",
        "snippet": "    def min_value(self, key, value, spider=None):\n        pass",
        "begin_line": 80,
        "end_line": 81,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.__init__#24",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n            raise NotConfigured\n        try:\n            # stdlib's resource module is only available on unix platforms.\n            self.resource = import_module('resource')\n        except ImportError:\n            raise NotConfigured\n\n        self.crawler = crawler\n        self.warned = False\n        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')\n        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n        self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n        self.mail = MailSender.from_settings(crawler.settings)\n        crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n        crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)",
        "begin_line": 24,
        "end_line": 42,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.from_crawler#45",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 45,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.get_virtual_size#48",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.get_virtual_size(self)",
        "snippet": "    def get_virtual_size(self):\n        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n        if sys.platform != 'darwin':\n            # on Mac OS X ru_maxrss is in bytes, on Linux it is in KB\n            size *= 1024\n        return size",
        "begin_line": 48,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.engine_started#55",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.engine_started(self)",
        "snippet": "    def engine_started(self):\n        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n        self.tasks = []\n        tsk = task.LoopingCall(self.update)\n        self.tasks.append(tsk)\n        tsk.start(self.check_interval, now=True)\n        if self.limit:\n            tsk = task.LoopingCall(self._check_limit)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)\n        if self.warning:\n            tsk = task.LoopingCall(self._check_warning)\n            self.tasks.append(tsk)\n            tsk.start(self.check_interval, now=True)",
        "begin_line": 55,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.engine_stopped#70",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.engine_stopped(self)",
        "snippet": "    def engine_stopped(self):\n        for tsk in self.tasks:\n            if tsk.running:\n                tsk.stop()",
        "begin_line": 70,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage.update#75",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage.update(self)",
        "snippet": "    def update(self):\n        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())",
        "begin_line": 75,
        "end_line": 76,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._check_limit#78",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._check_limit(self)",
        "snippet": "    def _check_limit(self):\n        if self.get_virtual_size() > self.limit:\n            self.crawler.stats.set_value('memusage/limit_reached', 1)\n            mem = self.limit/1024/1024\n            logger.error(\"Memory usage exceeded %(memusage)dM. Shutting down Scrapy...\",\n                         {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/limit_notified', 1)\n\n            open_spiders = self.crawler.engine.open_spiders\n            if open_spiders:\n                for spider in open_spiders:\n                    self.crawler.engine.close_spider(spider, 'memusage_exceeded')\n            else:\n                self.crawler.stop()",
        "begin_line": 78,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._check_warning#97",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._check_warning(self)",
        "snippet": "    def _check_warning(self):\n        if self.warned: # warn only once\n            return\n        if self.get_virtual_size() > self.warning:\n            self.crawler.stats.set_value('memusage/warning_reached', 1)\n            mem = self.warning/1024/1024\n            logger.warning(\"Memory usage reached %(memusage)dM\",\n                           {'memusage': mem}, extra={'crawler': self.crawler})\n            if self.notify_mails:\n                subj = \"%s warning: memory usage reached %dM at %s\" % \\\n                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                self._send_report(self.notify_mails, subj)\n                self.crawler.stats.set_value('memusage/warning_notified', 1)\n            self.warned = True",
        "begin_line": 97,
        "end_line": 110,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.memusage.MemoryUsage._send_report#112",
        "src_path": "scrapy/extensions/memusage.py",
        "class_name": "scrapy.extensions.memusage.MemoryUsage",
        "signature": "scrapy.extensions.memusage.MemoryUsage._send_report(self, rcpts, subject)",
        "snippet": "    def _send_report(self, rcpts, subject):\n        \"\"\"send notification mail with some additional useful info\"\"\"\n        stats = self.crawler.stats\n        s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n        s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n        s += \"Current memory usage           : %dM\\r\\n\" % (self.get_virtual_size()/1024/1024)\n\n        s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n        s += \"\\r\\n\"\n        s += pformat(get_engine_status(self.crawler.engine))\n        s += \"\\r\\n\"\n        self.mail.send(rcpts, subject, s)",
        "begin_line": 112,
        "end_line": 123,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.job.job_dir#3",
        "src_path": "scrapy/utils/job.py",
        "class_name": "scrapy.utils.job",
        "signature": "scrapy.utils.job.job_dir(settings)",
        "snippet": "def job_dir(settings):\n    path = settings['JOBDIR']\n    if path and not os.path.exists(path):\n        os.makedirs(path)\n    return path",
        "begin_line": 3,
        "end_line": 7,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.flatten#15",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.flatten(x)",
        "snippet": "def flatten(x):\n    \"\"\"flatten(sequence) -> list\n\n    Returns a single, flat list which contains all elements retrieved\n    from the sequence and all recursively contained sub-sequences\n    (iterables).\n\n    Examples:\n    >>> [1, 2, [3,4], (5,6)]\n    [1, 2, [3, 4], (5, 6)]\n    >>> flatten([[[1,2,3], (42,None)], [4,5], [6], 7, (8,9,10)])\n    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]\n    >>> flatten([\"foo\", \"bar\"])\n    ['foo', 'bar']\n    >>> flatten([\"foo\", [\"baz\", 42], \"bar\"])\n    ['foo', 'baz', 42, 'bar']\n    \"\"\"\n    return list(iflatten(x))",
        "begin_line": 15,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004610419548178884,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.iflatten#35",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.iflatten(x)",
        "snippet": "def iflatten(x):\n    \"\"\"iflatten(sequence) -> iterator\n\n    Similar to ``.flatten()``, but returns iterator instead\"\"\"\n    for el in x:\n        if is_listlike(el):\n            for el_ in flatten(el):\n                yield el_\n        else:\n            yield el",
        "begin_line": 35,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046707146193367583,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.is_listlike#47",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.is_listlike(x)",
        "snippet": "def is_listlike(x):\n    \"\"\"\n    >>> is_listlike(\"foo\")\n    False\n    >>> is_listlike(5)\n    False\n    >>> is_listlike(b\"foo\")\n    False\n    >>> is_listlike([b\"foo\"])\n    True\n    >>> is_listlike((b\"foo\",))\n    True\n    >>> is_listlike({})\n    True\n    >>> is_listlike(set())\n    True\n    >>> is_listlike((x for x in range(3)))\n    True\n    >>> is_listlike(six.moves.xrange(5))\n    True\n    \"\"\"\n    return hasattr(x, \"__iter__\") and not isinstance(x, (six.text_type, bytes))",
        "begin_line": 47,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000445632798573975,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.to_unicode#97",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_unicode(text, encoding=None, errors='strict')",
        "snippet": "def to_unicode(text, encoding=None, errors='strict'):\n    \"\"\"Return the unicode representation of a bytes object `text`. If `text`\n    is already an unicode object, return it as-is.\"\"\"\n    if isinstance(text, six.text_type):\n        return text\n    if not isinstance(text, (bytes, six.text_type)):\n        raise TypeError('to_unicode must receive a bytes, str or unicode '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.decode(encoding, errors)",
        "begin_line": 97,
        "end_line": 107,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.020833333333333332,
            "pseudo_dstar_susp": 0.020833333333333332,
            "pseudo_tarantula_susp": 0.020833333333333332,
            "pseudo_op2_susp": 0.020833333333333332,
            "pseudo_barinel_susp": 0.020833333333333332
        }
    },
    {
        "name": "scrapy.utils.python.to_bytes#110",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_bytes(text, encoding=None, errors='strict')",
        "snippet": "def to_bytes(text, encoding=None, errors='strict'):\n    \"\"\"Return the binary representation of `text`. If `text`\n    is already a bytes object, return it as-is.\"\"\"\n    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, six.string_types):\n        raise TypeError('to_bytes must receive a unicode, str or bytes '\n                        'object, got %s' % type(text).__name__)\n    if encoding is None:\n        encoding = 'utf-8'\n    return text.encode(encoding, errors)",
        "begin_line": 110,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.25,
            "pseudo_tarantula_susp": 0.25,
            "pseudo_op2_susp": 0.25,
            "pseudo_barinel_susp": 0.25
        }
    },
    {
        "name": "scrapy.utils.python.to_native_str#123",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.to_native_str(text, encoding=None, errors='strict')",
        "snippet": "def to_native_str(text, encoding=None, errors='strict'):\n    \"\"\" Return str representation of `text`\n    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n    if six.PY2:\n        return to_bytes(text, encoding, errors)\n    else:\n        return to_unicode(text, encoding, errors)",
        "begin_line": 123,
        "end_line": 129,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.02,
            "pseudo_dstar_susp": 0.02,
            "pseudo_tarantula_susp": 0.02,
            "pseudo_op2_susp": 0.02,
            "pseudo_barinel_susp": 0.02
        }
    },
    {
        "name": "scrapy.utils.python.memoizemethod_noargs#165",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.memoizemethod_noargs(method)",
        "snippet": "def memoizemethod_noargs(method):\n    \"\"\"Decorator to cache the result of a method (without arguments) using a\n    weak reference to its object\n    \"\"\"\n    cache = weakref.WeakKeyDictionary()\n    @wraps(method)\n    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]\n    return new_method",
        "begin_line": 165,
        "end_line": 175,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.new_method#171",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.new_method(self, *args, **kwargs)",
        "snippet": "    def new_method(self, *args, **kwargs):\n        if self not in cache:\n            cache[self] = method(self, *args, **kwargs)\n        return cache[self]",
        "begin_line": 171,
        "end_line": 174,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.isbinarytext#181",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.isbinarytext(text)",
        "snippet": "def isbinarytext(text):\n    \"\"\"Return True if the given text is considered binary, or False\n    otherwise, by looking for binary bytes at their chars\n    \"\"\"\n    if not isinstance(text, bytes):\n        raise TypeError(\"text must be bytes, got '%s'\" % type(text).__name__)\n    return any(c in _BINARYCHARS for c in text)",
        "begin_line": 181,
        "end_line": 187,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.get_func_args#190",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_func_args(func, stripself=False)",
        "snippet": "def get_func_args(func, stripself=False):\n    \"\"\"Return the argument name list of a callable\"\"\"\n    if inspect.isfunction(func):\n        func_args, _, _, _ = inspect.getargspec(func)\n    elif inspect.isclass(func):\n        return get_func_args(func.__init__, True)\n    elif inspect.ismethod(func):\n        return get_func_args(func.__func__, True)\n    elif inspect.ismethoddescriptor(func):\n        return []\n    elif isinstance(func, partial):\n        return [x for x in get_func_args(func.func)[len(func.args):]\n                if not (func.keywords and x in func.keywords)]\n    elif hasattr(func, '__call__'):\n        if inspect.isroutine(func):\n            return []\n        elif getattr(func, '__name__', None) == '__call__':\n            return []\n        else:\n            return get_func_args(func.__call__, True)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n    if stripself:\n        func_args.pop(0)\n    return func_args",
        "begin_line": 190,
        "end_line": 214,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.get_spec#217",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.get_spec(func)",
        "snippet": "def get_spec(func):\n    \"\"\"Returns (args, kwargs) tuple for a function\n    >>> import re\n    >>> get_spec(re.match)\n    (['pattern', 'string'], {'flags': 0})\n\n    >>> class Test(object):\n    ...     def __call__(self, val):\n    ...         pass\n    ...     def method(self, val, flags=0):\n    ...         pass\n\n    >>> get_spec(Test)\n    (['self', 'val'], {})\n\n    >>> get_spec(Test.method)\n    (['self', 'val'], {'flags': 0})\n\n    >>> get_spec(Test().method)\n    (['self', 'val'], {'flags': 0})\n    \"\"\"\n\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        spec = inspect.getargspec(func)\n    elif hasattr(func, '__call__'):\n        spec = inspect.getargspec(func.__call__)\n    else:\n        raise TypeError('%s is not callable' % type(func))\n\n    defaults = spec.defaults or []\n\n    firstdefault = len(spec.args) - len(defaults)\n    args = spec.args[:firstdefault]\n    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n    return args, kwargs",
        "begin_line": 217,
        "end_line": 251,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.equal_attributes#254",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python",
        "signature": "scrapy.utils.python.equal_attributes(obj1, obj2, attributes)",
        "snippet": "def equal_attributes(obj1, obj2, attributes):\n    \"\"\"Compare two objects attributes\"\"\"\n    # not attributes given return False by default\n    if not attributes:\n        return False\n\n    for attr in attributes:\n        # support callables like itemgetter\n        if callable(attr):\n            if not attr(obj1) == attr(obj2):\n                return False\n        else:\n            # check that objects has attribute\n            if not hasattr(obj1, attr):\n                return False\n            if not hasattr(obj2, attr):\n                return False\n            # compare object attributes\n            if not getattr(obj1, attr) == getattr(obj2, attr):\n                return False\n    # all attributes equal\n    return True",
        "begin_line": 254,
        "end_line": 275,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__init__#280",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__init__(self, default_factory)",
        "snippet": "    def __init__(self, default_factory):\n        self.default_factory = default_factory\n        self._weakdict = weakref.WeakKeyDictionary()",
        "begin_line": 280,
        "end_line": 282,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.python.WeakKeyCache.__getitem__#284",
        "src_path": "scrapy/utils/python.py",
        "class_name": "scrapy.utils.python.WeakKeyCache",
        "signature": "scrapy.utils.python.WeakKeyCache.__getitem__(self, key)",
        "snippet": "    def __getitem__(self, key):\n        if key not in self._weakdict:\n            self._weakdict[key] = self.default_factory(key)\n        return self._weakdict[key]",
        "begin_line": 284,
        "end_line": 287,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.ScrapyXPathExpr.from_xpath#12",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.ScrapyXPathExpr",
        "signature": "scrapy.selector.csstranslator.ScrapyXPathExpr.from_xpath(cls, xpath, textnode=False, attribute=None)",
        "snippet": "    def from_xpath(cls, xpath, textnode=False, attribute=None):\n        x = cls(path=xpath.path, element=xpath.element, condition=xpath.condition)\n        x.textnode = textnode\n        x.attribute = attribute\n        return x",
        "begin_line": 12,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.ScrapyXPathExpr.__str__#18",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.ScrapyXPathExpr",
        "signature": "scrapy.selector.csstranslator.ScrapyXPathExpr.__str__(self)",
        "snippet": "    def __str__(self):\n        path = super(ScrapyXPathExpr, self).__str__()\n        if self.textnode:\n            if path == '*':\n                path = 'text()'\n            elif path.endswith('::*/*'):\n                path = path[:-3] + 'text()'\n            else:\n                path += '/text()'\n\n        if self.attribute is not None:\n            if path.endswith('::*/*'):\n                path = path[:-2]\n            path += '/@%s' % self.attribute\n\n        return path",
        "begin_line": 18,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_element#44",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_element(self, selector)",
        "snippet": "    def xpath_element(self, selector):\n        xpath = super(TranslatorMixin, self).xpath_element(selector)\n        return ScrapyXPathExpr.from_xpath(xpath)",
        "begin_line": 44,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_pseudo_element#48",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_pseudo_element(self, xpath, pseudo_element)",
        "snippet": "    def xpath_pseudo_element(self, xpath, pseudo_element):\n        if isinstance(pseudo_element, FunctionalPseudoElement):\n            method = 'xpath_%s_functional_pseudo_element' % (\n                pseudo_element.name.replace('-', '_'))\n            method = _unicode_safe_getattr(self, method, None)\n            if not method:\n                raise ExpressionError(\n                    \"The functional pseudo-element ::%s() is unknown\"\n                % pseudo_element.name)\n            xpath = method(xpath, pseudo_element)\n        else:\n            method = 'xpath_%s_simple_pseudo_element' % (\n                pseudo_element.replace('-', '_'))\n            method = _unicode_safe_getattr(self, method, None)\n            if not method:\n                raise ExpressionError(\n                    \"The pseudo-element ::%s is unknown\"\n                    % pseudo_element)\n            xpath = method(xpath)\n        return xpath",
        "begin_line": 48,
        "end_line": 67,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_attr_functional_pseudo_element#69",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_attr_functional_pseudo_element(self, xpath, function)",
        "snippet": "    def xpath_attr_functional_pseudo_element(self, xpath, function):\n        if function.argument_types() not in (['STRING'], ['IDENT']):\n            raise ExpressionError(\n                \"Expected a single string or ident for ::attr(), got %r\"\n                % function.arguments)\n        return ScrapyXPathExpr.from_xpath(xpath,\n            attribute=function.arguments[0].value)",
        "begin_line": 69,
        "end_line": 75,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.csstranslator.TranslatorMixin.xpath_text_simple_pseudo_element#77",
        "src_path": "scrapy/selector/csstranslator.py",
        "class_name": "scrapy.selector.csstranslator.TranslatorMixin",
        "signature": "scrapy.selector.csstranslator.TranslatorMixin.xpath_text_simple_pseudo_element(self, xpath)",
        "snippet": "    def xpath_text_simple_pseudo_element(self, xpath):\n        \"\"\"Support selecting text nodes using ::text pseudo-element\"\"\"\n        return ScrapyXPathExpr.from_xpath(xpath, textnode=True)",
        "begin_line": 77,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.__init__#27",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.__init__(self, item=None, selector=None, response=None, **context)",
        "snippet": "    def __init__(self, item=None, selector=None, response=None, **context):\n        if selector is None and response is not None:\n            selector = self.default_selector_class(response)\n        self.selector = selector\n        context.update(selector=selector, response=response)\n        if item is None:\n            item = self.default_item_class()\n        self.item = context['item'] = item\n        self.context = context\n        self._values = defaultdict(list)",
        "begin_line": 27,
        "end_line": 36,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00048355899419729207,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_value#38",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def add_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._add_value(k, v)\n        else:\n            self._add_value(field_name, value)",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_value#48",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_value(self, field_name, value, *processors, **kw)",
        "snippet": "    def replace_value(self, field_name, value, *processors, **kw):\n        value = self.get_value(value, *processors, **kw)\n        if value is None:\n            return\n        if not field_name:\n            for k, v in six.iteritems(value):\n                self._replace_value(k, v)\n        else:\n            self._replace_value(field_name, value)",
        "begin_line": 48,
        "end_line": 56,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._add_value#58",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._add_value(self, field_name, value)",
        "snippet": "    def _add_value(self, field_name, value):\n        value = arg_to_iter(value)\n        processed_value = self._process_input_value(field_name, value)\n        if processed_value:\n            self._values[field_name] += arg_to_iter(processed_value)",
        "begin_line": 58,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00042354934349851756,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._replace_value#64",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._replace_value(self, field_name, value)",
        "snippet": "    def _replace_value(self, field_name, value):\n        self._values.pop(field_name, None)\n        self._add_value(field_name, value)",
        "begin_line": 64,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004730368968779565,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_value#68",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_value(self, value, *processors, **kw)",
        "snippet": "    def get_value(self, value, *processors, **kw):\n        regex = kw.get('re', None)\n        if regex:\n            value = arg_to_iter(value)\n            value = flatten([extract_regex(regex, x) for x in value])\n\n        for proc in processors:\n            if value is None:\n                break\n            proc = wrap_loader_context(proc, self.context)\n            value = proc(value)\n        return value",
        "begin_line": 68,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.load_item#81",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.load_item(self)",
        "snippet": "    def load_item(self):\n        item = self.item\n        for field_name in tuple(self._values):\n            value = self.get_output_value(field_name)\n            if value is not None:\n                item[field_name] = value\n        return item",
        "begin_line": 81,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_value#89",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_value(self, field_name)",
        "snippet": "    def get_output_value(self, field_name):\n        proc = self.get_output_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        try:\n            return proc(self._values[field_name])\n        except Exception as e:\n            raise ValueError(\"Error with output processor: field=%r value=%r error='%s: %s'\" % \\\n                (field_name, self._values[field_name], type(e).__name__, str(e)))",
        "begin_line": 89,
        "end_line": 96,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_collected_values#98",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_collected_values(self, field_name)",
        "snippet": "    def get_collected_values(self, field_name):\n        return self._values[field_name]",
        "begin_line": 98,
        "end_line": 99,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_input_processor#101",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_input_processor(self, field_name)",
        "snippet": "    def get_input_processor(self, field_name):\n        proc = getattr(self, '%s_in' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'input_processor', \\\n                self.default_input_processor)\n        return proc",
        "begin_line": 101,
        "end_line": 106,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00046425255338904364,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_output_processor#108",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_output_processor(self, field_name)",
        "snippet": "    def get_output_processor(self, field_name):\n        proc = getattr(self, '%s_out' % field_name, None)\n        if not proc:\n            proc = self._get_item_field_attr(field_name, 'output_processor', \\\n                self.default_output_processor)\n        return proc",
        "begin_line": 108,
        "end_line": 113,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004342162396873643,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._process_input_value#115",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._process_input_value(self, field_name, value)",
        "snippet": "    def _process_input_value(self, field_name, value):\n        proc = self.get_input_processor(field_name)\n        proc = wrap_loader_context(proc, self.context)\n        return proc(value)",
        "begin_line": 115,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00042354934349851756,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_item_field_attr#120",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_item_field_attr(self, field_name, key, default=None)",
        "snippet": "    def _get_item_field_attr(self, field_name, key, default=None):\n        if isinstance(self.item, Item):\n            value = self.item.fields[field_name].get(key, default)\n        else:\n            value = default\n        return value",
        "begin_line": 120,
        "end_line": 125,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._check_selector_method#127",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._check_selector_method(self)",
        "snippet": "    def _check_selector_method(self):\n        if self.selector is None:\n            raise RuntimeError(\"To use XPath or CSS selectors, \"\n                \"%s must be instantiated with a selector \"\n                \"or a response\" % self.__class__.__name__)",
        "begin_line": 127,
        "end_line": 131,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_xpath#133",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def add_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 133,
        "end_line": 135,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_xpath#137",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_xpath(self, field_name, xpath, *processors, **kw)",
        "snippet": "    def replace_xpath(self, field_name, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 137,
        "end_line": 139,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_xpath#141",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_xpath(self, xpath, *processors, **kw)",
        "snippet": "    def get_xpath(self, xpath, *processors, **kw):\n        values = self._get_xpathvalues(xpath, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 141,
        "end_line": 143,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_xpathvalues#149",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_xpathvalues(self, xpaths, **kw)",
        "snippet": "    def _get_xpathvalues(self, xpaths, **kw):\n        self._check_selector_method()\n        xpaths = arg_to_iter(xpaths)\n        return flatten([self.selector.xpath(xpath).extract() for xpath in xpaths])",
        "begin_line": 149,
        "end_line": 152,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005558643690939411,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.add_css#154",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.add_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def add_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.add_value(field_name, values, *processors, **kw)",
        "begin_line": 154,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006064281382656155,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.replace_css#158",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.replace_css(self, field_name, css, *processors, **kw)",
        "snippet": "    def replace_css(self, field_name, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        self.replace_value(field_name, values, *processors, **kw)",
        "begin_line": 158,
        "end_line": 160,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader.get_css#162",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader.get_css(self, css, *processors, **kw)",
        "snippet": "    def get_css(self, css, *processors, **kw):\n        values = self._get_cssvalues(css, **kw)\n        return self.get_value(values, *processors, **kw)",
        "begin_line": 162,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.loader.__init__.ItemLoader._get_cssvalues#166",
        "src_path": "scrapy/loader/__init__.py",
        "class_name": "scrapy.loader.__init__.ItemLoader",
        "signature": "scrapy.loader.__init__.ItemLoader._get_cssvalues(self, csss, **kw)",
        "snippet": "    def _get_cssvalues(self, csss, **kw):\n        self._check_selector_method()\n        csss = arg_to_iter(csss)\n        return flatten([self.selector.css(css).extract() for css in csss])",
        "begin_line": 166,
        "end_line": 169,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.httpobj.urlparse_cached#8",
        "src_path": "scrapy/utils/httpobj.py",
        "class_name": "scrapy.utils.httpobj",
        "signature": "scrapy.utils.httpobj.urlparse_cached(request_or_response)",
        "snippet": "def urlparse_cached(request_or_response):\n    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n    Request or Response object\n    \"\"\"\n    if request_or_response not in _urlparse_cache:\n        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n    return _urlparse_cache[request_or_response]",
        "begin_line": 8,
        "end_line": 14,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.1,
            "pseudo_dstar_susp": 0.1,
            "pseudo_tarantula_susp": 0.1,
            "pseudo_op2_susp": 0.1,
            "pseudo_barinel_susp": 0.1
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.__init__#11",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.__init__(self, crawler)",
        "snippet": "    def __init__(self, crawler):\n        self.crawler = crawler\n        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):\n            raise NotConfigured\n\n        self.debug = crawler.settings.getbool(\"AUTOTHROTTLE_DEBUG\")\n        self.target_concurrency = crawler.settings.getfloat(\"AUTOTHROTTLE_TARGET_CONCURRENCY\")\n        crawler.signals.connect(self._spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(self._response_downloaded, signal=signals.response_downloaded)",
        "begin_line": 11,
        "end_line": 19,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle.from_crawler#22",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls(crawler)",
        "begin_line": 22,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._spider_opened#25",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._spider_opened(self, spider)",
        "snippet": "    def _spider_opened(self, spider):\n        self.mindelay = self._min_delay(spider)\n        self.maxdelay = self._max_delay(spider)\n        spider.download_delay = self._start_delay(spider)",
        "begin_line": 25,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._min_delay#30",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._min_delay(self, spider)",
        "snippet": "    def _min_delay(self, spider):\n        s = self.crawler.settings\n        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))",
        "begin_line": 30,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._max_delay#34",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._max_delay(self, spider)",
        "snippet": "    def _max_delay(self, spider):\n        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')",
        "begin_line": 34,
        "end_line": 35,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._start_delay#37",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._start_delay(self, spider)",
        "snippet": "    def _start_delay(self, spider):\n        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))",
        "begin_line": 37,
        "end_line": 38,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._response_downloaded#40",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._response_downloaded(self, response, request, spider)",
        "snippet": "    def _response_downloaded(self, response, request, spider):\n        key, slot = self._get_slot(request, spider)\n        latency = request.meta.get('download_latency')\n        if latency is None or slot is None:\n            return\n\n        olddelay = slot.delay\n        self._adjust_delay(slot, latency, response)\n        if self.debug:\n            diff = slot.delay - olddelay\n            size = len(response.body)\n            conc = len(slot.transferring)\n            logger.info(\n                \"slot: %(slot)s | conc:%(concurrency)2d | \"\n                \"delay:%(delay)5d ms (%(delaydiff)+d) | \"\n                \"latency:%(latency)5d ms | size:%(size)6d bytes\",\n                {\n                    'slot': key, 'concurrency': conc,\n                    'delay': slot.delay * 1000, 'delaydiff': diff * 1000,\n                    'latency': latency * 1000, 'size': size\n                },\n                extra={'spider': spider}\n            )",
        "begin_line": 40,
        "end_line": 62,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._get_slot#64",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._get_slot(self, request, spider)",
        "snippet": "    def _get_slot(self, request, spider):\n        key = request.meta.get('download_slot')\n        return key, self.crawler.engine.downloader.slots.get(key)",
        "begin_line": 64,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.throttle.AutoThrottle._adjust_delay#68",
        "src_path": "scrapy/extensions/throttle.py",
        "class_name": "scrapy.extensions.throttle.AutoThrottle",
        "signature": "scrapy.extensions.throttle.AutoThrottle._adjust_delay(self, slot, latency, response)",
        "snippet": "    def _adjust_delay(self, slot, latency, response):\n        \"\"\"Define delay adjustment policy\"\"\"\n\n        # If a server needs `latency` seconds to respond then\n        # we should send a request each `latency/N` seconds\n        # to have N requests processed in parallel\n        target_delay = latency / self.target_concurrency\n\n        # Adjust the delay to make it closer to target_delay\n        new_delay = (slot.delay + target_delay) / 2.0\n\n        # If target delay is bigger than old delay, then use it instead of mean.\n        # It works better with problematic sites.\n        new_delay = max(target_delay, new_delay)\n\n        # Make sure self.mindelay <= new_delay <= self.max_delay\n        new_delay = min(max(self.mindelay, new_delay), self.maxdelay)\n\n        # Dont adjust delay if response status != 200 and new delay is smaller\n        # than old one, as error pages (and redirections) are usually small and\n        # so tend to reduce latency, thus provoking a positive feedback by\n        # reducing delay instead of increase.\n        if response.status != 200 and new_delay <= slot.delay:\n            return\n\n        slot.delay = new_delay",
        "begin_line": 68,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.lxmldocument._factory#11",
        "src_path": "scrapy/selector/lxmldocument.py",
        "class_name": "scrapy.selector.lxmldocument",
        "signature": "scrapy.selector.lxmldocument._factory(response, parser_cls)",
        "snippet": "def _factory(response, parser_cls):\n    url = response.url\n    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'\n    parser = parser_cls(recover=True, encoding='utf8')\n    return etree.fromstring(body, parser=parser, base_url=url)",
        "begin_line": 11,
        "end_line": 15,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004580852038479157,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.lxmldocument.LxmlDocument.__new__#23",
        "src_path": "scrapy/selector/lxmldocument.py",
        "class_name": "scrapy.selector.lxmldocument.LxmlDocument",
        "signature": "scrapy.selector.lxmldocument.LxmlDocument.__new__(cls, response, parser=etree.HTMLParser)",
        "snippet": "    def __new__(cls, response, parser=etree.HTMLParser):\n        cache = cls.cache.setdefault(response, {})\n        if parser not in cache:\n            obj = object_ref.__new__(cls)\n            cache[parser] = _factory(response, parser)\n        return cache[parser]",
        "begin_line": 23,
        "end_line": 28,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004580852038479157,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.common.newsetter#2",
        "src_path": "scrapy/http/common.py",
        "class_name": "scrapy.http.common",
        "signature": "scrapy.http.common.newsetter(self, value)",
        "snippet": "    def newsetter(self, value):\n        c = self.__class__.__name__\n        msg = \"%s.%s is not modifiable, use %s.replace() instead\" % (c, attrname, c)\n        raise AttributeError(msg)",
        "begin_line": 2,
        "end_line": 5,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.__init__#10",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.__init__(self, jobdir=None)",
        "snippet": "    def __init__(self, jobdir=None):\n        self.jobdir = jobdir",
        "begin_line": 10,
        "end_line": 11,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.from_crawler#14",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        obj = cls(job_dir(crawler.settings))\n        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n        return obj",
        "begin_line": 14,
        "end_line": 18,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_closed#20",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_closed(self, spider)",
        "snippet": "    def spider_closed(self, spider):\n        if self.jobdir:\n            with open(self.statefn, 'wb') as f:\n                pickle.dump(spider.state, f, protocol=2)",
        "begin_line": 20,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.spider_opened#25",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        if self.jobdir and os.path.exists(self.statefn):\n            with open(self.statefn, 'rb') as f:\n                spider.state = pickle.load(f)\n        else:\n            spider.state = {}",
        "begin_line": 25,
        "end_line": 30,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.spiderstate.SpiderState.statefn#33",
        "src_path": "scrapy/extensions/spiderstate.py",
        "class_name": "scrapy.extensions.spiderstate.SpiderState",
        "signature": "scrapy.extensions.spiderstate.SpiderState.statefn(self)",
        "snippet": "    def statefn(self):\n        return os.path.join(self.jobdir, 'spider.state')",
        "begin_line": 33,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.request.request_fingerprint#20",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_fingerprint(request, include_headers=None)",
        "snippet": "def request_fingerprint(request, include_headers=None):\n    \"\"\"\n    Return the request fingerprint.\n\n    The request fingerprint is a hash that uniquely identifies the resource the\n    request points to. For example, take the following two urls:\n\n    http://www.example.com/query?id=111&cat=222\n    http://www.example.com/query?cat=222&id=111\n\n    Even though those are two different URLs both point to the same resource\n    and are equivalent (ie. they should return the same response).\n\n    Another example are cookies used to store session ids. Suppose the\n    following page is only accesible to authenticated users:\n\n    http://www.example.com/members/offers.html\n\n    Lot of sites use a cookie to store the session id, which adds a random\n    component to the HTTP Request and thus should be ignored when calculating\n    the fingerprint.\n\n    For this reason, request headers are ignored by default when calculating\n    the fingeprint. If you want to include specific headers use the\n    include_headers argument, which is a list of Request headers to include.\n\n    \"\"\"\n    if include_headers:\n        include_headers = tuple([to_bytes(h.lower())\n                                 for h in sorted(include_headers)])\n    cache = _fingerprint_cache.setdefault(request, {})\n    if include_headers not in cache:\n        fp = hashlib.sha1()\n        fp.update(to_bytes(request.method))\n        fp.update(to_bytes(canonicalize_url(request.url)))\n        fp.update(request.body or b'')\n        if include_headers:\n            for hdr in include_headers:\n                if hdr in request.headers:\n                    fp.update(hdr)\n                    for v in request.headers.getlist(hdr):\n                        fp.update(v)\n        cache[include_headers] = fp.hexdigest()\n    return cache[include_headers]",
        "begin_line": 20,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.request.request_authenticate#66",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_authenticate(request, username, password)",
        "snippet": "def request_authenticate(request, username, password):\n    \"\"\"Autenticate the given request (in place) using the HTTP basic access\n    authentication mechanism (RFC 2617) and the given username and password\n    \"\"\"\n    request.headers['Authorization'] = basic_auth_header(username, password)",
        "begin_line": 66,
        "end_line": 70,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.request.request_httprepr#73",
        "src_path": "scrapy/utils/request.py",
        "class_name": "scrapy.utils.request",
        "signature": "scrapy.utils.request.request_httprepr(request)",
        "snippet": "def request_httprepr(request):\n    \"\"\"Return the raw HTTP representation (as bytes) of the given request.\n    This is provided only for reference since it's not the actual stream of\n    bytes that will be send when performing the request (that's controlled\n    by Twisted).\n    \"\"\"\n    parsed = urlparse_cached(request)\n    path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))\n    s = to_bytes(request.method) + b\" \" + to_bytes(path) + b\" HTTP/1.1\\r\\n\"\n    s += b\"Host: \" + to_bytes(parsed.hostname) + b\"\\r\\n\"\n    if request.headers:\n        s += request.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += request.body\n    return s",
        "begin_line": 73,
        "end_line": 87,
        "comment": "",
        "is_bug": true,
        "susp": {
            "pseudo_ochiai_susp": 0.25,
            "pseudo_dstar_susp": 0.25,
            "pseudo_tarantula_susp": 0.25,
            "pseudo_op2_susp": 0.25,
            "pseudo_barinel_susp": 0.25
        }
    },
    {
        "name": "scrapy.utils.http.decode_chunked_transfer#9",
        "src_path": "scrapy/utils/http.py",
        "class_name": "scrapy.utils.http",
        "signature": "scrapy.utils.http.decode_chunked_transfer(chunked_body)",
        "snippet": "def decode_chunked_transfer(chunked_body):\n    \"\"\"Parsed body received with chunked transfer encoding, and return the\n    decoded body.\n\n    For more info see:\n    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n    \"\"\"\n    body, h, t = '', '', chunked_body\n    while t:\n        h, t = t.split('\\r\\n', 1)\n        if h == '0':\n            break\n        size = int(h, 16)\n        body += t[:size]\n        t = t[size+2:]\n    return body",
        "begin_line": 9,
        "end_line": 25,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.test.get_crawler#23",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_crawler(spidercls=None, settings_dict=None)",
        "snippet": "def get_crawler(spidercls=None, settings_dict=None):\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n    from scrapy.settings import Settings\n    from scrapy.spiders import Spider\n\n    runner = CrawlerRunner(Settings(settings_dict))\n    return runner._create_crawler(spidercls or Spider)",
        "begin_line": 23,
        "end_line": 33,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.test.get_pythonpath#35",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_pythonpath()",
        "snippet": "def get_pythonpath():\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module('scrapy').__path__[0]\n    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')",
        "begin_line": 35,
        "end_line": 39,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.test.get_testenv#41",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.get_testenv()",
        "snippet": "def get_testenv():\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env['PYTHONPATH'] = get_pythonpath()\n    return env",
        "begin_line": 41,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.test.assert_samelines#49",
        "src_path": "scrapy/utils/test.py",
        "class_name": "scrapy.utils.test",
        "signature": "scrapy.utils.test.assert_samelines(testcase, text1, text2, msg=None)",
        "snippet": "def assert_samelines(testcase, text1, text2, msg=None):\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)",
        "begin_line": 49,
        "end_line": 53,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "conftest.chdir#38",
        "src_path": "conftest.py",
        "class_name": "conftest",
        "signature": "conftest.chdir(tmpdir)",
        "snippet": "def chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()",
        "begin_line": 38,
        "end_line": 40,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.015625,
            "pseudo_dstar_susp": 0.015625,
            "pseudo_tarantula_susp": 0.015625,
            "pseudo_op2_susp": 0.015625,
            "pseudo_barinel_susp": 0.015625
        }
    },
    {
        "name": "scrapy.utils.log.failure_to_exc_info#18",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log",
        "signature": "scrapy.utils.log.failure_to_exc_info(failure)",
        "snippet": "def failure_to_exc_info(failure):\n    \"\"\"Extract exc_info from Failure instances\"\"\"\n    if isinstance(failure, Failure):\n        return (failure.type, failure.value, failure.getTracebackObject())",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.log.LogCounterHandler.__init__#155",
        "src_path": "scrapy/utils/log.py",
        "class_name": "scrapy.utils.log.LogCounterHandler",
        "signature": "scrapy.utils.log.LogCounterHandler.__init__(self, crawler, *args, **kwargs)",
        "snippet": "    def __init__(self, crawler, *args, **kwargs):\n        super(LogCounterHandler, self).__init__(*args, **kwargs)\n        self.crawler = crawler",
        "begin_line": 155,
        "end_line": 157,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_any_domain#20",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_any_domain(url, domains)",
        "snippet": "def url_is_from_any_domain(url, domains):\n    \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n    host = parse_url(url).netloc.lower()\n    if not host:\n        return False\n    domains = [d.lower() for d in domains]\n    return any((host == d) or (host.endswith('.%s' % d)) for d in domains)",
        "begin_line": 20,
        "end_line": 26,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url.url_is_from_spider#29",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.url_is_from_spider(url, spider)",
        "snippet": "def url_is_from_spider(url, spider):\n    \"\"\"Return True if the url belongs to the given spider\"\"\"\n    return url_is_from_any_domain(url,\n        [spider.name] + list(getattr(spider, 'allowed_domains', [])))",
        "begin_line": 29,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url.canonicalize_url#39",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.canonicalize_url(url, keep_blank_values=True, keep_fragments=False, encoding=None)",
        "snippet": "def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                     encoding=None):\n    \"\"\"Canonicalize the given url by applying the following procedures:\n\n    - sort query arguments, first by key, then by value\n    - percent encode paths and query arguments. non-ASCII characters are\n      percent-encoded using UTF-8 (RFC-3986)\n    - normalize all spaces (in query arguments) '+' (plus symbol)\n    - normalize percent encodings case (%2f -> %2F)\n    - remove query arguments with blank values (unless keep_blank_values is True)\n    - remove fragments (unless keep_fragments is True)\n\n    The url passed can be a str or unicode, while the url returned is always a\n    str.\n\n    For examples see the tests in tests/test_utils_url.py\n    \"\"\"\n\n    scheme, netloc, path, params, query, fragment = parse_url(url)\n    keyvals = parse_qsl(query, keep_blank_values)\n    keyvals.sort()\n    query = urlencode(keyvals)\n\n    # XXX: copied from w3lib.url.safe_url_string to add encoding argument\n    # path = to_native_str(path, encoding)\n    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'\n\n    path = safe_url_string(_unquotepath(path)) or '/'\n    fragment = '' if not keep_fragments else fragment\n    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))",
        "begin_line": 39,
        "end_line": 68,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004399472063352398,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url._unquotepath#71",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url._unquotepath(path)",
        "snippet": "def _unquotepath(path):\n    for reserved in ('2f', '2F', '3f', '3F'):\n        path = path.replace('%' + reserved, '%25' + reserved.upper())\n    return unquote(path)",
        "begin_line": 71,
        "end_line": 74,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004399472063352398,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url.parse_url#77",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.parse_url(url, encoding=None)",
        "snippet": "def parse_url(url, encoding=None):\n    \"\"\"Return urlparsed url from the given argument (which could be an already\n    parsed url)\n    \"\"\"\n    if isinstance(url, ParseResult):\n        return url\n    return urlparse(to_native_str(url, encoding))",
        "begin_line": 77,
        "end_line": 83,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004342162396873643,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.url.escape_ajax#86",
        "src_path": "scrapy/utils/url.py",
        "class_name": "scrapy.utils.url",
        "signature": "scrapy.utils.url.escape_ajax(url)",
        "snippet": "def escape_ajax(url):\n    \"\"\"\n    Return the crawleable url according to:\n    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n\n    >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?k1=v1&k2=v2#!key=value\")\n    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html?#!key=value\")\n    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n    >>> escape_ajax(\"www.example.com/ajax.html#!\")\n    'www.example.com/ajax.html?_escaped_fragment_='\n\n    URLs that are not \"AJAX crawlable\" (according to Google) returned as-is:\n\n    >>> escape_ajax(\"www.example.com/ajax.html#key=value\")\n    'www.example.com/ajax.html#key=value'\n    >>> escape_ajax(\"www.example.com/ajax.html#\")\n    'www.example.com/ajax.html#'\n    >>> escape_ajax(\"www.example.com/ajax.html\")\n    'www.example.com/ajax.html'\n    \"\"\"\n    defrag, frag = urldefrag(url)\n    if not frag.startswith('!'):\n        return url\n    return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])",
        "begin_line": 86,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.037037037037037035,
            "pseudo_dstar_susp": 0.037037037037037035,
            "pseudo_tarantula_susp": 0.037037037037037035,
            "pseudo_op2_susp": 0.037037037037037035,
            "pseudo_barinel_susp": 0.037037037037037035
        }
    },
    {
        "name": "scrapy.crawler.Crawler.__init__#26",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.Crawler",
        "signature": "scrapy.crawler.Crawler.__init__(self, spidercls, settings=None)",
        "snippet": "    def __init__(self, spidercls, settings=None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n\n        self.spidercls = spidercls\n        self.settings = settings.copy()\n        self.spidercls.update_settings(self.settings)\n\n        self.signals = SignalManager(self)\n        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n\n        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n        logging.root.addHandler(handler)\n        # lambda is assigned to Crawler attribute because this way it is not\n        # garbage collected after leaving __init__ scope\n        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n\n        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n        self.logformatter = lf_cls.from_crawler(self)\n        self.extensions = ExtensionManager.from_crawler(self)\n\n        self.settings.freeze()\n        self.crawling = False\n        self.spider = None\n        self.engine = None",
        "begin_line": 26,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner.__init__#111",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner.__init__(self, settings=None)",
        "snippet": "    def __init__(self, settings=None):\n        if isinstance(settings, dict) or settings is None:\n            settings = Settings(settings)\n        self.settings = settings\n        self.spider_loader = _get_spider_loader(settings)\n        self._crawlers = set()\n        self._active = set()",
        "begin_line": 111,
        "end_line": 117,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.crawler.CrawlerRunner._create_crawler#165",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler.CrawlerRunner",
        "signature": "scrapy.crawler.CrawlerRunner._create_crawler(self, spidercls)",
        "snippet": "    def _create_crawler(self, spidercls):\n        if isinstance(spidercls, six.string_types):\n            spidercls = self.spider_loader.load(spidercls)\n        return Crawler(spidercls, self.settings)",
        "begin_line": 165,
        "end_line": 168,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.crawler._get_spider_loader#278",
        "src_path": "scrapy/crawler.py",
        "class_name": "scrapy.crawler",
        "signature": "scrapy.crawler._get_spider_loader(settings)",
        "snippet": "def _get_spider_loader(settings):\n    \"\"\" Get SpiderLoader instance from settings \"\"\"\n    if settings.get('SPIDER_MANAGER_CLASS'):\n        warnings.warn(\n            'SPIDER_MANAGER_CLASS option is deprecated. '\n            'Please use SPIDER_LOADER_CLASS.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n                            settings.get('SPIDER_LOADER_CLASS'))\n    loader_cls = load_object(cls_path)\n    try:\n        verifyClass(ISpiderLoader, loader_cls)\n    except DoesNotImplement:\n        warnings.warn(\n            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n            'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n            'Please add all missing methods to avoid unexpected runtime errors.',\n            category=ScrapyDeprecationWarning, stacklevel=2\n        )\n    return loader_cls.from_settings(settings.frozencopy())",
        "begin_line": 278,
        "end_line": 298,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__init__#25",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__init__(self, name=None, **kwargs)",
        "snippet": "    def __init__(self, name=None, **kwargs):\n        if name is not None:\n            self.name = name\n        elif not getattr(self, 'name', None):\n            raise ValueError(\"%s must have a name\" % type(self).__name__)\n        self.__dict__.update(kwargs)\n        if not hasattr(self, 'start_urls'):\n            self.start_urls = []",
        "begin_line": 25,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.000510986203372509,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.update_settings#79",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.update_settings(cls, settings)",
        "snippet": "    def update_settings(cls, settings):\n        settings.setdict(cls.custom_settings or {}, priority='spider')",
        "begin_line": 79,
        "end_line": 80,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.handles_request#83",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.handles_request(cls, request)",
        "snippet": "    def handles_request(cls, request):\n        return url_is_from_spider(request.url, cls)",
        "begin_line": 83,
        "end_line": 84,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.spiders.__init__.Spider.__str__#92",
        "src_path": "scrapy/spiders/__init__.py",
        "class_name": "scrapy.spiders.__init__.Spider",
        "signature": "scrapy.spiders.__init__.Spider.__str__(self)",
        "snippet": "    def __str__(self):\n        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))",
        "begin_line": 92,
        "end_line": 93,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.__init__#17",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        formdata = kwargs.pop('formdata', None)\n        if formdata and kwargs.get('method') is None:\n            kwargs['method'] = 'POST'\n\n        super(FormRequest, self).__init__(*args, **kwargs)\n\n        if formdata:\n            items = formdata.items() if isinstance(formdata, dict) else formdata\n            querystr = _urlencode(items, self.encoding)\n            if self.method == 'POST':\n                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')\n                self._set_body(querystr)\n            else:\n                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)",
        "begin_line": 17,
        "end_line": 31,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form.FormRequest.from_response#34",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form.FormRequest",
        "signature": "scrapy.http.request.form.FormRequest.from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None, clickdata=None, dont_click=False, formxpath=None, **kwargs)",
        "snippet": "    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n                      clickdata=None, dont_click=False, formxpath=None, **kwargs):\n        kwargs.setdefault('encoding', response.encoding)\n        form = _get_form(response, formname, formid, formnumber, formxpath)\n        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n        url = _get_form_url(form, kwargs.pop('url', None))\n        method = kwargs.pop('method', form.method)\n        return cls(url=url, method=method, formdata=formdata, **kwargs)",
        "begin_line": 34,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._get_form_url#44",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form_url(form, url)",
        "snippet": "def _get_form_url(form, url):\n    if url is None:\n        return form.action or form.base_url\n    return urljoin(form.base_url, url)",
        "begin_line": 44,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._urlencode#50",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._urlencode(seq, enc)",
        "snippet": "def _urlencode(seq, enc):\n    values = [(to_bytes(k, enc), to_bytes(v, enc))\n              for k, vs in seq\n              for v in (vs if is_listlike(vs) else [vs])]\n    return urlencode(values, doseq=1)",
        "begin_line": 50,
        "end_line": 54,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0008143322475570033,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._get_form#57",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_form(response, formname, formid, formnumber, formxpath)",
        "snippet": "def _get_form(response, formname, formid, formnumber, formxpath):\n    \"\"\"Find the form element \"\"\"\n    from scrapy.selector.lxmldocument import LxmlDocument\n    root = LxmlDocument(response, lxml.html.HTMLParser)\n    forms = root.xpath('//form')\n    if not forms:\n        raise ValueError(\"No <form> element found in %s\" % response)\n\n    if formname is not None:\n        f = root.xpath('//form[@name=\"%s\"]' % formname)\n        if f:\n            return f[0]\n\n    if formid is not None:\n        f = root.xpath('//form[@id=\"%s\"]' % formid)\n        if f:\n            return f[0]\n            \n    # Get form element from xpath, if not found, go up\n    if formxpath is not None:\n        nodes = root.xpath(formxpath)\n        if nodes:\n            el = nodes[0]\n            while True:\n                if el.tag == 'form':\n                    return el\n                el = el.getparent()\n                if el is None:\n                    break\n        raise ValueError('No <form> element found with %s' % formxpath)\n\n    # If we get here, it means that either formname was None\n    # or invalid\n    if formnumber is not None:\n        try:\n            form = forms[formnumber]\n        except IndexError:\n            raise IndexError(\"Form number %d not found in %s\" %\n                             (formnumber, response))\n        else:\n            return form",
        "begin_line": 57,
        "end_line": 97,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._get_inputs#100",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_inputs(form, formdata, dont_click, clickdata, response)",
        "snippet": "def _get_inputs(form, formdata, dont_click, clickdata, response):\n    try:\n        formdata = dict(formdata or ())\n    except (ValueError, TypeError):\n        raise ValueError('formdata should be a dict or iterable of tuples')\n\n    inputs = form.xpath('descendant::textarea'\n                        '|descendant::select'\n                        '|descendant::input[@type!=\"submit\" and @type!=\"image\" and @type!=\"reset\"'\n                        'and ((@type!=\"checkbox\" and @type!=\"radio\") or @checked)]')\n    values = [(k, u'' if v is None else v)\n              for k, v in (_value(e) for e in inputs)\n              if k and k not in formdata]\n\n    if not dont_click:\n        clickable = _get_clickable(clickdata, form)\n        if clickable and clickable[0] not in formdata and not clickable[0] is None:\n            values.append(clickable)\n\n    values.extend(formdata.items())\n    return values",
        "begin_line": 100,
        "end_line": 120,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._value#123",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._value(ele)",
        "snippet": "def _value(ele):\n    n = ele.name\n    v = ele.value\n    if ele.tag == 'select':\n        return _select_value(ele, n, v)\n    return n, v",
        "begin_line": 123,
        "end_line": 128,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.http.request.form._get_clickable#146",
        "src_path": "scrapy/http/request/form.py",
        "class_name": "scrapy.http.request.form",
        "signature": "scrapy.http.request.form._get_clickable(clickdata, form)",
        "snippet": "def _get_clickable(clickdata, form):\n    \"\"\"\n    Returns the clickable element specified in clickdata,\n    if the latter is given. If not, it returns the first\n    clickable element found\n    \"\"\"\n    clickables = [el for el in form.xpath('.//input[@type=\"submit\"]')]\n    if not clickables:\n        return\n\n    # If we don't have clickdata, we just use the first clickable element\n    if clickdata is None:\n        el = clickables[0]\n        return (el.name, el.value)\n\n    # If clickdata is given, we compare it to the clickable elements to find a\n    # match. We first look to see if the number is specified in clickdata,\n    # because that uniquely identifies the element\n    nr = clickdata.get('nr', None)\n    if nr is not None:\n        try:\n            el = list(form.inputs)[nr]\n        except IndexError:\n            pass\n        else:\n            return (el.name, el.value)\n\n    # We didn't find it, so now we build an XPath expression out of the other\n    # arguments, because they can be used as such\n    xpath = u'.//*' + \\\n            u''.join(u'[@%s=\"%s\"]' % c for c in six.iteritems(clickdata))\n    el = form.xpath(xpath)\n    if len(el) == 1:\n        return (el[0].name, el[0].value)\n    elif len(el) > 1:\n        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n                         \"in clickdata: %r\" % (el, clickdata))\n    else:\n        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))",
        "begin_line": 146,
        "end_line": 184,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.__init__#14",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.__init__(self, stats, interval=60.0)",
        "snippet": "    def __init__(self, stats, interval=60.0):\n        self.stats = stats\n        self.interval = interval\n        self.multiplier = 60.0 / self.interval",
        "begin_line": 14,
        "end_line": 17,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.from_crawler#20",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n        if not interval:\n            raise NotConfigured\n        o = cls(crawler.stats, interval)\n        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n        return o",
        "begin_line": 20,
        "end_line": 27,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_opened#29",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_opened(self, spider)",
        "snippet": "    def spider_opened(self, spider):\n        self.pagesprev = 0\n        self.itemsprev = 0\n\n        self.task = task.LoopingCall(self.log, spider)\n        self.task.start(self.interval)",
        "begin_line": 29,
        "end_line": 34,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.log#36",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.log(self, spider)",
        "snippet": "    def log(self, spider):\n        items = self.stats.get_value('item_scraped_count', 0)\n        pages = self.stats.get_value('response_received_count', 0)\n        irate = (items - self.itemsprev) * self.multiplier\n        prate = (pages - self.pagesprev) * self.multiplier\n        self.pagesprev, self.itemsprev = pages, items\n\n        msg = (\"Crawled %(pages)d pages (at %(pagerate)d pages/min), \"\n               \"scraped %(items)d items (at %(itemrate)d items/min)\")\n        log_args = {'pages': pages, 'pagerate': prate,\n                    'items': items, 'itemrate': irate}\n        logger.info(msg, log_args, extra={'spider': spider})",
        "begin_line": 36,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.logstats.LogStats.spider_closed#49",
        "src_path": "scrapy/extensions/logstats.py",
        "class_name": "scrapy.extensions.logstats.LogStats",
        "signature": "scrapy.extensions.logstats.LogStats.spider_closed(self, spider, reason)",
        "snippet": "    def spider_closed(self, spider, reason):\n        if self.task.running:\n            self.task.stop()",
        "begin_line": 49,
        "end_line": 51,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extension.ExtensionManager._get_mwlist_from_settings#14",
        "src_path": "scrapy/extension.py",
        "class_name": "scrapy.extension.ExtensionManager",
        "signature": "scrapy.extension.ExtensionManager._get_mwlist_from_settings(cls, settings)",
        "snippet": "    def _get_mwlist_from_settings(cls, settings):\n        return build_component_list(settings['EXTENSIONS_BASE'], \\\n            settings['EXTENSIONS'])",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.crawled#33",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.crawled(self, request, response, spider)",
        "snippet": "    def crawled(self, request, response, spider):\n        flags = ' %s' % str(response.flags) if response.flags else ''\n        return {\n            'level': logging.DEBUG,\n            'msg': CRAWLEDMSG,\n            'args': {\n                'status': response.status,\n                'request': request,\n                'referer': request.headers.get('Referer'),\n                'flags': flags,\n            }\n        }",
        "begin_line": 33,
        "end_line": 44,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.scraped#46",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.scraped(self, item, response, spider)",
        "snippet": "    def scraped(self, item, response, spider):\n        src = response.getErrorMessage() if isinstance(response, Failure) else response\n        return {\n            'level': logging.DEBUG,\n            'msg': SCRAPEDMSG,\n            'args': {\n                'src': src,\n                'item': item,\n            }\n        }",
        "begin_line": 46,
        "end_line": 55,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.dropped#57",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.dropped(self, item, exception, response, spider)",
        "snippet": "    def dropped(self, item, exception, response, spider):\n        return {\n            'level': logging.WARNING,\n            'msg': DROPPEDMSG,\n            'args': {\n                'exception': exception,\n                'item': item,\n            }\n        }",
        "begin_line": 57,
        "end_line": 65,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.logformatter.LogFormatter.from_crawler#68",
        "src_path": "scrapy/logformatter.py",
        "class_name": "scrapy.logformatter.LogFormatter",
        "signature": "scrapy.logformatter.LogFormatter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        return cls()",
        "begin_line": 68,
        "end_line": 69,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.push#14",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.push(self, obj)",
        "snippet": "        def push(self, obj):\n            s = serialize(obj)\n            super(SerializableQueue, self).push(s)",
        "begin_line": 14,
        "end_line": 16,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00039872408293460925,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.squeues.SerializableQueue.pop#18",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues.SerializableQueue",
        "signature": "scrapy.squeues.SerializableQueue.pop(self)",
        "snippet": "        def pop(self):\n            s = super(SerializableQueue, self).pop()\n            if s:\n                return deserialize(s)",
        "begin_line": 18,
        "end_line": 21,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00039872408293460925,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.squeues._pickle_serialize#25",
        "src_path": "scrapy/squeues.py",
        "class_name": "scrapy.squeues",
        "signature": "scrapy.squeues._pickle_serialize(obj)",
        "snippet": "def _pickle_serialize(obj):\n    try:\n        return pickle.dumps(obj, protocol=2)\n    except pickle.PicklingError as e:\n        raise ValueError(str(e))",
        "begin_line": 25,
        "end_line": 29,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.00040766408479412964,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.response.get_base_url#26",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_base_url(response)",
        "snippet": "def get_base_url(response):\n    \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n    if response not in _baseurl_cache:\n        text = response.body_as_unicode()[0:4096]\n        _baseurl_cache[response] = html.get_base_url(text, response.url,\n            response.encoding)\n    return _baseurl_cache[response]",
        "begin_line": 26,
        "end_line": 32,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0005763688760806917,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.response.get_meta_refresh#38",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.get_meta_refresh(response)",
        "snippet": "def get_meta_refresh(response):\n    \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n    if response not in _metaref_cache:\n        text = response.body_as_unicode()[0:4096]\n        text = _noscript_re.sub(u'', text)\n        text = _script_re.sub(u'', text)\n        _metaref_cache[response] = html.get_meta_refresh(text, response.url,\n            response.encoding)\n    return _metaref_cache[response]",
        "begin_line": 38,
        "end_line": 46,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.response.response_httprepr#61",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.response_httprepr(response)",
        "snippet": "def response_httprepr(response):\n    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n    is provided only for reference, since it's not the exact stream of bytes\n    that was received (that's not exposed by Twisted).\n    \"\"\"\n    s = b\"HTTP/1.1 \" + to_bytes(str(response.status)) + b\" \" + \\\n        to_bytes(RESPONSES.get(response.status, b'')) + b\"\\r\\n\"\n    if response.headers:\n        s += response.headers.to_string() + b\"\\r\\n\"\n    s += b\"\\r\\n\"\n    s += response.body\n    return s",
        "begin_line": 61,
        "end_line": 72,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.utils.response.open_in_browser#75",
        "src_path": "scrapy/utils/response.py",
        "class_name": "scrapy.utils.response",
        "signature": "scrapy.utils.response.open_in_browser(response, _openfunc=webbrowser.open)",
        "snippet": "def open_in_browser(response, _openfunc=webbrowser.open):\n    \"\"\"Open the given response in a local web browser, populating the <base>\n    tag for external links to work\n    \"\"\"\n    from scrapy.http import HtmlResponse, TextResponse\n    # XXX: this implementation is a bit dirty and could be improved\n    body = response.body\n    if isinstance(response, HtmlResponse):\n        if b'<base' not in body:\n            repl = '<head><base href=\"%s\">' % response.url\n            body = body.replace(b'<head>', to_bytes(repl))\n        ext = '.html'\n    elif isinstance(response, TextResponse):\n        ext = '.txt'\n    else:\n        raise TypeError(\"Unsupported response type: %s\" %\n                        response.__class__.__name__)\n    fd, fname = tempfile.mkstemp(ext)\n    os.write(fd, body)\n    os.close(fd)\n    return _openfunc(\"file://%s\" % fname)",
        "begin_line": 75,
        "end_line": 95,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.SafeXMLParser.__init__#21",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SafeXMLParser",
        "signature": "scrapy.selector.unified.SafeXMLParser.__init__(self, *args, **kwargs)",
        "snippet": "    def __init__(self, *args, **kwargs):\n        kwargs.setdefault('resolve_entities', False)\n        super(SafeXMLParser, self).__init__(*args, **kwargs)",
        "begin_line": 21,
        "end_line": 23,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified._st#35",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._st(response, st)",
        "snippet": "def _st(response, st):\n    if st is None:\n        return 'xml' if isinstance(response, XmlResponse) else 'html'\n    elif st in ('xml', 'html'):\n        return st\n    else:\n        raise ValueError('Invalid type: %s' % st)",
        "begin_line": 35,
        "end_line": 41,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified._response_from_text#44",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified",
        "signature": "scrapy.selector.unified._response_from_text(text, st)",
        "snippet": "def _response_from_text(text, st):\n    rt = XmlResponse if st == 'xml' else HtmlResponse\n    return rt(url='about:blank', encoding='utf-8',\n              body=to_bytes(text, 'utf-8'))",
        "begin_line": 44,
        "end_line": 47,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.__init__#69",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.__init__(self, response=None, text=None, type=None, namespaces=None, _root=None, _expr=None)",
        "snippet": "    def __init__(self, response=None, text=None, type=None, namespaces=None,\n                 _root=None, _expr=None):\n        self.type = st = _st(response, type or self._default_type)\n        self._parser = _ctgroup[st]['_parser']\n        self._csstranslator = _ctgroup[st]['_csstranslator']\n        self._tostring_method = _ctgroup[st]['_tostring_method']\n\n        if text is not None:\n            response = _response_from_text(text, st)\n\n        if response is not None:\n            _root = LxmlDocument(response, self._parser)\n\n        self.response = response\n        self.namespaces = dict(self._default_namespaces)\n        if namespaces is not None:\n            self.namespaces.update(namespaces)\n        self._root = _root\n        self._expr = _expr",
        "begin_line": 69,
        "end_line": 87,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.xpath#89",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.xpath(self, query)",
        "snippet": "    def xpath(self, query):\n        try:\n            xpathev = self._root.xpath\n        except AttributeError:\n            return SelectorList([])\n\n        try:\n            result = xpathev(query, namespaces=self.namespaces,\n                             smart_strings=self._lxml_smart_strings)\n        except etree.XPathError:\n            msg = u\"Invalid XPath: %s\" % query\n            raise ValueError(msg if six.PY3 else msg.encode(\"unicode_escape\"))\n\n        if type(result) is not list:\n            result = [result]\n\n        result = [self.__class__(_root=x, _expr=query,\n                                 namespaces=self.namespaces,\n                                 type=self.type)\n                  for x in result]\n        return SelectorList(result)",
        "begin_line": 89,
        "end_line": 109,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.css#111",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.css(self, query)",
        "snippet": "    def css(self, query):\n        return self.xpath(self._css2xpath(query))",
        "begin_line": 111,
        "end_line": 112,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector._css2xpath#114",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector._css2xpath(self, query)",
        "snippet": "    def _css2xpath(self, query):\n        return self._csstranslator.css_to_xpath(query)",
        "begin_line": 114,
        "end_line": 115,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004955401387512388,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.re#117",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.re(self, regex)",
        "snippet": "    def re(self, regex):\n        return extract_regex(regex, self.extract())",
        "begin_line": 117,
        "end_line": 118,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.001,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.Selector.extract#120",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.Selector",
        "signature": "scrapy.selector.unified.Selector.extract(self)",
        "snippet": "    def extract(self):\n        try:\n            return etree.tostring(self._root,\n                                  method=self._tostring_method,\n                                  encoding=\"unicode\",\n                                  with_tail=False)\n        except (AttributeError, TypeError):\n            if self._root is True:\n                return u'1'\n            elif self._root is False:\n                return u'0'\n            else:\n                return six.text_type(self._root)",
        "begin_line": 120,
        "end_line": 132,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.selector.unified.SelectorList.extract#184",
        "src_path": "scrapy/selector/unified.py",
        "class_name": "scrapy.selector.unified.SelectorList",
        "signature": "scrapy.selector.unified.SelectorList.extract(self)",
        "snippet": "    def extract(self):\n        return [x.extract() for x in self]",
        "begin_line": 184,
        "end_line": 185,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0004510599909788002,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__#59",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.__init__(self, uri, _stdout=sys.stdout)",
        "snippet": "    def __init__(self, uri, _stdout=sys.stdout):\n        self._stdout = _stdout",
        "begin_line": 59,
        "end_line": 60,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.open#62",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        return self._stdout",
        "begin_line": 62,
        "end_line": 63,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.StdoutFeedStorage.store#65",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "signature": "scrapy.extensions.feedexport.StdoutFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        pass",
        "begin_line": 65,
        "end_line": 66,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.__init__#72",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.__init__(self, uri)",
        "snippet": "    def __init__(self, uri):\n        self.path = file_uri_to_path(uri)",
        "begin_line": 72,
        "end_line": 73,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.open#75",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.open(self, spider)",
        "snippet": "    def open(self, spider):\n        dirname = os.path.dirname(self.path)\n        if dirname and not os.path.exists(dirname):\n            os.makedirs(dirname)\n        return open(self.path, 'ab')",
        "begin_line": 75,
        "end_line": 79,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0021645021645021645,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FileFeedStorage.store#81",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FileFeedStorage",
        "signature": "scrapy.extensions.feedexport.FileFeedStorage.store(self, file)",
        "snippet": "    def store(self, file):\n        file.close()",
        "begin_line": 81,
        "end_line": 82,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0007347538574577516,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.__init__#141",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.__init__(self, settings)",
        "snippet": "    def __init__(self, settings):\n        self.settings = settings\n        self.urifmt = settings['FEED_URI']\n        if not self.urifmt:\n            raise NotConfigured\n        self.format = settings['FEED_FORMAT'].lower()\n        self.storages = self._load_components('FEED_STORAGES')\n        self.exporters = self._load_components('FEED_EXPORTERS')\n        if not self._storage_supported(self.urifmt):\n            raise NotConfigured\n        if not self._exporter_supported(self.format):\n            raise NotConfigured\n        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n        uripar = settings['FEED_URI_PARAMS']\n        self._uripar = load_object(uripar) if uripar else lambda x, y: None",
        "begin_line": 141,
        "end_line": 156,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    },
    {
        "name": "scrapy.extensions.feedexport.FeedExporter.from_crawler#159",
        "src_path": "scrapy/extensions/feedexport.py",
        "class_name": "scrapy.extensions.feedexport.FeedExporter",
        "signature": "scrapy.extensions.feedexport.FeedExporter.from_crawler(cls, crawler)",
        "snippet": "    def from_crawler(cls, crawler):\n        o = cls(crawler.settings)\n        crawler.signals.connect(o.open_spider, signals.spider_opened)\n        crawler.signals.connect(o.close_spider, signals.spider_closed)\n        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n        return o",
        "begin_line": 159,
        "end_line": 164,
        "comment": "",
        "is_bug": false,
        "susp": {
            "pseudo_ochiai_susp": 0.0007692307692307692,
            "pseudo_dstar_susp": 0.0007692307692307692,
            "pseudo_tarantula_susp": 0.0007692307692307692,
            "pseudo_op2_susp": 0.0006653359946773121,
            "pseudo_barinel_susp": 0.0007692307692307692
        }
    }
]