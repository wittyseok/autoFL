[
    {
        "name": "tests.keras.metrics_test.test_metrics#32",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_metrics()",
        "snippet": "def test_metrics():\n    y_a = K.variable(np.random.random((6, 7)))\n    y_b = K.variable(np.random.random((6, 7)))\n    for metric in all_metrics:\n        output = metric(y_a, y_b)\n        print(metric.__name__)\n        assert K.eval(output).shape == (6,)",
        "begin_line": 32,
        "end_line": 38,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_sparse_metrics#42",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_sparse_metrics()",
        "snippet": "def test_sparse_metrics():\n    for metric in all_sparse_metrics:\n        y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n        y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n        assert K.eval(metric(y_a, y_b)).shape == (6,)",
        "begin_line": 42,
        "end_line": 46,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_serialize#49",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_serialize()",
        "snippet": "def test_serialize():\n    '''This is a mock 'round trip' of serialize and deserialize.\n    '''\n\n    class MockMetric:\n        def __init__(self):\n            self.__name__ = \"mock_metric\"\n\n    mock = MockMetric()\n    found = metrics.serialize(mock)\n    assert found == \"mock_metric\"\n\n    found = metrics.deserialize('mock_metric',\n                                custom_objects={'mock_metric': True})\n    assert found is True",
        "begin_line": 49,
        "end_line": 63,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.MockMetric.test_serialize#49",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.MockMetric",
        "signature": "tests.keras.metrics_test.MockMetric.test_serialize()",
        "snippet": "def test_serialize():\n    '''This is a mock 'round trip' of serialize and deserialize.\n    '''\n\n    class MockMetric:\n        def __init__(self):\n            self.__name__ = \"mock_metric\"\n\n    mock = MockMetric()\n    found = metrics.serialize(mock)\n    assert found == \"mock_metric\"\n\n    found = metrics.deserialize('mock_metric',\n                                custom_objects={'mock_metric': True})\n    assert found is True",
        "begin_line": 49,
        "end_line": 63,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.MockMetric.__init__#54",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.MockMetric",
        "signature": "tests.keras.metrics_test.MockMetric.__init__(self)",
        "snippet": "        def __init__(self):\n            self.__name__ = \"mock_metric\"",
        "begin_line": 54,
        "end_line": 55,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_invalid_get#66",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_invalid_get()",
        "snippet": "def test_invalid_get():\n\n    with pytest.raises(ValueError):\n        metrics.get(5)",
        "begin_line": 66,
        "end_line": 69,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_top_k_categorical_accuracy#75",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_top_k_categorical_accuracy()",
        "snippet": "def test_top_k_categorical_accuracy():\n    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n                                                               k=3))\n    assert success_result == 1\n    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n                                                               k=2))\n    assert partial_result == 0.5\n    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n                                                               k=1))\n    assert failure_result == 0",
        "begin_line": 75,
        "end_line": 86,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_sparse_top_k_categorical_accuracy#92",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_sparse_top_k_categorical_accuracy()",
        "snippet": "def test_sparse_top_k_categorical_accuracy():\n    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n    y_true = K.variable(np.array([[1], [0]]))\n    success_result = K.eval(\n        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=3))\n\n    assert success_result == 1\n    partial_result = K.eval(\n        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=2))\n\n    assert partial_result == 0.5\n    failure_result = K.eval(\n        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=1))\n\n    assert failure_result == 0",
        "begin_line": 92,
        "end_line": 106,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.test_stateful_metrics#111",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.test_stateful_metrics(metrics_mode)",
        "snippet": "def test_stateful_metrics(metrics_mode):\n    np.random.seed(1334)\n\n    class BinaryTruePositives(keras.layers.Layer):\n        \"\"\"Stateful Metric to count the total true positives over all batches.\n\n        Assumes predictions and targets of shape `(samples, 1)`.\n\n        # Arguments\n            name: String, name for the metric.\n        \"\"\"\n\n        def __init__(self, name='true_positives', **kwargs):\n            super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.true_positives = K.variable(value=0, dtype='int32')\n\n        def reset_states(self):\n            K.set_value(self.true_positives, 0)\n\n        def __call__(self, y_true, y_pred):\n            \"\"\"Computes the number of true positives in a batch.\n\n            # Arguments\n                y_true: Tensor, batch_wise labels\n                y_pred: Tensor, batch_wise predictions\n\n            # Returns\n                The total number of true positives seen this epoch at the\n                    completion of the batch.\n            \"\"\"\n            y_true = K.cast(y_true, 'int32')\n            y_pred = K.cast(K.round(y_pred), 'int32')\n            correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\n            true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\n            current_true_pos = self.true_positives * 1\n            self.add_update(K.update_add(self.true_positives,\n                                         true_pos),\n                            inputs=[y_true, y_pred])\n            return current_true_pos + true_pos\n\n    metric_fn = BinaryTruePositives()\n    config = metrics.serialize(metric_fn)\n    metric_fn = metrics.deserialize(\n        config, custom_objects={'BinaryTruePositives': BinaryTruePositives})\n\n    # Test on simple model\n    inputs = keras.Input(shape=(2,))\n    outputs = keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)\n    model = keras.Model(inputs, outputs)\n\n    if metrics_mode == 'list':\n        model.compile(optimizer='sgd',\n                      loss='binary_crossentropy',\n                      metrics=['acc', metric_fn])\n    elif metrics_mode == 'dict':\n        model.compile(optimizer='sgd',\n                      loss='binary_crossentropy',\n                      metrics={'out': ['acc', metric_fn]})\n\n    samples = 1000\n    x = np.random.random((samples, 2))\n    y = np.random.randint(2, size=(samples, 1))\n\n    val_samples = 10\n    val_x = np.random.random((val_samples, 2))\n    val_y = np.random.randint(2, size=(val_samples, 1))\n\n    # Test fit and evaluate\n    history = model.fit(x, y, validation_data=(val_x, val_y), epochs=1, batch_size=10)\n    outs = model.evaluate(x, y, batch_size=10)\n    preds = model.predict(x)\n\n    def ref_true_pos(y_true, y_pred):\n        return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))\n\n    # Test correctness (e.g. updates should have been run)\n    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n\n    # Test correctness of the validation metric computation\n    val_preds = model.predict(val_x)\n    val_outs = model.evaluate(val_x, val_y, batch_size=10)\n    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n    np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)\n\n    # Test with generators\n    gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]\n    val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]\n    history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,\n                                  validation_data=iter(val_gen), validation_steps=val_samples)\n    outs = model.evaluate_generator(iter(gen), steps=samples)\n    preds = model.predict_generator(iter(gen), steps=samples)\n\n    # Test correctness of the metric re ref_true_pos()\n    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n\n    # Test correctness of the validation metric computation\n    val_preds = model.predict_generator(iter(val_gen), steps=val_samples)\n    val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)\n    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n    np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)",
        "begin_line": 111,
        "end_line": 211,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.BinaryTruePositives.test_stateful_metrics#111",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.BinaryTruePositives",
        "signature": "tests.keras.metrics_test.BinaryTruePositives.test_stateful_metrics(metrics_mode)",
        "snippet": "def test_stateful_metrics(metrics_mode):\n    np.random.seed(1334)\n\n    class BinaryTruePositives(keras.layers.Layer):\n        \"\"\"Stateful Metric to count the total true positives over all batches.\n\n        Assumes predictions and targets of shape `(samples, 1)`.\n\n        # Arguments\n            name: String, name for the metric.\n        \"\"\"\n\n        def __init__(self, name='true_positives', **kwargs):\n            super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.true_positives = K.variable(value=0, dtype='int32')\n\n        def reset_states(self):\n            K.set_value(self.true_positives, 0)\n\n        def __call__(self, y_true, y_pred):\n            \"\"\"Computes the number of true positives in a batch.\n\n            # Arguments\n                y_true: Tensor, batch_wise labels\n                y_pred: Tensor, batch_wise predictions\n\n            # Returns\n                The total number of true positives seen this epoch at the\n                    completion of the batch.\n            \"\"\"\n            y_true = K.cast(y_true, 'int32')\n            y_pred = K.cast(K.round(y_pred), 'int32')\n            correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\n            true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\n            current_true_pos = self.true_positives * 1\n            self.add_update(K.update_add(self.true_positives,\n                                         true_pos),\n                            inputs=[y_true, y_pred])\n            return current_true_pos + true_pos\n\n    metric_fn = BinaryTruePositives()\n    config = metrics.serialize(metric_fn)\n    metric_fn = metrics.deserialize(\n        config, custom_objects={'BinaryTruePositives': BinaryTruePositives})\n\n    # Test on simple model\n    inputs = keras.Input(shape=(2,))\n    outputs = keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)\n    model = keras.Model(inputs, outputs)\n\n    if metrics_mode == 'list':\n        model.compile(optimizer='sgd',\n                      loss='binary_crossentropy',\n                      metrics=['acc', metric_fn])\n    elif metrics_mode == 'dict':\n        model.compile(optimizer='sgd',\n                      loss='binary_crossentropy',\n                      metrics={'out': ['acc', metric_fn]})\n\n    samples = 1000\n    x = np.random.random((samples, 2))\n    y = np.random.randint(2, size=(samples, 1))\n\n    val_samples = 10\n    val_x = np.random.random((val_samples, 2))\n    val_y = np.random.randint(2, size=(val_samples, 1))\n\n    # Test fit and evaluate\n    history = model.fit(x, y, validation_data=(val_x, val_y), epochs=1, batch_size=10)\n    outs = model.evaluate(x, y, batch_size=10)\n    preds = model.predict(x)\n\n    def ref_true_pos(y_true, y_pred):\n        return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))\n\n    # Test correctness (e.g. updates should have been run)\n    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n\n    # Test correctness of the validation metric computation\n    val_preds = model.predict(val_x)\n    val_outs = model.evaluate(val_x, val_y, batch_size=10)\n    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n    np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)\n\n    # Test with generators\n    gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(x, y)]\n    val_gen = [(np.array([x0]), np.array([y0])) for x0, y0 in zip(val_x, val_y)]\n    history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,\n                                  validation_data=iter(val_gen), validation_steps=val_samples)\n    outs = model.evaluate_generator(iter(gen), steps=samples)\n    preds = model.predict_generator(iter(gen), steps=samples)\n\n    # Test correctness of the metric re ref_true_pos()\n    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n\n    # Test correctness of the validation metric computation\n    val_preds = model.predict_generator(iter(val_gen), steps=val_samples)\n    val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)\n    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n    np.testing.assert_allclose(val_outs[2], history.history['val_true_positives'][-1], atol=1e-5)",
        "begin_line": 111,
        "end_line": 211,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.BinaryTruePositives.__init__#123",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.BinaryTruePositives",
        "signature": "tests.keras.metrics_test.BinaryTruePositives.__init__(self, name='true_positives', **kwargs)",
        "snippet": "        def __init__(self, name='true_positives', **kwargs):\n            super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n            self.stateful = True\n            self.true_positives = K.variable(value=0, dtype='int32')",
        "begin_line": 123,
        "end_line": 126,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.BinaryTruePositives.reset_states#128",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.BinaryTruePositives",
        "signature": "tests.keras.metrics_test.BinaryTruePositives.reset_states(self)",
        "snippet": "        def reset_states(self):\n            K.set_value(self.true_positives, 0)",
        "begin_line": 128,
        "end_line": 129,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.BinaryTruePositives.__call__#131",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test.BinaryTruePositives",
        "signature": "tests.keras.metrics_test.BinaryTruePositives.__call__(self, y_true, y_pred)",
        "snippet": "        def __call__(self, y_true, y_pred):\n            \"\"\"Computes the number of true positives in a batch.\n\n            # Arguments\n                y_true: Tensor, batch_wise labels\n                y_pred: Tensor, batch_wise predictions\n\n            # Returns\n                The total number of true positives seen this epoch at the\n                    completion of the batch.\n            \"\"\"\n            y_true = K.cast(y_true, 'int32')\n            y_pred = K.cast(K.round(y_pred), 'int32')\n            correct_preds = K.cast(K.equal(y_pred, y_true), 'int32')\n            true_pos = K.cast(K.sum(correct_preds * y_true), 'int32')\n            current_true_pos = self.true_positives * 1\n            self.add_update(K.update_add(self.true_positives,\n                                         true_pos),\n                            inputs=[y_true, y_pred])\n            return current_true_pos + true_pos",
        "begin_line": 131,
        "end_line": 150,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.metrics_test.ref_true_pos#184",
        "src_path": "tests/keras/metrics_test.py",
        "class_name": "tests.keras.metrics_test",
        "signature": "tests.keras.metrics_test.ref_true_pos(y_true, y_pred)",
        "snippet": "    def ref_true_pos(y_true, y_pred):\n        return np.sum(np.logical_and(y_pred > 0.5, y_true == 1))",
        "begin_line": 184,
        "end_line": 185,
        "comment": "",
        "is_bug": false
    }
]