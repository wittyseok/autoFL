[
    {
        "name": "tests.keras.legacy.conftest.clear_session_after_test#6",
        "src_path": "tests/keras/legacy/conftest.py",
        "class_name": "tests.keras.legacy.conftest",
        "signature": "tests.keras.legacy.conftest.clear_session_after_test()",
        "snippet": "def clear_session_after_test():\n    \"\"\"This wrapper runs for all the tests in the legacy directory (recursively).\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', message=r'(.+) Keras 2 ',\n                                category=UserWarning)\n        yield",
        "begin_line": 6,
        "end_line": 12,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.conftest.clear_session_after_test#6",
        "src_path": "tests/conftest.py",
        "class_name": "tests.conftest",
        "signature": "tests.conftest.clear_session_after_test()",
        "snippet": "def clear_session_after_test():\n    \"\"\"Test wrapper to clean up after TensorFlow and CNTK tests.\n\n    This wrapper runs for all the tests in the keras test suite.\n    \"\"\"\n    yield\n    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n        K.clear_session()",
        "begin_line": 6,
        "end_line": 13,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_get_updates_for#20",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_get_updates_for()",
        "snippet": "def test_get_updates_for():\n    a = Input(shape=(2,))\n    dense_layer = Dense(1)\n    dense_layer.add_update(0, inputs=a)\n    dense_layer.add_update(1, inputs=None)\n\n    assert dense_layer.get_updates_for(a) == [0]\n    assert dense_layer.get_updates_for(None) == [1]",
        "begin_line": 20,
        "end_line": 27,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_get_losses_for#30",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_get_losses_for()",
        "snippet": "def test_get_losses_for():\n    a = Input(shape=(2,))\n    dense_layer = Dense(1)\n    dense_layer.add_loss(0, inputs=a)\n    dense_layer.add_loss(1, inputs=None)\n\n    assert dense_layer.get_losses_for(a) == [0]\n    assert dense_layer.get_losses_for(None) == [1]",
        "begin_line": 30,
        "end_line": 37,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_trainable_weights#40",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_trainable_weights()",
        "snippet": "def test_trainable_weights():\n    a = Input(shape=(2,))\n    b = Dense(1)(a)\n    model = Model(a, b)\n\n    weights = model.weights\n    assert model.trainable_weights == weights\n    assert model.non_trainable_weights == []\n\n    model.trainable = False\n    assert model.trainable_weights == []\n    assert model.non_trainable_weights == weights\n\n    model.trainable = True\n    assert model.trainable_weights == weights\n    assert model.non_trainable_weights == []\n\n    model.layers[1].trainable = False\n    assert model.trainable_weights == []\n    assert model.non_trainable_weights == weights\n\n    # sequential model\n    model = Sequential()\n    model.add(Dense(1, input_dim=2))\n    weights = model.weights\n\n    assert model.trainable_weights == weights\n    assert model.non_trainable_weights == []\n\n    model.trainable = False\n    assert model.trainable_weights == []\n    assert model.non_trainable_weights == weights\n\n    model.trainable = True\n    assert model.trainable_weights == weights\n    assert model.non_trainable_weights == []\n\n    model.layers[0].trainable = False\n    assert model.trainable_weights == []\n    assert model.non_trainable_weights == weights",
        "begin_line": 40,
        "end_line": 79,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_valid_compute_mask#82",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_valid_compute_mask()",
        "snippet": "def test_valid_compute_mask():\n    model = Sequential()\n    model.add(Dense(1, input_dim=2))\n    assert model.layers[0].supports_masking is True\n    assert model.layers[0].compute_mask([model.input], [0., 1.]) == [0., 1.]",
        "begin_line": 82,
        "end_line": 86,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_invalid_compute_mask#89",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_invalid_compute_mask()",
        "snippet": "def test_invalid_compute_mask():\n    model = Sequential()\n    model.add(Conv2D(1, [2, 2], input_shape=[3, 3, 1]))\n    assert model.layers[0].supports_masking is False\n    assert model.layers[0].compute_mask([model.input], [None]) is None\n\n    mask = np.array([[0., 1.], [1., 0.]])\n    with pytest.raises(TypeError):\n        model.layers[0].compute_mask([model.input], [mask])\n    with pytest.raises(TypeError):\n        model.layers[0].compute_mask([model.input], mask)",
        "begin_line": 89,
        "end_line": 99,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_get_layer#102",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_get_layer()",
        "snippet": "def test_get_layer():\n    model = Sequential()\n    model.add(Dense(1, input_dim=2))\n    with pytest.raises(ValueError):\n        model.get_layer(index=5)\n    with pytest.raises(ValueError):\n        model.get_layer(index=None)\n    with pytest.raises(ValueError):\n        model.get_layer(name='conv')",
        "begin_line": 102,
        "end_line": 110,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_learning_phase#113",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_learning_phase()",
        "snippet": "def test_learning_phase():\n    a = Input(shape=(32,), name='input_a')\n    b = Input(shape=(32,), name='input_b')\n\n    a_2 = Dense(16, name='dense_1')(a)\n    dp = Dropout(0.5, name='dropout')\n    b_2 = dp(b)\n\n    assert not a_2._uses_learning_phase\n    assert b_2._uses_learning_phase\n\n    # test merge\n    m = layers.concatenate([a_2, b_2])\n    assert m._uses_learning_phase\n\n    # Test recursion\n    model = Model([a, b], [a_2, b_2])\n    print(model.input_spec)\n    assert model.uses_learning_phase\n\n    c = Input(shape=(32,), name='input_c')\n    d = Input(shape=(32,), name='input_d')\n\n    c_2, b_2 = model([c, d])\n    assert c_2._uses_learning_phase\n    assert b_2._uses_learning_phase\n\n    # try actually running graph\n    fn = K.function(model.inputs + [K.learning_phase()], model.outputs)\n    input_a_np = np.random.random((10, 32))\n    input_b_np = np.random.random((10, 32))\n    fn_outputs_no_dp = fn([input_a_np, input_b_np, 0])\n    fn_outputs_dp = fn([input_a_np, input_b_np, 1])\n    # output a: nothing changes\n    assert fn_outputs_no_dp[0].sum() == fn_outputs_dp[0].sum()\n    # output b: dropout applied\n    assert fn_outputs_no_dp[1].sum() != fn_outputs_dp[1].sum()",
        "begin_line": 113,
        "end_line": 149,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_layer_call_arguments#152",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_layer_call_arguments()",
        "snippet": "def test_layer_call_arguments():\n    # Test the ability to pass and serialize arguments to `call`.\n    inp = layers.Input(shape=(2,))\n    x = layers.Dense(3)(inp)\n    x = layers.Dropout(0.5)(x, training=True)\n    model = Model(inp, x)\n    assert not model.uses_learning_phase\n\n    # Test that argument is kept when applying the model\n    inp2 = layers.Input(shape=(2,))\n    out2 = model(inp2)\n    assert not out2._uses_learning_phase\n\n    # Test that argument is kept after loading a model\n    config = model.get_config()\n    model = Model.from_config(config)\n    assert not model.uses_learning_phase",
        "begin_line": 152,
        "end_line": 168,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_node_construction#171",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_node_construction()",
        "snippet": "def test_node_construction():\n    ####################################################\n    # test basics\n\n    a = Input(shape=(32,), name='input_a')\n    b = Input(shape=(32,), name='input_b')\n\n    assert a._keras_shape == (None, 32)\n    a_layer, a_node_index, a_tensor_index = a._keras_history\n    b_layer, b_node_index, b_tensor_index = b._keras_history\n    assert len(a_layer._inbound_nodes) == 1\n    assert a_tensor_index is 0\n    node = a_layer._inbound_nodes[a_node_index]\n    assert node.outbound_layer == a_layer\n\n    assert isinstance(node.inbound_layers, list)\n    assert node.inbound_layers == []\n    assert isinstance(node.input_tensors, list)\n    assert node.input_tensors == [a]\n    assert isinstance(node.input_masks, list)\n    assert node.input_masks == [None]\n    assert isinstance(node.input_shapes, list)\n    assert node.input_shapes == [(None, 32)]\n\n    assert isinstance(node.output_tensors, list)\n    assert node.output_tensors == [a]\n    assert isinstance(node.output_shapes, list)\n    assert node.output_shapes == [(None, 32)]\n    assert isinstance(node.output_masks, list)\n    assert node.output_masks == [None]\n\n    dense = Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    assert len(dense._inbound_nodes) == 2\n    assert len(dense._outbound_nodes) == 0\n    assert dense._inbound_nodes[0].inbound_layers == [a_layer]\n    assert dense._inbound_nodes[0].outbound_layer == dense\n    assert dense._inbound_nodes[1].inbound_layers == [b_layer]\n    assert dense._inbound_nodes[1].outbound_layer == dense\n\n    assert dense._inbound_nodes[0].input_tensors == [a]\n    assert dense._inbound_nodes[1].input_tensors == [b]\n\n    assert dense._inbound_nodes[0].get_config()['inbound_layers'] == ['input_a']\n    assert dense._inbound_nodes[1].get_config()['inbound_layers'] == ['input_b']\n\n    # test layer properties\n    test_layer = Dense(16, name='test_layer')\n    a_test = test_layer(a)\n    assert K.int_shape(test_layer.kernel) == (32, 16)\n    assert test_layer.input == a\n    assert test_layer.output == a_test\n    assert test_layer.input_mask is None\n    assert test_layer.output_mask is None\n    assert test_layer.input_shape == (None, 32)\n    assert test_layer.output_shape == (None, 16)\n\n    with pytest.raises(AttributeError):\n        dense.input\n    with pytest.raises(AttributeError):\n        dense.output\n    with pytest.raises(AttributeError):\n        dense.input_mask\n    with pytest.raises(AttributeError):\n        dense.output_mask\n\n    assert dense.get_input_at(0) == a\n    assert dense.get_input_at(1) == b\n    assert dense.get_output_at(0) == a_2\n    assert dense.get_output_at(1) == b_2\n    assert dense.get_input_shape_at(0) == (None, 32)\n    assert dense.get_input_shape_at(1) == (None, 32)\n    assert dense.get_output_shape_at(0) == (None, 16)\n    assert dense.get_output_shape_at(1) == (None, 16)\n    assert dense.get_input_mask_at(0) is None\n    assert dense.get_input_mask_at(1) is None\n    assert dense.get_output_mask_at(0) is None\n    assert dense.get_output_mask_at(1) is None",
        "begin_line": 171,
        "end_line": 250,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_multi_input_layer#253",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_multi_input_layer()",
        "snippet": "def test_multi_input_layer():\n    ####################################################\n    # test multi-input layer\n    a = Input(shape=(32,), name='input_a')\n    b = Input(shape=(32,), name='input_b')\n\n    dense = Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    assert merged._keras_shape == (None, 16 * 2)\n    merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n\n    assert merge_node_index == 0\n    assert merge_tensor_index == 0\n\n    assert len(merge_layer._inbound_nodes) == 1\n    assert len(merge_layer._outbound_nodes) == 0\n\n    assert len(merge_layer._inbound_nodes[0].input_tensors) == 2\n    assert len(merge_layer._inbound_nodes[0].inbound_layers) == 2\n\n    c = Dense(64, name='dense_2')(merged)\n    d = Dense(5, name='dense_3')(c)\n\n    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n    assert len(model.layers) == 6\n    expected_shapes = [(None, 64), (None, 5)]\n    assert model.compute_output_shape([(None, 32), (None, 32)]) == expected_shapes\n    assert model.compute_mask([a, b], [None, None]) == [None, None]\n    assert model.compute_output_shape([(None, 32), (None, 32)]) == expected_shapes\n\n    # we don't check names of first 2 layers (inputs) because\n    # ordering of same-level layers is not fixed\n    expected_names = ['dense_1', 'merge', 'dense_2', 'dense_3']\n    assert [l.name for l in model.layers][2:] == expected_names\n    assert [l.name for l in model._input_layers] == ['input_a', 'input_b']\n    assert [l.name for l in model._output_layers] == ['dense_2', 'dense_3']\n\n    # actually run model\n    fn = K.function(model.inputs, model.outputs)\n    input_a_np = np.random.random((10, 32))\n    input_b_np = np.random.random((10, 32))\n    fn_outputs = fn([input_a_np, input_b_np])\n    assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n\n    # test get_source_inputs\n    assert get_source_inputs(c) == [a, b]\n\n    # serialization / deserialization\n    json_config = model.to_json()\n    recreated_model = model_from_json(json_config)\n    recreated_model.compile('rmsprop', 'mse')\n\n    assert [l.name for l in recreated_model.layers][2:] == expected_names\n    assert [l.name for l in recreated_model._input_layers] == ['input_a', 'input_b']\n    assert [l.name for l in recreated_model._output_layers] == ['dense_2', 'dense_3']\n\n    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n    input_a_np = np.random.random((10, 32))\n    input_b_np = np.random.random((10, 32))\n    fn_outputs = fn([input_a_np, input_b_np])\n    assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]",
        "begin_line": 253,
        "end_line": 316,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_recursion#319",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_recursion()",
        "snippet": "def test_recursion():\n    ####################################################\n    # test recursion\n\n    a = Input(shape=(32,), name='input_a')\n    b = Input(shape=(32,), name='input_b')\n\n    dense = Dense(16, name='dense_1')\n    a_2 = dense(a)\n    b_2 = dense(b)\n    merged = layers.concatenate([a_2, b_2], name='merge')\n    c = Dense(64, name='dense_2')(merged)\n    d = Dense(5, name='dense_3')(c)\n\n    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n\n    e = Input(shape=(32,), name='input_e')\n    f = Input(shape=(32,), name='input_f')\n    g, h = model([e, f])\n\n    # g2, h2 = model([e, f])\n\n    assert g._keras_shape == c._keras_shape\n    assert h._keras_shape == d._keras_shape\n\n    # test separate manipulation of different layer outputs\n    i = Dense(7, name='dense_4')(h)\n\n    final_model = Model(inputs=[e, f], outputs=[i, g], name='final')\n    assert len(final_model.inputs) == 2\n    assert len(final_model.outputs) == 2\n    assert len(final_model.layers) == 4\n\n    # we don't check names of first 2 layers (inputs) because\n    # ordering of same-level layers is not fixed\n    expected_shapes = [(10, 7), (10, 64)]\n    assert [layer.name for layer in final_model.layers][2:] == ['model', 'dense_4']\n    assert model.compute_mask([e, f], [None, None]) == [None, None]\n    assert final_model.compute_output_shape([(10, 32), (10, 32)]) == expected_shapes\n\n    # run recursive model\n    fn = K.function(final_model.inputs, final_model.outputs)\n    input_a_np = np.random.random((10, 32))\n    input_b_np = np.random.random((10, 32))\n    fn_outputs = fn([input_a_np, input_b_np])\n    assert [x.shape for x in fn_outputs] == [(10, 7), (10, 64)]\n\n    # test serialization\n    model_config = final_model.get_config()\n    print(json.dumps(model_config, indent=4))\n    recreated_model = Model.from_config(model_config)\n\n    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n    input_a_np = np.random.random((10, 32))\n    input_b_np = np.random.random((10, 32))\n    fn_outputs = fn([input_a_np, input_b_np])\n    assert [x.shape for x in fn_outputs] == [(10, 7), (10, 64)]\n\n    ####################################################\n    # test multi-input multi-output\n\n    j = Input(shape=(32,), name='input_j')\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    o = Input(shape=(32,), name='input_o')\n    p = Input(shape=(32,), name='input_p')\n    q, r = model([o, p])\n\n    assert n._keras_shape == (None, 5)\n    assert q._keras_shape == (None, 64)\n    s = layers.concatenate([n, q], name='merge_nq')\n    assert s._keras_shape == (None, 64 + 5)\n\n    # test with single output as 1-elem list\n    multi_io_model = Model([j, k, o, p], [s])\n\n    fn = K.function(multi_io_model.inputs, multi_io_model.outputs)\n    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n                     np.random.random((10, 32)), np.random.random((10, 32))])\n    assert [x.shape for x in fn_outputs] == [(10, 69)]\n\n    # test with single output as tensor\n    multi_io_model = Model([j, k, o, p], s)\n\n    fn = K.function(multi_io_model.inputs, multi_io_model.outputs)\n    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n                     np.random.random((10, 32)), np.random.random((10, 32))])\n    # note that the output of the K.function will still be a 1-elem list\n    assert [x.shape for x in fn_outputs] == [(10, 69)]\n\n    # test serialization\n    model_config = multi_io_model.get_config()\n    recreated_model = Model.from_config(model_config)\n\n    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n                     np.random.random((10, 32)), np.random.random((10, 32))])\n    # note that the output of the K.function will still be a 1-elem list\n    assert [x.shape for x in fn_outputs] == [(10, 69)]\n\n    config = model.get_config()\n    Model.from_config(config)\n\n    model.summary()\n    json_str = model.to_json()\n    model_from_json(json_str)\n\n    yaml_str = model.to_yaml()\n    model_from_yaml(yaml_str)\n\n    ####################################################\n    # test invalid graphs\n\n    # input is not an Input tensor\n    j = Input(shape=(32,), name='input_j')\n    j = Dense(32)(j)\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n\n    with pytest.raises(ValueError):\n        Model([j, k], [m, n])\n\n    # disconnected graph\n    j = Input(shape=(32,), name='input_j')\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with pytest.raises(ValueError):\n        Model([j], [m, n])\n\n    # redundant outputs\n    j = Input(shape=(32,), name='input_j')\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    # this should work with a warning\n    Model([j, k], [m, n, n])\n\n    # redundant inputs\n    j = Input(shape=(32,), name='input_j')\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with pytest.raises(ValueError):\n        Model([j, k, j], [m, n])\n\n    # i have not idea what I'm doing: garbage as inputs/outputs\n    j = Input(shape=(32,), name='input_j')\n    k = Input(shape=(32,), name='input_k')\n    m, n = model([j, k])\n    with pytest.raises(ValueError):\n        Model([j, k], [m, n, 0])\n\n    ####################################################\n    # test calling layers/models on TF tensors\n\n    if K.backend() == 'tensorflow':\n        import tensorflow as tf\n        j = Input(shape=(32,), name='input_j')\n        k = Input(shape=(32,), name='input_k')\n        m, n = model([j, k])\n        tf_model = Model([j, k], [m, n])\n\n        j_tf = tf.placeholder(dtype=K.floatx())\n        k_tf = tf.placeholder(dtype=K.floatx())\n        m_tf, n_tf = tf_model([j_tf, k_tf])\n        assert m_tf.get_shape().as_list() == [None, 64]\n        assert n_tf.get_shape().as_list() == [None, 5]\n\n        # test merge\n        layers.concatenate([j_tf, k_tf], axis=1)\n        layers.add([j_tf, k_tf])\n\n        # test tensor input\n        x = tf.placeholder(shape=(None, 2), dtype=K.floatx())\n        InputLayer(input_tensor=x)\n\n        x = Input(tensor=x)\n        Dense(2)(x)",
        "begin_line": 319,
        "end_line": 495,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_load_layers#498",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_load_layers()",
        "snippet": "def test_load_layers():\n    from keras.layers import ConvLSTM2D, TimeDistributed\n    from keras.layers import Bidirectional, Conv2D, Input\n    from keras.models import Model\n\n    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n        inputs = Input(shape=(10, 20, 20, 1))\n    else:\n        inputs = Input(shape=(10, 1, 20, 20))\n    td_conv = TimeDistributed(Conv2D(15, (5, 5)))(inputs)\n    bi_conv = Bidirectional(ConvLSTM2D(10, (3, 3)), merge_mode='concat')(td_conv)\n    model = Model(inputs=inputs, outputs=bi_conv)\n\n    weight_value_tuples = []\n\n    # TimeDistributed Conv2D layer\n    # use 'channels_first' data format to check that\n    # the function is being called correctly for Conv2D\n    # old: (filters, stack_size, kernel_rows, kernel_cols)\n    # new: (kernel_rows, kernel_cols, stack_size, filters)\n    weight_tensor_td_conv_old = list()\n    weight_tensor_td_conv_old.append(np.zeros((15, 1, 5, 5)))\n    weight_tensor_td_conv_old.append(np.zeros((15,)))\n    td_conv_layer = model.layers[1]\n    td_conv_layer.layer.data_format = 'channels_first'\n    weight_tensor_td_conv_new = saving.preprocess_weights_for_loading(\n        td_conv_layer,\n        weight_tensor_td_conv_old,\n        original_keras_version='1')\n    symbolic_weights = td_conv_layer.weights\n    assert (len(symbolic_weights) == len(weight_tensor_td_conv_new))\n    weight_value_tuples += zip(symbolic_weights, weight_tensor_td_conv_new)\n\n    # Bidirectional ConvLSTM2D layer\n    # old ConvLSTM2D took a list of 12 weight tensors,\n    # returns a list of 3 concatenated larger tensors.\n    weights_bi_conv_old = []\n    for j in range(2):  # bidirectional\n        for i in range(4):\n            weights_bi_conv_old.append(np.zeros((3, 3, 15, 10)))  # kernel\n            weights_bi_conv_old.append(np.zeros((3, 3, 10, 10)))  # recurrent kernel\n            weights_bi_conv_old.append(np.zeros((10,)))  # bias\n\n    bi_convlstm_layer = model.layers[2]\n    weights_bi_conv_new = saving.preprocess_weights_for_loading(\n        bi_convlstm_layer,\n        weights_bi_conv_old,\n        original_keras_version='1')\n\n    symbolic_weights = bi_convlstm_layer.weights\n    assert (len(symbolic_weights) == len(weights_bi_conv_new))\n    weight_value_tuples += zip(symbolic_weights, weights_bi_conv_new)\n\n    K.batch_set_value(weight_value_tuples)\n\n    assert np.all(K.eval(model.layers[1].weights[0]) == weight_tensor_td_conv_new[0])\n    assert np.all(K.eval(model.layers[1].weights[1]) == weight_tensor_td_conv_new[1])\n    assert np.all(K.eval(model.layers[2].weights[0]) == weights_bi_conv_new[0])\n    assert np.all(K.eval(model.layers[2].weights[1]) == weights_bi_conv_new[1])\n    assert np.all(K.eval(model.layers[2].weights[2]) == weights_bi_conv_new[2])\n    assert np.all(K.eval(model.layers[2].weights[3]) == weights_bi_conv_new[3])\n    assert np.all(K.eval(model.layers[2].weights[4]) == weights_bi_conv_new[4])\n    assert np.all(K.eval(model.layers[2].weights[5]) == weights_bi_conv_new[5])",
        "begin_line": 498,
        "end_line": 560,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.convert_weights#563",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.convert_weights(layer, weights)",
        "snippet": "def convert_weights(layer, weights):\n    if layer.__class__.__name__ == 'GRU':\n        W = [np.split(w, 3, axis=-1) for w in weights]\n        return sum(map(list, zip(*W)), [])\n    elif layer.__class__.__name__ in ('LSTM', 'ConvLSTM2D'):\n        W = [np.split(w, 4, axis=-1) for w in weights]\n        for w in W:\n            w[2], w[1] = w[1], w[2]\n        return sum(map(list, zip(*W)), [])\n    elif layer.__class__.__name__ == 'Conv2DTranspose':\n        return [np.transpose(weights[0], (2, 3, 0, 1)), weights[1]]\n    return weights",
        "begin_line": 563,
        "end_line": 574,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading#584",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading(layer)",
        "snippet": "def test_preprocess_weights_for_loading(layer):\n    # A model is needed to initialize weights.\n    _ = Sequential([layer])\n    weights1 = layer.get_weights()\n    weights2 = saving.preprocess_weights_for_loading(\n        layer, convert_weights(layer, weights1),\n        original_keras_version='1')\n    assert all([np.allclose(x, y, 1e-5)\n                for (x, y) in zip(weights1, weights2)])",
        "begin_line": 584,
        "end_line": 592,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_for_model#601",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_for_model(layer)",
        "snippet": "def test_preprocess_weights_for_loading_for_model(layer):\n    model = Sequential([layer])\n    weights1 = model.get_weights()\n    weights2 = saving.preprocess_weights_for_loading(\n        model, convert_weights(layer, weights1),\n        original_keras_version='1')\n    assert all([np.allclose(x, y, 1e-5)\n                for (x, y) in zip(weights1, weights2)])",
        "begin_line": 601,
        "end_line": 608,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_rnn_should_be_idempotent#616",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, args)",
        "snippet": "def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, args):\n    \"\"\"\n    Loading weights from a RNN class to itself should not convert the weights.\n    \"\"\"\n    # layer can be instantiated only for supported backends\n    layer = layer_class(**args)\n    # A model is needed to initialize weights.\n    _ = Sequential([layer])\n    weights1 = layer.get_weights()\n    weights2 = saving.preprocess_weights_for_loading(layer, weights1)\n    assert all([np.allclose(x, y, 1e-5) for (x, y) in zip(weights1, weights2)])",
        "begin_line": 616,
        "end_line": 626,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent#634",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent(layer_class, args)",
        "snippet": "def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent(layer_class,\n                                                                       args):\n    test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, args)",
        "begin_line": 634,
        "end_line": 636,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_recursion_with_bn_and_loss#639",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_recursion_with_bn_and_loss()",
        "snippet": "def test_recursion_with_bn_and_loss():\n    model1 = Sequential([\n        layers.Dense(5, input_dim=5, activity_regularizer='l1'),\n        layers.BatchNormalization(),\n        layers.Dense(5),\n    ])\n\n    print('NEW MODEL')\n    inputs = layers.Input(shape=(5,))\n    outputs = model1(inputs)\n    model2 = Model(inputs=inputs, outputs=outputs)\n\n    assert len(model1.updates) == 2\n    assert len(model2.updates) == 2\n    assert len(model1.losses) == 1\n    assert len(model2.losses) == 1, model2.layers[1]._per_input_losses\n\n    model1.compile(optimizer='sgd', loss='categorical_crossentropy')\n    model2.compile(optimizer='sgd', loss='categorical_crossentropy')\n\n    x = np.ones((3, 5))\n    y = np.ones((3, 5))\n    model1.fit(x, y, verbose=0, epochs=1)\n    model2.fit(x, y, verbose=0, epochs=1)",
        "begin_line": 639,
        "end_line": 662,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_activity_regularization_with_model_composition#665",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_activity_regularization_with_model_composition()",
        "snippet": "def test_activity_regularization_with_model_composition():\n\n    def reg(x):\n        return K.sum(x)\n\n    net_a_input = Input((2,))\n    net_a = net_a_input\n    net_a = Dense(2, kernel_initializer='ones',\n                  use_bias=False,\n                  activity_regularizer=reg)(net_a)\n    model_a = Model([net_a_input], [net_a])\n\n    net_b_input = Input((2,))\n    net_b = model_a(net_b_input)\n    model_b = Model([net_b_input], [net_b])\n\n    model_b.compile(optimizer='sgd', loss=None)\n    x = np.ones((1, 2))\n    loss = model_b.evaluate(x)\n    assert loss == 4",
        "begin_line": 665,
        "end_line": 684,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.reg#667",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.reg(x)",
        "snippet": "    def reg(x):\n        return K.sum(x)",
        "begin_line": 667,
        "end_line": 668,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_shared_layer_depth_is_correct#687",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_shared_layer_depth_is_correct()",
        "snippet": "def test_shared_layer_depth_is_correct():\n    # Basic outline here: we have a shared embedding layer, and two inputs that\n    # go through different depths of computation in the graph before\n    # the final output.  We need the computed depth of the input layers to be\n    # the same, because they both pass through the embedding layer before anything\n    # else happens.  That's what we're testing.\n    from keras.layers import Embedding, Input, Dense, Concatenate\n    from keras.models import Model\n    input1 = Input(shape=(10,), name='input1')\n    input2 = Input(shape=(10,), name='input2')\n    embedding_layer = Embedding(name='embedding', input_dim=5, output_dim=10)\n    embedded_input1 = embedding_layer(input1)\n    embedded_input2 = embedding_layer(input2)\n    transformed_input2 = Dense(6)(Dense(5)(Dense(3)(embedded_input2)))\n    final_output = Dense(2)(Concatenate()([embedded_input1, transformed_input2]))\n    model = Model(inputs=[input1, input2], outputs=final_output)\n    input1_depth = -1\n    input2_depth = -1\n    for depth, layers in model._layers_by_depth.items():\n        for layer in layers:\n            if layer.name == 'input1':\n                input1_depth = depth\n            if layer.name == 'input2':\n                input2_depth = depth\n    assert input1_depth != -1\n    assert input1_depth == input2_depth",
        "begin_line": 687,
        "end_line": 712,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth#715",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth()",
        "snippet": "def test_layer_sharing_at_heterogeneous_depth():\n    x_val = np.random.random((10, 5))\n\n    x = Input(shape=(5,))\n    A = Dense(5, name='A')\n    B = Dense(5, name='B')\n    output = A(B(A(B(x))))\n    M = Model(x, output)\n\n    output_val = M.predict(x_val)\n\n    config = M.get_config()\n    weights = M.get_weights()\n\n    M2 = Model.from_config(config)\n    M2.set_weights(weights)\n\n    output_val_2 = M2.predict(x_val)\n    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)",
        "begin_line": 715,
        "end_line": 733,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth_with_concat#736",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth_with_concat()",
        "snippet": "def test_layer_sharing_at_heterogeneous_depth_with_concat():\n    input_shape = (16, 9, 3)\n    input_layer = Input(shape=input_shape)\n\n    A = Dense(3, name='dense_A')\n    B = Dense(3, name='dense_B')\n    C = Dense(3, name='dense_C')\n\n    x1 = B(A(input_layer))\n    x2 = A(C(input_layer))\n    output = layers.concatenate([x1, x2])\n\n    M = Model(inputs=input_layer, outputs=output)\n\n    x_val = np.random.random((10, 16, 9, 3))\n    output_val = M.predict(x_val)\n\n    config = M.get_config()\n    weights = M.get_weights()\n\n    M2 = Model.from_config(config)\n    M2.set_weights(weights)\n\n    output_val_2 = M2.predict(x_val)\n    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)",
        "begin_line": 736,
        "end_line": 760,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth_order#763",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_layer_sharing_at_heterogeneous_depth_order()",
        "snippet": "def test_layer_sharing_at_heterogeneous_depth_order():\n    # This tests for the bug in this issue\n    # https://github.com/keras-team/keras/issues/11159\n    # It occurs with layer sharing at heterogeneous depth when\n    # the layers need to be applied in an order that differs from\n    # the order that occurs in the config.\n\n    input_shape = (1, 12)\n    input_layer = Input(shape=input_shape)\n\n    A = Dense(12, name='layer_a')\n    r1 = layers.Reshape((12,))(input_layer)\n    Aout1 = A(r1)\n\n    r2 = layers.Reshape((12,))(A(input_layer))\n    Aout2 = A(r2)\n\n    # Note: if the order of the layers in the concat is\n    # changed to ([Aout1, Aout2]) the bug doesn't trigger\n    c1 = layers.concatenate([Aout2, Aout1])\n    output = Dense(2, name='layer_b')(c1)\n\n    M = Model(inputs=input_layer, outputs=output)\n\n    x_val = np.random.random((10,) + input_shape)\n    output_val = M.predict(x_val)\n\n    config = M.get_config()\n    weights = M.get_weights()\n\n    M2 = Model.from_config(config)\n    M2.set_weights(weights)\n\n    output_val_2 = M2.predict(x_val)\n    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)",
        "begin_line": 763,
        "end_line": 797,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_multi_output_mask#800",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_multi_output_mask()",
        "snippet": "def test_multi_output_mask():\n    \"\"\"Fixes #7589\"\"\"\n    class TestMultiOutputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiOutputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            return [K.abs(inputs), K.abs(inputs)]\n\n        def compute_output_shape(self, input_shape):\n            out_shape = super(TestMultiOutputLayer, self).compute_output_shape(\n                input_shape)\n            return [out_shape, out_shape]\n\n    class TestMultiInputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiInputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            negative, positive = inputs\n            return negative + positive\n\n    input_layer = Input(shape=(16, 16, 3))\n    x, y = TestMultiOutputLayer()(input_layer)\n    z = TestMultiInputLayer()([x, y])\n    _ = Model(inputs=input_layer, outputs=z)\n    assert K.int_shape(z)[1:] == (16, 16, 3)",
        "begin_line": 800,
        "end_line": 826,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiOutputLayer.test_multi_output_mask#800",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiOutputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiOutputLayer.test_multi_output_mask()",
        "snippet": "def test_multi_output_mask():\n    \"\"\"Fixes #7589\"\"\"\n    class TestMultiOutputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiOutputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            return [K.abs(inputs), K.abs(inputs)]\n\n        def compute_output_shape(self, input_shape):\n            out_shape = super(TestMultiOutputLayer, self).compute_output_shape(\n                input_shape)\n            return [out_shape, out_shape]\n\n    class TestMultiInputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiInputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            negative, positive = inputs\n            return negative + positive\n\n    input_layer = Input(shape=(16, 16, 3))\n    x, y = TestMultiOutputLayer()(input_layer)\n    z = TestMultiInputLayer()([x, y])\n    _ = Model(inputs=input_layer, outputs=z)\n    assert K.int_shape(z)[1:] == (16, 16, 3)",
        "begin_line": 800,
        "end_line": 826,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiOutputLayer.__init__#803",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiOutputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiOutputLayer.__init__(self, **kwargs)",
        "snippet": "        def __init__(self, **kwargs):\n            super(TestMultiOutputLayer, self).__init__(**kwargs)",
        "begin_line": 803,
        "end_line": 804,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiOutputLayer.call#806",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiOutputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiOutputLayer.call(self, inputs, **kwargs)",
        "snippet": "        def call(self, inputs, **kwargs):\n            return [K.abs(inputs), K.abs(inputs)]",
        "begin_line": 806,
        "end_line": 807,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiOutputLayer.compute_output_shape#809",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiOutputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiOutputLayer.compute_output_shape(self, input_shape)",
        "snippet": "        def compute_output_shape(self, input_shape):\n            out_shape = super(TestMultiOutputLayer, self).compute_output_shape(\n                input_shape)\n            return [out_shape, out_shape]",
        "begin_line": 809,
        "end_line": 812,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiInputLayer.test_multi_output_mask#800",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiInputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiInputLayer.test_multi_output_mask()",
        "snippet": "def test_multi_output_mask():\n    \"\"\"Fixes #7589\"\"\"\n    class TestMultiOutputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiOutputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            return [K.abs(inputs), K.abs(inputs)]\n\n        def compute_output_shape(self, input_shape):\n            out_shape = super(TestMultiOutputLayer, self).compute_output_shape(\n                input_shape)\n            return [out_shape, out_shape]\n\n    class TestMultiInputLayer(Layer):\n        def __init__(self, **kwargs):\n            super(TestMultiInputLayer, self).__init__(**kwargs)\n\n        def call(self, inputs, **kwargs):\n            negative, positive = inputs\n            return negative + positive\n\n    input_layer = Input(shape=(16, 16, 3))\n    x, y = TestMultiOutputLayer()(input_layer)\n    z = TestMultiInputLayer()([x, y])\n    _ = Model(inputs=input_layer, outputs=z)\n    assert K.int_shape(z)[1:] == (16, 16, 3)",
        "begin_line": 800,
        "end_line": 826,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiInputLayer.__init__#815",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiInputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiInputLayer.__init__(self, **kwargs)",
        "snippet": "        def __init__(self, **kwargs):\n            super(TestMultiInputLayer, self).__init__(**kwargs)",
        "begin_line": 815,
        "end_line": 816,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.TestMultiInputLayer.call#818",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology.TestMultiInputLayer",
        "signature": "tests.keras.engine.test_topology.TestMultiInputLayer.call(self, inputs, **kwargs)",
        "snippet": "        def call(self, inputs, **kwargs):\n            negative, positive = inputs\n            return negative + positive",
        "begin_line": 818,
        "end_line": 820,
        "comment": "",
        "is_bug": false
    },
    {
        "name": "tests.keras.engine.test_topology.test_constant_initializer_with_numpy#829",
        "src_path": "tests/keras/engine/test_topology.py",
        "class_name": "tests.keras.engine.test_topology",
        "signature": "tests.keras.engine.test_topology.test_constant_initializer_with_numpy()",
        "snippet": "def test_constant_initializer_with_numpy():\n    model = Sequential()\n    model.add(Dense(2, input_shape=(3,),\n                    kernel_initializer=Constant(np.ones((3, 2)))))\n    model.add(Dense(3))\n    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n\n    json_str = model.to_json()\n    model_from_json(json_str).summary()\n\n    yaml_str = model.to_yaml()\n    model_from_yaml(yaml_str).summary()",
        "begin_line": 829,
        "end_line": 840,
        "comment": "",
        "is_bug": false
    }
]